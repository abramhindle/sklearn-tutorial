@relation large.json
@attribute id integer
@attribute owner string
@attribute content string
@data
3226,'tokengeek','[Brightbox] Update Brightbox gem to add storage\nThis adds support for our Orbit storage service to the `Fog` binary.\r\n\r\nDue to the whitelisting of supported services, without these changes\r\nversions of `fog-brightbox >= 0.4.0` would error when the `shindo` tests\r\nwere starting so all Brightbox tests would be skipped.'
3204,'icco','Regression in Fock.mock causing CarrierWave specs to fail\nIt seems that fog 1.24.0 introduced a regression not present in 1.23.0 which is causing CarrierWave\'s test suite to fail. Here\'s a reproduction:\r\n\r\n``` ruby\r\nrequire \'fog\'\r\n\r\nFog.mock!\r\n\r\n[\'AWS\', \'Google\'].each do |provider|\r\n  connection = ::Fog::Storage.new({ provider: provider, google_storage_access_key_id: \'xtz\', google_storage_secret_access_key: \'yyy\', aws_access_key_id: \'blah\', aws_secret_access_key: \'moo\' })\r\n\r\n  directory = connection.directories.create(key: "xyz")\r\n  directory.files.create(body: "this is stuff", key: "moo.txt")\r\n\r\n  p directory.files.get("moo.txt").body\r\nend\r\n```\r\n\r\nOn 1.23.0 this prints:\r\n\r\n```\r\n"this is stuff"\r\n"this is stuff"\r\n```\r\n\r\nBut on 1.24.0 this fails like this:\r\n\r\n```\r\n"this is stuff"\r\n/Users/dev/.rvm/gems/ruby-2.1.2/gems/fog-1.24.0/lib/fog/google/requests/storage/get_object.rb:67:in `get_object\': object_name is required (ArgumentError)\r\n\tfrom /Users/dev/.rvm/gems/ruby-2.1.2/gems/fog-1.24.0/lib/fog/google/models/storage/files.rb:62:in `get\'\r\n\tfrom /Users/dev/.rvm/gems/ruby-2.1.2/gems/fog-1.24.0/lib/fog/google/models/storage/file.rb:31:in `body\'\r\n\tfrom test.rb:11:in `block in <main>\'\r\n\tfrom test.rb:5:in `each\'\r\n\tfrom test.rb:5:in `<main>\'\r\n```\r\n\r\nEverything works fine, but it seems that the `File` object for Google has not received a `key`. Printing out the file object looks like this:\r\n\r\n```\r\n  <Fog::Storage::Google::File\r\n    key=nil,\r\n    cache_control=nil,\r\n    content_disposition=nil,\r\n    content_encoding=nil,\r\n    content_length=13,\r\n    content_md5=nil,\r\n    content_type=nil,\r\n    etag="938e47592dc66e9082e241733ac79328",\r\n    expires=nil,\r\n    last_modified="Fri, 17 Oct 2014 07:54:03 +0000",\r\n    metadata={},\r\n    owner=nil,\r\n    storage_class=nil\r\n  >\r\n```\r\n\r\nNote that `key` is nil. This is not the case when running with AWS, and I don\'t think it\'s supposed to be, this is causing the subsequent failure when trying to read the body.'
3195,'icco','[google|dns] Add missing models, requests and tests\n- Add Zone model and tests\r\n- Add Record models, requests and tests\r\n- Add Change models, requests and tests\r\n- Add Project models, requests and tests\r\n- Add examples'
3181,'nirvdrum','Fixed attribute passing for add_interface\nServer model has an `add_interface` method. This method allows attributes to be passed so you can configure options on the interface. \r\n\r\nUnfortunately it doesn\'t pass these options to the create_interface call. This in turn leaves `attributes` empty for `create_vm.rb#create_interface` and subsequently `create_vm.rb#create_nic_backing`.\r\n\r\n When you define a `DistributedVirtualPortgroup` as the network attribute, and `create_nic_backing` doesn\'t have `attributes[:datacenter]`, you will end up with a newly created StandardPortgroup instead of the correct `DistributedVirtualPortgroup` in your Backing info for the interface. \r\n\r\nTo solve this you will need to pass along the `datacenter: \'name-your-dc-here\'` param when you call server.add_interface, along with `type`, and `network`. \r\n\r\nTested in ESX 5.5. See debug code below to replicate issue.\r\n\r\n```ruby\r\nv = Fog::Compute[:vsphere]\r\n\r\nc = v.datacenters.get(\'data-center-name-here\').virtual_machines.get(\'path/to/my/virtual/machine\')\r\n\r\n>> c.add_interface type: RbVmomi::VIM::VirtualE1000, network: \'network-name-here\', datacenter: \'data-center-name-here\'\r\n/home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/modify_vm_interface.rb:10\r\ninterface=get_interface_from_options(vmid, options)\r\n\r\n[5, 14] in /home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/modify_vm_interface.rb\r\n   5\r\n   6          def add_vm_interface(vmid, options = {})\r\n   7            raise ArgumentError, "instance id is a required parameter" unless vmid\r\n   8            debugger\r\n   9\r\n=> 10            interface=get_interface_from_options(vmid, options)\r\n   11            vm_reconfig_hardware(\'instance_uuid\' => vmid, \'hardware_spec\' => {\'deviceChange\'=>[create_interface(interface)]})\r\n   12          end\r\n   13\r\n   14          def destroy_vm_interface(vmid, options = {})\r\n(rdb:1) options\r\n{:type=>VirtualE1000, :network=>"network-name-here", :datacenter=>"data-center-name-here"}\r\n(rdb:1) n\r\n/home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/modify_vm_interface.rb:11\r\nvm_reconfig_hardware(\'instance_uuid\' => vmid, \'hardware_spec\' => {\'deviceChange\'=>[create_interface(interface)]})\r\n\r\n[6, 15] in /home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/modify_vm_interface.rb\r\n   6          def add_vm_interface(vmid, options = {})\r\n   7            raise ArgumentError, "instance id is a required parameter" unless vmid\r\n   8            debugger\r\n   9\r\n   10            interface=get_interface_from_options(vmid, options)\r\n=> 11            vm_reconfig_hardware(\'instance_uuid\' => vmid, \'hardware_spec\' => {\'deviceChange\'=>[create_interface(interface)]})\r\n   12          end\r\n   13\r\n   14          def destroy_vm_interface(vmid, options = {})\r\n   15            raise ArgumentError, "instance id is a required parameter" unless vmid\r\n(rdb:1) s\r\n/home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb:79\r\n}\r\n\r\n[74, 83] in /home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb\r\n   74                    :label   => nic.name,\r\n   75                    :summary => nic.summary,\r\n   76                  },\r\n   77                :backing     => create_nic_backing(nic, attributes),\r\n   78                :addressType => \'generated\')\r\n=> 79            }\r\n   80          end\r\n   81\r\n   82          def create_controller operation = :add, controller_key = 1000, bus_id = 0\r\n   83            {\r\n(rdb:1) attributes\r\n{}\r\n(rdb:1) s\r\n\r\n# <snip> I hit next three times </snip>\r\n\r\n/home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb:53\r\nraw_network = get_raw_network(nic.network, attributes[:datacenter], if nic.virtualswitch then nic.virtualswitch end)\r\n\r\n[48, 57] in /home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb\r\n   48            end\r\n   49            devices.flatten\r\n   50          end\r\n   51\r\n   52          def create_nic_backing nic, attributes\r\n=> 53            raw_network = get_raw_network(nic.network, attributes[:datacenter], if nic.virtualswitch then nic.virtualswitch end)\r\n   54\r\n   55            if raw_network.kind_of? RbVmomi::VIM::DistributedVirtualPortgroup\r\n   56              RbVmomi::VIM.VirtualEthernetCardDistributedVirtualPortBackingInfo(\r\n   57                :port => RbVmomi::VIM.DistributedVirtualSwitchPortConnection(\r\n(rdb:1) attributes\r\n{}\r\n(rdb:1) n\r\n/home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb:55\r\nif raw_network.kind_of? RbVmomi::VIM::DistributedVirtualPortgroup\r\n\r\n[50, 59] in /home/vagrant/.rvm/gems/ruby-1.9.3-p547/gems/fog-1.16.0/lib/fog/vsphere/requests/compute/create_vm.rb\r\n   50          end\r\n   51\r\n   52          def create_nic_backing nic, attributes\r\n   53            raw_network = get_raw_network(nic.network, attributes[:datacenter], if nic.virtualswitch then nic.virtualswitch end)\r\n   54\r\n=> 55            if raw_network.kind_of? RbVmomi::VIM::DistributedVirtualPortgroup\r\n   56              RbVmomi::VIM.VirtualEthernetCardDistributedVirtualPortBackingInfo(\r\n   57                :port => RbVmomi::VIM.DistributedVirtualSwitchPortConnection(\r\n   58                  :portgroupKey => raw_network.key,\r\n   59                  :switchUuid   => raw_network.config.distributedVirtualSwitch.uuid\r\n(rdb:1) raw_network\r\nnil\r\n(rdb:1) raw_network = get_raw_network(nic.network, \'data-center-name-here\', if nic.virtualswitch then nic.virtualswitch end)\r\nDistributedVirtualPortgroup("dvportgroup-100")\r\n(rdb:1) raw_network.kind_of? RbVmomi::VIM::DistributedVirtualPortgroup\r\ntrue\r\n(rdb:1) get_raw_network(nic.network, attributes[:datacenter], if nic.virtualswitch then nic.virtualswitch end).kind_of? RbVmomi::VIM::DistributedVirtualPortgroup\r\nfalse\r\n(rdb:1)\r\n```'
3161,'icco','Google compute tests are failing due to Not Found error\nTravis has been failing for a while, mostly from `tests/google/models/compute/target_pools_tests.rb` failing.\r\n\r\nE.g. https://travis-ci.org/fog/fog/jobs/35336198 (and many others)\r\n\r\n@icco Can you have a look, assign a minion or something please?'
3157,'tokengeek','Number of 1.9 style hashes introduced in testing\nTravis is showing failures due to 1.9 hashes being introduced into the tests.\r\n\r\nThese should be fixed to be hash rocket versions.\r\n\r\ne.g. https://travis-ci.org/fog/fog/jobs/34821735'
3125,'icco','[google|dns] Improve non-mocked tests, add support for get_managed_zone, with tests.\n@icco please ignore the previous pull requests, this one has both commits.\r\nThis is for #2865'
3078,'icco',"[google|compute] added more HTTP load balancing resources\n'add backend services backend', global forwarding rules, target http proxies, and url maps"
3077,'icco','[google|sql] Complete support for Google Cloud SQL\n- Add models, requests and tests for Flags\r\n- Add models, requests and tests for Operations\r\n- Add models, requests and tests for Instances\r\n- Add models, requests and tests for SslCerts\r\n- Add models and requests for BackupRuns\r\n- Add examples\r\n\r\nSolves https://github.com/fog/fog/issues/2976'
3033,'icco','[google|compute] Added read metadata from file functionality\nThe patch adds the ability to read metadata from files allowing users to specify startup scripts or arbitrary metadata for instances in separate files.'
2977,'icco',"[google|sql] Initial support for Google Cloud SQL\nThis commit adds the initial support for Google Cloud SQL:\r\n- Adds a new service 'SQL' to the existing 'Google' provider\r\n- Creates new shared methods to be reused by different services\r\n- Add requests, models and tests for Tiers\r\n\r\nSee #2976 for more details"
2869,'icco','[google|compute] Add Region tests\nAdd "Region" tests'
2864,'tokengeek',"Extracted fog-brightbox code to own repo\nThis is more a work in progress update for @geemus and other core folks.\r\n\r\nI've extracted the `providers/brightbox` subtree out to https://github.com/brightbox/fog-brightbox\r\n\r\nHaving tried to work within a subdirectory for the last month or two has highlighted a number of problems - most of which I think we guessed on the original issue https://github.com/fog/fog/issues/2681\r\n\r\nThe main killer which will affect other users later is that from an application basis, Bundler can not reference a gemspec in a subdirectory of a git repo.\r\n\r\nSo it has been impossible for our CI server or applications to reference the `master` version of `fog-brightbox` which has led us down the road of building prerelease gems and hosting gems internally.\r\n\r\nSo this will basically just lead to more requests for fog releases because the only clean way for provider gems to be used are either cutting a gem release or unpacking gems and using `path`\r\n\r\nI'm currently at the junction where I have an application that can reference `fog-core/master` but not `fog-brightbox/master` and I can't cut a prerelease gem that references the unreleased gem.\r\n\r\nSo in the application I'm forced to update the `Gemfile` to reference `fog-core` AND create a prerelease gem. The worst of both worlds.\r\n\r\nNow I knew this was a problem when we last discussed it but it's hit the point where it's too much work to justify and I'm really behind on a project working around this one problem.\r\n\r\nIf anyone can come up with workable alternatives I'll be happy to discuss.\r\n\r\nSo my intention is to clean up the main repo next week to cut down on things like loading providers tests, gemfiles referencing sub paths etc."
2854,'icco','[google|compute] Improve Disks support\n- Add missing "Disk" properties\r\n- Remove unnecessary property aliases\r\n- Use "Operation" instead of the "backoff_if_unfound" method when inserting a "Disk" or "Snapshot"\r\n- When destroying a "Disk" asyncronous return the "Operation" model instead of the raw operation response\r\n- Add "list_aggregatted_disks" request\r\n- If zone is not set on "all" and "get" methods, use the list aggregated disks request (1 API call) instead of fetching all zones\r\n- Fix a bug when inserting a snapshot (description was not merged at the body request)\r\n- Fix model tests'
2845,'tokengeek',"[fog-brightbox] Prepare for v0.0.2 release\nSorry for the noise - I'm planning a release of `fog-brightbox_v0.0.2` and since it's a subproject in the main repo, commits, PRs and tags end up on the main project.\r\n\r\n@geemus I'm starting to rethink the providers approach again!"
2834,'icco','Google Cloud Storage: does not match the server certificate (OpenSSL::SSL::SSLError)\nHiya,\r\n\r\nI have an issue which is aparently already solved for S3 connections. I configured my GCS bucket with a CNAME which produces now the OpenSSL error.\r\n```ruby\r\nExcon::Errors::SocketError (hostname "aaa.bbb.com.storage.googleapis.com" does not match the server certificate (OpenSSL::SSL::SSLError))\r\n```\r\nFor S3, it could have been solved by adding path_style: true to the Fog config. Unfortunately, this is not working for GCS configurations, it does not seem to be supported:\r\n```ruby\r\n[fog][WARNING] Unrecognized arguments: path_style\r\n```\r\nAre there any other suggestions?\r\n\r\nin application.rb\r\n```ruby\r\nconfig.paperclip_defaults = {\r\n      storage: :fog,\r\n      fog_credentials: {\r\n        provider: \'Google\',\r\n        google_storage_access_key_id: \'ccc\',\r\n        google_storage_secret_access_key: \'ddd\',\r\n        # path_style: true\r\n      },\r\n      fog_public: true,\r\n      fog_directory: \'aaa.bbb.com\',\r\n      fog_host: \'http://aaa.bbb.com\'\r\n    }\r\n```\r\n\r\nSetup:\r\nRuby 2.1.1\r\nRails 4.0.4\r\nPaperclip 4.1.1\r\nFog 1.21.0'
2830,'tokengeek','[Brightbox] Test error when required args missing\nThis adds a test to confirm existing behaviour that ArgumentError is\r\nraised when `brightbox_client_id` or `brightbox_secret` is omitted.'
2814,'tokengeek','[Brightbox] Update testing to MiniTest::Spec\nUsing spec form is going to be the standard way to testing (but relying\r\non asserts). This update allows the test(s) to be picked up by the main\r\nrake task again.'
2793,'tokengeek','Provider examples are appearing in Yard API docs\nJust noticed that there is a lot of Rackspace only stuff in the API docs.\r\n\r\nhttp://www.rubydoc.info/gems/fog/toplevel\r\n\r\nMost are coming from `lib/fog/rackspace/examples` which when processed appear to be declaring constants and methods in the top level.\r\n\r\nI\'m assuming that requiring "fog/rackspace" isn\'t picking these up as well and it is just the options (or lack of) I\'m passing to `yardoc`. However since rubydoc is doing the same we probably need to exclude all examples globally.'
2752,'tokengeek','[Brightbox] Updates for gem/module\nAdds support file and implements minitest "hello world"'
2743,'icco',"[google|compute] Determine a way to search private (premium) projects\nSee PR: https://github.com/fog/fog/pull/2534#issuecomment-32297119\r\n\r\nPlease determine a way to be able to add premium or private projects (i.e. rhel-cloud) to the list of projects searched for images.\r\n\r\nThis currently requires us to keep a separate fork of fog and keep pulling in our commit to add rhel-cloud to the project search list to search for images. It seems that when this commit was pulled into fog, it caused problems for others because they don't have access to the project since it is a premium project.\r\n\r\nSee PR link for details from @icco and @geemus "
2741,'tokengeek','Brightbox test updates\nUpdates to not rename users and accounts during test runs.\r\n\r\nFixes a timing issue now SQL instance snapshots have to be complete before source SQL instance can be deleted.'
2700,'tokengeek','[Brightbox] Extract to provider module [GH-2674]\nThis moves all the Brightbox lib files to a `providers` subdirectory as a prototype of moving the repo files around.\r\n\r\nA new gemspec exists which has built `fog-brightbox v0.0.1` - https://rubygems.org/gems/fog-brightbox\r\n\r\nI have a branch of our CLI application (https://github.com/brightbox/brightbox-cli) using this which appears to be working.\r\n\r\n`fog` now depends on `fog-brightbox` which brings all the functionality back.\r\n\r\nThe shindo tests have not been moved and (locally) pass.'
2696,'krames',"[rackspace] Add Virtual Interfaces to Servers\nAs far as I can find in Fog, there's not way to add virtual interfaces to servers with fog:\r\nhttp://docs.rackspace.com/servers/api/v2/cn-devguide/content/api_virt_interfaces.html\r\n\r\nThis is something we need for our workflow we're using with `fog`, so I'll see if I can get this implemented :+1: "
2693,'tokengeek','[internetarchive] Correct test tagging\nThe provider is declaring itself as "internetarchive" which does not\r\nmatch the tags "internet_archive" used on the tests.\r\n\r\nSince we blacklist the tests that are run, it meant they continued to\r\nrun even if no credentials were present.'
2692,'tokengeek','[Brightbox] Support Cloud SQL maintenance windows\nEnables setting of `maintenance_weekday` and `maintenance_hour` values\r\nto specify the maintenance window where updates may be applied to the\r\nSQL instance.'
2684,'tokengeek','[Brightbox] Destroy snapshot after completion\nThis works around an issue where you can not detect if a Cloud SQL\r\nInstance is being snapshotted.\r\n\r\nA new API restriction is that you can no longer delete an instance that\r\nis being snapshotted so this suddenly began failing.\r\n\r\nNow we look for a new snapshot and poll until it is ready only then can\r\nwe safely issue the destroy command.'
2675,'elight',"Fog::Storage.new causing all providers to be required\nTo replicate:\r\n\r\n```ruby\r\nrequire 'fog/rackspace'\r\n\r\np Fog.providers\r\nFog::Storage.new(args)\r\np Fog.providers\r\n```\r\n\r\nVery different output between the two <code>p</code>s."
2674,'tokengeek','Extract Brightbox provider to module\nWith our push towards fog becoming modular and the release of `fog-core` we need to get a few providers extracted and work on the pain points.\r\n\r\nSo this is for the **Brightbox** provider.\r\n\r\nSo what I have working is a `fog-brightbox` gem that is a fork of `fog` using `fog-core` and `fog-json`.\r\n\r\nThis works:\r\n* in isolation (Shindo tests)\r\n* when as a dependency required by our CLI tool (https://github.com/brightbox/brightbox-cli)\r\n* when as a dependency required by `fog`\r\n\r\nThe issues remaining are:\r\n\r\n- [ ] Official repo required so `fog/fog-brightbox` is needed (FAO @geemus)\r\n- [ ] Our Shindo helper (now in a different gem) is not getting picked up so in https://github.com/fog/fog/blob/master/tests/compute/helper.rb the `Brightbox::Compute::TestSupport` is not available.\r\n\r\n'
2662,'tokengeek','Fix Joyent service declarations\nWhen the Joyent provider was merged in it was after a reworking of how\r\nrequires and services were declared.\r\n\r\nThis meant the call to `service` was no longer correct.\r\n\r\nThis updates the signature and the mocked tests now pass (locally at\r\nleast) however the structuring and placement of the files may not be\r\ninconsistent with the results of [GH-1712]'
2630,'tokengeek','Prototype replacing Shindo testing with minitest\nWe discussed changing testing frameworks as part of #1266\r\n\r\nThe decision was to switch to using `minitest` with tagging support.\r\n\r\nWe do not want to replace everything immediately but bring in the gems, update some of the shared tests and core tests.\r\n\r\nSorting out helpers so they are available.\r\n\r\nSince Shindo does broad tests (1 expensive operation, many assertions) we need to work out the best structure for the tests so you can see a good level of detail.\r\n\r\nThis work should not affect mocking work or dividing into providers since providers may be able to use their own testing framework.'
2624,'geemus',"Arrange fog 1.20.0 release\n@geemus It's been about a month since 1.19.0 was released.\r\n\r\nCan we get a new version cut?\r\n\r\nThanks."
2623,'tokengeek','[Brightbox] Remove old #destroy request\nAll the "destroy" methods were renamed to "delete" but this file was not\r\nremoved.'
2620,'tokengeek',"[Brightbox] Add support for Cloud SQL service\nThis adds support for Brightbox's Cloud SQL DBaaS.\r\n\r\nIt is currently still part of the Compute service so all resources can\r\nbe managed from one connection to the service."
2615,'tokengeek',"Make Coveralls opt-in\nDue to looking for a value of 'false' whenever I run tests locally\r\nwithout the `COVERAGE` env variable coverage is very slowly, still\r\ncalculated - it then aborts without doing anything because I'm not using\r\na CI setup.\r\n\r\nThis adds the setting in for Travis to opt in and should exclude\r\ncoverage work for anyone running tests locally."
2614,'tokengeek','[Brightbox] Add CloudIp#destination_id\nAbstraction over the number of possible mapped targets to consistently\r\naccess the identifier.'
2575,'tokengeek',"Update Nokogiri version\nPlease check http://www.osvdb.org/show/osvdb/101458. Nokogiri Version 1.5.11 should be installed. I didn't had time to run tests on this."
2556,'icco','[google|storage] Add put_object_acl request\nThis PR adds `put_object_acl` request method (similar with `put_bucket_acl` request in #2555). I tested it with following code:\r\n```ruby\r\nrequire \'./lib/fog/google\'\r\n \r\nstorage = Fog::Storage::Google.new(...)\r\n \r\ncurrent_acl = storage.get_object_acl(\'acl-test-12312312323\', \'test\').body\r\n \r\nnew_acl = current_acl\r\nnew_acl[\'AccessControlList\'].push(\r\n  {"Scope" => {"type" => \'AllUsers\'}, "Permission"=> \'READ\'}\r\n)\r\n \r\nstorage.put_object_acl(\'acl-test-12312312323\', \'test\', new_acl)\r\n```\r\n\r\nBTW, after #2509 will be accepted, I plan to move methods that are similar with `put_bucket_acl` [request](https://github.com/allomov/fog/blob/47f5a04c3e3e7fd43e4c1482f7a515c59bc61c0a/lib/fog/google/requests/storage/put_bucket_acl.rb) to [requests helper](https://github.com/allomov/fog/blob/4574ff45b64002a8b9f2c7fcd9eedf9b54a702f1/lib/fog/google/helpers/requests_helper.rb).\r\n\r\nBest wishes :boy:'
2555,'icco','[google|storage] Fix put_bucket_acl request\nI don\'t see how `put_bucket_acl` request could work before and rewrote it using [current documentation for ACLs](https://developers.google.com/storage/docs/accesscontrol#About-Access-Control-Lists). I tested it with following code: \r\n```ruby\r\nrequire \'./lib/fog/google\'\r\n \r\nstorage = Fog::Storage::Google.new(...)\r\n \r\ncurrent_acl = storage.get_bucket_acl(\'acl-test-12312312323\').body\r\n \r\nnew_acl = current_acl\r\nnew_acl[\'AccessControlList\'].push({"Scope" => {"type" => \'AllUsers\'}, "Permission"=> \'READ\'})\r\n \r\nstorage.put_bucket_acl(\'acl-test-12312312323\', new_acl)\r\n```'
2542,'icco',"[google] Disk.ready? should not reload the data\nit's done in wait_for in other models and providers"
2535,'icco','[google|compute] more additions and corrections\n'
2534,'icco','[google|compute] Add rhel-cloud to project search list\n'
2533,'icco','[google|compute] auth needs additional scope to insert images\n'
2523,'icco','[google|compute] Improve support for Tags, Addresses, Snapshots\n Some additions to help get the google compute support up to complete v1 support. Specifically I added delete snapshot, address support, tag support for firewalls and setTag for instances. Still more to do to get to complete v1 support. But I like the google compute api. It is very clean and straightforward.  Sorry i did not do mocks....'
2509,'icco','[google] Fix Google Engine tests\nGoogle Engine live tests fail after v1 update. More fixes coming.'
2503,'icco','[google|compute] Rescue from Fog::Errors::NotFound instead of Excon\nUse Fog::Errors::NotFound as raised by build_excon_response'
2494,'icco','Update to Google Compute Engine API v1\nplus some disk goodies'
2437,'tokengeek','[Brightbox] Code style clean up\nFirst pass at cleaning up the code to a better, more consistent style.'
2435,'tokengeek',"[Brightbox] Clean up Cloud IP mapping code\nWhen passing an object that responds to `identity` which is all of our\r\nresources, we can pass the resource's identifier in to the API call to\r\nmake it easier to use without maintaining a list of classes.\r\n\r\nServer's need to use the interface for the destination so they override\r\na new `mapping_identity` method which is picked up first.\r\n\r\nAnything else (such as a String identifier) is passed directly to the\r\nAPI request as before."
2433,'tokengeek','[Brightbox] Minor cleanup\nFixes a yard tag typo and removes a commented constant.'
2422,'icco','[google] Raise Fog::Errors::NotFound on 404\n'
2421,'icco','[google] Implement disk mocks and enable tests\nAvoid repeating api_version\r\nAdd persistent disk to insert_server mock\r\nUse Fog::Mock.delay properly instead of Fog.timeout'
2396,'nirvdrum','"[WARNING] Unable to load the \'unf\' gem." Does fog depend on unf or not?\nWhen I run my rspec tests, I get a warning:\r\n\r\n[fog][WARNING] Unable to load the \'unf\' gem. Your AWS strings may not be properly encoded.\r\n\r\nIf unf is necessary, let us depend on it. If not, let us remove the warning. I am willing to make a pull request in either case. Just let me know.\r\n\r\nEnvironment:\r\n- Mac OSX Mountain Lion\r\n- Rails 4.0.1\r\n- fog 1.18.0\r\n- rspec 2.14.1\r\n\r\nHow to reproduce:\r\n- strike up a Rails app\r\n- add fog and rspec to Gemfile (in my case fog is a dependency to asset_sync)\r\n- run "rspec" on the command line'
2378,'icco','[google|compute] Add support for instance tags\n'
2376,'tokengeek','[Brightbox] Add new SSL cert metadata attributes\nNow adding `valid_from` and `issuer` to model'
2371,'tokengeek',"[Brightbox] Fix test issue with reusing servers\nTo save time, we hoped to use an existing server to read the interface\r\nJSON and it did a naive selection of the first server.\r\n\r\nServer's that are deleted have two relevant characteristics. 1) They no\r\nlonger have interfaces and 2) they are reported in the output in their\r\ndeleted state for an hour after being removed.\r\n\r\nSince the test was not filtering for active servers, subsequent runs\r\ncould pick up a deleted server and fail to find the identifier for it's\r\nmissing first interface.\r\n\r\nThis simplifies matters by creating a new server for the test."
2361,'icco',"[google|compute] Support for tokens\nHi there,\r\n\r\n Currently the google compute object only allows to use a certificate file to authz with google. It'd be nice to be able to do the same with an access/refresh token. That way fog could easily be used by a web page able to get the an oauth2 token."
2329,'tokengeek','[Brightbox] Add SSL settings to load balancer\nThis allows passing a SSL cert and key to a load balancer to use SSL.\r\n\r\nIf a certificate is present, the expiry time and certificate subject are\r\navailable.'
2317,'tokengeek','[core] Make `ruby-libvirt` dependency optional\nThe `ruby-libvirt` dependency was commented out since it is rarely used\r\nand a pain for developers to prepare.\r\n\r\nThe comment was removed in https://github.com/fog/fog/commit/b373e55a8f32b24c11fe4b20e773c1428df9d811\r\nto protect JRuby but made it a requirement for MRI (& others).\r\n\r\nThis adds another guard based around the setting of `FOG_USE_LIBVIRT`\r\nenvironment variable.\r\n\r\nThis hopefully will mean anyone wanting to use it can just set that\r\nrather than editing the gemspec.\r\n\r\nFixes #2316'
2316,'tokengeek','LibVirt has been added as a dependency by accident\nIn https://github.com/fog/fog/commit/b373e55a8f32b24c11fe4b20e773c1428df9d811 to guard against JRUBY using `ruby-libvirt` the comment was removed.\r\n\r\nThis has made `ruby-libvirt` a dependency for everyone again.\r\n\r\nFix shortly...'
2313,'geemus',"Semantic Versioning (and Pessimistic Versioning Constraint) recommendation\nI noticed some projects using Fog are setting very specific versioning constraint.  For example, https://github.com/mitchellh/vagrant-rackspace/issues/34 happened because both vagrant-aws and vagrant-rackspace are using 3 digits of precision.\r\n\r\nThe fog RELEASE.md says that Fog follows semantic versioning.  Several other projects have put up notices suggesting to use 2 digits of precision for projects that follow semver.  See https://github.com/intridea/multi_json, for example.\r\n\r\nI added a similar notice, but softened the language because I don't think there is a clear understanding on what semantic versioning means for a project like Fog, which has multiple providers backed by services that are all evolving at their own pace."
2302,'nosborn','[vcloud_director] Mocking for tasks.\n'
2298,'MarcGrimme','[vSphere] Implemented feature to specify a socket cpu layout as specified\n[vSphere] Implemented feature to specify a socket cpu layout as specified in vmware API. If not used numCoresPerSocket is left out and default VMware behaviour is used.\r\n\r\nExample for creation of vm:\r\n```\r\ncompute.servers.create("name" => "myname", "cluster" => compute.datacenters.first.clusters.first.name, "datacenter" => compute.datacenters.first.name, "memory_mb"=>1024, "cpus"=>2, "corespersocket"=>2,"guest_id"=>"rhel6_64Guest", "interfaces"=>[interface,], "volumes"=>[volume])\r\n```\r\nWill end up with 1 socket with 2 cores instead of 2 sockets with 1 core each. \r\nThis seams to have some performance advantages (might be dependent on guest OS).\r\n\r\n[vSphere] also fixed bug in vm_reconfig_memory: wrong memory value passed to reconfig_hardware (memory in bytes instead of memory in MB).'
2280,'nosborn','[vcloud_director] Change input options structure.\n  #put_guest_customization_section_vapp\r\n  #put_network_connection_system_section_vapp\r\n\r\nAlthough no longer documented, the previous options structure is still\r\naccepted and is mutated into the new structure. This behaviour is likely\r\nto be deprecated at some point.'
2266,'nosborn','[vcloud_director] Do ensure_list in request methods.\nMitigation for Fog::ToHashDocument. This is (still) a short-term fix\r\nuntil we get to real response parsers.'
2260,'nosborn','[vcloud_director] Configure edge gateway services\nConfigures firewall, nat and load balancer services for an edge gateway. \r\n\r\nImplemented service layer to accept a configuration hash and do post to the vmware api.\r\n\r\naccepting all the required as well as optional fields specified in vmware api documentation. User can opt to not provide the optional fields.\r\n'
2244,'geemus','Ship a new version of fog\nThere have been a [ton of changes](https://github.com/fog/fog/compare/v1.15.0...master) since the last release back in August. I think it’s about time to release version 1.16.0. Tests are passing on Travis. What issues need to be resolved before a new gem can be pushed? Perhaps you could create a tag or a milestone for these issues so they could be prioritized?'
2230,'nosborn','[vcloud_director] Implement more API requests.\n'
2217,'nosborn','[vcloud_director] Enable media uploading\n'
2207,'nosborn','[vcloud_director|tests] Fix some edge cases.\n'
2204,'nosborn','[vcloud_director] Add remaining vApp/VM power actions.\n'
2203,'nosborn','[vcloud_director] Add get_supported_systems_info request.\n'
2202,'nosborn','[vcloud_director] Request method renaming.\nThis renames most request methods in line with page names in the vCloud\r\nAPI documentation. Previous method names remain as deprecations.'
2196,'nosborn','[vcloud_director] More mocking (1.8.7 compliant)\n'
2194,'nosborn','Revert "[vcloud_director] More mocking."\nThis reverts commit b19f2f0e7eddb80eb78d65757fb2dfbb8db19dd0.\r\n\r\nSeems to contain a 1.9-ism, backing out until found.'
2193,'icco','[google] Add mock data\nMock data for google provider'
2189,'nosborn','[vcloud_director] Better mocking and documentation.\n'
2184,'nosborn','[vcloud_director] Add get_supported_versions request.\n'
2182,'icco','[google] Create zone and zones models\nThe models were missing with a TODO\r\n\r\nPreparing a pull request with mock data too'
2178,'nosborn','[vcloud_director] Start mocking requests.\n- also adds get_current_session request'
2145,'kkanev','CloudSigma Firewall Policies\nAdd model for Rule and FW Policies\r\nRetrieve FW Policies'
2139,'eyardimci','Fixes redshift typo that prevented parsing more than one reserved_node in response\n'
2138,'krames','failing mock rackspace test\n@krames could you take a quick look at this: https://travis-ci.org/fog/fog/jobs/11272207#L320 (seems likely to be a quick fix, but I figured I would let you take a look just in case). Thanks!'
2124,'tokengeek','Brightbox updates\nSeries of updates to the Brightbox provider'
2112,'tokengeek','Introduction of VCR has broken live tests\nUpdated fog to run just our provider tests against my dev server and VCR errors started appearing.\r\n\r\n```\r\n$ FOG_RC=config/dev_app FOG_MOCK=false bundle exec shindont +brightbox\r\n...\r\n  Fog::Compute[:brightbox] | flavors (brightbox)\r\n      tests/compute/models/flavors_tests.rb\r\n        success\r\n        Fog::Compute[:brightbox] | flavors (brightbox)\r\n      - succeeds\r\n\r\n      tests/compute/models/flavors_tests.rb\r\n\r\n\r\n================================================================================\r\nAn HTTP request has been made that VCR does not know how to handle:\r\n  POST http://honcho.dev/token\r\n\r\nThere is currently no cassette in use. There are a few ways\r\nyou can configure VCR to handle this request:\r\n...\r\n  35 failed, 1 pending, 60 succeeded in 7.347761 seconds\r\n```\r\n\r\nThere are a number of problems here:\r\n\r\n* VCR needs to be correctly set up on a per provider basis\r\n* VCR is an alternative mocking system so should not be running when `FOG_MOCK = false`\r\n* An alternative mocking system gives us another combination of tests not being ran. The CI passes our official tests.\r\n* VCR is using Webmock rather than the Excon adapter\r\n* VCR can pick up and record sensitive information so guidance needs to be in place. I\'m assuming the vcloud instance recorded is an off Internet instance because reversing the Basic Authorization is trivial.\r\n* ISTR someone asking about using VCR before and "no" was said for some reasons. I don\'t know if the reasons still stand or not.\r\n\r\nI\'m going to work on the initial problems to get our providers specs working again. Hopefully as simple as allowing non VCR HTTP calls.\r\n\r\nI don\'t mind VCR (although I\'ve been fighting with it on another project) so I don\'t need convincing it could be a good idea it\'s just currently preventing my doing any testing.\r\n\r\nVCR was introduced as part of the VcloudDirector work in https://github.com/fog/fog/commit/99dd30fe3f66577c14917f5dc83dc6f767e7b033\r\n\r\n/cc @restebanez @geemus '
2106,'icco','Create a Snapshot based on a Disk\nThis patch introduces 2 ways of inserting a snapshot.\r\n1. By calling disk#create_snapshot\r\n2. By "inserting" into the snapshot collection (snapshot#create)\r\n\r\nThe latter doesn\'t directly map back to the Google API (create is not exposed on the snapshots collection), but I\'ve implemented it here in any case. I\'m open to discussing whether we should to this (the latter) at all (because validating all the params becomes a little more complex).'
2079,'kkanev','[CloudSigma] Volumes not working correctly\nHello,\r\n\r\nI tried to add a volume to a server with this command:\r\n\r\n```\r\nserver.mount_volume(\'9816464e-b519-4869-a691-5d34c9ea7e3f\', device = \'virtio\', dev_channel = \'0:0\', boot_order = \'1\')\r\n```\r\n\r\nWhen I do a pp.server it\'s looks fine:\r\n\r\n```\r\n<Fog::Compute::CloudSigma::Server\r\n[...]\r\n    volumes=[    <Fog::Compute::CloudSigma::MountPoint\r\n      device="virtio",\r\n      dev_channel="0:0",\r\n      drive="9816464e-b519-4869-a691-5d34c9ea7e3f",\r\n      boot_order="1"\r\n    >],\r\n    nics=[]\r\n  >\r\n```\r\n\r\nIf I do a server.update I got this kind of error:\r\n\r\n```\r\nFog::CloudSigma::Errors::RequestError: User fbf100e6-636a-41df-bbe9-914d690b4bb9 does not have (\'MOUNT\',) resource permissions on 9816464e-b519-4869-a691-5d34c9ea7e3f\r\n        from /var/lib/gems/1.9.1/gems/excon-0.25.3/lib/excon/middlewares/expects.rb:10:in `response_call\'\r\n        from /var/lib/gems/1.9.1/gems/excon-0.25.3/lib/excon/middlewares/response_parser.rb:8:in `response_call\'\r\n        from /var/lib/gems/1.9.1/gems/excon-0.25.3/lib/excon/connection.rb:349:in `response\'\r\n        from /var/lib/gems/1.9.1/gems/excon-0.25.3/lib/excon/connection.rb:247:in `request\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/core/connection.rb:57:in `request\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/core/deprecated/connection.rb:20:in `request\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/cloudsigma/connection.rb:54:in `request\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/cloudsigma/connection.rb:108:in `update_request\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/cloudsigma/requests/update_server.rb:8:in `update_server\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/lib/fog/cloudsigma/models/server.rb:54:in `update\'\r\n        from (irb):75:in `<top (required)>\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/bin/fog:76:in `block in <top (required)>\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/bin/fog:76:in `catch\'\r\n        from /var/lib/gems/1.9.1/gems/fog-1.15.0/bin/fog:76:in `<top (required)>\'\r\n        from /usr/local/bin/fog:23:in `load\'\r\n        from /usr/local/bin/fog:23:in `<main>\'\r\n```\r\n\r\nWhen I start the server and take a look to the VNC console, the server doesn\'t find any boot device. If I add the volume using the web ui it works but I can see the added device with fog.\r\n\r\nFeel free to ask me anymore details.'
2069,'amoghe','[google|compute] Fix some issues with GCE examples and disk requests.\n@amoghe please take a look. Some small issues found while running `rake google:smoke:compute`.'
2068,'icco',"[google|compute] Fix up the 'disk' api\nA recent patch to introduce support for creating disks from\r\nsnapshots had a few problems:\r\n1. Incorrect URL (to the snapshot resource) was being sent\r\n2. Ruby syntax errors.\r\n\r\nI looked into adding Shindo tests for this, but am not able to\r\nsince this request relies on a pre-existing snapshot which we\r\nuse for disk creation."
2065,'icco','[google|compute] get/list snapshots\nThis patch...\r\n 1. Adds the Snapshot collection and model\r\n 2. Adds the get/list requests\r\n 3. adds some examples\r\n'
2064,'icco','[google|compute] Accept/propagate the correct params when creating a new disk\nThis patch tries to match the [API](https://developers.google.com/compute/docs/reference/latest/disks/insert)  w.r.t disk insert(create). The user is now expected to pass one of ```image_name``` or ```size``` and ```snapshot``` when creating a disk.\r\n'
2059,'icco',"[google|compute] Add 'status' attribute to GCE images.\nSubsequent 'get' or 'reload' of the image will cause this to be\r\npopulated, and the user will be able to tell when it is 'READY'."
2045,'kevinykchan',"[joyent|storage| storage provider for joyent manta service\nI'm currently working on this\r\n\r\nprogress: https://github.com/kevinykchan/fog/tree/joyent-manta"
2044,'tokengeek','[Brightbox] Collaboration model updates\nThis expands on the new collaboration models that enable management of users to add model actions mapping to the lower level requests.\r\n\r\nAlso fixes `connection` deprecation warnings introduced in the branch.'
2028,'icco',"[google|compute] Query global projects when get/list'ing compute images.\n    This patch refactors some of the code that would query global\r\n    projects when get'ing images. It makes the list of global projects\r\n    a constant on the Images collection, so that both 'get' and 'list'\r\n    can use the same list of projects to query.\r\n    \r\n    Also, when bootstrapping/create'ing a server, the validation of\r\n    the specified image name is done by trying to 'get' the image\r\n    instead of 'save'ing it.\r\n"
2025,'icco','Google Storage: generation of public URLs is broken\nSpecifically, this check (reformatted for better readability):\r\n\r\n```ruby\r\n  def public_url\r\n    requires :directory, :key\r\n\r\n     if service.get_object_acl(directory.key, key).\\\r\n       body[\'AccessControlList\'].detect { |entry|\r\n         entry[\'Scope\'][\'type\'] == \'AllUsers\' && entry[\'Permission\'] == \'READ\'\r\n       }\r\n      if directory.key.to_s =~ /^(?:[a-z]|\\d(?!\\d{0,2}(?:\\.\\d{1,3}){3}$))(?:[a-z0-9]|\\.(?![\\.\\-])|\\-(?![\\.])){1,61}[a-z0-9]$/\r\n        "https://#{directory.key}.storage.googleapis.com/#{key}"\r\n      else\r\n        "https://storage.googleapis.com/#{directory.key}/#{key}"\r\n      end\r\n    else\r\n      nil\r\n    end\r\n  end\r\n```\r\n\r\nNotice that it tries to check ```entry[\'Scope\'][\'type\'] == \'AllUsers\'``` and ```entry[\'Permission\'] == \'READ\'```\r\n\r\nWith some debugging information, turns out that the object returned is:\r\n\r\n```\r\n{ "Owner"=> { "ID"=> someID },\r\n  "AccessControlList"=>\r\n  [\r\n      { \r\n         "Scope"=> { \r\n                     #<struct Nokogiri::XML::SAX::Parser::Attribute> => nil,\r\n                     "ID" => someID,\r\n                   },\r\n        "Permission"=>"FULL_CONTROL"\r\n      }\r\n  ]\r\n},\r\n\r\n```\r\n\r\nIf you look at the ```AccessControlList``` entry, you will notice that its "value" is a an array (of ACLs). Each item in this array is a Hash with 2 pairs, the first key being ```Scope``` and the next is ```Permission```. Now, the code indexes into the ```Permission``` correctly, but check for ```type``` is always bad (because there is no type). What we want instead is to access the ```SAX::Parser::Attribute``` object (whose value is always nil), because it contains the information we want, namely:\r\n\r\n```\r\n#<struct Nokogiri::XML::SAX::Parser::Attribute\r\n           localname="type",\r\n           prefix=nil,\r\n           uri=nil,\r\n           value="UserById">\r\n```\r\n\r\n... we want to check if its ```value=AllUsers``` and if allUsers have ```Permission=READ```'
2020,'icco',"[google] client.images doesn't list google public images\nclient.images only lists images on client's project.\r\nthis add on listing all images provided by google\r\n"
2015,'icco','Google Compute disk object not being populated correctly\nThe response on the wire comes back correctly, see...\r\n\r\n```ruby\r\n{\r\n "kind"=>"compute#disk",\r\n "id"=>"REDACTED",\r\n "creationTimestamp"=>"2013-08-01T21:33:58.508-07:00",\r\n "zone"=>\r\n  "https://www.googleapis.com/compute/v1beta15/projects/redacted/zones/us-central1-a",\r\n "status"=>"READY",\r\n "name"=>"REDACTED",\r\n "sizeGb"=>"100",\r\n "sourceSnapshot"=>\r\n  "https://www.googleapis.com/compute/v1beta15/projects/REDACTED/global/snapshots/REDACTED",\r\n "sourceSnapshotId"=>"REDACTED",\r\n "selfLink"=>\r\n  "https://www.googleapis.com/compute/v1beta15/projects/redacted/zones/us-central1-a/disks/test-disk"\r\n}\r\n```\r\n\r\nBut, by the time control returns to my code, I have this object...\r\n\r\n```ruby\r\n<Fog::Compute::Google::Disk\r\n    name="test-disk",\r\n    kind=nil,\r\n    id=nil,\r\n    creation_timestamp=nil,\r\n    zone_name="us-central1-a",\r\n    status=nil,\r\n    description=nil,\r\n    size_gb=nil,\r\n    self_link=nil,\r\n    image_name=nil,\r\n   source_snapshot="https://www.googleapis.com/compute/v1beta15/projects/REDACTED/global/snapshots/test-snapshot"\r\n  >\r\n```'
2009,'icco','[google|compute] Add support for network and external_ip during server creation\n* Add support for an other network than default\r\n   ("default" network is still default network)\r\n * Can specify if creation or not of an external_ip\r\n   (External_ip is still created by default)\r\n\r\nThoughts ?'
2003,'icco','[google|compute] all method in Google::Compute::Servers should have filters argument\nI think this is a convention in all Compute providers.\r\n\r\nThoughts ?'
1987,'icco','[google] Add service level documentation and simple smoke tests.\nThis is a rough draft at adding some specifics about the Google section of Fog. It adds some simple smoke tests for compute and a little documentation for Google Cloud Storage and Google Compute Engine.'
1960,'icco','[google|compute] Destroy for disk.\nAdds the methods to destroy a disk and get the zone of a disk.\r\nAlso changed google/compute.rb to raise errors rather than throw them as is done in other providers in fog. This also makes the rescue I implemented in the last pull request actually work.'
1950,'icco',"servers.boostrap not working correctly for Google Compute\nWhen calling ```bootstrap``` to start a new GCE server, this is the code that executes:\r\n\r\n```ruby\r\n  defaults { ... some default params ... }\r\n\r\n  server = create(defaults.merge(new_attributes)\r\n  server.wait_for { sshable? }\r\n```\r\n\r\nIn the ```create``` method, it does the following\r\n\r\n```ruby\r\n  object = new(attributes)\r\n  object.save\r\n  object\r\n```\r\n\r\nand within the ```save``` it makes a call to ```service.insert_server```\r\n\r\nNow, the problem here is that the ```new``` will return a Server object, that is not the complete representation of the remote server (it is only local). This is because that call returns the ```object``` (variable) which is _not_ updated with the response from the API call (because the server is still in the PENDING state --> which is an artifact of the non-RESTful way in which the GCE api behaves?).\r\n\r\nSo the the subsequent call to ``sshable?``` (in the wait_for loop) will never succeed, because trying to query whether the server is sshable? needs for its ```public_ip_address``` field to be set, which never gets set in this case, because of the code flow described above.\r\n\r\nThe fix for this would be to subsequently call ```reload``` on the server (till its public IP is populated, presumably when the server transitions out of PENDING), and then wait for the ```sshable?``` call to return true. Alternatively, we could query the operation id for the status of the operation, till the server is ready, and then call ```reload``` on the server.\r\n\r\nNote: IMHO, these are 'workarounds' to the quirkiness of the GCE API, which unlike the other APIs, does not immediately return a full representation of the vm (eg - on AWS, the RunInstances call returns the full representation of the server(s), therefore doesnt suffer from this problem)"
1946,'icco',"[google|compute] Infer the 'image' URL correctly when inserting a server.\nWhen inserting (creating) a new server, using a private (project\r\nspecific) image, the code would infer the resource URI for the\r\nimage incorrectly, because it would try to find the image in the\r\ncurrent project using an incorrect method to access the name of\r\nthe project. This patch fixes the variable reference, and changes\r\nthe loop logic to try all known project names in the same fashion.\r\n\r\nRefs #1933"
1933,'icco','Image URL being inferred incorrectly for Google Compute Engine instances\n**This only exists in 1.12 (see comment below for the behavior on master)**\r\n\r\nWhen creating an instance in GCE, the ```insert_server``` tries to check if the specified ```image_name``` exists in the project. It does so by making this call:\r\n\r\n```ruby\r\n  if get_image(image_name, @project).data[\'code\'] == 200\r\n```\r\n\r\nWhereas, ```get_image``` returns an ```Excon::Response``` (which doesnt have the ```\'code\'``` key. Instead, the check should be:\r\n\r\n```ruby\r\n  if get_image(image_name, @project).data[:status] == 200\r\n```\r\n\r\nWithout this, trying to spin up instances using a custom image will always fail (silently), because the image will never be found in the project, and fog will assume the image exists in the "global" pool, and proceed to send out the ```insert_server``` request using the broken image url.\r\n\r\n_The silent failure also points to another interesting "problem" here:_\r\n\r\n1. When creating an instance based off a custom image, fog defaults to using the global image pool to construct the URI for the image (as described above).\r\n2. This request is sent out on the wire, and \r\n   * the immediate response from GCE is always a ```:status=200, @data[\'status\'] = PENDING``` .\r\n   * At this point, the code returns a server object back to the caller (because its 200).\r\n   * On inspecting the "operations" section of the GCE console, I can see that it eventually transitions into an "INVALID_FIELD_VALUE: Invalid value for field \'resource.images\'", because the specified image does not exist in the global pool.\r\n   * So the cloud never actually spun up a server really.\r\n3. Now, as for the users code...\r\n   * If the (server creation) call was made using either a ```create``` and ```save```, then the users calls for ```ready?``` will always return false, and I dont know how else he/she is expected to know something went wrong (without querying for the operation)\r\n   * If this code path was executed via ```bootstrap``` , the call will block for a long time, and then fail due to the timeout (since ```ready?``` never returns true).\r\n\r\nThe former (3.a) case is where I was bitten.\r\n\r\n'
1930,'icco','[google|compute] Add support for Disks and Micro Instances\nAdded Disks collection and Disk model to Google Cloud Compute and implemented a new insert server service method that allows all options for inserting a server to be used rather than just a subset. These changes were necessary to be able to create micro instances as a persistent disk must be manually specified in the micro instance request which was not possible before.'
1925,'rubiojr',"Create Debian packages for Debian stable and the current Ubuntu LTS release\nI'd like to use this issue to document the whole process. Any input is greatly appreciated.\r\n\r\nThe idea is to maintain up2date deb packages for Fog users currently on LTS/stable releases still using Ruby 1.8.7.\r\n\r\nSee also #1878.\r\n\r\n\r\n\r\n"
1914,'rubiojr','Added support for [xenserver] snapshot\nAdded support for snapshot on xenserver.. this is my first PR on fog, tried to keep the things as much similar on other parts of the code as I have studied..\r\n\r\nsorry if any mistake ;)'
1910,'tokengeek',"[Brightbox] Adds collaborator support\nUpdates Brightbox's fog provider to support collaborations in the API."
1873,'tokengeek','[Brightbox] Expose expires_in value for the access token\n'
1872,'jedi4ever','[openvz] Fixes #1871 test helper callback\nThis moves the setting up of an `at_exit` callback from when the file\r\nis required to when the matching helper is used.\r\n\r\nThis prevents it failing on any run where OpenVZ is excluded.'
1868,'seanhandley','This withstands naming/renaming issues.\nFollows up https://github.com/fog/fog/pull/1867\r\n\r\nand\r\n\r\nhttps://github.com/fog/fog/pull/1867#discussion_r4588518'
1867,'seanhandley','Fix issue 1414\nSee #1414'
1866,'rubiojr',"Fog Xenserver Net::ReadTimeout on certain actions like power_off\nHi, \r\n\r\nI'm using fog (1.11.1) as part of a script to spin up - and power control  vms on Citrix Xenserver. So far this looks really great except I receive timeouts on shutting down a vm for example. \r\n\r\nI cannot find a way to configure this timeout. It looks like this is set to 30 seconds somewhere. Shutting down a vm may take longer, for me it's even fine if it takes 300 seconds at least allow me to set this somewhere. This does not happen on every request, only on slower hosts or vms that are slow on shutdown sequence. \r\n\r\nThis is part of the code to setup a connection \r\n\r\n          @connection = Fog::Compute.new({\r\n                                             :provider => 'XenServer',\r\n                                             :xenserver_url => @host,\r\n                                             :xenserver_username => @user,\r\n                                             :xenserver_password => @pass\r\n                                         })\r\n\r\nActual statement for shutting down the vm is vm.stop \r\n\r\noutput: \r\n\r\n```\r\n/usr/local/rvm/gems/ruby-2.0.0-p0/gems/fog-1.11.1/lib/fog/xenserver.rb:41:in `eval': Net::ReadTimeout (Net::ReadTimeout)\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/protocol.rb:152:in `rbuf_fill'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/protocol.rb:134:in `readuntil'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/protocol.rb:144:in `readline'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http/response.rb:39:in `read_status_line'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http/response.rb:28:in `read_new'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http.rb:1405:in `block in transport_request'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http.rb:1402:in `catch'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http.rb:1402:in `transport_request'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http.rb:1375:in `request'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/net/http.rb:1321:in `request_post'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/xmlrpc/client.rb:477:in `do_rpc'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/xmlrpc/client.rb:281:in `call2'\r\n\tfrom /usr/local/rvm/rubies/ruby-2.0.0-p0/lib/ruby/2.0.0/xmlrpc/client.rb:262:in `call'\r\n\tfrom (eval):1:in `request'\r\n\tfrom /usr/local/rvm/gems/ruby-2.0.0-p0/gems/fog-1.11.1/lib/fog/xenserver.rb:41:in `eval'\r\n\tfrom /usr/local/rvm/gems/ruby-2.0.0-p0/gems/fog-1.11.1/lib/fog/xenserver.rb:41:in `request'\r\n\tfrom /usr/local/rvm/gems/ruby-2.0.0-p0/gems/fog-1.11.1/lib/fog/xenserver/requests/compute/shutdown_server.rb:8:in `shutdown_server'\r\n\tfrom /usr/local/rvm/gems/ruby-2.0.0-p0/gems/fog-1.11.1/lib/fog/xenserver/models/compute/server.rb:184:in `stop'\r\n\tfrom /usr/local/rvm/gems/ruby-2.0.0-p0/gems/xenfoo-0.0.1/lib/xenfoo/action.rb:63:in `poweroff_vm'\r\n```\r\n\r\nTo me it seems Fog is using xmlrpc over ruby-net to setup the connection to the host. Where can I tweak some values? Any help is really appreciated. \r\n "
1854,'kevinykchan',"[joyent|compute] support for http-signature-auth using ssh-agent\nThis PR enables authenticating against Joyent's API using ssh-agent, this eliminates the need to enter a passphrase every time when making API requests."
1818,'tokengeek','[Brightbox] Updates to delete requests and doc fixes\nWe renamed our API docs to use "delete" rather than "destroy". This renames the files and methods whilst keeping "destroy" variants in place.\r\n\r\nAlso fixes many yardoc errors across the generated docs for requests.'
1804,'kevinykchan','joyent server resize -- server resize appears successful even if API call fails\nThis addresses an issue reported at kevinykchan/knife-joyent#39'
1793,'tokengeek','[Brightbox] Updates image selector for name format\nBrightbox is now moving to using the default Ubuntu Cloud Images\r\n(http://cloud-images.ubuntu.com/) where we are also using the naming\r\nconvention used there.\r\n\r\nSo the image selector has been updated to select the latest version of\r\nUbuntu (i386, official Brightbox) image if no image is specified with\r\nthe new format.'
1792,'tokengeek','[Brightbox] Updates image selector for name format\nBrightbox is now moving to using the default Ubuntu Cloud Images\r\n(http://cloud-images.ubuntu.com/) where we are also using the naming\r\nconvention used there.\r\n\r\nSo the image selector has been updated to select the latest version of\r\nUbuntu (i386, official Brightbox) image if no image is specified with\r\nthe new format.'
1711,'rubiojr','Added service_catalog method to OpenStack::Identity\nThe serviceCatalog hash is returned during authentication.  This holds important information on service endpoints and available regions and it is useful to be able to view all information.'
1705,'rubiojr','[openstack|compute] Add volume attachment methods\nAdded new methods to server:\r\n\r\n- Attach volume to a server\r\n- Detach volume from a server\r\n- Retrieve volumes attached to a server\r\n\r\nAdded volume tests.'
1699,'rubiojr',"[openstack] Retrieve supported API version for Image & Network services\nSome environments don't set the API version for endpoints in service catalog (as a default glance or quantum endpoint in a devstack env), so if you try to use Fog::Image of Fog::Network it'll fail with a 300 (Multiple Choices) error. Also, Fog doesn't support some latest API versions (i.e. glance v2).\r\n\r\nInstead of failing, Fog will now check if the service catalog returns an API version and if it is supported by Fog. If not, it'll try to retrieve the supported API endpoint.\r\n"
1681,'krames','SSH timeout not configurable\nIt looks to me like I might have hardcoded that value to 30. \r\n\r\nhttps://github.com/fog/fog/blob/master/lib/fog/core/ssh.rb#L49\r\n\r\nLine 49 should have been:\r\n\r\noptions[:timeout] ||= 30\r\n\r\nNot \r\n\r\noptions[:timeout] = 30\r\n\r\n\r\nLet me submit a fix for that, which should go out in the next release. \r\n\r\nIn the meantime, here is a patch to fix the issue.\r\n\r\nhttps://gist.github.com/krames/5204730\r\n\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/ruby-fog/x_p-QNOBnJ8'
1666,'rubiojr',"Openstack legacy authentication\nMy cloud provider only supports the legacy v1.0 style auth. The method exists in /lib/fog/openstack.rb as authenticate_v1, but /lib/fog/openstack/storage.rb only uses authenticate_v2 within it's authenticate method. Could it be possible to create an option to choose the v1.0 style auth method?"
1530,'rubiojr','[openstack|tests] fix deprecation warnings\nThis is intended to be a semi-long-running pull request to fix/cleanup/add openstack tests. The first commit just fixes some deprecated calls. More soon.'
1490,'geemus','[tests] Changes to format testing helper\n* Adds and documents Shindo::Tests#data_matches_schema\r\n* Deprecates Shindo::Tests#format method\r\n* Strict mode is replace by expandable options\r\n* Uses Fog::Logger instead of p for debug output\r\n* Shindo::Tests#formats_kernel removed (was private)\r\n* Shindo::Tests#confirm_data_matches_schema added\r\n* #confirm_data_matches_schema uses yield\r\n\r\nRelated to issue #1477\r\n\r\nShindo::Tests#formats incorrectly treated missing keys as valid if their\r\nvalue is allowed to be "nullable" or NilClass. These keys are not\r\noptional. They are required but can be the value nil or the Nullable\r\nclass.\r\n\r\nUnfortunately due to lack of documentation and a bug in the code the\r\ncurrent Nullable classes can be interpreted as optional and work as such.\r\n\r\nThis replaces the old format checker with a new version that supports\r\noptions to control the matching behaviour related to extra keys in\r\neither the data or the schema.\r\n\r\nThis corrects the behaviour so extra keys in either the schema or the\r\nsupplied data fail the check unless non strict has been used.\r\n\r\nA legacy version of the #formats test is in place that passes the\r\noptions so it behaves as the old version does.\r\n\r\nPremptive patches have been added to fix those tests that were already\r\nbroken but passed the original checks.'
1488,'tokengeek',"[Brightbox] General updates\nRequest to update firewall policies and a server's compatibility mode flag have been added to the API. This pull request adds these to fog"
1477,'tokengeek','Formats helper does not complain when "nullable" values are missing\nWhen comparing API output with `Shindo::Tests#formats` it should complain when a key is missing.\r\n\r\nThere is a bug such that this does not happen when the value is expected to be `NilClass` or any "nullable" class such as `Fog::Nullable::String`\r\n\r\n### Expected\r\n\r\n```\r\n# :a is missing, invalid format\r\nformats_kernel({}, {:a => Fog::Nullable::String}) # => false\r\n```\r\n\r\n### Actual\r\n```\r\nformats_kernel({}, {:a => Fog::Nullable::String}) # => true\r\n```\r\n\r\nThese of course need to be tested in the context of `Shindo`. I have added test cases to `tests/helpers/formats_helper_tests.rb` to confirm. Will push the test cases to a suitable branch when I have an issue number.'
1460,'tokengeek','[core] Fix service instance variable being included when doing model.to_json\n'
1427,'geemus','[docs] Creates release policy document to discuss\nThis documents the current release policy as it appears (based on the\r\nRakefile) but may not be comprehensive.\r\n\r\nFor github issue #1406'
1419,'tokengeek','List of maintainers/provider and support levels.\nIt would be useful for people for us to list who has volunteered to maintain which particular provider/service or see which code is not being actively maintained.\r\n\r\nFor support levels you have providers where the companies are involved, the providers where it is a community effort with a volunteer or providers that are not quite there or abandoned.'
1417,'tokengeek','[Brightbox] Adds #dns_name to server\nLittle known fact is that the public version of the domain name of a\r\nBrightbox cloud server is the `fqdn` prefixed with public.\r\n\r\nThis is now fixed here until it is fixed in the main API and the value\r\ncan be referenced directly from the response.'
1415,'geemus','Deprecate connection option\nFor #1392 \r\n\r\nThis adds accessors to handle deprecation of the `connection` option, which as discussed on the ticket is actually an instance of a subclass of `Fog::Service` not `Fog::Connection` as implied.\r\n\r\nSo now passing `service` upon creation is the expected means of declaring the parent/containing service. `connection` still works but warns.\r\n\r\nLeft the tests as is so we can see the deprecated behaviour still works.\r\n\r\nSome models need to set up the service first so that they can refer there for additional data, a preliminary API call etc. These have been altered as well.\r\n\r\nIt then changes in small groups (per provider or provider/service) the calls. This change affects most models since they all piggy back on the service for requests.'
1414,'seanhandley','serverlove/models/compute/server.rb does not appear to parse\nA simple test program, such as (serverlove.rb):\r\n---\r\nrequire \'rubygems\'\r\nrequire \'fog\'\r\n\r\nFog.mock!\r\n\r\nconn = Fog::Compute.new({\r\n  :serverlove_uuid    => "XXXX",\r\n  :serverlove_api_key => "XXXX",\r\n  :provider           => "Serverlove"\r\n})\r\n\r\nserver = conn.servers.create( { :name => \'test\', :cpu => 500, :memory => 512 } )\r\n\r\nResults in the error:\r\n/Library/Ruby/Gems/1.8/gems/fog-1.8.0/lib/fog/serverlove/models/compute/server.rb:38:in `save\': undefined method `defaults\' for #<Fog::Compute::Serverlove::Server:0x1025a5fe8> (NoMethodError)\r\n\tfrom /Library/Ruby/Gems/1.8/gems/fog-1.8.0/lib/fog/core/collection.rb:50:in `create\'\r\n\tfrom ./serverlove.rb:14\r\n\r\nI\'ve tried this on Ruby 1.8.7 under Mac OS X and Ubuntu with the same error.'
1406,'tokengeek',"Documented release process\nAt the moment fog releases are done (sort of) monthly by @geemus or on demand.\r\n\r\nEvery so often someone asks for a release, frequently because of a new AWS endpoint or a feature they have added.\r\n\r\n(Putting aside solutions/pending pull requests to make endpoints dynamic, not the issue)\r\n\r\nIt would be useful to document this so people wouldn't need to ask.\r\n\r\nFurther to this however is that I'm starting to work on some major refactoring of fog that may make `master` unstable.\r\n\r\nSo we can end up with the simple change (new endpoint) stuck behind an unstable change that needs full testing by all providers before we can consider a stable release. Or a new version of a provider's API.\r\n\r\nSo I think it's worth a discussion about how we should schedule future releases, relate them to branches and document that.\r\n\r\n* Do we offer a `stable` branch? `1.8_branch` ?\r\n* Aim for even minor releases being stable (1.8) and odd (1.9) less so?\r\n* Issue Release Candidates so early adopters can help test without it getting picked up by bundler for most people?\r\n* Any projects we can learn from?\r\n\r\nSome of the complexity disappears when fog goes modular and providers can version their own changes and fog is a stable snapshot of those versions, but it would be good to start somewhere.\r\n\r\nSo after a bit of discussion I'll add `RELEASE.md` to the project root with a write up. Who to speak to. Where to base changes. Which rake tasks to run.\r\n\r\nCheers!"
1392,'tokengeek','Service is misnamed as `connection` within collection and models\nWhen the service code was rewritten, unfortunately `connection` was used instead of "service" which has passed down two levels and is available throughout all the models.\r\n\r\nSo within a `Fog::Model` we have `connection` attribute which is a `Fog::Service::Real` which in many cases has it\'s own `@connection` variable which is a `Fog::Connection`\r\n\r\nThis has caught me out a few times and is a source of confusion.\r\n\r\nSo the service generator passes itself as `connection` here: https://github.com/fog/fog/blob/master/lib/fog/core/service.rb#L94\r\n\r\nCollections are then built and passed the same to merge in merge to their own `connection` accessor: https://github.com/fog/fog/blob/master/lib/fog/core/collection.rb#L108\r\n\r\nFinally when collection creates a model it does the same: https://github.com/fog/fog/blob/master/lib/fog/core/model.rb#L7\r\n\r\nIn both `Collection` and `Model` it is an accessor attribute which means it may have been picked up and changed by existing code.\r\n\r\nI think we should look at replacing these with `service` so what is being dealt with is clearer.\r\n\r\nDeprecating `connection` and `connection=` as well so they still function as is.\r\n\r\nI also think it would be cleaner to initialise the service in `initialize ` rather than exposing accessor methods.\r\n\r\nI\'m happy to look at this as long as we agree it is worth fixing.'
1391,'tokengeek','[Brightbox] Standardise request\nWhilst DRYing up the code we replaced the standard version of `#request`\r\nwith one that had clearer arguments and returned parsed data.\r\n\r\nThis was mostly fine but introduced a few problems:\r\n\r\n* mock functionality expects the original interface\r\n* headers returned in request are not available\r\n\r\nThis makes the newer version available as `#wrapped_request` and as a\r\ndeprecated form through `#request`.'
1390,'tokengeek',"`Fog.credentials` is a global we should phase out of provider code\nSummary: `Fog.credentials` is a global parameter whose use has been encouraged by some of the earlier examples so has been picked up by several providers.\r\n\r\nWe spent a while looking at altering which sets of `.fog` files we should be able to load for #1314 . Specifically it was for ascending back up parent directories looking.\r\n\r\nWe could not safely allow that because that changes the possible output for `Fog.credentials` to change depending on the environment.\r\n\r\nOne thing this highlighted is that several providers and a few places in core are using a pattern where we are reading values out of `Fog.credentials` at a low level, inside the connection. Most often the `initialize` methods.\r\n\r\nSo the main reason we could not change how the files were loaded was that the accessing credentials were combined with loading of the credentials and is treated it as a global resource.\r\n\r\nSo you create a new Compute service instance with options but `Fog.credentials` is called as a global but *may* have loaded credentials from wherever they had located a file which *may* have added a setting that was wrong.\r\n\r\nIf you use the service Factory `Compute[:provider]` it loads and passes the options anyway but this wasn't documented.\r\n\r\n* I think we need to deprecate `Fog.credentials` so it starts warning.\r\n* Then affected code is updated to not rely on it.\r\n* Then we remove it in 2.0\r\n\r\nThe use in core code is that of public/private SSH key which I think is different enough from service credentials to be handled in another way."
1389,'geemus','[core] Adds #persisted? to Fog models\nFog models have been able to use #new_record? for years to use an\r\nActiveRecord like interface to check for peristence of a resource.\r\n\r\nWhen Rails 3 switched to using ActiveModel API this changed to\r\nbe #persisted?\r\n\r\nThis deprecates #new_record? and adds #persisted? as the first little\r\nstep to making fog models easier to use using ActiveModel API.\r\n\r\nAs discussed on the tickes, ActiveModel is not planned to become a\r\ndirect dependency for fog.\r\n\r\nRelates to #1276'
1335,'rubiojr','Fog::XenServer::InvalidLogin\nFog raises `Fog::XenServer::InvalidLogin` error, even though I can connect to the server with these login using the code below;\n\n      def initialize(host, username, password)\n        @factory = XMLRPC::Client.new(host, \'/\')\n        @factory.set_parser(XMLRPC::XMLParser::REXMLStreamParser.new)\n        response = @factory.call(\'session.login_with_password\', username, password )\n        raise Fog::XenServer::InvalidLogin.new unless response["Status"] =~ /Success/\n        @credentials = response["Value"]\n      end\n    \nThis code is actually part of `Fog::Xenserver` module but It does not raise any error when I type it directly to the irb.\n'
1320,'tokengeek',"[Brightbox] Refresh tokens\nThis adds support for refresh tokens to authenticate with Brightbox's Compute service.\n\nAlso major clean up of the internals to allow better testing and maintenance."
1314,'tokengeek',"Find credentials\n@postmodern seems to have lost track of PR748, so I've tweaked it and added tests to make it acceptable for merging."
1310,'tokengeek',"User-Agent change uses Fog::VERSION and drags in excess code.\nWe have been using just the Brightbox part of fog in our CLI happily with `require 'fog/brightbox'` but the User-Agent change I added causes a dependency on `Fog::VERSION`\n\n`Fog::VERSION` is defined in `lib/fog.rb` which also pulls in the rest of fog.\n\nSo I'm reverting immediately and will look to reapply"
1280,'geemus',"move /docs into own repo, move hosting to github pages\nFor various historical reasons fog.io was in the same repo and hosted on S3 static pages, but I think the time has come for it to have it's own repo and that the whole setup can be greatly simplified by putting this on to github pages instead of s3 static stuff."
1270,'tokengeek','Update contributor guidelines with notes about 1.8.7 support\nWhen discussing #1250 the general consensus is that dropping 1.8.7 support is not beneficial and may be disruptive for users of some LTS distros.\r\n\r\nSo the contributors documentation needs to be updated pointing out that fog does support 1.8.7 and will do for the foreseeable future.\r\n\r\nAs such all patches should be expected to be compatible with 1.8.7 and any dependencies should be the same.'
1260,'brianhartsock','[storage] Rackspace Access-Control-Allow-Origin\nAdd support for Rackspace Files Access-Control-Allow-Origin and Origin headers'
1244,'seanhandley','Bumped mocked maximum value for Provisioned IOPS\nThis change only affects the mock for AWS volumes. AWS announced today that they are going to increase the maximum soon.'
1214,'tokengeek','[Brightbox] Server initialisation fails unless a config file is in place\nAffects Fog 1.6\r\n\r\nDiscovered by @rubiojr as part of https://github.com/rubiojr/knife-brightbox/issues/6\r\n\r\nWe introduced a dynamic lookup in https://github.com/fog/fog/commit/b221b97ba62622ea60f623af6d41c6db9d9798ec to query the list of images available and pick the most suitable default rather than hardcoding it in Fog.\r\n\r\nUnfortunately using `Fog::Compute[:brightbox]` as the basis of the connection actually creates a new connection based on details in the `~/.fog` file to perform this lookup.\r\n\r\nNever spotted this because I always have this file in place but if you are creating your own connection with parameters and do not have this file `Fog::Compute::Brightbox::Server.new` fails because this second connection doesn\'t have the required arguments.\r\n\r\nLooking further the `connection` that should be used is not available at that point in time (is `nil`) so it\'s not a simple exchange.\r\n\r\nThe connection attribute is merged in afterwards so currently there isn\'t a way to use an existing connection in a model\'s `initialize` that I can see.\r\n\r\n```\r\n$ connection = Fog::Compute.new(:brightbox_client_id => "cli-xxxxx", :brightbox_secret => "xxxxxxxx", :provider => "Brightbox")\r\n#<Fog::Compute::Brightbox::Real>\r\n$ connection.servers.first\r\nArgumentError: Missing required arguments: brightbox_client_id, brightbox_secret\r\nfrom /Users/paul/code/fog/lib/fog/core/service.rb:208:in `validate_options\'\r\n```'
1142,'bradgignac','Add Accept header with application/json media type to OpenStack requests\nThe Fog code expects JSON data to be returned, and in some cases enforces this by appending `.json` to the URI. However, this is not employed consistently. Since the OpenStack  specification does not require JSON to be the default media type many calls will fail if connecting to an implementation that returns XML or even some other media as a default.\n\nI have added an Accept header to the requests for compute, identity and volume services, set to `application/json`. This has been tested on an OpenStack implementation configured to default to XML representations.'
886,'kevinykchan','[joyent|compute] Support for DSA keys for auth\n'
868,'kevinykchan',"Use MultiJSON #dump and #load rather than #encode and #decode\nIn version 1.3.0 MultiJSON replaced the methods encode and decode with dump and load. The commit also deprecated the encode and decode methods (https://github.com/intridea/multi_json/commit/e90fd6cb1b0293eb0c73c2f4eb0f7a1764370216).\r\n\r\nIn version 2.0 of MultiJSON encode and decode will be removed entirely. This commit replaces the methods throughout fog for every provider. It also updates the gemspec to ~> 1.3 to ensure that 1.3.0 or higher is required so that the new methods are availible.\r\n\r\nThe changes are pretty simple and I've tested them on the Brightbox Cloud but I don't have the ability to test all the other providers."
866,'xtoddx','OpenStack Update\nIn relation to: https://github.com/fog/fog/issues/848'
845,'benton','Parsing error for EC instances with networkInterfaceSet tag\nHello, all.\r\n\r\nI am working on a problem I\'m having with Fog that manifests as "empty" (that is, with no "instanceID" identity field) instances. If the following code produces anything other than an empty Array, then the bug is present:\r\n\r\n    no_ids = Fog::Compute[:aws].servers.select {|s| s.id == nil}\r\n\r\nI can\'t tell if it\'s related to any of the other open issues, but it seems not. But here\'s a running history, and a fix.\r\n\r\n\r\nFog EC2 Server Collection Parsing Error\r\n===\r\n\r\nFirst, I noticed that some of our EC2 server instances were returning with no IDs.\r\nThe following code should always produce an empty array, but for fog >= 1.2, I kept getting entries in the following Array:\r\n\r\n    no_ids = Fog::Compute[:aws].servers.select {|s| s.id == nil}\r\n\r\n\r\nUpon further investigation, found that the ID-less instances do not actually correspond to running instances. Here I compare the output of the EC2 command-line utility against Fog:\r\n\r\n    ╭─@Diderot.local ~/projects/fog ‹ruby-1.9.3› ‹master›\r\n    ╰─ ec2-describe-instances | grep INSTANCE | wc -l\r\n         115\r\n    ╭─@Diderot.local ~/projects/fog ‹ruby-1.9.3› ‹master›\r\n    ╰─ bundle exec fog\r\n     Welcome to fog interactive!\r\n     :default provides AWS, VirtualBox and Vmfusion\r\n    >> Fog::Compute[:aws].servers.count\r\n    117\r\n\r\nHmmmm, something\'s wrong with the Fog Collection `Compute[:aws].servers`. Here\'s the Fog code that parses the response body from Excon (from `lib/fog/aws/models/compute/servers.rb`):\r\n\r\n    data = connection.describe_instances(filters).body\r\n    load(\r\n      data[\'reservationSet\'].map do |reservation|\r\n        reservation[\'instancesSet\'].map do |instance|\r\n          instance.merge(:groups => reservation[\'groupSet\'])\r\n        end\r\n      end.flatten\r\n    )\r\n\r\nSo, we\'ve got 2 XML structures that wrap our instances: `reservationSet` and `instancesSet`. Each gets a call to `map`, so as long as the outer call returns the correct count, all is well. Let\'s try this code in IRB to confirm that we\'ve located the problem...\r\n\r\n    ╭─@Diderot.local ~/projects/fog ‹ruby-1.9.3› ‹master›\r\n    ╰─ bundle exec ./bin/fog\r\n      Welcome to fog interactive!\r\n      :default provides AWS, VirtualBox and Vmfusion\r\n    >> (Fog::Compute[:aws].describe_instances.body[\'reservationSet\'].map {|r| r[\'instancesSet\']}).flatten.count\r\n    117\r\n\r\nThere\'s the incorrect count. What if we eliminate parsing of the inner XML and just count `reservationSet`?\r\n\r\n    >> Fog::Compute[:aws].describe_instances.body[\'reservationSet\'].flatten.count\r\n    115\r\n\r\nThere\'s our correct number.\r\nAt least one of these `reservationSet`s must have more than one `instancesSet`...\r\n\r\n    >> fat_sets = Fog::Compute[:aws].describe_instances.body[\'reservationSet\'].select {|r| r[\'instancesSet\'].count > 1}\r\n      [...]\r\n    >> fat_sets.count\r\n    2\r\n    >> fat_sets.map {|set| set[\'instancesSet\'].count}\r\n    [2, 2]\r\n    >> fat_sets.first[\'instancesSet\'][0]\r\n    {"blockDeviceMapping"=>[{"deviceName"=>"/dev/sda1", "volumeId"=>"vol-1b8c2672", "status"=>"attached", "attachTime"=>2011-03-24 19:38:20 UTC, "deleteOnTermination"=>true}], "instanceState"=>{"code"=>80, "name"=>"stopped"}, "monitoring"=>{"state"=>false}, "placement"=>{"availabilityZone"=>"us-east-1c", "groupName"=>nil, "tenancy"=>"default"}, "productCodes"=>[], "stateReason"=>{"code"=>0}, "tagSet"=>{}, "instanceId"=>"i-0745236c", "imageId"=>"ami-f11ff098", "privateDnsName"=>nil, "dnsName"=>nil, "reason"=>"User initiated (2011-03-24 19:37:49 GMT)", "keyName"=>"CorpIT_NAS", "amiLaunchIndex"=>0, "instanceType"=>"m1.small", "launchTime"=>2011-03-21 19:58:07 UTC, "platform"=>"windows", "subnetId"=>"subnet-b78544de", "vpcId"=>"vpc-b28544db", "privateIpAddress"=>"10.225.50.8", "architecture"=>"i386", "rootDeviceType"=>"ebs", "clientToken"=>nil}\r\n    >> fat_sets.first[\'instancesSet\'][1]\r\n    {"blockDeviceMapping"=>[], "instanceState"=>{}, "monitoring"=>{}, "placement"=>{}, "productCodes"=>[], "stateReason"=>{}, "tagSet"=>{}}\r\n\r\nThere it is -- an empty item in `instancesSet`. There\'s one in both the `fat_sets`.\r\n\r\n\r\nThe Fix\r\n===\r\n\r\nOK, so our extra instances are being caused by an empty `instancesSet`. \r\nWhat if we just filter those out?\r\n\r\n    >> (Fog::Compute[:aws].describe_instances.body[\'reservationSet\'].map do |r| \r\n    ?>   r[\'instancesSet\'].select {|i| i[\'instanceId\'] != nil}\r\n    >> end).flatten.count\r\n    117\r\n\r\nWorks just fine. The code fix goes into line 59 of the relevant parser,\r\n`lib/fog/aws/parsers/compute/describe_instances.rb`:\r\n\r\n    if @instance[\'instanceId\'] != nil\r\n      @reservation[\'instancesSet\'] << @instance\r\n    end\r\n\r\n(The existing code is just the middle line, without the test for nil identity.)\r\n\r\n---\r\n\r\nI will try to answer the following questions before submitting a pull request:\r\n\r\n  * Why is this happening in the first place? \r\n    Does the XML response from Amazon have some empty tags? \r\n    Or is there some other bug in the parser that I\'m missing?\r\n  * Under what conditions does this problem manifest?\r\n  * How can I write a test for it?\r\n\r\nDoes anyone know how I  can easily look at the whole response body (pre-parsing) when parsing with SAX?\r\n\r\n- benton\r\n\r\n'
697,'jeffmccune','[vsphere] Add the ability to create linked clones in vsphere\nAdd the ability to create linked clones in vsphere'
692,'jeffmccune','Feature/master/robustness and similar to vsphere\n This patch has several purposes.  Firstly this patch brings the Fog vmfusion provider in line with the recently released v0.4.0 version of the Fission project by aligning various method names.  During this sync up servers.rb was modified to obtain all VM states and passing them to the raw object instead of doing so when attributes are obtained inside server.rb.  This improves performance a lot since it reduces the need to run vmrun for every VM on the system.  Other changes being made to the provider are so that it returns data and acts more similar to the vsphere provider while still keeping backward compatibility with the original implementation; which was to be more similar to various cloud providers.'
691,'jeffmccune',"[vsphere] this patch allows the ability to create 'blank' vms in vsphere rather than cloning\nthis patch allows the ability to create 'blank' vms in vsphere rather than cloning"
613,'jeffmccune','vsphere provider should create blank VMs\nThe vsphere provider should have a method to create blank VMs. This would support the provisioning model where you create a blank VM and PXE boot into a kickstart or preseed install. '
603,'brianhartsock',"[rackspace|dns] DNS Records failure test breaks\nThis is probably a bug in the Rackspace API, but the failure test on records is not working.\r\n\r\nGET /v1.0/389090/domains/2959812/records/k-6543282 is returning 500 from the API.  I'll email the Rackspace DNS API team, but I wanted to make sure we documented it in case other people saw the same issue."
602,'brianhartsock','[rackspace|dns] Callback errors should thrown on ERROR status\n<code>\r\n{"request"=>"{\\"domains\\":[{\\"name\\":\\"fogzonetests.com\\",\\"emailAddress\\":\\"fog@example.com\\"}]}", "error"=>{"message"=>"The object already exists.", "code"=>409, "details"=>"<?xml version=\\"1.0\\" encoding=\\"UTF-8\\"?>\\n\\n  Hash: Zone label conflicts with the existing zone because parent/sub domain of fogzonetests.com belongs to another owner\\n\\n"}, "status"=>"ERROR", "verb"=>"POST", "jobId"=>"1af8bd49-4860-4a3b-9256-8f6eb9467dd8", "callbackUrl"=>"https://dns.api.rackspacecloud.com:443/v1.0/<redact>/status/1af8bd49-4860-4a3b-9256-8f6eb9467dd8", "requestUrl"=>"http://dns.api.rackspacecloud.com:443/v1.0/<redact>/domains"}\r\n</code>\r\n\r\nThe above callback status response should trigger an exception.'
586,'redzebra','Auto Scaling Group object contains All instances and not just the member ones\nHello,\r\n\r\nI have got an object of my AutoScaling group and all the attributes of it are correct, however the the instances property contains all instances from all groups and not just the one I have selected.\r\n\r\n\t# Create a connection to the required cloud service\r\n\tcf = Fog::AWS::CloudFormation.new(\r\n\t\t:aws_access_key_id => "somekey",\r\n\t\t:aws_secret_access_key => "secret",\r\n\t\t:region => "eu-west-1" \r\n\t  )\r\n\r\n\tresources = cf.describe_stack_resources("StackName" => "MyStack").body[\'StackResources\']\r\n\r\n\t# Now that we have some instance ids get information about each one\r\n\tcompute = Fog::Compute.new(:provider => config["cloud"]["service"],\r\n\t\t\t\t\t\t\t\t:aws_access_key_id => "somekey",\r\n\t\t\t\t\t\t\t\t:aws_secret_access_key => "secret",\r\n\t\t\t\t\t\t\t\t:region => "eu-west-1")  \r\n\t  \r\n\t# loop round the resources that are returned\r\n\tinstances = Array.new\r\n\tresources.each do |resource|\r\n\t  \r\n\t  # determine the type of resource\r\n\t  if (resource[\'ResourceType\'] == "AWS::EC2::Instance")    \r\n\t\t# get the instance id\r\n\t\tinstances << resource[\'PhysicalResourceId\']\r\n\t  elsif (resource[\'ResourceType\'] == "AWS::AutoScaling::AutoScalingGroup")\r\n\r\n\t\t# Create an AutoScaling object\r\n\t\tas = Fog::AWS::AutoScaling.new(:aws_access_key_id => "somekey",\r\n\t\t\t\t\t\t\t\t\t:aws_secret_access_key => "secret",\r\n\t\t\t\t\t\t\t\t\t:region => "eu-west-1")  \r\n\t\t\r\n\t\t\r\n\t\t# use the Fog class to get the machines with the relevant tags\r\n\t\tasgroup = as.groups.get(resource[\'PhysicalResourceId\'])\r\n\r\n\t\tputs asgroup.inspect\r\n\t  end\r\n\tend\r\n\r\nI can parse this list to extract only the instances relevant to the group, but I thought this is what the \'groups.get\' function was meant to provide.  If it is what this is meant to do, is this a bug and if so can it be looked at?\r\n\r\nThanks very much,\r\n\r\nRussell'
544,'nightshade427',"After creating a server, cannot ssh to it\n``>> compute = Fog::Compute.new(:provider => 'Linode')\r\n>> server = compute.servers[0]\r\n>> server.ssh('mkdir -p infrastructure')\r\nNoMethodError: undefined method `public_ip_address' for #<Fog::Compute::Linode::Server:0x00000101350550>\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/lib/fog/core/attributes.rb:181:in `block in missing_attributes'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/lib/fog/core/attributes.rb:180:in `each'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/lib/fog/core/attributes.rb:180:in `missing_attributes'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/lib/fog/core/attributes.rb:161:in `requires'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/lib/fog/compute/models/server.rb:18:in `ssh'\r\n  from (irb):13:in `<top (required)>'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/bin/fog:52:in `block in <top (required)>'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/bin/fog:52:in `catch'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/gems/fog-1.0.0/bin/fog:52:in `<top (required)>'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/bin/fog:19:in `load'\r\n  from ~/.rvm/gems/ruby-1.9.2-p290@infrastructure/bin/fog:19:in `<main>'``\r\n\r\nI'm running ruby 1.9.2 and Fog v1.0"
382,'lstoll',"Feature request: automatic retry on AWS 500 or 'throttling' error\nI'm the principal author of dew (https://github.com/playup/dew). Dew contains automated tests which create / destroy AWS resources, and we run a 'stress test' to determine where intermittent failures occur in our scripts.\r\n\r\nOne thing we've found is that AWS will occasionally return an ISE (code 500) or 403 Throttling. We can work around this in our own scripts by retrying, but I'm wondering if it would be a better idea to make this part of Fog or Excon.\r\n\r\nI would imagine that it'd be an option on instantiation, eg :retries => 5, :retrywait => 1000. (retry up to five times, wait 1sec between retries).\r\n\r\nIf the Fog maintainers think this is a good idea I'd be tempted to implement this myself."
278,'geemus','[storage|google&rackspace] should auto-paginate for each\nFor consistency and ease of use, these providers should be updated to match up with the aws changes introduced here:\r\nhttps://github.com/geemus/fog/pull/269\r\n\r\nYou should be able to take those changes and add them in more or less the same way, but with a little tweaking around how the actual pagination works.\r\n\r\nLet me know if there are any questions or how I can help, thanks!'
3139,'nirvdrum',"[vsphere] new default dest_folder in vm_clone\nfixes fog/fog#3134\r\n\r\nWe want to allow someone to not set the dest_folder option because they\r\nmight not have a desired folder structure and not care. In case they\r\nfollow common examples and use a folder called 'templates/' for templates\r\nwe don't want to create the new vm in that folder. It is better to\r\ndefault to the root folder.\r\n\r\nConflicts:\r\n\tlib/fog/vsphere/requests/compute/vm_clone.rb"
3121,'nirvdrum','[vsphere] find network by name and dvswitch\ngiven a name and a switch name\r\nor just a name and whether a dvs is desired\r\nor just a name\r\n\r\nWhen you just want the first (or only) distributed virtual switch on a given cluster, you for example call `get_raw_network(name, datacenter_name, true)` This enables a network admin to choose where a new machine will be placed on the network by moving a network to a distributed virtual switch or non distributed virtual switch.'
3117,'nirvdrum',"[vsphere] get resource pool without name\nThis appears to be the best way to allow cloning into a chosen cluster which is not using any resource pools. Luckily the cluster is able to return a default resource pool. I have this working with vsphere 5.1 and rbvmomi 1.6.0.\r\nThe option to the vm_clone method would look like this `'resource_pool' => ['cluster5', nil]`"
2986,'mwhagedorn','tests/hp/block_storage_tests.rb fails without network connection\nThere are 5 failing tests such as:\r\n\r\n```\r\n  \r\n  Fog::HP::BlockStorage (hp, blockstorage)\r\n    Test good credentials + returns {:auth_token=>"auth_token", :endpoint_url=>"http://127.0.0.1/bpath/", :service_catalog=>{:"Block Storage"=>{:zone=>"http://127.0.0.1/bpath/"}}, :expires=>"2014-06-10T16:42:17+02:00"}\r\n    Test expired credentials\r\n      - raises Excon::Errors::Unauthorized\r\n      getaddrinfo: Name or service not known (SocketError) (Excon::Errors::SocketError)\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/socket.rb:181:in `getaddrinfo\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/socket.rb:181:in `connect\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/ssl_socket.rb:131:in `connect\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/socket.rb:28:in `initialize\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/ssl_socket.rb:9:in `initialize\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/connection.rb:414:in `new\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/connection.rb:414:in `socket\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/connection.rb:126:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/middlewares/mock.rb:42:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/middlewares/instrumentor.rb:22:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/middlewares/base.rb:15:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/middlewares/base.rb:15:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/middlewares/base.rb:15:in `request_call\'\r\n        /usr/share/gems/gems/excon-0.33.0/lib/excon/connection.rb:269:in `request\'\r\n        /usr/share/gems/gems/fog-core-1.22.0/lib/fog/core/connection.rb:56:in `request\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/lib/fog/xml.rb:24:in `request\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/lib/fog/hp/core.rb:198:in `authenticate_v2\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/lib/fog/hp/block_storage.rb:121:in `initialize\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/tests/hp/block_storage_tests.rb:26:in `new\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/tests/hp/block_storage_tests.rb:26:in `block (3 levels) in <top (required)>\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:132:in `instance_eval\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:132:in `assert\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:108:in `raises\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/tests/hp/block_storage_tests.rb:26:in `block (2 levels) in <top (required)>\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:79:in `instance_eval\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:79:in `tests\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/tests/hp/block_storage_tests.rb:24:in `block in <top (required)>\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:79:in `instance_eval\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:79:in `tests\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:38:in `initialize\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:13:in `new\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo.rb:13:in `tests\'\r\n        /builddir/build/BUILD/rubygem-fog-1.22.0/usr/share/gems/gems/fog-1.22.0/tests/hp/block_storage_tests.rb:3:in `<top (required)>\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo/bin.rb:61:in `load\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo/bin.rb:61:in `block (2 levels) in run_in_thread\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo/bin.rb:58:in `each\'\r\n        /usr/share/gems/gems/shindo-0.3.8/lib/shindo/bin.rb:58:in `block in run_in_thread\'\r\n```\r\n\r\nThey fails when the host cannot be resolved, e.g. when /etc/resolv.conf is not available. This is default configuration for Fedora build infrastructure.\r\n\r\nAlthouhg not a show stopper, I would appreciate if this could be fixed.'
2877,'icco',"Move Google to a separate Gem\nI imagine this process will be talked about a little at the summit, but I'd love to do this this summer.\r\n\r\nI'd be curious about your thoughts on doing this @geemus.\r\n\r\nQuestions that have popped in my head:\r\n\r\n * Should the gem be owned by the fog organization or Google Cloud? \r\n * Testing policies?\r\n * Licensing?\r\n * Switch to semantic versioning?\r\n\r\netc."
2783,'tokengeek',"Add support for global 'compatible' and 'proprietary' options to service\nQuick idea to discuss.\n\nSo along with 'provider' could we not add 'mode' or 'proprietary' as options to base service? \n\nThis dictates if models are created by including a module set by the providers and makes requests public.\n\nFor fog 1.x proprietary is true and exposes requests and uncommon methods that have been added.\n\nFog 2.0/3.0 we change it so it is false. So only the agreed public api to a service and its models are visible.\n\nRather than a boolean we could allow 'mode' and you can declare if you want warnings, proprietary mode or compatible mode.\n\nWe can use this to allow libraries and users to clean up their use of an abstraction layer so they can actually abstract things.\n\nThis is actually a core change but will eventually need work on most providers.\n"
2626,'tokengeek',"Replace/phase out current data validator with JSON schema\nThis was just raised on #1266 \r\n\r\nThe current code for testing if the data from an API response is limited and a bit buggy (https://github.com/fog/fog/blob/master/lib/fog/schema/data_validator.rb)\r\n\r\nI already had a major clean up on it whilst keeping backward compatibility but there's still a few corner cases I find and it's missing functionality.\r\n\r\nAt Brightbox we've been testing our JSON output using https://github.com/hoxworth/json-schema with much better results.\r\n\r\nDespite the name it is not JSON specific and it can verify one hash structure against the schema in another. It just uses JSON schema's terms/names/keys/structure not *only JSON*.\r\n\r\nUnlike a real XML validator it's fairly lightweight so could be included and used by either JSON or XML apis to ensure the `data` hash after parsing matches a schema.\r\n\r\nSo I propose that we add a new version of the helper that can be used to replace our undocumented schemas over a transitional period.\r\n\r\nThis will add new helpers to https://github.com/fog/fog/blob/master/tests/helpers/formats_helper.rb and require updates to the schemas.\r\n\r\nI expect including schema files (rather than schemas as Ruby hashes) would be acceptable but making external calls to gain a schema would not be."
2501,'icco','[google|compute] v1 update and enhancement (fixing tests, Networks setup, Server metadata update, Disk attachment, etc.)\nSince changes needed to move Fog to GCE v1 was made by @carlossg in #2494 I decided to share my work on this field. I plan to add support for Networks setup, Server metadata update and Disks attachment. \r\nCurrently **work is in progress** and I just want to hear feedback from @icco about following things:\r\n- since GCE returns on every operation and as I understand we have synchronous API here, I think we need to add  [wait](https://github.com/allomov/fog/blob/b73e99b1cd09da041a9f3493e3ad588886dc011b/lib/fog/google/models/compute/operation.rb#L45-L55) method to Operation and [use it to determine when operation ends](https://github.com/allomov/fog/blob/b73e99b1cd09da041a9f3493e3ad588886dc011b/lib/fog/google/models/compute/image.rb#L56-L57), `backoff_if_unfound` method can raise exception after 1 sec while inserting take more time;\r\n- you can use `Fog.wait_for` [method](https://github.com/allomov/fog/blob/b73e99b1cd09da041a9f3493e3ad588886dc011b/lib/fog/core/wait_for.rb) within `Compute#backoff_if_unfound` [method](https://github.com/fog/fog/blob/master/lib/fog/google/compute.rb#L104-L122) to DRY this method;\r\n- since we fill only `source` field of `raw_data` attribute for `Image` instance we can escape using additional hash and assign only source URL to `raw_data` attribute, it will reduce repetition (it is the worse idea I have because it will make new version incompatible with previous). Also this field can be refactored to accept `Fog::Storage::Google::File` object.\r\n\r\nThank you for your attention . :sparkling_heart:'
1741,'nirvdrum','[vsphere|compute] add template model details\nCurrently\r\n    templates = @fog.list_templates(options)\r\nwill only return the `id` field.\r\n\r\n@jeffmccune  this PR pads out the model with a bunch of other fields.\r\n'
362,'dylanegan',"Mock only accepts string options\nHi,\r\n\r\nIt's possible I have this wrong, but it looks like the Fog::AWS::Storage::Real class can accept symbols for the options hash, but the Mock class can only accept strings.\r\n\r\nSo whereas this seems to work fine when interacting directly with S3:\r\n\r\n<pre>\r\nresponse = @directory.connection.get_bucket(@directory.key, :prefix => prefix, :max_keys => MAX_KEYS, :marker => marker)\r\n</pre>\r\n\r\nIt must be called like so in order for the Mock implementation to be satisfied:\r\n\r\n<pre>\r\nresponse = @directory.connection.get_bucket(@directory.key, 'prefix' => prefix, 'max-keys' => MAX_KEYS, 'marker' => marker)\r\n</pre>\r\n\r\nSince all the options have defaults or get ignored otherwise, this could trip someone up rather easily."
