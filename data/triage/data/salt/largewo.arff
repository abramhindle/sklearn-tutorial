@relation large.json
@attribute id integer
@attribute owner string
@attribute content string
@data
18249,'thatch45','fix grains filter_by function when using salt-ssh\nThis is very similar to #18238 and 7975b0a11.\r\n\r\nFor a more verbose description see #18238.\r\n'
18196,'thatch45','Fix for pyobjects __salt__ wrapper (SaltObject)\nThis is a proposed fix for #18088 where the `__salt__` object cannot simply be iterated over when used via `salt-ssh`. The existing implementation of `SaltObject` (the pyobjects renderer wrapper for more object-oriented access to functions registered in the `__salt__` dictionary) requires iteration to resolve all modules in advance.\r\n\r\nThe new implementation changes this so it does not need to prepare the list of modules in advance but it just wraps anything passed and forwards it to the underlying dictionary.'
18194,'s0undt3ch','Replace python six module with salt.utils.six\n@thatch45 @rallytime \r\n\r\nRefs: https://github.com/saltstack/salt/issues/18171'
18176,'thatch45','file.directory reports false state execution success\nSalt Minion \\:\r\n\r\n    c:\\salt>salt-minion --versions-report\r\n                Salt: 2014.1.10\r\n                Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n               Jinja2: 2.2.1\r\n               M2Crypto: 0.20.2\r\n               msgpack-python: 0.1.13\r\n               msgpack-pure: Not Installed\r\n               pycrypto: 2.0.1\r\n               PyYAML: 3.10\r\n               PyZMQ: 2.2.0.1\r\n               ZMQ: 3.2.4\r\n\r\n\r\nSalt Master \\:\r\n\r\n    (salt)-bash-4.1$ salt --versions-report\r\n               Salt: 2014.7.0rc2\r\n             Python: 2.7.8 (default, Sep 24 2014, 11:50:00)\r\n             Jinja2: 2.7.3\r\n           M2Crypto: 0.22\r\n     msgpack-python: 0.4.2\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6.1\r\n            libnacl: Not Installed\r\n             PyYAML: 3.11\r\n              ioflo: Not Installed\r\n              PyZMQ: 14.3.1\r\n              RAET: Not Installed\r\n               ZMQ: 4.0.4\r\n               Mako: 1.0.0\r\n\r\nSalt state that I am attempting to run \\:\r\n\r\n    create-extra-folder:\r\n        file.directory:\r\n         - name: /etc/nginx/conf.d/extra/\r\n         - makedirs: True\r\n         - clean: True\r\n         - require_in : copy-upstream-conf\r\n\r\n    copy-upstream-conf:\r\n       file.copy:\r\n        - source: /tmp/upstream.conf\r\n        - name: /etc/nginx/conf.d/extra/\r\n        - force: True\r\n\r\nResult of the "salt-call state.sls nginx.sls " \\:\r\n```\r\n   ----------\r\n      ID: create-extra-folder\r\n      Function: file.directory\r\n      Name: /etc/nginx/conf.d/extra/\r\n      Result: True\r\n      Comment: Directory /etc/nginx/conf.d/extra updated\r\n      Changes:\r\n      ----------\r\n        /etc/nginx/conf.d/extra:\r\n             New Dir\r\n   ----------\r\n      ID: copy-upstream-conf\r\n      Function: file.copy\r\n      Name: /etc/nginx/conf.d/extra/\r\n      Result: False\r\n      Comment: The target directory /etc/nginx/conf.d/extra is not present\r\n      Changes:\r\n```\r\n\r\nThe name attribute in copy-upstream-conf state had to be updated with appending the file name to succeed :\r\n\r\n```\r\n   copy-upstream-conf:\r\n      file.copy:\r\n        - source: /tmp/upstream.conf\r\n        - name: /etc/nginx/conf.d/extra/**upstream.conf**\r\n```'
18168,'thatch45','states/network.py managed() - no way to manage IP aliases in 2014.7\nLets consider this sls:\r\n```\r\neth0:\r\n  network.managed:\r\n    - name: "eth0"\r\n    - enabled: True\r\n    - type: eth\r\n    - proto: none\r\n    - ipaddr: "10.10.223.30"\r\n    - netmask: "255.255.255.0"\r\n    - dns:\r\n      - 8.8.8.8\r\n      - 8.8.4.4\r\n\r\neth0_1:\r\n  network.managed:\r\n    - name: "eth0:1"\r\n    - enabled: True\r\n    - type: eth\r\n    - ipaddr: "10.10.223.29"\r\n    - netmask: "255.255.255.0"\r\n\r\neth0_2:\r\n  network.managed:\r\n    - name: "eth0:2"\r\n    - enabled: True\r\n    - type: eth\r\n    - ipaddr: "10.10.223.38"\r\n    - netmask: "255.255.255.0"\r\n```\r\nIn 2014.1 you would have eth0 with 3 aliases on it because in modules/rh_ip.py up() it runs "ifup eth0:1" which legit on RHEL systems\r\n\r\nIn 2014.7 you cant do that anymore, because salt.utils.network.interfaces() returns new format of array:\r\n```\r\n{\'lo\': {\'hwaddr\': \'00:00:00:00:00:00\', \'up\': True, \'inet\': [{\'broadcast\': None, \'netmask\': \'255.0.0.0\', \'label\': \'lo\', \'address\': \'127.0.0.1\'}], \'inet6\': [{\'prefixlen\': \'128\', \'address\': \'::1\'}]}, \'eth0\': {\'hwaddr\': \'d6:be:8f:2c:bc:57\', \'inet6\': [{\'prefixlen\': \'64\', \'address\': \'fe80::d4be:8fff:fe2c:bc57\'}], \'up\': True, \'inet\': [{\'broadcast\': \'10.10.223.255\', \'netmask\': \'255.255.255.0\', \'label\': \'eth0\', \'address\': \'10.10.223.30\'}], \'secondary\': [{\'broadcast\': \'10.10.223.255\', \'netmask\': \'255.255.255.0\', \'label\': \'eth0:1\', \'type\': \'inet\', \'address\': \'10.10.223.29\'}, {\'broadcast\': \'10.10.223.255\', \'netmask\': \'255.255.255.0\', \'label\': \'eth0:2\', \'type\': \'inet\', \'address\': \'10.10.223.38\'}]}}\r\n```\r\n\r\nWhich in short is `{\'lo\':{}, \'eth0\':{\'secondary\': [{\'label\': \'eth0:1\'}, {\'label\': \'eth0:2\'}]}`, so when you have \r\nstates/network.py, line 282: `interface_status = salt.utils.network.interfaces()[name].get(\'up\')` you will get exception trying to get name="eth0:1" from the dict.\r\n\r\nObviously the solution is either:\r\n- revert back to old dist format and consider "eth0:1" and etc as separate interfaces, as it used to be\r\n- add to type "alias" and treat the interface the other way, like with type="bond"\r\n\r\n\r\n'
18145,'thatch45','Quotes for at mod\n'
18137,'thatch45','hg: allow pull from non default repo\n'
18113,'whiteinge','Docs Building Errors\nI went to manually build the docs today on the docs VM and one of the states was failing because `six` wasn\'t included. I added it to the list of mocked modules in #18099. After that was merged, I tried again, and now I am getting a new error:\r\n```\r\n----------\r\n          ID: makedocs\r\n    Function: cmd.wait\r\n        Name: /usr/bin/sphinx-build -qE /root/salt/doc /var/www/saltdocs/salt-develop\r\n      Result: False\r\n     Comment: Command "/usr/bin/sphinx-build -qE /root/salt/doc /var/www/saltdocs/salt-develop" run\r\n     Changes:\r\n              ----------\r\n              pid:\r\n                  23574\r\n              retcode:\r\n                  1\r\n              stderr:\r\n\r\n                  Exception occurred:\r\n                    File "/usr/lib64/python2.6/logging/__init__.py", line 900, in setLoggerClass\r\n                      if not issubclass(klass, Logger):\r\n                  TypeError: issubclass() arg 1 must be a class\r\n                  The full traceback has been saved in /tmp/sphinx-err-Ei6UEg.log, if you want to report the issue to the developers.\r\n                  Please also report this if it was a user error, so that a better error message can be provided next time.\r\n                  Either send bugs to the mailing list at <http://groups.google.com/group/sphinx-users/>,\r\n                  or report them in the tracker at <http://bitbucket.org/birkenfeld/sphinx/issues/>. Thanks!\r\n              stdout:\r\n\r\n----------\r\n```\r\nping @whiteinge @s0undt3ch @thatch45 '
18100,'ssgward',"`modules.network.ip_addrs` ignores `include_loopback=True` on Windows\nRunning `salt -C 'G@os:Windows' network.ip_addrs include_loopback=True` doesn't return any loopback interface results. It works as expected for any Linux minions.\r\n\r\n**Minion (Windows 7 Enterprise en_US):**\r\n```\r\n              Salt: 2014.1.10\r\n             Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n             Jinja2: 2.7.1\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.4.2\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.11\r\n              PyZMQ: 14.1.1\r\n                ZMQ: 4.0.4\r\n```\r\n\r\n**Master (Ubuntu 14.04 amd64):**\r\n```\r\n               Salt: 2014.1.13\r\n             Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n             Jinja2: 2.7.2\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.3.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 14.0.1\r\n                ZMQ: 4.0.4\r\n```"
18070,'thatch45','Initial sketch of django returner\nRefs #17160'
18057,'thatch45','Make mine module event-based\nCloses #16864'
18010,'thatch45','Making salt/daemons/flo/core python3 compatible\n'
17945,'ssgward','2014.7.0 fileserver.update returns error\n```\r\n# salt-run fileserver.update\r\n[ERROR   ] Git fileserver backend is enabled in master config file, but could not be loaded, are pygit2 and libgit2 installed?\r\n```\r\nBut GitPython is installed and remote git repositories for salt and pillar work correctly.\r\n\r\n```\r\n# cat /etc/redhat-release \r\nCentOS release 6.6 (Final)\r\n# salt --versions-report\r\n           Salt: 2014.7.0\r\n         Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.3.1\r\n           RAET: Not Installed\r\n            ZMQ: 3.2.4\r\n           Mako: Not Installed\r\n```'
17706,'rallytime','Fixed namespace issue in cloud/clouds/cloudstack.py which was breaking i...\nFixed namespace issue in cloud/clouds/cloudstack.py which was breaking its destroy method.\r\n\r\nThis is the traceback, before the patch:\r\n```\r\n[ERROR   ] There was an error destroying machines: global name \'__active_provider_name__\' is not defined\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/cloud/cli.py", line 166, in run\r\n    ret = mapper.destroy(names, cached=True)\r\n  File "/usr/lib/python2.6/site-packages/salt/cloud/__init__.py", line 957, in destroy\r\n    ret = self.clouds[fun](name)\r\n  File "/usr/lib64/python2.6/contextlib.py", line 34, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File "/usr/lib/python2.6/site-packages/salt/utils/context.py", line 41, in func_globals_inject\r\n    yield\r\n  File "/usr/lib/python2.6/site-packages/salt/cloud/__init__.py", line 957, in destroy\r\n    ret = self.clouds[fun](name)\r\n  File "/usr/lib/python2.6/site-packages/salt/cloud/clouds/cloudstack.py", line 455, in destroy\r\n    node = get_node(conn, name)\r\n  File "/usr/lib/python2.6/site-packages/salt/cloud/libcloudfuncs.py", line 105, in get_node\r\n    salt.utils.cloud.cache_node(salt.utils.cloud.simple_types_filter(node.__dict__), __active_provider_name__, __opts__)\r\nNameError: global name \'__active_provider_name__\' is not defined\r\n```'
17687,'basepi',"No syntax checking of id in /etc/salt/minion_id \n1. Issue: can't delete psp3 key\r\n```\r\n[root@psp2 ~]# salt-key -L\r\nAccepted Keys:\r\npsp2\r\npsp3\r\npsp3\r\nUnaccepted Keys:\r\nRejected Keys:\r\n[root@psp2 ~]# salt-key -y -d psp3\r\nThe key glob 'psp3' does not match any accepted, unaccepted or rejected keys.\r\n[root@psp2 ~]# date\r\nTue Nov 11 06:22:48 PST 2014\r\n[root@psp2 ~]# \r\n```\r\n2. root cause: minion_id was set twice in the /etc/salt/minion_id file\r\n```\r\n[root@psp3 ~]# cat /etc/salt/minion_id \r\npsp3\r\npsp3\r\n[root@psp3 ~]# date\r\nTue Nov 11 06:23:43 PST 2014\r\n[root@psp3 ~]#\r\n``` \r\n3. Solution: please tighten up minion_id syntax check.\r\n4. tested using 2014.7.0 rpm package for both minion and master.\r\n```\r\n[root@psp3 ~]# salt-minion --version\r\nsalt-minion 2014.7.0 (Helium)\r\n[root@psp3 ~]# \r\n[root@psp3 ~]# rpm -q salt\r\nsalt-2014.7.0-3.el6.noarch\r\n[root@psp3 ~]# \r\n```"
17289,'thatch45','Minion and salt commands generating two minion keys under race conditions\n#### Summary: ####\r\nMinion will generate two minion keys under certain race conditions, which results in salt-minion with a key that will never work until you delete and re-add the key on salt-master.\r\n\r\n#### Description: ####\r\nIf you start salt-minion and run a salt call, for example salt-call state.highstate; or from master, salt \'dev.test\' state.highstate, the minion will generate two minion keys. Both the salt-minion and the salt call will generate minion keys instead of just one of them. The salt function will usually over write the minion daemon\'s key, but the minion already sent a message to salt-master before the overwrite. Because of this, you need to delete the key on salt-master and add the new key.\r\nUnder certain conditions, I have added the first key with salt-key and then ran "salt \'dev.test\' state.highstate" and it also generated another minion key. This happens if you add the key and then run highstate really quickly.\r\n\r\n#### Steps to Reproduce: Run the following script on a vagrant box. ####\r\n**salt_crit.sh**\r\n#!/bin/bash\r\napt-get update\r\napt-get install -y salt-master salt-minion\r\necho "master: 10.0.2.15" > /etc/salt/minion.d/master.conf\r\n/etc/init.d/salt-minion stop\r\nsalt-minion start -l trace &>/tmp/salt-minion.log &\r\nsalt-call state.highstate -l trace &> /tmp/highstate.log &\r\n\r\n#### LOGS ####\r\n\r\n\r\n**/etc/salt/pki/minion/minion.pub**\r\n-----BEGIN PUBLIC KEY-----\r\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA9ycQzHR5N5SX2EAmrDL5\r\n......\r\n.....\r\n-----END PUBLIC KEY-----\r\n\r\n**"Salt-Master log:"**\r\n[INFO    ] Clear payload received with command _auth\r\n[INFO    ] Authentication request from dev.test\r\n[INFO    ] New public key for dev.test placed in pending\r\n[DEBUG   ] Sending event - data = {\'id\': \'dev.test\', \'_stamp\': \'2014-11-08T00:51:13.949049\', \'result\': True, \'pub\': \'-----BEGIN PUBLIC KEY-----\\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEA9ycQzHR5N5SX2EAmrDL5\r\n....\r\n\r\n**"Salt-Master log:"**\r\n[INFO    ] Clear payload received with command _auth\r\n[INFO    ] Authentication request from dev.test\r\n[ERROR   ] Authentication attempt from dev.test failed, the public key in pending did not match. This may be an attempt to compromise the Salt cluster.\r\n[DEBUG   ] Sending event - data = {\'_stamp\': \'2014-11-08T00:51:14.902917\', \'result\': False, \'pub\': \'-----BEGIN PUBLIC KEY-----\\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAw8zaNjrndyzgHjIRY7se\r\n....\r\n\r\n**/etc/salt/pki/minion/minion.pub**\r\n-----BEGIN PUBLIC KEY-----\r\nMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAw8zaNjrndyzgHjIRY7se\r\n....\r\n\r\n\r\n**"Salt-Minion trace log:"**\r\n[DEBUG   ] Reading configuration from /etc/salt/minion\r\n[DEBUG   ] Including configuration from \'/etc/salt/minion.d/master.conf\'\r\n[DEBUG   ] Reading configuration from /etc/salt/minion.d/master.conf\r\n[DEBUG   ] Using cached minion ID from /etc/salt/minion_id: dev.test\r\n.....\r\n[TRACE   ] Device ram14 does not report itself as an SSD\r\n[TRACE   ] Device ram15 does not report itself as an SSD\r\n[DEBUG   ] Attempting to authenticate with the Salt Master at 10.0.2.15\r\n[INFO    ] Generating keys: /etc/salt/pki/minion\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\r\n[INFO    ] Waiting for minion key to be accepted by the master.\r\n[INFO    ] Waiting 10 seconds before retry.\r\n[CRITICAL] The Salt Master has rejected this minion\'s public key!\r\nTo repair this issue, delete the public key for this minion on the Salt Master and restart this minion.\r\nOr restart the Salt Master in open mode to clean out the keys. The Salt Minion will now exit.\r\n\r\n**"Salt Highstate Trace Log:"**\r\n[DEBUG   ] Reading configuration from /etc/salt/minion\r\n[DEBUG   ] Including configuration from \'/etc/salt/minion.d/master.conf\'\r\n[DEBUG   ] Reading configuration from /etc/salt/minion.d/master.conf\r\n[DEBUG   ] Using cached minion ID from /etc/salt/minion_id: dev.test\r\n......\r\n[TRACE   ] Device ram15 does not report itself as an SSD\r\n[INFO    ] Generating keys: /etc/salt/pki/minion\r\n[CRITICAL] The Salt Master has rejected this minion\'s public key!\r\nTo repair this issue, delete the public key for this minion on the Salt Master and restart this minion.\r\nOr restart the Salt Master in open mode to clean out the keys. The Salt Minion will now exit.'
17282,'thatch45','BTRFS module\nHi!\r\nSince BTRFS is a flagship filesystem in **SUSE Linux Enterprise 12** and is offered by default (along with XFS for ``/home``, module of which already merged), I would like to introduce BTRFS management module.\r\n\r\n*NOTE: The module has some obvious duplication with XFS module, but this is intentionally: once current version of BTRFS module is merged, they will be brought to the common procedures, since they differ and need an additional polishing.*\r\n\r\nThis module allows you to make filesystems, manage RAID arrays, scan, resize, get all required information and even reliably migrate from Ext2/Ext3/Ext4. Some additional features are still coming, but it already does initial not so very bad job and therefore I would like to just "release it early". :-)\r\n\r\nSome examples:\r\n```\r\ne212:~ # salt \'*\' btrfs.info /foo\r\nd122.suse.de:\r\n    ----------\r\n    /dev/sdb1:\r\n        ----------\r\n        device_id:\r\n            1\r\n        size:\r\n            5.00GiB\r\n        used:\r\n            0.00B\r\n    /dev/sdb2:\r\n        ----------\r\n        device_id:\r\n            2\r\n        size:\r\n            4.99GiB\r\n        used:\r\n            1.28GiB\r\n    label:\r\n        None\r\n    uuid:\r\n        2bf3713d-603e-4fc3-a671-ce6b55517642\r\ne212:~ # salt \'*\' btrfs.delete /foo /dev/sdb2 force=True\r\nd122.suse.de:\r\n    ----------\r\n    /dev/sdb1:\r\n        ----------\r\n        device_id:\r\n            1\r\n        size:\r\n            5.00GiB\r\n        used:\r\n            800.00MiB\r\n    label:\r\n        None\r\n    uuid:\r\n        2bf3713d-603e-4fc3-a671-ce6b55517642\r\ne212:~ # salt \'*\' btrfs.mkfs /dev/sdb2 label=\'My Great Storage\'\r\nd122.suse.de:\r\n    ----------\r\n    /dev/sdb2:\r\n        ----------\r\n        device_id:\r\n            1\r\n        size:\r\n            4.99GiB\r\n        used:\r\n            546.62MiB\r\n    label:\r\n        \'My Great Storage\'\r\n    log:\r\n        Btrfs v3.16+20140829\r\n        See http://btrfs.wiki.kernel.org for more information.\r\n\r\n        Turning ON incompat feature \'extref\': increased hardlink limit per file to 65536\r\n        Turning ON incompat feature \'skinny-metadata\': reduced-size metadata extent refs\r\n        fs created label bubamara on /dev/sdb2\r\n           nodesize 16384 leafsize 16384 sectorsize 4096 size 4.99GiB\r\n    uuid:\r\n        067275df-7265-46a6-aed1-2aee720d979f\r\n\r\ne212:~ # salt \'*\' btrfs.add /foo /dev/sdb2 force=True\r\nd122.suse.de:\r\n    ----------\r\n    /dev/sdb1:\r\n        ----------\r\n        device_id:\r\n            1\r\n        size:\r\n            5.00GiB\r\n        used:\r\n            0.00B\r\n    /dev/sdb2:\r\n        ----------\r\n        device_id:\r\n            2\r\n        size:\r\n            4.99GiB\r\n        used:\r\n            1.28GiB\r\n    label:\r\n        None\r\n    log:\r\n        Done, had to relocate 3 out of 3 chunks\r\n    uuid:\r\n        2bf3713d-603e-4fc3-a671-ce6b55517642\r\n\r\ne212:~ # salt \'*\' btrfs.properties /foo set="label=\'My great storage\'"\r\nd122.suse.de:\r\n    None\r\ne212:~ # salt \'*\' btrfs.properties /foo\r\nd122.suse.de:\r\n    ----------\r\n    compression:\r\n        ----------\r\n        description:\r\n            Set/get compression for a file or directory\r\n        value:\r\n            N/A\r\n    label:\r\n        ----------\r\n        description:\r\n            Set/get label of device.\r\n        value:\r\n            My great storage\r\n    ro:\r\n        ----------\r\n        description:\r\n            Set/get read-only flag of subvolume.\r\n        value:\r\n            false\r\n```'
17273,'rallytime',"salt commands stuck with raet, ok with zmq\nHi,\r\n\r\nAlready using salt in production, and to have a try at 2014.7, I installed it in an Ubuntu Server 14.04.1 VM (using install_salt.sh). Master and minion are on the same VM.\r\n\r\nConfig files are default, and everything is fine.\r\n\r\nIf I try with transport: raet in master and minion config, commands timeout or give an error (minion did not reply). I can't see anything in the logs with max debug level. raet has been installed with pip and libsodium has been compiled from the latest tarball.\r\n\r\nHow could I proceed to debug?\r\n\r\nCould anyone try the same in a similar setup (with VMs it is quite easy) and confirm?\r\n\r\nThanks in advance!\r\n\r\nDamien"
17263,'s0undt3ch','Fix VT execution environment\nThis fixes #17262 \r\n\r\ncritical fix /cc @thatch45 @basepi'
17262,'s0undt3ch',"VT is badly damaged since a while\nI just synced with develop and we discovered that env dealing in vt.py is broken:\r\n\r\n\r\nOLD env before sync:\r\n```\r\n\r\n                                        testenv: Running 'env'\r\n                                        MAIL=/var/mail/ald-user\r\n                                        USER=ald-user\r\n                                        LC_TIME=fr_FR.UTF-8\r\n                                        SHLVL=1\r\n                                        BUILDOUT_ORIGINAL_PYTHONPATH=\r\n                                        HOME=/home/users/ald-user\r\n                                        LC_CTYPE=fr_FR.UTF-8\r\n                                        LC_MONETARY=fr_FR.UTF-8\r\n                                        LOGNAME=ald-user\r\n                                        _=/salt-venv/bin/python\r\n                                        TERM=screen\r\n                                        LC_COLLATE=fr_FR.UTF-8\r\n                                        PATH=/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games\r\n                                        LC_ADDRESS=fr_FR.UTF-8\r\n                                        LANG=fr_FR.UTF-8\r\n                                        LC_TELEPHONE=fr_FR.UTF-8\r\n                                        LC_MESSAGES=fr_FR.UTF-8\r\n                                        SHELL=/bin/bash\r\n                                        LC_NAME=fr_FR.UTF-8\r\n                                        LC_MEASUREMENT=fr_FR.UTF-8\r\n                                        LC_IDENTIFICATION=fr_FR.UTF-8\r\n                                        ED=/usr/bin/vim\r\n                                        LC_ALL=C\r\n                                        PWD=/srv/projects/ald/data/zeocluster\r\n                                        LC_NUMERIC=fr_FR.UTF-8\r\n                                        PYTHONPATH=/srv/projects/ald/data/zeocluster/parts/buildout\r\n                                        LC_PAPER=fr_FR.UTF-8\r\n                                        EDITOR=/usr/bin/vim\r\n                                        Unused options for testenv: 'update-command'.\r\n\r\n```\r\nNOw:\r\n```\r\n                          Updating testenv.\r\n                          testenv: Running env\r\n                          BUILDOUT_ORIGINAL_PYTHONPATH=\r\n                          LC_ALL=C\r\n                          PWD=/srv/projects/ald/data/zeocluster\r\n                          PYTHONPATH=/srv/projects/ald/data/zeocluster/parts/buildout\r\n```\r\n"
17256,'jfindlay','Interface ordering in kvm.\nI try to provision a kvm-guest with 3 interfaces. virt.nic is correctly reflected in hypervisors pillar.items:\r\n\r\n```\r\n       triple:\r\n            ----------\r\n            eth0:\r\n                ----------\r\n                bridge:\r\n                    xbr0\r\n            eth1:\r\n                ----------\r\n                bridge:\r\n                    virbr1\r\n            eth2:\r\n                ----------\r\n                bridge:\r\n                    virbr2\r\n```\r\nHowever the guest will reverse or randomize the bridge to interface order, assigning eth0 to virbr2. \r\nHow can I force a match between pillar data and guest interfaces and make sure the external bridge xbr0 always ends up as eth0 in the guest?\r\n'
17243,'rallytime',"Enable pillar/compound matching in mine/publish, with no pillar globbing support\nSTATUS: Merge away! (Assuming tests pass, of course)\r\n\r\nI have manually tested both mine and publish for compound/pillar matching and it's working now, and successfully preventing pillar globbing.\r\n\r\nThis should be reviewed carefully. With this patch, pillar and compound matching will work for both mine and publish functions. However, pillar matches will use exact matching, globbing is disabled. This is primarily accomplished with the changes to `subdict_match` in `salt.utils.minions.CkMinions`. I also added two new match types, `pillar_exact` and `compound_pillar_exact`, and ensure that those matches are used instead of `pillar` and `compound` respectively for all publish and mine queries."
17228,'pitatus','Module run error with data.update error in 2014.1.13\nThis (below) ran fine in 2014.1.11 with no errors:\r\n\r\n    setjre7homeminiondata:\r\n      module.run:\r\n        - name: data.update\r\n        - key: md_jre7home\r\n        - value: {{ _jrepath }}\r\n\r\nIn 2014.1.13, this produces the following error, and a False result:\r\n\r\n    ----------\r\n              ID: setjre7homeminiondata\r\n        Function: module.run\r\n            Name: data.update\r\n            Result: False\r\n         Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib/python2.6/site-packages/salt/state.py", line 1383, in call\r\n                  self.verify_ret(ret)\r\n                File "/usr/lib64/python2.6/contextlib.py", line 34, in __exit__\r\n                  self.gen.throw(type, value, traceback)\r\n                File "/usr/lib/python2.6/site-packages/salt/utils/context.py", line 43, in func_globals_inject\r\n                  yield\r\n                File "/usr/lib/python2.6/site-packages/salt/state.py", line 1382, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib/python2.6/site-packages/salt/states/module.py", line 203, in run\r\n                  if ret[\'changes\'].get(\'ret\', {}).get(\'retcode\', 0) != 0:\r\n              AttributeError: \'bool\' object has no attribute \'get\'\r\n     Changes:   \r\n    ----------\r\n\r\nSince the formulas I have that use this are not set to fail fast, they work (for now), but if my sponsor ends up requesting a true "clean" (i.e., fail fast enabled) run, then this will fail other formulas/states.'
17124,'cro',"cache/publish_auth directory filling up\nThis Github issue is synchronized with Zendesk,\n\n**Zendesk ticket ID:** [109](https://saltstack.zendesk.com/agent/#/tickets/109)\n**Priority:** high\n**Zendesk assignee:** N/A\n **Component:** salt-master\n\n\n**Original ticket content:**\n\nHello,\r\n\r\nOn our production saltmaster we've notice that there are now 500684 files in our cache/publish_auth directory. As far as we can make out, salt attempts to stat every one of those files when we make a simple salt grain lookup on some of our nodes (we ran it through strace), such as:\r\n\r\nsalt -G 'role:aempublisher' grains.item role aem_group_id aemready\r\n\r\nIs this another known issue? There was something very similar with files in the cache/jobs directory filling up, which again I believe has been fixed in 2014.7 (but we've not upgraded to that release yet as it's not fully supported).\r\n\r\nFor now, is it safe to delete those files (or, say, all files older than 24 hours)?\r\n\r\nRegards,\r\nJames\n"
17120,'thatch45','Move the scheduler to its own process\nFixes #17000'
16874,'terminalmage','Salt-SSH: pkg module on RH based systems dependent on yum-utils. Need to remove that dependency\nUsing 2014.7.0rc6\r\n\r\nTested on CentOS 6.5 (ssh-minion on Rackspace)\r\n\r\nProbably not describing/wording this entirely accurately. Contact me for other details if needed.\r\n\r\nIn talking with Tom, we can add this to the Known Issues list of 2014.7.0 and fix in 2014.7.1.\r\n\r\nRan the following command from master: salt-ssh gwtest state.sls apache\r\n\r\nHere is the apache.sls:\r\n```\r\napache:\r\n  pkg.installed:\r\n    {% if grains[\'os_family\'] == \'RedHat\' %}\r\n    - name: httpd\r\n    {% elif grains[\'os_family\'] == \'Debian\' %}\r\n    - name: apache2\r\n    {% endif %}\r\n```\r\nGet the following results on master:\r\n```\r\ngwtest:\r\n----------\r\n          ID: apache\r\n    Function: pkg.installed\r\n        Name: httpd\r\n      Result: False\r\n     Comment: The following package(s) were not found, and no possible matches were found in the package db: httpd\r\n     Started: 23:49:35.606090\r\n    Duration: 51.497 ms\r\n     Changes:\r\n\r\nSummary\r\n------------\r\nSucceeded: 0\r\nFailed:    1\r\n------------\r\nTotal states run:     1\r\n```\r\nWhen troubleshooting on ssh-minion, ran this command: `python salt-call --local -l debug -c /tmp/.root_1ec649__salt/ pkg.list_pkgs`\r\n\r\nGet this error:\r\n```\r\n[ERROR   ] Command \'repoquery --plugins --queryformat="%{NAME}_|-%{VERSION}_|-%{RELEASE}_|-%{ARCH}_|-%{REPOID}" --all --pkgnarrow=installed\' failed with return code: 127\r\n[ERROR   ] stderr: /bin/bash: repoquery: command not found\r\n```\r\nInstalled yum-utils on ssh-minion\r\n\r\nRe-ran with success the same command: `python salt-call --local -l debug -c /tmp/.root_1ec649__salt/ pkg.list_pkgs`\r\n\r\nRe-ran state file from master against same minion with a success.\r\n\r\nLooks like we have a dependency on `yum-utils` for this module. Should probably remove this dep.\r\n\r\n'
16864,'cachedout','Multi Master Mine Sync\nMake the minions report the mine data to each mine\r\nthis is probably a dup, please replace with an older one'
16831,'UtahDave',"Unable to backup files using salt\nHello,\r\nI am using the salt backup functionality under file.managed and getting the following error:\r\nUnable to manage file: [Error 123] The filename, directory name, or volume label syntax is incorrect: 'C:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\file_backup\\\\:'\r\n\r\nMy sls file is as follows:\r\n```\r\ntestfile.txt:\r\n  file:\r\n    - name: D:\\Program Files\\Test\\testfile.txt\r\n    - managed\r\n    - source: salt://test-files/test/testfile1.txt\r\n    - backup: minion\r\n```"
16826,'basepi','The explanation for prereq seem to be wrong.\nOn http://docs.saltstack.com/en/latest/ref/states/requisites.html#prereq, I read a paragraph\r\n`If the "changes" key contains a populated dictionary, it means that the pre-required state expects changes to occur when the state is actually executed, as opposed to the test-run. The pre-required state will now actually run. If the pre-required state executes successfully, the pre-requiring state will then execute. If the pre-required state fails, the pre-requiring state will not execute.`\r\n\r\nI think `pre-required` and `pre-requiring` are exchanged in some places of the paragraph.\r\n\r\nAm I right?'
16583,'terminalmage','Salt Runner cast string to int\nI get a curious bug when i\'m using number with underscore. Salt remove _ and make an int. \r\nIn this example, v1 must be 1_0_0 and not 100. \r\n\r\nExample:\r\n\r\n    $ salt --version\r\n    salt 2014.1.11 (Hydrogen)\r\n\r\n    $ cat myrunner.py\r\n    import salt.output\r\n    def test(v1,v2):\r\n        data = {"v1":v1, "v2":v2, "realv2":v2.replace(\'.\', \'_\'), "typev1":type(v1), "typev2":type(v2)}\r\n        salt.output.display_output(data, \'yaml\', __opts__)\r\n        return data\r\n\r\n    $ salt-run myrunner.test v1="1_0_0" v2="1.0.0"\r\n        realv2: \'1_0_0\'\r\n        v1: 100\r\n        v2: 1.0.0\r\n\r\n    $ python\r\n    >>>> import salt.runner\r\n    >>>> opts = salt.config.master_config(\'/etc/salt/master\')\r\n    >>>> runner = salt.runner.RunnerClient(opts)\r\n    >>>> arg = []\r\n    >>>> arg.append("v1=1_0_0")\r\n    >>>> arg.append("v2=1.0.0")\r\n    >>>> runner.cmd(fun=\'myrunner.test\', arg=arg)\r\n    realv2: \'1_0_0\'\r\n    v1: 100\r\n    v2: 1.0.0\r\n    {\'v1\': 100, \'v2\': \'1.0.0\', \'realv2\': \'1_0_0\'}\r\n'
16567,'s0undt3ch','salt.utils.thin does now know how to handle python egg\'s\n```\r\n(Py27)lepto:salt s0undt3ch$ ~/.virtualenvs/Py27/bin/salt-ssh -c ~/.salt-conf/ -i \\* test.ping\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n    self.run()\r\n  File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/Users/s0undt3ch/salt/salt/client/ssh/__init__.py", line 325, in handle_routine\r\n    stdout, stderr, retcode = single.run()\r\n  File "/Users/s0undt3ch/salt/salt/client/ssh/__init__.py", line 613, in run\r\n    stdout, stderr, retcode = self.cmd_block()\r\n  File "/Users/s0undt3ch/salt/salt/client/ssh/__init__.py", line 783, in cmd_block\r\n    cmd_str = self._cmd_str()\r\n  File "/Users/s0undt3ch/salt/salt/client/ssh/__init__.py", line 725, in _cmd_str\r\n    thin_sum = salt.utils.thin.thin_sum(self.opts[\'cachedir\'], \'sha1\')\r\n  File "/Users/s0undt3ch/salt/salt/utils/thin.py", line 166, in thin_sum\r\n    thintar = gen_thin(cachedir)\r\n  File "/Users/s0undt3ch/salt/salt/utils/thin.py", line 142, in gen_thin\r\n    os.chdir(os.path.dirname(top))\r\nOSError: [Errno 20] Not a directory: \'/Users/s0undt3ch/.virtualenvs/Py27/lib/python2.7/site-packages/PyYAML-3.11-py2.7-macosx-10.7-x86_64.egg\'\r\n```\r\n\r\n/CC @thatch45 '
16370,'rallytime',"onchanges and onfail requisites fail with 'One or more requisite failed' instead of no action\n```yaml\r\nthis_will_fail:\r\n  test.fail_without_changes\r\n\r\ntake_some_recovery_action:\r\n  test.succeed_without_changes:\r\n    - onfail:\r\n      - test: this_will_fail\r\n\r\nthis_wont_fail:\r\n  test.succeed_without_changes\r\n\r\ndont_take_some_recovery_action:\r\n  test.succeed_without_changes:\r\n    - onfail:\r\n      - test: this_wont_fail\r\n\r\n# ----------------\r\n\r\nthis_will_have_changes:\r\n  test.succeed_with_changes\r\n\r\ndo_something_if_has_changes:\r\n  test.succeed_without_changes:\r\n    - onchanges:\r\n      - test: this_will_have_changes\r\n\r\nthis_wont_have_changes:\r\n  test.succeed_without_changes\r\n\r\nthis_shouldnt_run:\r\n  test.succeed_without_changes:\r\n    - onchanges:\r\n      - test: this_wont_have_changes\r\n```\r\n\r\nResults in:\r\n\r\n```\r\n          ID: this_will_fail\r\n    Function: test.fail_without_changes\r\n      Result: False\r\n     Comment: Failure!\r\n     Started: 15:45:13.619242\r\n    Duration: 0.893 ms\r\n     Changes:\r\n----------\r\n          ID: take_some_recovery_action\r\n    Function: test.succeed_without_changes\r\n      Result: True\r\n     Comment: Success!\r\n     Started: 15:45:13.620738\r\n    Duration: 0.852 ms\r\n     Changes:\r\n----------\r\n          ID: this_wont_fail\r\n    Function: test.succeed_without_changes\r\n      Result: True\r\n     Comment: Success!\r\n     Started: 15:45:13.621859\r\n    Duration: 0.861 ms\r\n     Changes:\r\n----------\r\n          ID: dont_take_some_recovery_action\r\n    Function: test.succeed_without_changes\r\n      Result: False\r\n     Comment: One or more requisite failed\r\n     Started:\r\n    Duration:\r\n     Changes:\r\n----------\r\n          ID: this_will_have_changes\r\n    Function: test.succeed_with_changes\r\n      Result: True\r\n     Comment: Success!\r\n     Started: 15:45:13.623244\r\n    Duration: 0.817 ms\r\n     Changes:\r\n              ----------\r\n              testing:\r\n                  ----------\r\n                  new:\r\n                      Something pretended to change\r\n                  old:\r\n                      Unchanged\r\n----------\r\n          ID: do_something_if_has_changes\r\n    Function: test.succeed_without_changes\r\n      Result: True\r\n     Comment: Success!\r\n     Started: 15:45:13.624661\r\n    Duration: 0.846 ms\r\n     Changes:\r\n----------\r\n          ID: this_wont_have_changes\r\n    Function: test.succeed_without_changes\r\n      Result: True\r\n     Comment: Success!\r\n     Started: 15:45:13.625775\r\n    Duration: 0.921 ms\r\n     Changes:\r\n----------\r\n          ID: this_shouldnt_run\r\n    Function: test.succeed_without_changes\r\n      Result: False\r\n     Comment: One or more requisite failed\r\n     Started:\r\n    Duration:\r\n     Changes:\r\n```\r\n\r\nThe ``dont_take_some_recovery_action`` and ``this_shouldnt_run`` states should not execute."
16275,'cro','salt-minion using raet transport will not start second time\nWhen configured for raet, a salt-minion will not start after its keys have been accepted by the master.  Stacktrace below.  To reproduce, with -master and -minion not running on a machine:\r\n\r\n```\r\nrm -rf /var/cache/salt\r\nrm -rf /var/run/salt\r\nrm -rf /etc/salt/pki\r\nsalt-master &\r\nsalt-minion &\r\nsalt-key -Ay\r\nsalt \\* test.ping\r\n```\r\nthis should return True for the minion.  Stop the salt-minion.  The next time you start the salt-minion you will get:\r\n```\r\nTraceback (most recent call last):\r\n  File "/vagrant/salt/scripts/salt-minion", line 14, in <module>\r\n    salt_minion()\r\n  File "/vagrant/salt/salt/scripts.py", line 56, in salt_minion\r\n    minion.start()\r\n  File "/vagrant/salt/salt/__init__.py", line 264, in start\r\n    self.minion.tune_in()\r\n  File "/vagrant/salt/salt/daemons/flo/__init__.py", line 136, in tune_in\r\n    consolepath=consolepath,\r\n  File "/root/src/ioflo/ioflo/app/run.py", line 120, in run\r\n    skedder.run()\r\n  File "/root/src/ioflo/ioflo/base/skedding.py", line 264, in run\r\n    status = tasker.runner.send(tasker.desire)\r\n  File "/root/src/ioflo/ioflo/base/framing.py", line 477, in makeRunner\r\n    self.recur() #.desire may change here\r\n  File "/root/src/ioflo/ioflo/base/framing.py", line 339, in recur\r\n    frame.recur()\r\n  File "/root/src/ioflo/ioflo/base/framing.py", line 1117, in recur\r\n    act()\r\n  File "/root/src/ioflo/ioflo/base/acting.py", line 62, in __call__\r\n    return (self.actor(**self.parms))\r\n  File "/root/src/ioflo/ioflo/base/acting.py", line 258, in __call__\r\n    return self.action(**kwa)\r\n  File "/vagrant/salt/salt/daemons/flo/core.py", line 674, in action\r\n    self.udp_stack.value.serviceAllRx()\r\n  File "/root/src/raet/raet/stacking.py", line 441, in serviceAllRx\r\n    self.serviceRxes()\r\n  File "/root/src/raet/raet/stacking.py", line 310, in serviceRxes\r\n    self._handleOneRx()\r\n  File "/root/src/raet/raet/road/stacking.py", line 437, in _handleOneRx\r\n    self.processRx(packet)\r\n  File "/root/src/raet/raet/road/stacking.py", line 561, in processRx\r\n    trans.receive(packet)\r\n  File "/root/src/raet/raet/road/transacting.py", line 1466, in receive\r\n    self.cookie()\r\n  File "/root/src/raet/raet/road/transacting.py", line 1631, in cookie\r\n    self.initiate()\r\n  File "/root/src/raet/raet/road/transacting.py", line 1642, in initiate\r\n    fqdn = self.remote.fqdn.ljust(128, \' \')\r\nstruct.error: argument for \'s\' must be a string\r\n```\r\n'
16165,'thatch45',"Salt-ssh isn't deploying custom grains.\nI have the following custom grain:\r\n\r\ncat /srv/salt/_grains/dave.py\r\n```\r\ndef zzz():\r\n    me = {}\r\n    me['zzzzz'] = 'Dave grain'\r\n    return me\r\n```\r\n\r\nIt is not getting deployed to salt-ssh minions.\r\n\r\nI did see that custom modules and states are being synced to:\r\n```\r\n/tmp/.root_salt/running_data/var/cache/salt/minion/files/base/_modules\r\n/tmp/.root_salt/running_data/var/cache/salt/minion/files/base/_states\r\n```\r\nBut there is no:\r\n```\r\n/tmp/.root_salt/running_data/var/cache/salt/minion/files/base/_grains\r\n```"
16147,'thatch45',"Salt-ssh password based authentication fails\n```\r\nweb1.eng.domain.com:\r\n  host: web1.eng.domain.com\r\n  user: root\r\n  passwd: password\r\n```\r\nDebug output demonstrates:\r\n```\r\n[DEBUG   ] Executing command: ssh web1.eng.domain.com  -o KbdInteractiveAuthentication=no -o PasswordAuthentication=no -o GSSAPIAuthentication=no -o ConnectTimeout=65 -o Port=22 -o IdentityFile=/etc/salt/pki/master/ssh/salt-ssh.rsa -o User=root  /bin/sh << 'EOF'\r\n```\r\nThe -o PasswordAuthentication=no there appears to be incorrect-- am I doing something idiotic here?\r\n\r\nThe same behavior manifests both in 2014.1 and HEAD as of 4 minutes ago."
16070,'thatch45','salt-ssh stacktraces and hangs waiting for defunct processes to return\nI\'m running salt-ssh on the latest from the 2014.7 branch.\r\nI have 5 identical Ubuntu 14.04 vms. I ran the following command on all 5 and 3 returned successfully and the salt-ssh sits there and waits indefinitely for the others to return.  In another terminal I run ps aux and there are 2 salt-ssh defunct processes.\r\n\r\nHere\'s the command and the stacktrace.\r\n```\r\nroot@boucha:~# salt-ssh \'*\' state.sls docker\r\nProcess Process-2:\r\nProcess Process-5:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\nTraceback (most recent call last):\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 321, in handle_routine\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 321, in handle_routine\r\n    stdout, stderr, retcode = single.run()\r\n    stdout, stderr, retcode = single.run()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 602, in run\r\n    stdout = self.run_wfunc()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 602, in run\r\n    stdout = self.run_wfunc()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 696, in run_wfunc\r\n    result = self.wfuncs[self.fun](*self.args, **self.kwargs)\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 696, in run_wfunc\r\n    result = self.wfuncs[self.fun](*self.args, **self.kwargs)\r\n  File "/root/salt/salt/client/ssh/wrapper/state.py", line 69, in sls\r\n  File "/root/salt/salt/client/ssh/wrapper/state.py", line 69, in sls\r\n    __pillar__)\r\n    __pillar__)\r\n  File "/root/salt/salt/client/ssh/state.py", line 160, in prep_trans_tar\r\n  File "/root/salt/salt/client/ssh/state.py", line 160, in prep_trans_tar\r\n    files = file_client.cache_dir(name, saltenv)\r\n    files = file_client.cache_dir(name, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 225, in cache_dir\r\n  File "/root/salt/salt/fileclient.py", line 225, in cache_dir\r\n    ret.append(self.cache_file(\'salt://\' + fn_, saltenv))\r\n    ret.append(self.cache_file(\'salt://\' + fn_, saltenv))\r\n  File "/root/salt/salt/fileclient.py", line 148, in cache_file\r\n    return self.get_url(path, \'\', True, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 148, in cache_file\r\n  File "/root/salt/salt/fileclient.py", line 520, in get_url\r\n    return self.get_file(url, dest, makedirs, saltenv)\r\n    return self.get_url(path, \'\', True, saltenv)root@boucha:~# salt-ssh \'*\' state.sls docker\r\nProcess Process-2:\r\nProcess Process-5:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\nTraceback (most recent call last):\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 321, in handle_routine\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 321, in handle_routine\r\n    stdout, stderr, retcode = single.run()\r\n    stdout, stderr, retcode = single.run()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 602, in run\r\n    stdout = self.run_wfunc()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 602, in run\r\n    stdout = self.run_wfunc()\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 696, in run_wfunc\r\n    result = self.wfuncs[self.fun](*self.args, **self.kwargs)\r\n  File "/root/salt/salt/client/ssh/__init__.py", line 696, in run_wfunc\r\n    result = self.wfuncs[self.fun](*self.args, **self.kwargs)\r\n  File "/root/salt/salt/client/ssh/wrapper/state.py", line 69, in sls\r\n  File "/root/salt/salt/client/ssh/wrapper/state.py", line 69, in sls\r\n    __pillar__)\r\n    __pillar__)\r\n  File "/root/salt/salt/client/ssh/state.py", line 160, in prep_trans_tar\r\n  File "/root/salt/salt/client/ssh/state.py", line 160, in prep_trans_tar\r\n    files = file_client.cache_dir(name, saltenv)\r\n    files = file_client.cache_dir(name, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 225, in cache_dir\r\n  File "/root/salt/salt/fileclient.py", line 225, in cache_dir\r\n    ret.append(self.cache_file(\'salt://\' + fn_, saltenv))\r\n    ret.append(self.cache_file(\'salt://\' + fn_, saltenv))\r\n  File "/root/salt/salt/fileclient.py", line 148, in cache_file\r\n    return self.get_url(path, \'\', True, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 148, in cache_file\r\n  File "/root/salt/salt/fileclient.py", line 520, in get_url\r\n    return self.get_file(url, dest, makedirs, saltenv)\r\n    return self.get_url(path, \'\', True, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 940, in get_file\r\n  File "/root/salt/salt/fileclient.py", line 520, in get_url\r\n    hash_server = self.hash_file(path, saltenv)\r\n    return self.get_file(url, dest, makedirs, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 1151, in hash_file\r\n  File "/root/salt/salt/fileclient.py", line 940, in get_file\r\n    return channel.send(load)\r\n    hash_server = self.hash_file(path, saltenv)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 566, in send\r\n  File "/root/salt/salt/fileclient.py", line 1151, in hash_file\r\n    return channel.send(load)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 566, in send\r\n    return getattr(self.fs, cmd)(load)\r\n    return getattr(self.fs, cmd)(load)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 433, in file_hash\r\n  File "/root/salt/salt/fileserver/__init__.py", line 433, in file_hash\r\n    return self.servers[fstr](load, fnd)\r\n    return self.servers[fstr](load, fnd)\r\n  File "/root/salt/salt/fileserver/roots.py", line 220, in file_hash\r\n  File "/root/salt/salt/fileserver/roots.py", line 220, in file_hash\r\n    os.makedirs(cache_dir)\r\n    os.makedirs(cache_dir)\r\n  File "/usr/lib/python2.7/os.py", line 157, in makedirs\r\n  File "/usr/lib/python2.7/os.py", line 157, in makedirs\r\n    mkdir(name, mode)\r\n    mkdir(name, mode)\r\nOSError: [Errno 17] File exists: \'/tmp/.root_salt/running_data/var/cache/salt/minion/roots/hash/base/_states\'\r\nOSError: [Errno 17] File exists: \'/tmp/.root_salt/running_data/var/cache/salt/minion/roots/hash/base/_states\'\r\n\r\n  File "/root/salt/salt/fileclient.py", line 940, in get_file\r\n  File "/root/salt/salt/fileclient.py", line 520, in get_url\r\n    hash_server = self.hash_file(path, saltenv)\r\n    return self.get_file(url, dest, makedirs, saltenv)\r\n  File "/root/salt/salt/fileclient.py", line 1151, in hash_file\r\n  File "/root/salt/salt/fileclient.py", line 940, in get_file\r\n    return channel.send(load)\r\n    hash_server = self.hash_file(path, saltenv)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 566, in send\r\n  File "/root/salt/salt/fileclient.py", line 1151, in hash_file\r\n    return channel.send(load)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 566, in send\r\n    return getattr(self.fs, cmd)(load)\r\n    return getattr(self.fs, cmd)(load)\r\n  File "/root/salt/salt/fileserver/__init__.py", line 433, in file_hash\r\n  File "/root/salt/salt/fileserver/__init__.py", line 433, in file_hash\r\n    return self.servers[fstr](load, fnd)\r\n    return self.servers[fstr](load, fnd)\r\n  File "/root/salt/salt/fileserver/roots.py", line 220, in file_hash\r\n  File "/root/salt/salt/fileserver/roots.py", line 220, in file_hash\r\n    os.makedirs(cache_dir)\r\n    os.makedirs(cache_dir)\r\n  File "/usr/lib/python2.7/os.py", line 157, in makedirs\r\n  File "/usr/lib/python2.7/os.py", line 157, in makedirs\r\n    mkdir(name, mode)\r\n    mkdir(name, mode)\r\nOSError: [Errno 17] File exists: \'/tmp/.root_salt/running_data/var/cache/salt/minion/roots/hash/base/_states\'\r\nOSError: [Errno 17] File exists: \'/tmp/.root_salt/running_data/var/cache/salt/minion/roots/hash/base/_states\'\r\n<-- snip --> # the rest is the successful results from the other 3 minions.\r\n```\r\n\r\nOutput of ps aux | grep salt:\r\n```\r\nroot@boucha:~/salt# ps aux | grep salt\r\nroot     10733  0.0  0.0  42084   464 ?        Ss   20:46   0:00 su -c salt-master\r\nroot     10734  0.0  2.4 663780 12112 ?        Sl   20:46   0:02 /usr/bin/python /usr/local/bin/salt-master\r\n<-- snip -->\r\nroot     13829 94.4  4.9 127676 24684 pts/1    R+   21:30   7:16 /usr/bin/python /usr/local/bin/salt-ssh * state.sls docker\r\nroot     13837  0.2  0.0      0     0 pts/1    Z+   21:30   0:01 [salt-ssh] <defunct>\r\nroot     13840  0.2  0.0      0     0 pts/1    Z+   21:30   0:00 [salt-ssh] <defunct>\r\nroot     14199  0.0  0.1   9388   928 pts/0    S+   21:38   0:00 grep --color=auto salt\r\n```\r\n\r\nVersion of salt on my master:\r\n```\r\nroot@boucha:~/salt# salt-ssh --version\r\nsalt-ssh 2014.7.0rc2-299-g3ce4387 (Helium)\r\n```'
15941,'s0undt3ch',"VT can say that it is not alive but data is still in the buffer\njust use vt to run a, then inspect the isalive method. more data is in the recv and the terminal is not alive:\r\n\r\n```\r\nimport salt.utils.vt\r\nterm = salt.utils.vt.Terminal('ls -l /etc', shell=True, stream_stdout=False, stream_stderr=False)\r\nterm.isalive() == False\r\nterm.recv()\r\n```\r\nSo you will see that the term is accurately no longer alive because the routine has finished, but we don't know if the recv buffer has been flushed without calling recv until it returns None\r\nSee #15934\r\n\r\nSo it would probably make sense to have a method that returns if the recv buffer is empty, and you can know that you are done with a shell once the recv buffer is empty and the is_alive returns False"
15913,'terminalmage','Numeric state-id leads to exception\nI accidentally created a state with a numeric id (`0: pkg.installed`) and instead of an error of the config-parser I got this:\r\n\r\n```\r\n    The minion function caused an exception: Traceback (most recent call last):\r\n      File "/usr/lib/pymodules/python2.7/salt/minion.py", line 995, in _thread_return\r\n        return_data = func(*args, **kwargs)\r\n      File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 295, in highstate\r\n        whitelist=kwargs.get(\'whitelist\')\r\n      File "/usr/lib/pymodules/python2.7/salt/state.py", line 2801, in call_highstate\r\n        return self.state.call_high(high)\r\n      File "/usr/lib/pymodules/python2.7/salt/state.py", line 1921, in call_high\r\n        errors += self.verify_high(high)\r\n      File "/usr/lib/pymodules/python2.7/salt/state.py", line 856, in verify_high\r\n        if name.startswith(\'__\'):\r\n    AttributeError: \'int\' object has no attribute \'startswith\'\r\n```\r\n\r\nNot important at all, but probably an easy fix.'
15763,'rallytime','Allow creation of jobs with a known JID \nWe should be able to pass in a JID when we kick off any kinds of jobs (salt, salt-cloud, runner, wheel etc) via salt api. The passed in JID should be used as the JID for this job.'
15690,'thatch45',"salt-call and salt treat unicode differently\nSteps to reproduce:\r\n```\r\n# salt MINION grains.setval unicode ગુજરાતી  # it works\r\n```\r\nThen\r\n```\r\n# salt --out=raw MINION grains.get unicode\r\n{'MINION': '\\xe0\\xaa\\x97\\xe0\\xab\\x81\\xe0\\xaa\\x9c\\xe0\\xaa\\xb0\\xe0\\xaa\\xbe\\xe0\\xaa\\xa4\\xe0\\xab\\x80'}\r\n```\r\nBut:\r\n```\r\n# salt-call --out=raw grains.get unicode\r\n{'local': u'\\u0a97\\u0ac1\\u0a9c\\u0ab0\\u0abe\\u0aa4\\u0ac0'}\r\n```\r\n\r\nI prefer the second variant. It's better to stick with `unicode`, than with `bytes`, especially because of https://github.com/saltstack/salt/issues/11995."
15547,'terminalmage','Wrong number of GPUs returned for num_gpus grain\nI\'m using Salt 2014.1.10 to manage a RHEL 6.5 cluster. All the nodes on the cluster have 2 GPUs installed, but the num_gpus grain contains the value 1.\r\n\r\nHere is the output of lspci (notice the two lines that say "3D controller: NVIDIA"):\r\n\r\n00:00.0 Host bridge: Intel Corporation Xeon E5 v2/Core i7 DMI2 (rev 04)\r\n00:01.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 1a (rev 04)\r\n00:02.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 2a (rev 04)\r\n00:02.2 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 2c (rev 04)\r\n00:03.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 3a (rev 04)\r\n00:05.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 VTd/Memory Map/Misc (rev 04)\r\n00:05.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 IIO RAS (rev 04)\r\n00:11.0 PCI bridge: Intel Corporation C600/X79 series chipset PCI Express Virtual Root Port (rev 05)\r\n00:16.0 Communication controller: Intel Corporation C600/X79 series chipset MEI Controller #1 (rev 05)\r\n00:16.1 Communication controller: Intel Corporation C600/X79 series chipset MEI Controller #2 (rev 05)\r\n00:1a.0 USB controller: Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #2 (rev 05)\r\n00:1c.0 PCI bridge: Intel Corporation C600/X79 series chipset PCI Express Root Port 1 (rev b5)\r\n00:1c.4 PCI bridge: Intel Corporation C600/X79 series chipset PCI Express Root Port 5 (rev b5)\r\n00:1c.7 PCI bridge: Intel Corporation C600/X79 series chipset PCI Express Root Port 8 (rev b5)\r\n00:1d.0 USB controller: Intel Corporation C600/X79 series chipset USB2 Enhanced Host Controller #1 (rev 05)\r\n00:1e.0 PCI bridge: Intel Corporation 82801 PCI Bridge (rev a5)\r\n00:1f.0 ISA bridge: Intel Corporation C600/X79 series chipset LPC Controller (rev 05)\r\n00:1f.2 SATA controller: Intel Corporation C600/X79 series chipset 6-Port SATA AHCI Controller (rev 05)\r\n01:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\r\n01:00.1 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)\r\n02:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS 2208 [Thunderbolt] (rev 05)\r\n04:00.0 3D controller: NVIDIA Corporation GK110GL [Tesla K20Xm] (rev a1)\r\n07:00.0 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\r\n07:00.1 Ethernet controller: Intel Corporation I350 Gigabit Network Connection (rev 01)\r\n08:00.0 PCI bridge: Renesas Technology Corp. SH7757 PCIe Switch [PS]\r\n09:00.0 PCI bridge: Renesas Technology Corp. SH7757 PCIe Switch [PS]\r\n09:01.0 PCI bridge: Renesas Technology Corp. SH7757 PCIe Switch [PS]\r\n0a:00.0 PCI bridge: Renesas Technology Corp. SH7757 PCIe-PCI Bridge [PPB]\r\n0b:00.0 VGA compatible controller: Matrox Electronics Systems Ltd. G200eR2\r\n3f:08.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 0 (rev 04)\r\n3f:09.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 1 (rev 04)\r\n3f:0a.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 0 (rev 04)\r\n3f:0a.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 1 (rev 04)\r\n3f:0a.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 2 (rev 04)\r\n3f:0a.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 3 (rev 04)\r\n3f:0b.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 UBOX Registers (rev 04)\r\n3f:0b.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 UBOX Registers (rev 04)\r\n3f:0c.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0c.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0c.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0c.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0c.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0d.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0d.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0d.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0d.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0d.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n3f:0e.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Home Agent 0 (rev 04)\r\n3f:0e.1 Performance counters: Intel Corporation Xeon E5 v2/Core i7 Home Agent 0 (rev 04)\r\n3f:0f.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Target Address/Thermal Registers (rev 04)\r\n3f:0f.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 RAS Registers (rev 04)\r\n3f:0f.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n3f:0f.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n3f:0f.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n3f:0f.5 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n3f:10.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 0 (rev 04)\r\n3f:10.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 1 (rev 04)\r\n3f:10.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 0 (rev 04)\r\n3f:10.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 1 (rev 04)\r\n3f:10.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 2 (rev 04)\r\n3f:10.5 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 3 (rev 04)\r\n3f:10.7 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 3 (rev 04)\r\n3f:13.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 R2PCIe (rev 04)\r\n3f:13.1 Performance counters: Intel Corporation Xeon E5 v2/Core i7 R2PCIe (rev 04)\r\n3f:13.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Registers (rev 04)\r\n3f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)\r\n3f:16.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 System Address Decoder (rev 04)\r\n3f:16.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)\r\n3f:16.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)\r\n40:01.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 1a (rev 04)\r\n40:02.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 2a (rev 04)\r\n40:03.0 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 3a (rev 04)\r\n40:03.2 PCI bridge: Intel Corporation Xeon E5 v2/Core i7 PCI Express Root Port 3c (rev 04)\r\n40:05.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 VTd/Memory Map/Misc (rev 04)\r\n40:05.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 IIO RAS (rev 04)\r\n42:00.0 3D controller: NVIDIA Corporation GK110GL [Tesla K20Xm] (rev a1)\r\n7f:08.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 0 (rev 04)\r\n7f:09.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Link 1 (rev 04)\r\n7f:0a.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 0 (rev 04)\r\n7f:0a.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 1 (rev 04)\r\n7f:0a.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 2 (rev 04)\r\n7f:0a.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Power Control Unit 3 (rev 04)\r\n7f:0b.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 UBOX Registers (rev 04)\r\n7f:0b.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 UBOX Registers (rev 04)\r\n7f:0c.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0c.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0c.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0c.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0c.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0d.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0d.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0d.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0d.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0d.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Unicast Registers (rev 04)\r\n7f:0e.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Home Agent 0 (rev 04)\r\n7f:0e.1 Performance counters: Intel Corporation Xeon E5 v2/Core i7 Home Agent 0 (rev 04)\r\n7f:0f.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Target Address/Thermal Registers (rev 04)\r\n7f:0f.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 RAS Registers (rev 04)\r\n7f:0f.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n7f:0f.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n7f:0f.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n7f:0f.5 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 0 Channel Target Address Decoder Registers (rev 04)\r\n7f:10.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 0 (rev 04)\r\n7f:10.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 1 (rev 04)\r\n7f:10.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 0 (rev 04)\r\n7f:10.3 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 1 (rev 04)\r\n7f:10.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 2 (rev 04)\r\n7f:10.5 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 Thermal Control 3 (rev 04)\r\n7f:10.7 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Integrated Memory Controller 1 Channel 0-3 ERROR Registers 3 (rev 04)\r\n7f:13.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 R2PCIe (rev 04)\r\n7f:13.1 Performance counters: Intel Corporation Xeon E5 v2/Core i7 R2PCIe (rev 04)\r\n7f:13.4 System peripheral: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Registers (rev 04)\r\n7f:13.5 Performance counters: Intel Corporation Xeon E5 v2/Core i7 QPI Ring Performance Ring Monitoring (rev 04)\r\n7f:16.0 System peripheral: Intel Corporation Xeon E5 v2/Core i7 System Address Decoder (rev 04)\r\n7f:16.1 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)\r\n7f:16.2 System peripheral: Intel Corporation Xeon E5 v2/Core i7 Broadcast Registers (rev 04)\r\n\r\n'
15444,'terminalmage','Clarify that GitFS does not currently work in masterless\nApparently the docs do not make clear the fact that custom fileserver backends do not work with masterless minions.\r\n\r\nRefs https://github.com/saltstack/salt/issues/6660#issuecomment-54109402'
15218,'whiteinge',"Arguments to runner functions not passed through in salt-api\n```text\r\n% curl -b /tmp/cookies.txt -sSi -H 'Accept: application/x-yaml' http://localhost:8000 -d client='runner' -d fun='jobs.lookup_jid' -d jid=20140823233206283030\r\nHTTP/1.1 200 OK\r\nContent-Length: 128\r\nAccess-Control-Expose-Headers: GET, POST\r\nAccess-Control-Allow-Credentials: true\r\nVary: Accept-Encoding\r\nServer: CherryPy/3.3.0\r\nAllow: GET, HEAD, POST\r\nCache-Control: private\r\nDate: Sun, 24 Aug 2014 05:36:15 GMT\r\nAccess-Control-Allow-Origin: *\r\nContent-Type: application/x-yaml\r\nSet-Cookie: session_id=4e32b0091aa7037917ed5ee9d2054e26bc31904e; expires=Sun, 24 Aug 2014 15:36:15 GMT; Path=/\r\n\r\nreturn:\r\n- 'Exception occurred in runner jobs.lookup_jid: SaltInvocationError: lookup_jid takes\r\n  at least 1 argument (0 given)'\r\n```"
15074,'s0undt3ch','salt-call --out-indent should have an --append / -a option to avoid overwriting\nI have a cron job on a minion that contains the following:\r\n\r\n```\r\n*/5 * * * * salt-call -l quiet --log-file-level error --out-indent 4 --out-file /var/log/salt/cron state.sls somestate\r\n```\r\n\r\nInstead of /var/log/salt/cron being appended to with every execution, the file is clobbered. This was unexpected to me, as log files are typically appended to. However this functionality may be expected and relied upon by some people.\r\n\r\nI propose a --append / -a option (similar to the GNU tee coreutils command) to allow the user the option to append output to the file, as opposed to overwriting it.\r\n\r\nThis issue was split out from #14979, which now focuses on a different salt-call issue.'
15015,'whiteinge','500 response when logging out of rest_cherrypy app\n'
14885,'whiteinge','Bad format string in npm.installed state\n```\r\n************* Module salt.states.npm\r\nsalt/states/npm.py:135: [E1305(too-many-format-args), installed] Too many arguments for format string\r\n```\r\n\r\nIntroduced in ed4bb5119ea47a72cafd8639f8abcef3b202d07e.\r\nCC: @whiteinge'
14860,'basepi','id in minion configuration can match in yaml\nI have minions with hostnames that are pretty similar to a date, ie 4017-16-20. When I explicitly name my minions in the id field, salt-minion never starts and complains with this: \r\n\r\n```\r\n[ERROR   ] month must be in 1..12\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 195, in parse_args\r\n    process_option_func()\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 349, in process_config_dir\r\n    self.config = self.setup_config()\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 1399, in setup_config\r\n    minion_id=True)\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 688, in minion_config\r\n    overrides = load_config(path, env_var, DEFAULT_MINION_OPTS[\'conf_file\'])\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 589, in load_config\r\n    opts = _read_conf_file(path)\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 516, in _read_conf_file\r\n    conf_opts = yaml.safe_load(conf_file.read()) or {}\r\n  File "/usr/lib/python2.6/dist-packages/yaml/__init__.py", line 75, in safe_load\r\n    return load(stream, SafeLoader)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/__init__.py", line 58, in load\r\n    return loader.get_single_data()\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 44, in get_single_data\r\n    return self.construct_document(node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 53, in construct_document\r\n    for dummy in generator:\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 403, in construct_yaml_map\r\n    value = self.construct_mapping(node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 213, in construct_mapping\r\n    return BaseConstructor.construct_mapping(self, node, deep=deep)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 138, in construct_mapping\r\n    value = self.construct_object(value_node, deep=deep)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 93, in construct_object\r\n    data = constructor(self, node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 317, in construct_yaml_timestamp\r\n    return datetime.date(year, month, day)\r\nValueError: month must be in 1..12\r\nUsage: salt-minion\r\n\r\nsalt-minion: error: Error while processing <bound method Minion.process_config_dir of <salt.Minion object at 0xa09bf8c>>: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 195, in parse_args\r\n    process_option_func()\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 349, in process_config_dir\r\n    self.config = self.setup_config()\r\n  File "/usr/lib/pymodules/python2.6/salt/utils/parsers.py", line 1399, in setup_config\r\n    minion_id=True)\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 688, in minion_config\r\n    overrides = load_config(path, env_var, DEFAULT_MINION_OPTS[\'conf_file\'])\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 589, in load_config\r\n    opts = _read_conf_file(path)\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 516, in _read_conf_file\r\n    conf_opts = yaml.safe_load(conf_file.read()) or {}\r\n  File "/usr/lib/python2.6/dist-packages/yaml/__init__.py", line 75, in safe_load\r\n    return load(stream, SafeLoader)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/__init__.py", line 58, in load\r\n    return loader.get_single_data()\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 44, in get_single_data\r\n    return self.construct_document(node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 53, in construct_document\r\n    for dummy in generator:\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 403, in construct_yaml_map\r\n    value = self.construct_mapping(node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 213, in construct_mapping\r\n    return BaseConstructor.construct_mapping(self, node, deep=deep)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 138, in construct_mapping\r\n    value = self.construct_object(value_node, deep=deep)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 93, in construct_object\r\n    data = constructor(self, node)\r\n  File "/usr/lib/python2.6/dist-packages/yaml/constructor.py", line 317, in construct_yaml_timestamp\r\n    return datetime.date(year, month, day)\r\nValueError: month must be in 1..12\r\n```\r\n\r\nThe fine cats on IRC helped to establish this and recommended me to wrap my hostname in single quotes in the id field, which took care of my problem. Might be worth mentioning this in the configuration file. It\'s a yaml issue, not salt. '
14855,'s0undt3ch',"Salt Version contains 'n/a' \nI installed Salt using the bootstrap script, and when I do a versions report, there is an `n/a` in the versioning number for Salt itself:\r\n```\r\n# salt --versions-report\r\n           Salt: 2014.7.0-n/a-8b2cbbb\r\n         Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.0.1\r\n           RAET: Not Installed\r\n            ZMQ: 4.0.4\r\n           Mako: Not Installed\r\n```\r\n@s0undt3ch - I think this might be related to your changes to the versioning system. "
14788,'basepi',"manage.versions wrong number sorting\nHello,\r\n\r\nJust updated some minions to 2014.1.10 (from 2014.1.7).\r\nThe manage.versions command now states that the 2014.1.7 minions are newer then the 2014.1.10 master.\r\nThink it's a number sorting issue since salt thinks 7 is higher then 10 and it seems it's only comparing the first number it sees.\r\n\r\n```\r\n# salt-run manage.versions\r\nMinion newer than master:\r\n    ----------\r\n    centos.mydomain.org:\r\n        2014.1.7\r\n    opsview.mydomain.org:\r\n        2014.1.7\r\n    w7.mydomain.org:\r\n        2014.1.7\r\nUp to date:\r\n    ----------\r\n    apps.mydomain.org:\r\n        2014.1.10\r\n    crashplan.mydomain.org:\r\n        2014.1.10\r\n    download.mydomain.org:\r\n        2014.1.10\r\n    neo.mydomain.org:\r\n        2014.1.10\r\n    plex.mydomain.org:\r\n        2014.1.10\r\n    www.mydomain.org:\r\n        2014.1.10\r\n    zimbra.mydomain.org:\r\n        2014.1.10\r\n```\r\n```\r\n# dpkg -l | grep salt       \r\nii  salt-common                        2014.1.10-1precise1                 Shared libraries that salt requires for all packages\r\nii  salt-master                        2014.1.10-1precise1                 This package provides a remote manager to administer servers via salt\r\nii  salt-minion                        2014.1.10-1precise1                 This package represents the client package for salt\r\n```\r\n\r\nOr am i seeing it wrong ?\r\n\r\nRegards,\r\n\r\nDamon"
14768,'terminalmage',"/var/cache/salt installed 755 by deb, not installed by rpm; causes commands/clients to fail\nAccessing the master through client.LocalClient fails even when the user is in the ACL list, but only on RPM systems\r\n\r\nThe root of the problem is that /var/cache/salt is created by the Debian packaging (as mode 0755), but is not created by the RPM packaging, and the master running as root creates the dirs as 770.\r\n\r\nI'm not sure of the correct fix, but it seems like individual files in /var/cache/salt are carefully permission-managed, so perhaps the right answer is to add /var/cache/salt to the RPM packaging?"
14634,'rallytime','\'unless\' documentation isn\'t logically plausible\nhttp://docs.saltstack.com/en/latest/ref/states/requisites.html\r\n\r\n22.25.15.2 states \r\n\r\n"Use unless to only run if any of the specified commands return False."\r\n\r\nImplying unless = (!A or !B)\r\n\r\nand then says "This state will not run if the vim-enhanced package is already installed, or if /usr/bin/vim exists"\r\n\r\nImplying unless = !(A or B), which is subtly different.\r\n\r\nOnlyif is defined as !unless, which could be !(!A or !B) [ (A and B) ], or (A or B), if my De Morgan\'s laws hold true!'
14358,'terminalmage','Use install instead of installmissing if Chocolatey >= 0.9.8.24\nX-ref: #13980\n\nThis also adds a chocolatey.chocolatey_version function and does some\nDRY cleanup so that the error message and exception are raised in\n_find_chocolatey() instead of in 6-8 different places.'
14240,'cachedout',"gitfs pillars broken in salt 2014.1.7\nI'm experiencing major problems with my pillar data and gitfs after upgrading from 2014.1.3 to 2014.1.7 (I did not try the intermediate versions since most of them have various severe regressions of their own). Downgrading to 2014.1.3 and blowing away /var/cache/salt/master/ restores pillar functionality. I've tried removing all traces of Salt from the system and doing a clean install of 2014.1.7, same result.\r\n\r\nAfter the upgrade, none of my pillar data is sent to minions. It is not present in pillar.items output and highstates fail due to missing data. I see a string of messages like this in /var/log/salt/master:\r\n\r\n```\r\n2014-07-15 10:14:52,852 [root                                        ][ERROR   ] Unable to checkout branch master: 'git checkout origin/master' returned exit status 128: fatal: Unable to create '/var/cache/salt/master/pillar_gitfs/0/.git/index.lock': File exists.\r\n\r\nIf no other git process is currently running, this probably means a\r\ngit process crashed in this repository earlier. Make sure no other git\r\nprocess is running and remove the file manually to continue.\r\n```\r\n\r\nThe lockfile does not seem to actually exist, however. Even after entirely deleting /var/cache/salt/master/ and restarting salt-master, the error immediately returns. It also fails to actually check out the pillar data into /var/cache/salt/master/pillar_gitfs/. I just get a /0/ dir with nop.sls in it. Nothing else.\r\n\r\n```\r\nCentOS release 6.2 (Final)\r\n\r\n           Salt: 2014.1.7\r\n         Python: 2.6.6 (r266:84292, Jun 18 2012, 14:18:47)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.9.final\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.2\r\n            ZMQ: 3.2.3\r\n```\r\n\r\nNon-default master settings (all of the git repos exist and work fine on 2014.1.3):\r\n```\r\nauto_accept: True\r\next_pillar:\r\n  - git: master file:///srv/repos/sysops/conf/pillar.git\r\n  - git: production file:///srv/repos/sysops/conf/pillar.git\r\n  - git: stage file:///srv/repos/sysops/conf/pillar.git\r\nfileserver_backend:\r\n  - git\r\ngitfs_remotes:\r\n  - file:///srv/sls.git\r\nlog_level: warning\r\nlog_level_logfile: warning\r\nstate_verbose: False\r\ntimeout: 30\r\nworker_threads: 24\r\n```"
14220,'cro',"Running state.sls or state.highstate via salt-ssh returns: Undefined SHIM state\nNeed confirmation: I'm seeing the following error when running simple states via ``salt-ssh``: ``Undefined SHIM state``."
14205,'whiteinge','salt-api/run returning session error\nPosting against the salt-api /run endpoint with the following request returns a 500 from the rest_cherrypy server. \r\n\r\nNotes:\r\n* The api server is running behind nginx basic auth\r\n* Salt version `salt 2014.1.0-8755-g7d46ad7 (Hydrogen)`\r\n\r\nRequest:\r\n```shell\r\ncurl -ksS https://user:pass@api-server-endpoint.com/run \\\r\n-H \'Accept: application/x-yaml\' \\\r\n-d fun=\'test.ping\' \\\r\n-d client=\'local\' \\\r\n-d tgt=\'*\' \\\r\n-d username=\'user\' \\\r\n-d password=\'pass\' \\\r\n-d eauth=\'pam\'\r\n```\r\n\r\nAPI Log:\r\n```\r\n[DEBUG   ] Error while processing request for: /run\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/netapi/rest_cherrypy/app.py", line 299, in hypermedia_handler\r\n    ret = cherrypy.serving.request._hypermedia_inner_handler(*args, **kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/cherrypy/_cpdispatch.py", line 61, in __call__\r\n    return self.callable(*self.args, **self.kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/netapi/rest_cherrypy/app.py", line 1113, in POST\r\n    \'return\': list(self.exec_lowstate()),\r\n  File "/usr/lib/python2.7/dist-packages/salt/netapi/rest_cherrypy/app.py", line 524, in exec_lowstate\r\n    cherrypy.session.release_lock()\r\n  File "/usr/local/lib/python2.7/dist-packages/cherrypy/__init__.py", line 223, in __getattr__\r\n    child = getattr(serving, self.__attrname__)\r\nAttributeError: \'_Serving\' object has no attribute \'session\'\r\n[INFO    ] 127.0.0.1 - - [14/Jul/2014:17:37:59] "POST /run HTTP/1.0" 500 49 "" "curl/7.30.0"```'
14046,'basepi','salt batch -> not working (LocalClient / get_local_client call)\nIn the current release v2014.1.6 the \'salt -b\' batch command on the master ist not working anymore\r\n\r\n```\r\n# salt \'*\' -b 2 test.ping\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nAttributeError: \'module\' object has no attribute \'get_local_client\'\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt", line 10, in <module>\r\n    salt_main()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 135, in salt_main\r\n    client.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/__init__.py", line 82, in run\r\n    batch = salt.cli.batch.Batch(self.config, eauth)\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/batch.py", line 25, in __init__\r\n    self.local = salt.client.get_local_client(opts[\'conf_file\'])\r\nAttributeError: \'module\' object has no attribute \'get_local_client\'\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt", line 10, in <module>\r\n    salt_main()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 135, in salt_main\r\n    client.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/__init__.py", line 82, in run\r\n    batch = salt.cli.batch.Batch(self.config, eauth)\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/batch.py", line 25, in __init__\r\n    self.local = salt.client.get_local_client(opts[\'conf_file\'])\r\nAttributeError: \'module\' object has no attribute \'get_local_client\'\r\n```\r\n\r\nThe problem seem to be:\r\nIn the *develop* branch there is a change in ```salt/client/__init__.py``` from the method ```LocalConfig``` to ```get_local_config```. This is not yet merged into release *v2014.1.6*. \r\n\r\nSomehow it has already been changed in the file ```salt/cli/batch.py```. For the release there it should be reverted to the use of ```LocalConfig```.'
14019,'basepi','os.path.dirname not doing what you think in file.makedirs_ if the path excludes a trailing slash\nI just ran a simple test of what\'s going on in makedirs_ because I couldn\'t understand why a directory wasn\'t being created on the minion.  The sample below shows a directory I know to exist on the host, and the command that is being used in makedirs_ to assign the path to dirname.  Note that I did not include the trailing slash on the path.\r\n\r\n```\r\nPython 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\r\nType "help", "copyright", "credits" or "license" for more information.\r\n>>> import os.path\r\n>>> os.path.normpath(os.path.dirname(\'/etc/salt\'))\r\n\'/etc\'\r\n>>>\r\n```\r\n\r\nThe message to the user is wrong, since dirname was the value that was tested by os.path.isdir but path is the value sent back to the user in the return message.  \r\n\r\n```python\r\ndirname = os.path.normpath(os.path.dirname(path))\r\n\r\n    if os.path.isdir(dirname):\r\n        # There\'s nothing for us to do\r\n        return \'Directory {0!r} already exists\'.format(path)\r\n```\r\n\r\nI\'m not sure what would be preferable here as a solution:\r\n1. Change documentation to reflect that path must include the trailing slash?\r\n2. Append a trailing slash to path and expect os.path.normpath to fix any double slashes?\r\n3. Change the return message formatting to report on the actual value tested?\r\n\r\n'
13976,'terminalmage',"dangerous shadowing in boto_elb module\n```\r\n************* Module salt.modules.boto_elb\r\nsalt/modules/boto_elb.py:512: [W0621(redefined-outer-name), register_instances] Redefining name 'elb' from outer scope (line 47)\r\nsalt/modules/boto_elb.py:530: [W0621(redefined-outer-name), deregister_instances] Redefining name 'elb' from outer scope (line 47)\r\n```"
13937,'whiteinge','No man page for salt-unity script\nI got the following rpm warning when building salt from git develop as precursor to the new salt Helium release\r\n\r\nno-manual-page-for-binary salt-unity\r\nEach executable in standard binary directories should have a man page.\r\n\r\nCould this be provided?'
13932,'terminalmage','Documentation updates for gitfs\nSuggest ways to force a refresh of gitfs tree from a post-commit hook on the minion of the git server.'
13930,'whiteinge','salt-api doesn\'t check eauth when client is "runner"\nsalt-api [runner](https://github.com/saltstack/salt/blob/develop/salt/netapi/__init__.py#L83) use:\r\n<pre>\r\n    def runner(self, fun, **kwargs):\r\n        \'\'\'\r\n        Run `runner modules <all-salt.runners>`\r\n\r\n        Wraps :py:meth:`salt.runner.RunnerClient.low`.\r\n\r\n        :return: Returns the result from the runner module\r\n        \'\'\'\r\n        runner = salt.runner.RunnerClient(self.opts)\r\n        return runner.low(fun, kwargs)\r\n</pre>\r\n\r\n*runner.low* don\'t check eauth, so everyone can execute any runner module even if user don\'t exist. It\'s risky.'
13926,'terminalmage',"systemd is not always properly detected as a service provider on RHEL7\nWe have a state to configure each minion as we want it to be that is fired off via a reactor when a new minion's key is accepted. One of the most important things it does is ensures the salt-minion config is happy and everything is started.\r\n\r\nWe saw issues where it wouldn't detect that the system was running systemd for some odd reason and had to statically set it such as:\r\n\r\n```yaml\r\nsalt-minion:\r\n  service:\r\n    - running\r\n    - enable: True\r\n{%- if grains['os'] == 'RedHat'\r\n    and grains['osrelease'] >= '7.0' %}\r\n    - provider:\r\n      - service: systemd\r\n{%- endif %}\r\n```\r\n\r\nCan this be fixed?"
13878,'terminalmage','UnicodeEncodeError failure on cmd.run state enforcement with user argument\nWhen I call `env` program with `cmd.run` it works:\r\n```\r\n# salt \'*\' cmd.run env user=bin\r\nserver-vm:\r\n    LC_ALL=C\r\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\r\n    PWD=/root\r\n    LANG=POSIX\r\n    SHLVL=1\r\n    LC_CTYPE=ru_RU.UTF-8\r\n    _=/usr/bin/env\r\n```\r\nWhen I use the same command inside of state, it doesn\'t:\r\ncmd_fail.sls:\r\n```\r\ncmd_fail:\r\n  cmd.run:\r\n     - name: env\r\n     - user: bin\r\n```\r\n```\r\n# salt -v \'server-vm\' state.sls cmd_fail\r\nserver-vm:\r\n----------\r\n          ID: cmd_fail\r\n    Function: cmd.run\r\n        Name: env\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib/python2.7/site-packages/salt/state.py", line 1379, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib/python2.7/site-packages/salt/states/cmd.py", line 602, in run\r\n                  name, timeout=timeout, **cmd_kwargs\r\n                File "/usr/lib/python2.7/site-packages/salt/modules/cmdmod.py", line 806, in run_all\r\n                  saltenv=saltenv)\r\n                File "/usr/lib/python2.7/site-packages/salt/modules/cmdmod.py", line 408, in _run\r\n                  proc = salt.utils.timed_subprocess.TimedProc(cmd, **kwargs)\r\n                File "/usr/lib/python2.7/site-packages/salt/utils/timed_subprocess.py", line 24, in __init__\r\n                  self.process = subprocess.Popen(args, **kwargs)\r\n                File "/usr/lib64/python2.7/subprocess.py", line 709, in __init__\r\n                  errread, errwrite)\r\n                File "/usr/lib64/python2.7/subprocess.py", line 1326, in _execute_child\r\n                  raise child_exception\r\n              UnicodeEncodeError: \'ascii\' codec can\'t encode characters in position 0-1: ordinal not in range(128)\r\n     Changes:   \r\n\r\nSummary\r\n------------\r\nSucceeded: 0\r\nFailed:    1\r\n------------\r\nTotal:     1\r\n```\r\n\r\n_Software:_\r\nmaster: salt 2014.1.4 on Linux 3.2.0-4-amd64 #1 SMP Debian 7.3 3.2.57-3 x86_64 GNU/Linux\r\n\r\nminion: salt 2014.1.5 on Linux 3.11.10-17-desktop #1 SMP openSUSE 13.1 x86_64 GNU/Linux inside VirtualBox.\r\n\r\nIf I specify `root` as a user it works. If I use `name: \'su bin -c "env"\'` it works too. Changing `shell` to `/bin/bash` doesn\'t help.\r\n\r\nPS: it looks similar to https://github.com/saltstack/salt/issues/12611 and https://github.com/saltstack/salt/issues/12399 but in my case there is no unicode in env.'
13744,'thatch45','Stacktrace with listen_in when key is not valid\nMy state had the following:\r\n\r\n```yaml\r\nEnsure redis.conf is present:\r\n  file.managed:\r\n    - name: /etc/redis/redis.conf\r\n    - source: salt://redis/redis.conf\r\n    - template: jinja\r\n    - listen_in:\r\n      - supervisord: redis\r\n```\r\n\r\nBut I had removed my supervisord state for redis. This resulted in a stack trace:\r\n\r\n```\r\nKeyError: (\'supervisord\', \'redis\')\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 128, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/__init__.py", line 398, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/caller.py", line 186, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/caller.py", line 91, in call\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/state.py", line 288, in highstate\r\n    whitelist=kwargs.get(\'whitelist\')\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 2770, in call_highstate\r\n    return self.state.call_high(high)\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1906, in call_high\r\n    ret = self.call_listen(chunks, ret)\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1872, in call_listen\r\n    chunk = crefs[key]\r\nKeyError: (\'supervisord\', \'redis\')\r\n```'
13726,'thatch45','Stacktrace using new listen/listen_in feature\n```\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nKeyError: \'file_|-Ensure statsd supervisor configuration exists_|-/etc/supervisor/conf.d/statsd.conf_|-managed\'\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 128, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/__init__.py", line 398, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/caller.py", line 186, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/caller.py", line 91, in call\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/state.py", line 287, in highstate\r\n    whitelist=kwargs.get(\'whitelist\')\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 2768, in call_highstate\r\n    return self.state.call_high(high)\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1904, in call_high\r\n    ret = self.call_listen(chunks, ret)\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1869, in call_listen\r\n    if running[to_tag][\'changes\']:\r\n```'
13703,'terminalmage',"service.running not found\nUsing the latest develop:\r\n\r\nI get the following error:\r\n```\r\n----------\r\n          ID: autofs\r\n    Function: service.running\r\n      Result: False\r\n     Comment: State 'service.running' found in SLS 'curhel6.nfs' is unavailable\r\n     Started: \r\n     Duration: \r\n     Changes:   \r\n```\r\n\r\nHere's my state:\r\n```\r\nautofs:\r\n  pkg.installed:\r\n    - name: autofs\r\n  service:\r\n    - enable: True\r\n    - running\r\n    - watch: \r\n      - pkg: autofs\r\n```\r\n\r\n**Note**: This was working earlier. Something's broke after updating to the develop branch\r\n\r\nHere's my version's report:\r\n```\r\n[root@nitin-saltmaster ~]# salt --versions-report\r\n           Salt: 2014.1.0-8595-g2b16eea\r\n         Python: 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 2.2.0.1\r\n           RAET: Not Installed\r\n            ZMQ: 3.2.4\r\n```"
13673,'cro',"master_tops appears broken in 2014.1.5\nOn 2014.1.5, custom tops modules (located in {extension_modules}/tops) are loaded (``__virtual__`` function is called), but not utilized (``top`` function is not invoked).\r\n\r\nThe tops module (/srv/salt/modules/tops/test.py):\r\n```python\r\nimport logging\r\n\r\n\r\nlog = logging.getLogger(__name__)\r\n\r\n\r\ndef __virtual__():\r\n    log.warning('test loaded')\r\n    return 'test'\r\n\r\n\r\ndef top(**kwargs):\r\n    log.warning('test top')\r\n    return {'base': ['test']}\r\n```\r\n\r\nmaster config:\r\n```\r\nextension_modules: /srv/salt/modules/\r\nmaster_tops:\r\n  test: True\r\n```\r\n\r\n```\r\nsalt-call state.show_top\r\n```\r\n\r\nOn 2014.1.4:\r\n```\r\nlocal:\r\n    ----------\r\n    base:\r\n        - test\r\n```\r\n\r\nOn 2014.1.5:\r\n```\r\nlocal:\r\n    ----------\r\n```"
13672,'terminalmage','Allow traversing dicts nested within lists when matching\nSee https://groups.google.com/forum/#!topic/salt-users/_YjBAlAtlqo on the salt-users list for background info.'
13591,'UtahDave','Docker state id key error\nWhen running a state with docker.pulled on centos with docker 1.0 I get this error:\r\n\r\n```\r\n          ID: docker_index_image_mytraxwp\r\n    Function: docker.pulled\r\n        Name: docker.mytrax.co.jp/mytraxwp-app:latest\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib/python2.6/site-packages/salt/state.py", line 1509, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib64/python2.6/contextlib.py", line 34, in __exit__\r\n                  self.gen.throw(type, value, traceback)\r\n                File "/usr/lib/python2.6/site-packages/salt/utils/context.py", line 41, in func_globals_inject\r\n                  yield\r\n                File "/usr/lib/python2.6/site-packages/salt/state.py", line 1509, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib/python2.6/site-packages/salt/states/dockerio.py", line 239, in pulled\r\n                  previous_id = image_infos[\'out\'][\'id\'] if image_infos[\'status\'] else None\r\n              KeyError: \'id\'\r\n     Started: 07:26:49.861634\r\n     Duration: 313 ms\r\n     Changes:\r\n```\r\n\r\nThere seems to be some inconsistencies with the id, if I make the following changes it works, but it might affect other state commands too:\r\n\r\n```\r\n# diff dockerio.py dockerio.py2\r\n320c320\r\n<                    id_=infos[\'Id\'],\r\n---\r\n>                    id_=infos[\'id\'],\r\n350c350\r\n<                    id_=container_info[\'Id\'],\r\n---\r\n>                    id_=container_info[\'ID\'],\r\n1570c1570\r\n<                 if l.get(\'status\') == \'Download complete\' and l.get(\'Id\'):\r\n---\r\n>                 if l.get(\'status\') == \'Download complete\' and l.get(\'id\'):\r\n1678c1678\r\n<             if infos and infos.get(\'Id\', None):\r\n---\r\n>             if infos and infos.get(\'id\', None):\r\n1684c1684\r\n<                        id_=infos[\'Id\'],\r\n---\r\n>                        id_=infos[\'id\'],\r\n1686c1686\r\n<                            repotag, infos[\'Id\']))\r\n---\r\n>                            repotag, infos[\'id\']))\r\n```'
13544,'cro',"grains.setval returned index error after upgrade from 2014.1.4 to 2014.1.5\nHad a strange issue pop up.  I was running 2014.1.4.  I installed 2014.1.5 over the top of 2014.1.4 with a fresh checkout of v2014.1.5 from github using 'python setup.py install --force' and restarted the salt-master and salt-minion service.  \r\n\r\nI spun up two new nodes on Digital Ocean using salt-cloud and attempted to manually set a grain using salt 'nginx*' grains.setval role webserver.\r\n\r\nI received an error for both nodes indicating something to the affect of a malformed index, though unfortunately I didn't capture it exactly.  Afterwords the grains seemed to be corrupted as I received the same error when trying to apply a highstate.\r\n\r\nI downgraded to 2014.1.4 and the error went away.  I upgraded to 2014.1.5 again this morning using the same method and have not been able to reproduce the error.\r\n\r\nIt sounds like it may have been a caching issue, but its difficult to say since I haven't been able to reproduce.  \r\n\r\nThank you,\r\n-Mike"
13530,'rallytime',"Update Digital Ocean Configuration Docs\nCloud documentation regarding Digital Ocean's API change as referenced in fix #13525 needs to be updated."
13506,'terminalmage',"lift limitation of pygit2 transports if pygit2 >= 0.20\nWith regards to #9619, pygit2 0.20 supports ssh key authentication. I have not tried it myself, but it seems worth revisiting the transport limitation.\r\n\r\nI'm unsure whether the lack of availability of a 0.20 package in the common distros' repositories would be a deterrent, or if installing via pip is sufficient considering that gitfs is not a default, and therefore does not impact salt packaging."
13464,'rallytime',"Extending cloud provider needs redefinition of provider keyword\nI read the following from http://docs.saltstack.com/en/latest/topics/cloud/config.html\r\n![screen shot 2014-06-14 at 5 41 02 pm](https://cloud.githubusercontent.com/assets/3374962/3279673/a27e911c-f40c-11e3-8f53-02b184dac3d8.png)\r\n\r\nThere should be a correction in the documentation. `provider` is required for `my-productions-envs` as well.\r\n\r\nInstead of this\r\n```\r\nmy-productions-envs:\r\n  - extends: my-develop-envs:ibmsce\r\n    user: my-production-user@mycorp.com\r\n    location: us-east-1\r\n    availability_zone: us-east-1\r\n```\r\nIt should be this:\r\n```\r\nmy-productions-envs:\r\n  - extends: my-develop-envs:ibmsce\r\n    user: my-production-user@mycorp.com\r\n    location: us-east-1\r\n    availability_zone: us-east-1\r\n    provider: ibmsce\r\n```\r\n\r\nWithout it i get the following INFO message:\r\n```\r\n[INFO    ] There's at least one cloud driver details under the 'my-productions-envs' cloud provider alias which does not have the required 'provider' setting. Was probably just used as a holder for additional data. Removing it from the available providers listing\r\n```\r\n\r\nAnd it fails in the end with the following error message:\r\n```\r\n----------\r\n          ID: test.saltstackdev\r\n    Function: cloud.present\r\n      Result: False\r\n     Comment: Failed to create instance test.saltstackdev using profile <function profile at 0x361b1b8>, please check your configuration\r\n     Started: 17:36:53.376926\r\n     Duration: 378 ms\r\n     Changes:   \r\n----------\r\n```\r\n\r\n\r\nAnother thing to note here is, According to the following screenshot, should't the provider be already included under `my-productions-envs`?\r\n![screen shot 2014-06-14 at 5 55 44 pm](https://cloud.githubusercontent.com/assets/3374962/3279692/ac532a5c-f40e-11e3-86da-8f126af80bef.png)"
13325,'s0undt3ch','Traceback on creating many cloud instances\nThe following state creates 100 m3.xlarge EC2 instances . After it created 77:\r\n\r\n```\r\n$ salt-run state.sls env_qa1\r\n[WARNING ] Failed to spawn the VT: out of pty devices\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 198, in __init__\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 377, in _spawn\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 451, in __fork_ptys\r\n  File "/usr/lib/python2.7/pty.py", line 29, in openpty\r\n  File "/usr/lib/python2.7/pty.py", line 70, in _open_terminal\r\nOSError: out of pty devices\r\n[ERROR   ] Failed to upload file \'/tmp/.saltcloud/minion.pem\': Failed to spawn the VT. Error: out of pty devices\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/cloud.py", line 923, in scp_file\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 207, in __init__\r\nTerminalException: Failed to spawn the VT. Error: out of pty devices\r\n[WARNING ] Failed to spawn the VT: out of pty devices\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 198, in __init__\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 377, in _spawn\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 447, in __fork_ptys\r\n  File "/usr/lib/python2.7/pty.py", line 29, in openpty\r\n  File "/usr/lib/python2.7/pty.py", line 70, in _open_terminal\r\nOSError: out of pty devices\r\n[ERROR   ] Failed to execute command \'sudo chmod 600 /tmp/.saltcloud/minion.pem\': Failed to spawn the VT. Error: out of pty devices\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/cloud.py", line 1070, in root_cmd\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/vt.py", line 207, in __init__\r\nTerminalException: Failed to spawn the VT. Error: out of pty devices\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nOSError: [Errno 24] Too many open files: \'/usr/lib/pymodules/python2.7/salt/modules\'\r\nTraceback (most recent call last):\r\n  File "/usr/local/bin/salt-run", line 10, in <module>\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 95, in salt_run\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 362, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 218, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 125, in cmd\r\n  File "/usr/lib/pymodules/python2.7/salt/runners/state.py", line 85, in orchestrate\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 394, in sls\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1712, in call_high\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1436, in call_chunks\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1657, in call_chunk\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1412, in call\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 578, in load_modules\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 116, in minion_mods\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 650, in gen_functions\r\nOSError: [Errno 24] Too many open files: \'/usr/lib/pymodules/python2.7/salt/modules\'\r\nTraceback (most recent call last):\r\n  File "/usr/local/bin/salt-run", line 10, in <module>\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 95, in salt_run\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 362, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 218, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 125, in cmd\r\n  File "/usr/lib/pymodules/python2.7/salt/runners/state.py", line 85, in orchestrate\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 394, in sls\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1712, in call_high\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1436, in call_chunks\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1657, in call_chunk\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1412, in call\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 578, in load_modules\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 116, in minion_mods\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 650, in gen_functions\r\nOSError: [Errno 24] Too many open files: \'/usr/lib/pymodules/python2.7/salt/modules\'\r\nError in sys.excepthook:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/log/setup.py", line 663, in __global_logging_exception_handler\r\n  File "/usr/lib/python2.7/dist-packages/apport_python_hook.py", line 66, in apport_excepthook\r\nImportError: No module named fileutils\r\n\r\nOriginal exception was:\r\nTraceback (most recent call last):\r\n  File "/usr/local/bin/salt-run", line 10, in <module>\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 95, in salt_run\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 362, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 218, in run\r\n  File "/usr/lib/pymodules/python2.7/salt/runner.py", line 125, in cmd\r\n  File "/usr/lib/pymodules/python2.7/salt/runners/state.py", line 85, in orchestrate\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 394, in sls\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1712, in call_high\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1436, in call_chunks\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1657, in call_chunk\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1412, in call\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 578, in load_modules\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 116, in minion_mods\r\n  File "/usr/lib/pymodules/python2.7/salt/loader.py", line 650, in gen_functions\r\nOSError: [Errno 24] Too many open files: \'/usr/lib/pymodules/python2.7/salt/modules\'\r\n```\r\n\r\n```\r\n$ salt --versions-report\r\n           Salt: 2014.1.3\r\n         Python: 2.7.3 (default, Feb 27 2014, 19:58:35)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\nHost OS is Ubuntu 12.04.\r\n\r\nI can understand running out o PTYs, but a cleaner handling would be good.\r\n'
13249,'basepi','Run salt-ssh \'*\' test.ping fail because host password is full of numbers\nThe roster config is:\r\n\r\n```text\r\n[ray@localhost salt]$ cat /etc/salt/roster \r\nssh-vm109:\r\n  host: X.X.X.X\r\n  user: root\r\n  passwd: 123456789\r\n\r\n```\r\n\r\nRun salt-ssh \'*\' test.ping fail since of host passwd is like 123456789. salt thought it\'s a number, so len function hit exception\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 321, in handle_routine\r\n    stdout, stderr, retcode = single.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 572, in run\r\n    stdout, stderr, retcode = self.cmd_block()\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 696, in cmd_block\r\n    stdout, stderr, retcode = self.shell.exec_cmd(cmd)\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/shell.py", line 272, in exec_cmd\r\n    logmsg = logmsg.replace(self.passwd, (\'*\' * len(self.passwd))[:6])\r\nTypeError: object of type \'int\' has no len()\r\n```\r\n\r\nAnd salt version is:\r\n```text\r\n[ray@localhost salt]$ salt --versions-report\r\n           Salt: 2014.1.4\r\n         Python: 2.7.3 (default, Jul 24 2012, 11:41:34)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.4.2\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.3\r\n         PyYAML: 3.11\r\n          PyZMQ: 14.3.0\r\n            ZMQ: 4.0.4\r\n```\r\n\r\n'
13138,'basepi','\'docker.pulled\' unable to handle and pass along Docker image tags\nThis minimal working example:\r\n```\r\nprivate-registry-image:\r\n  docker.pulled:\r\n    - name: "registry:0.7.0"\r\n```\r\nraises the following error:\r\n```\r\n          ID: private-registry-image\r\n    Function: docker.pulled\r\n        Name: registry:0.7.0\r\n      Result: False\r\n     Comment: We did not get any expectable answer from docker\r\n              Traceback (most recent call last):\r\n                File "/usr/lib/pymodules/python2.7/salt/modules/dockerio.py", line 1627, in pull\r\n                  ret = client.pull(repo, tag=tag)\r\n                File "/usr/local/lib/python2.7/dist-packages/docker/client.py", line 620, in pull\r\n                  registry, repo_name = auth.resolve_repository_name(repository)\r\n                File "/usr/local/lib/python2.7/dist-packages/docker/auth/auth.py", line 55, in resolve_repository_name\r\n                  raise ValueError(\'Invalid repository name ({0})\'.format(repo_name))\r\n              ValueError: Invalid repository name (registry:0.7.0)\r\n     Changes:\r\n              ----------\r\n              registry:0.7.0:\r\n                  True\r\n```\r\n\r\nLooking at [states/dockerio.py#pulled@v2014.1.4](https://github.com/saltstack/salt/blob/v2014.1.4/salt/states/dockerio.py#L222) and [modules/dockerio.py#pull@v2014.1.4](https://github.com/saltstack/salt/blob/v2014.1.4/salt/modules/dockerio.py#L1566) there doesn\'t seem to be a way right now to pass both the image name **and its tag** from ``salt.states.dockerio.pulled`` to the ``salt.modules.dockerio.pull`` function.\r\n\r\nAlso, judging from [states/dockerio.py#pulled@v2014.1.4 docstring](https://github.com/saltstack/salt/blob/v2014.1.4/salt/states/dockerio.py#L237) there seems to be a documentation error:\r\n```\r\n    name\r\n        Tag of the image\r\n```\r\nwhere it probably meant to say "Image name".\r\n'
13137,'terminalmage',"states.file.serialize() tries to call nonexistent functions\n```\r\n************* Module salt.states.file\r\nsalt/states/file.py:3580: [E1101(no-member), serialize] Module 'salt.utils.serializers.yaml' has no 'load' member\r\nsalt/states/file.py:3582: [E1101(no-member), serialize] Module 'salt.utils.serializers.json' has no 'load' member\r\n```"
13065,'terminalmage',"states.npm.installed ignores `registry` argument\nFallout from #13033.\r\n```\r\n************* Module salt.states.npm\r\nsalt/states/npm.py:39: [W0613(unused-argument), installed] Unused argument 'registry'\r\n```\r\nThe relevant feature was added fairly recently:\r\n```\r\n    registry\r\n        The NPM registry from which to install the package\r\n\r\n        .. versionadded:: Helium\r\n```"
13045,'terminalmage',"mknod fails when pipe already exists\nWe're trying to create a pipe `fcgi.sock`.\r\n\r\nSalt state we specified is:\r\n\r\n```\r\n/var/run/myapp/fcgi.sock:\r\n  file.mknod:\r\n    - user: www-data\r\n    - group: www-data\r\n    - mode: 660\r\n    - ntype: p\r\n```\r\n\r\nThe first time the state runs it creates this socket correctly, but on subsequent runs it fails with the error:\r\n\r\n`File /var/run/myapp/fcgi.sock exists and cannot be overwritten`\r\n\r\ndespite the fact it should succeed, since the permissions for `fcgi.sock` are:\r\n\r\n`srw-rw----  1 www-data www-data    0 May 22 10:39 fcgi.sock`\r\n\r\n(cc @pwaller)"
13019,'UtahDave','Illegal characters on Windows filesystems\nA pkg.installed with an http location with a port defined causes a stacktrace on Windows when salt attempts to cache the file because it save the url as the path, which includes the \':\', which is illegal on ntfs\r\n\r\nHere\'s the stacktrace:\r\n```\r\ndev-app1.all.local:\r\n----------\r\n          ID: AllServices\r\n    Function: pkg.installed\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "salt/state.py", line 1378, in call\r\n                File "salt/states/pkg.py", line 518, in installed\r\n                File "salt/modules/win_pkg.py", line 540, in install\r\n                File "salt/modules/cp.py", line 310, in cache_file\r\n                File "salt/fileclient.py", line 143, in cache_file\r\n                File "salt/fileclient.py", line 521, in get_url\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 150, in makedirs\r\n                File "os.py", line 157, in makedirs\r\n              WindowsError: [Error 267] The directory name is invalid: \'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extrn_files\\\\base\\\\all-build.all.com:8080\'\r\n     Changes:\r\n```\r\n\r\nI was considering ways to deal with this. We could keep a list of common illegal characters and replace them with an underscore. The only problem there is keeping a list that would work across the various filesystems.\r\n\r\nA little googling gave me this:  http://stackoverflow.com/a/295150/114866\r\n\r\nI kind of like it.  It makes it filesystem safe and also is reversible. We wouldn\'t have the reversibility if we used md5, obviously.\r\n\r\nI haven\'t had a moment to dive into the code that lays down the files, but want to get some feedback.  Is either of the above solutions better? Should this be something that happens across all platforms, or should I just make this fix for Windows?\r\n\r\nThanks.'
12820,'terminalmage','repoquery command returning 0 results on systems with certain version of yum-utils\nAfter upgrading to 2014.1.3 I started noticing problems with some of my minions.  There were failing on any states that did anything with packages.  I ended up tracking it down to the repoquery command not returning any results.  Here is the specific command salt was running:\r\n\r\nrepoquery --queryformat="%{NAME}_|-%{VERSION}_|-%{RELEASE}_|-%{ARCH}_|-%{REPOID}" --all --pkgnarrow=installed\r\n\r\non a working system this would get you a list of RPMs.  On the systems failing you would get no output back.\r\n\r\nAll of my systems that were having issues were running yum-utils 1.1.16-14.  Upgrading to 1.1.16-21 fixed the issue for me and I now get the expected output from that repoquery command.  This was only on RHEL/Cent 5.x servers, it did not seem to effect any of my 6.x servers.'
12806,'UtahDave',"Can't query a sqlite DB in Windows minions\nI can't query a sqlite DB in all my Windows minions.\r\n\r\nThis seems to happen 'cause there is no bundled sqlite3 lib in the library.zip (c:\\salt\\salt-2014.1.0-5-g32d3463.win-amd64\\library.zip\\), maybe it has been forgotten.\r\n\r\nI don't know if this was on purpose."
12721,'rallytime',"restructure test cases and/or document CI logic\nAt present there is a rather large black box when it comes to the test case logic. \r\nThe test cases as run by Jenkins seem more monolithic and intertwined than necessary and this makes it hard to see the correct way to add new tests into the integration test suite which is of particular importance for a tool like salt. \r\n\r\nId like to either see the CI logic documented sufficiently that it is clear how to add new integration test steps for new test cases into the jenkins CI workflow. Or restructured so as to be far more obvious than it presently is. For instance I was unable to determine the correct way to augment the test suite to add new tests to prevent future regression of my changes in #11337. There is also a large lack of integration tests for application interoperability with managed software beyond install and service. Many of the module function issues currently reported as issues could be more effectively tackled and prevented from future breakage by making it easier for users to contribute test cases for these.  In the very least, improving the ability to add new test cases would allow contributors to 'prove working' a number of old unanswered issues and clean up any uncertainty regarding module/state functionality that may result from outstanding bugs that are actually fixed but not yet acknowledged as fixed by the original submitter.\r\n\r\nPossibly should be tagged in #12446 with regards to improving the test case documentation. "
12696,'terminalmage','Setting timezone in Ubuntu 14.04 fails\nTrying to set the timezone on a server that is updated from 12.04 to 14.04 seems to fail:\r\n```\r\nsalt \'saltminion\' state.sls ntp.timezone\r\nsaltminion:\r\n----------\r\n          ID: set-timezone\r\n    Function: timezone.system\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib/pymodules/python2.7/salt/state.py", line 1378, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib/pymodules/python2.7/salt/states/timezone.py", line 56, in system\r\n                  compzone = __salt__[\'timezone.zone_compare\'](name)\r\n                File "/usr/lib/pymodules/python2.7/salt/modules/timezone.py", line 157, in zone_compare\r\n                  with salt.utils.fopen(zonepath, \'r\') as fp_:\r\n                File "/usr/lib/pymodules/python2.7/salt/utils/__init__.py", line 1054, in fopen\r\n                  fhandle = open(*args, **kwargs)\r\n              IOError: [Errno 2] No such file or directory: \'/usr/share/zoneinfo/set-timezone\'\r\n     Changes:   \r\n\r\n\r\nsaltserver# cat /salt/base/ntp/timezone.sls:\r\nset-timezone:\r\n  timezone.system:\r\n    - name: {{ salt[\'pillar.get\']("site.timezone") }}\r\n    - utc: True\r\n\r\nsaltminion# salt-call pillar.get site:timezone\r\nlocal:\r\n    Europe/Oslo\r\n```\r\nIt seems as the modules work.\r\n```\r\nsaltserver# salt \'saltminion\' timezone.set_zone Europe/Oslo\r\nsaltminion:\r\n    True\r\n\r\nsaltserver# salt \'saltminion\' timezone.get_zone\r\nsaltminion:\r\n    Europe/Oslo\r\n```'
12651,'rallytime','file.append error\nsalt version: salt 2014.1.3\r\ni write sls like this:\r\n```\r\n/tmp/foo:\r\n    file.append:\r\n        - text:\r\n            - abcd\r\n            - 23456\r\n```\r\n\r\nwhen i run state.sls, i got error:\r\n\r\n----------\r\n          ID: /tmp/foo\r\n    Function: file.append\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib64/python2.7/site-packages/salt/state.py", line 1378, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib64/python2.7/site-packages/salt/states/file.py", line 2537, in append\r\n                  name, salt.utils.build_whitespace_split_regex(chunk)):\r\n                File "/usr/lib64/python2.7/site-packages/salt/utils/__init__.py", line 788, in build_whitespace_split_regex\r\n                  for line in text.splitlines():\r\n              AttributeError: \'int\' object has no attribute \'splitlines\'\r\n     Changes:\r\n\r\n'
12647,'pass-by-value','Two sources for ext_pillar cause minion to never end highstate\nIf I add two sources for ext_pillar like\r\n\r\n```\r\next_pillar:\r\n  - git: master https://git.example.com/one/saltstack-pillars.git\r\n  - git: master https://git.example.com/two/saltstack-pillars.git\r\n```\r\n\r\nminions never complete state.highstate. I use it to give another department in our company the opportunity to host their pillars in another repository.\r\n\r\nVersionreport for master and minion:\r\n\r\n```\r\nmaster:\r\n\r\n           Salt: 2014.1.3\r\n         Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n\r\nminion:\r\n\r\n           Salt: 2014.1.0\r\n         Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n```\r\n\r\nRegards,\r\n\r\nErwin Schliske'
12572,'terminalmage','Salt-call state.highstate -l debug is too noisy on yum-based systems due to repoquery\nRunning ```salt-call state.highstate --local -l debug``` on a yum-based OS displays every single available package in the yum repositories.\r\nThis is because the command ```repoquery --queryformat="%{NAME}_|-%{ARCH}" --pkgnarrow=all --all``` is being executed on every run. With debug enabled it salt-call shows the output of that command.\r\n\r\nWith just the default CentOS 6.5 repositories enabled that results in 17490 lines of extra output. I had to modify the scrollback length of my terminal emulator to even accommodate for it. I saved those 17490 lines into a file, it\'s 408 Kilobytes. [Scroll through it](https://gist.github.com/rasschaert/837340dfa62b8db43714#file-repoquery-log) and see for yourself how absolutely massive and pointless it is.\r\n\r\nI know that debug output by it\'s very nature is supposed to be noisy, but the noise to signal ratio here is staggering. How is that list of 17490 packages ever useful to anyone? Has anyone ever needed or used that particular section of the output?\r\n\r\nI believe an ocean of text like that is detrimental to actually being able to debug anything. IMHO those 17k lines of repoquery output have no place in the debug output. Curious to hear other views on this.\r\n\r\n---\r\n\r\nFiled this issue at the request of @basepi after posting the same thing in [this thread](https://groups.google.com/d/topic/salt-users/qCSOOl-Sf5k/discussion) on the Google group\r\n'
12505,'terminalmage','Better documentation for outputters\nIf I create my minion\'s /etc/salt/grains file with format in the salt documentation http://docs.saltstack.com/en/latest/topics/targeting/grains.html they don\'t get processed by jinja in my sls file. \r\n\r\nI\'m using salt-master/salt-minion version 2014.1.0.\r\n\r\n**sls file**\r\n```\r\n/etc/motd:\r\n    file.managed:\r\n        {% if grains[\'role\'] == \'lh\' %}\r\n        - source:\r\n            - salt://misc/motd.LH\r\n        - user: root\r\n        - group: root\r\n        - mode: 644\r\n        {% endif %}\r\n```\r\n\r\n**/etc/salt/grains that does NOT work**\r\n```\r\nrole: \r\n  - lh\r\n```\r\n\r\n**Output of grains.item**\r\n```\r\nDev: root@ misc]$ salt \'Host10000.com\' grains.item role\r\nHost10000.com:\r\n  role:\r\n      lh\r\n```\r\n\r\n**Result**\r\n```\r\nDev: root@ misc]$ salt \'Host10000.com\' state.sls misc\r\nHost10000.com:\r\n    Data failed to compile:\r\n----------\r\n    The state "/etc/motd" in sls misc is not formed as a list\r\n```\r\n\r\n**/etc/salt/grains file that DOES work**\r\nSetting the grain using grains.setval role lh creates the following working /etc/salt/grains file.\r\n \r\n```\r\nrole: lh\r\n```\r\n\r\n**Summary**\r\n- The output of grains.item looks the same with both styles of grains file\r\n- I can target minions by grains in my top.sls file successfully with either grains file\r\n- grains.setval role lh creates a working grains file\r\n- It only breaks when using jinja\r\n\r\n\r\n'
12464,'rallytime',"Test outputters directly\nWe need to have tests to make sure that all the outputter return their expected types of data. We probably don't need to bother with heavily-mocked unit tests since we're just wrapping libraries for YAML and JSON and such but we do need to test that something like a ```test.ping``` is returned with the proper output. Really simple integration tests are likely just fine here. This will help us catch issues wherein, for example, the loader might not be giving us back the right outputter even though we've requested it."
12444,'cachedout','--out json returns yaml\nMy develop branch is currently at 044dc215996a4508172cd96cf2c6eb1ccd3088b8 and when I use --out json it return yaml.\r\n\r\nSeems some basic tests on the outputters would be nice :)'
12388,'techhat','Salt Cloud can\'t delete vms from providers whose drivers don\'t have "optimize_providers" function\nRunning salt from develop I can successfully create a vm, but cannot delete a vm when using providers whose drivers don\'t have an "optimize_providers" function.\r\n\r\nMy server is an Ubuntu 12.04 vm:\r\n```\r\nroot@boucha:/etc/salt# salt --versions-report\r\n           Salt: 2014.1.0-5374-g871d218 (Hydrogen)\r\n         Python: 2.7.3 (default, Feb 27 2014, 19:58:35)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.22\r\n msgpack-python: 0.4.2\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.11\r\n          PyZMQ: 14.1.1\r\n            ZMQ: 4.0.4\r\n```\r\nProof of a vm on DigitalOcean:\r\n```\r\nroot@boucha:/etc/salt# salt davedocker100 test.ping\r\ndavedocker100:\r\n    True\r\n```\r\nAn attempt to delete the vm:\r\n```\r\nroot@boucha:/etc/salt# salt-cloud -d davedocker100\r\n[INFO    ] salt-cloud starting\r\n[ERROR   ] Public cloud provider openstack is not available\r\n[ERROR   ] Public cloud provider digital_ocean is not available\r\n[INFO    ] Starting new HTTPS connection (1): ec2.us-east-1.amazonaws.com\r\nNo machines were found to be destroyed\r\n```\r\nAn attempt to delete the vm in debug mode:\r\n```\r\nroot@boucha:/etc/salt# salt-cloud -d davedocker100 -l debug\r\n[DEBUG   ] Reading configuration from /etc/salt/cloud\r\n[DEBUG   ] Reading configuration from /etc/salt/master\r\n[DEBUG   ] Reading configuration from /etc/salt/cloud.providers\r\n[DEBUG   ] Missing configuration file: /etc/salt/cloud.profiles\r\n[DEBUG   ] Including configuration from \'/etc/salt/cloud.profiles.d/aws.conf\'\r\n[DEBUG   ] Reading configuration from /etc/salt/cloud.profiles.d/aws.conf\r\n[DEBUG   ] Including configuration from \'/etc/salt/cloud.profiles.d/do.conf\'\r\n[DEBUG   ] Reading configuration from /etc/salt/cloud.profiles.d/do.conf\r\n[DEBUG   ] Including configuration from \'/etc/salt/cloud.profiles.d/rackspace.conf\'\r\n[DEBUG   ] Reading configuration from /etc/salt/cloud.profiles.d/rackspace.conf\r\n\r\n<-- snip -->\r\n\r\n[DEBUG   ] The \'openstack\' cloud driver is unable to be optimized.\r\n[DEBUG   ] The \'digital_ocean\' cloud driver is unable to be optimized.\r\n[ERROR   ] Public cloud provider openstack is not available\r\n[ERROR   ] Public cloud provider digital_ocean is not available\r\n\r\n<-- snip -->\r\n[DEBUG   ] EC2 Request: https://ec2.us-east-1.amazonaws.com/\r\n[INFO    ] Starting new HTTPS connection (1): ec2.us-east-1.amazonaws.com\r\n<-- snip -->\r\n[DEBUG   ] EC2 Response Status Code: 200\r\nNo machines were found to be destroyed\r\n```'
12205,'whiteinge','List version number with version codename in documentation and with --version\nA lot of documentation shows: "Available since Hydrogen" or something similar. This is nice to know, but it\'s confusing to link the version number with the version codename, especially since salt/salt-call --version shows the version number and not the codename. When referencing either, both should be used to reduce confusion.'
12167,'UtahDave',"Does not honor master IP address and minion file seems corrupt after install.\nI've upgraded some of our Windows based 2014.1.0 and 2014.1.1 minions to 2014.1.3 from the following site using the Salt-Minion-2014.1.3-AMD64-Setup.exe installer:\r\nhttp://docs.saltstack.com/en/latest/topics/installation/windows.html\r\n\r\nWhen doing so, the installer does not honor the master IP given to it during the install.  I've tried with the /S /m=IPADDRESS and during the GUI install.  The c:\\salt\\conf\\minion file always contains master: salt on all of the servers I've tried.  I also tried a fresh install with the same result.  \r\n\r\nAlso, the carriage returns are no longer the same in the minion conf file.  I'm not sure exactly how to explain it, except that it's corrupt on every install/upgrade I've done so far.  So, even if I manually replace master: salt with master: MYIPADDRESS and restart the salt-minion service, it never talks to the salt master.  If I manually copy a minion file from a previous install and restart the salt-minion service, everything works fine again.\r\n"
12130,'cachedout',"file.append requires file to exist\nI'm using salt 2014.1.1-1precise1 on ubuntu 12.04.\r\n\r\nI want to append an ssh host key to a known_hosts file, but salt won't let me if the known_hosts file doesn't already exist:\r\n\r\n```\r\n$ salt 'append-test' state.single file.append name=/root/.ssh/known_hosts contents_pillar=github:hostkey\r\nappend-test:\r\n----------\r\n          ID: /root/.ssh/known_hosts\r\n    Function: file.append\r\n      Result: False\r\n     Comment: /root/.ssh/known_hosts: file not found\r\n     Changes:   \r\n\r\nSummary                                                                                    \r\n------------                                                                               \r\nSucceeded: 0\r\nFailed:    1\r\n------------\r\nTotal:     1\r\n```\r\n\r\nI was able to work around it by requiring file.touch first, but that is not ideal."
12098,'cachedout','Pillar overlay variables are inconsistently returned\nI have come across what appears to be a very serious bug in the way pillar overlays work.\r\n\r\nWe use a pillar structure similar to this:\r\n\r\n    # this contains "global" variables used throughout all regions\r\n    /srv/pillar/global/data.sls \r\n\r\n    # these contain variables specific to each region, sometimes \r\n    # overriding variables set in the global data.sls\r\n    /srv/pillar/iad3/data.sls\r\n    /srv/pillar/lon3/data.sls\r\n    /srv/pillar/hkg1/data.sls\r\n    /srv/pillar/ord1/data.sls\r\n    /srv/pillar/dfw2/data.sls\r\n    /srv/pillar/syd2/data.sls\r\n\r\nOur `/srv/pillar/top.sls` file looks like this:\r\n\r\n    base:\r\n      \'*\':\r\n        - global.data\r\n      {% for dc in \'dfw2\', \'hkg1\', \'iad3\', \'lon3\', \'ord1\', \'syd2\' %}\r\n      \'dc:{{ dc }}\':\r\n        - match: grain\r\n        - {{ dc }}.data\r\n      {% endfor %}\r\n\r\nThis was working wonderfully for us for our first few regions.  Pillar would load variables set in `global/data.sls` first, then override them with whatever was set in that regions `data.sls` file.  However, recently we expanded our salt configuration to include the full 6 regions listed in the pillar topfile, and that is when we began to notice the problem.\r\n\r\nIt boils down to:\r\n\r\n* If a variable is defined in `global/data.sls`, but not in `$region/data.sls`, then the global variable is used.\r\n* If a variable is not defined in `global/data.sl`s, but is defined in `$region/data.sls`, then region specific variable is used.\r\n* If a variable is defined in both `global/data.sls` and `$region/data.sls`, then in some cases the global variable is used, and other cases the region\'s variable is used. I can not find a reason why this happens only for certain regions.\r\n\r\nI have been able to reproduce this using Salt versions `2014.1.0-1precise1`, `2014.1.1-1precise1`, and `salt-2014.1.3.tar.gz` (via pip). In Salt 2014.1.3 the behavior changed again, as I\'ll show below.\r\n\r\nI have created a git repository with all files needed to reproduce this issue: https://github.com/corywright/salt-pillar-overlay-bug\r\n\r\nTo demonstrate this issue I created 7 Ubuntu Precise 12.04 instances, 1 as the salt-master and the other 6 with names similar to hostnames we use in production:\r\n\r\n    salt-master1.control.test.corywright.org\r\n    minion1.hkg1.test.corywright.org\r\n    minion1.lon3.test.corywright.org\r\n    minion1.ord1.test.corywright.org\r\n    minion1.iad3.test.corywright.org\r\n    minion1.dfw2.test.corywright.org\r\n    minion1.syd2.test.corywright.org\r\n\r\nIn our production environments we use the minion\'s hostname to indicate what region a server is in (the second label in the hostname, dfw2, hkg1, etc). A custom grain is used to retrieve this, and also to dictate which pillar files are served to that minion. The naming style of the test nodes is important in order for the rest of these steps to illustrate the bug.\r\n\r\nTo test 2014.1.0 and 2014.1.1 I used the [standard Ubuntu installation instructions](http://docs.saltstack.com/en/latest/topics/installation/ubuntu.html) to install salt-master on the salt-master1 node, and salt-minion on each minion node. To test 2014.1.3 I installed Salt using pip.\r\n\r\nI then cloned my [salt-pillar-overlay-bug](https://github.com/corywright/salt-pillar-overlay-bug) github repository into root\'s home directory on the salt-master1 server, and configured it to serve my files:\r\n\r\n    root@salt-master1:~# git clone https://github.com/corywright/salt-pillar-overlay-bug.git\r\n    root@salt-master1:~# ln -s /root/salt-pillar-overlay-bug/salt/ /srv/\r\n    root@salt-master1:~# ln -s /root/salt-pillar-overlay-bug/pillar/ /srv/\r\n    root@salt-master1:~# service salt-master restart\r\n\r\nNext I added the salt-master1\'s ip to `/etc/salt/minion.d/master.conf` and restarted salt-minion on each of the 6 minion servers. Once salt-minion was restarted I was able to accept all of the keys on salt-master1:\r\n\r\n    root@salt-master1:~# salt-key -L\r\n    Accepted Keys:\r\n    minion1.dfw2.test.corywright.org\r\n    minion1.hkg1.test.corywright.org\r\n    minion1.iad3.test.corywright.org\r\n    minion1.lon3.test.corywright.org\r\n    minion1.ord1.test.corywright.org\r\n    minion1.syd2.test.corywright.org\r\n    Unaccepted Keys:\r\n    Rejected Keys:\r\n\r\nFirst I tested that I could communicate with each minion:\r\n\r\n    root@salt-master1:~# salt --out=txt \'*\' test.ping\r\n    minion1.lon3.test.corywright.org: True\r\n    minion1.hkg1.test.corywright.org: True\r\n    minion1.syd2.test.corywright.org: True\r\n    minion1.ord1.test.corywright.org: True\r\n    minion1.iad3.test.corywright.org: True\r\n    minion1.dfw2.test.corywright.org: True\r\n\r\nThen I updated the pillar data that would be used to illustrate the bug:\r\n\r\n    root@salt-master1:~# salt --out=txt \'*\' saltutil.refresh_pillar\r\n    minion1.lon3.test.corywright.org: None\r\n    minion1.hkg1.test.corywright.org: None\r\n    minion1.syd2.test.corywright.org: None\r\n    minion1.ord1.test.corywright.org: None\r\n    minion1.dfw2.test.corywright.org: None\r\n    minion1.iad3.test.corywright.org: None\r\n\r\nNext I updated the custom grain that is used to detect the server\'s region:\r\n\r\n    root@salt-master1:~# salt --out=txt \'*\' saltutil.sync_all\r\n    minion1.lon3.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n    minion1.iad3.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n    minion1.dfw2.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n    minion1.hkg1.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n    minion1.ord1.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n    minion1.syd2.test.corywright.org: {\'outputters\': [], \'grains\': [\'grains.region\'], \'returners\': [], \'modules\': [], \'renderers\': [], \'states\': []}\r\n\r\nTest that the grains are working properly:\r\n\r\n    root@salt-master1:~# salt --out=txt \'*\' grains.item dc\r\n    minion1.hkg1.test.corywright.org: {\'dc\': \'hkg1\'}\r\n    minion1.lon3.test.corywright.org: {\'dc\': \'lon3\'}\r\n    minion1.iad3.test.corywright.org: {\'dc\': \'iad3\'}\r\n    minion1.ord1.test.corywright.org: {\'dc\': \'ord1\'}\r\n    minion1.syd2.test.corywright.org: {\'dc\': \'syd2\'}\r\n    minion1.dfw2.test.corywright.org: {\'dc\': \'dfw2\'}\r\n\r\nA quick glance at the `pillar/global/data.sls` data in the git repository will show the global variables I\'ve created for this test:\r\n\r\n    # pillar/global/data.sls\r\n    defined_only_in_global: global\r\n    defined_in_both_global_and_region: global\r\n\r\nAlso look at the variables that are in each region\'s `data.sls` file:\r\n\r\n    # pillar/dfw2/data.sls\r\n    defined_in_both_global_and_region: dfw2\r\n    defined_only_in_region: dfw2\r\n\r\nAt this point we can now test the pillar data to see what is being returned.  \r\n\r\nData that is only defined in the global `data.sls` file is returned as expected:\r\n\r\n    # behaves the same in all three versions I tested, 2014.1.0, 2014.1.1, and 2014.1.3\r\n    root@salt-master1:~# salt --out=txt \'*\' pillar.get defined_only_in_global\r\n    minion1.hkg1.test.corywright.org: global\r\n    minion1.dfw2.test.corywright.org: global\r\n    minion1.ord1.test.corywright.org: global\r\n    minion1.iad3.test.corywright.org: global\r\n    minion1.syd2.test.corywright.org: global\r\n    minion1.lon3.test.corywright.org: global\r\n\r\nAs is the data that was defined in each regions `data.sls` file, for version 2014.1.0 and 2014.1.1:\r\n\r\n    # for 2014.1.0 and 2014.1.1, see the next section for 2014.1.3 results\r\n    root@salt-master1:~# salt --out=txt \'*\' pillar.get defined_only_in_region\r\n    minion1.hkg1.test.corywright.org: hkg1\r\n    minion1.lon3.test.corywright.org: lon3\r\n    minion1.dfw2.test.corywright.org: dfw2\r\n    minion1.iad3.test.corywright.org: iad3\r\n    minion1.syd2.test.corywright.org: syd2\r\n    minion1.ord1.test.corywright.org: ord1\r\n\r\n    # 2014.1.3 returns nothing at all\r\n    root@salt-master1:~# salt --out=txt \'*\' pillar.get defined_only_in_region\r\n    root@salt-master1:~# salt \'*\' pillar.get defined_only_in_region\r\n    minion1.ord1.test.corywright.org:\r\n    \r\n    minion1.syd2.test.corywright.org:\r\n    \r\n    minion1.lon3.test.corywright.org:\r\n    \r\n    minion1.hkg1.test.corywright.org:\r\n    \r\n    minion1.dfw2.test.corywright.org:\r\n    \r\n    minion1.iad3.test.corywright.org:\r\n\r\nOddly enough, the `defined_only_in_region` variable does appear in pillar.items output and the values are consistent with those found in the output from 2014.1.0 and 2014.1.1:\r\n\r\n    root@salt-master1:~# salt \'*\' pillar.items\r\n    minion1.ord1.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            ord1\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            ord1\r\n    [...]\r\n    minion1.lon3.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            global\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            lon3\r\n    [...]\r\n    minion1.syd2.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            global\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            syd2\r\n    [...]\r\n    minion1.hkg1.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            hkg1\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            hkg1\r\n    [...]\r\n    minion1.iad3.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            iad3\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            iad3\r\n    [...]\r\n    minion1.dfw2.test.corywright.org:\r\n        ----------\r\n        defined_in_both_global_and_region:\r\n            dfw2\r\n        defined_only_in_global:\r\n            global\r\n        defined_only_in_region:\r\n            dfw2\r\n    [...]\r\n\r\nWhen we ask for the value of the `defined_in_both_global_and_region` pillar variable we see that it inconsistently returns regional data sometimes, and global data others:\r\n\r\n    # for 2014.1.0 and 2014.1.1, see the next section for 2014.1.3 results\r\n    root@salt-master1:~# salt --out=txt \'*\' pillar.get defined_in_both_global_and_region\r\n    minion1.hkg1.test.corywright.org: hkg1\r\n    minion1.lon3.test.corywright.org: global\r\n    minion1.ord1.test.corywright.org: ord1\r\n    minion1.iad3.test.corywright.org: iad3\r\n    minion1.dfw2.test.corywright.org: dfw2\r\n    minion1.syd2.test.corywright.org: global\r\n\r\nIn Salt 2014.1.3 it\'s even worse in that it never returns the region\'s variable that should be overlaid on top of the global definitions, instead always returning the global value:\r\n\r\n    root@salt-master1:~# salt --out=txt \'*\' pillar.get defined_in_both_global_and_region\r\n    minion1.lon3.test.corywright.org: global\r\n    minion1.dfw2.test.corywright.org: global\r\n    minion1.iad3.test.corywright.org: global\r\n    minion1.hkg1.test.corywright.org: global\r\n    minion1.syd2.test.corywright.org: global\r\n    minion1.ord1.test.corywright.org: global\r\n\r\n    root@salt-master1:~# salt --versions-report\r\n               Salt: 2014.1.3\r\n             Python: 2.7.3 (default, Feb 27 2014, 19:58:35)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.4.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.0.0\r\n                ZMQ: 3.2.2\r\n\r\nThis is a major problem for us as we use the overlays to define global variables that are used in most environments, and then override them in regions that deviate from standard variables.  I\'ve had to revert to defining all variables in all regions until we can get this issue fixed.\r\n\r\nI was waiting for a new release to test this again in case it was fixed. But 2014.1.3 has now been cut and the issue is still present.\r\n'
12086,'UtahDave','salt-minion in restart loop\nRunning salt-minion 2014.1.1 on our Windows servers. Over the last couple of days the minions on random different servers are breaking and in a constant service restart loop. So far I have seen it on Windows 2008 R2 64-bit and 32-bit servers. Not seen it yet on Windows 2003 R2 or Windows 2012 R2.\r\n\r\nThis is what I see in the event viewer. It repeats every 10 seconds. It has taken my Windows server out a couple of times and made them inoperable so I have to disable the salt-minion service on the Windows servers until further notice. When it is in the restart loop the salt-minion does not respond to test.pings from the master.\r\n\r\n```\r\n           Salt: 2014.1.1\r\n         Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.4.2\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.1.1\r\n            ZMQ: 4.0.4\r\n\r\n\r\nLog Name:      Application\r\nSource:        nssm\r\nDate:          4/17/2014 5:17:46 PM\r\nEvent ID:      1013\r\nTask Category: None\r\nLevel:         Information\r\nKeywords:      Classic\r\nUser:          N/A\r\nComputer:      QTS-MBX1-1.global.local\r\nDescription:\r\nProgram c:\\salt\\salt-minion.exe for service salt-minion exited with return code 1.\r\nEvent Xml:\r\n<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">\r\n  <System>\r\n    <Provider Name="nssm" />\r\n    <EventID Qualifiers="16384">1013</EventID>\r\n    <Level>4</Level>\r\n    <Task>0</Task>\r\n    <Keywords>0x80000000000000</Keywords>\r\n    <TimeCreated SystemTime="2014-04-17T17:17:46.000Z" />\r\n    <EventRecordID>304248336</EventRecordID>\r\n    <Channel>Application</Channel>\r\n    <Computer>QTS-MBX1-1.global.local</Computer>\r\n    <Security />\r\n  </System>\r\n  <EventData>\r\n    <Data>c:\\salt\\salt-minion.exe</Data>\r\n    <Data>salt-minion</Data>\r\n    <Data>1</Data>\r\n  </EventData>\r\n</Event>\r\nLog Name:      Application\r\nSource:        nssm\r\nDate:          4/17/2014 5:17:46 PM\r\nEvent ID:      1023\r\nTask Category: None\r\nLevel:         Information\r\nKeywords:      Classic\r\nUser:          N/A\r\nComputer:      QTS-MBX1-1.global.local\r\nDescription:\r\nKilling process tree of process 8672 for service salt-minion with exit code 1\r\nEvent Xml:\r\n<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">\r\n  <System>\r\n    <Provider Name="nssm" />\r\n    <EventID Qualifiers="16384">1023</EventID>\r\n    <Level>4</Level>\r\n    <Task>0</Task>\r\n    <Keywords>0x80000000000000</Keywords>\r\n    <TimeCreated SystemTime="2014-04-17T17:17:46.000Z" />\r\n    <EventRecordID>304248337</EventRecordID>\r\n    <Channel>Application</Channel>\r\n    <Computer>QTS-MBX1-1.global.local</Computer>\r\n    <Security />\r\n  </System>\r\n  <EventData>\r\n    <Data>salt-minion</Data>\r\n    <Data>8672</Data>\r\n    <Data>1</Data>\r\n  </EventData>\r\n</Event>\r\nLog Name:      Application\r\nSource:        nssm\r\nDate:          4/17/2014 5:17:46 PM\r\nEvent ID:      1027\r\nTask Category: None\r\nLevel:         Information\r\nKeywords:      Classic\r\nUser:          N/A\r\nComputer:      QTS-MBX1-1.global.local\r\nDescription:\r\nKilling PID 8672 in process tree of PID 8672 because service salt-minion is stopping.\r\nEvent Xml:\r\n<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">\r\n  <System>\r\n    <Provider Name="nssm" />\r\n    <EventID Qualifiers="16384">1027</EventID>\r\n    <Level>4</Level>\r\n    <Task>0</Task>\r\n    <Keywords>0x80000000000000</Keywords>\r\n    <TimeCreated SystemTime="2014-04-17T17:17:46.000Z" />\r\n    <EventRecordID>304248338</EventRecordID>\r\n    <Channel>Application</Channel>\r\n    <Computer>QTS-MBX1-1.global.local</Computer>\r\n    <Security />\r\n  </System>\r\n  <EventData>\r\n    <Data>8672</Data>\r\n    <Data>8672</Data>\r\n    <Data>salt-minion</Data>\r\n  </EventData>\r\n</Event>\r\nLog Name:      Application\r\nSource:        nssm\r\nDate:          4/17/2014 5:17:46 PM\r\nEvent ID:      1014\r\nTask Category: None\r\nLevel:         Information\r\nKeywords:      Classic\r\nUser:          N/A\r\nComputer:      QTS-MBX1-1.global.local\r\nDescription:\r\nService salt-minion action for exit code 1 is Restart. Attempting to restart c:\\salt\\salt-minion.exe.\r\nEvent Xml:\r\n<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">\r\n  <System>\r\n    <Provider Name="nssm" />\r\n    <EventID Qualifiers="16384">1014</EventID>\r\n    <Level>4</Level>\r\n    <Task>0</Task>\r\n    <Keywords>0x80000000000000</Keywords>\r\n    <TimeCreated SystemTime="2014-04-17T17:17:46.000Z" />\r\n    <EventRecordID>304248339</EventRecordID>\r\n    <Channel>Application</Channel>\r\n    <Computer>QTS-MBX1-1.global.local</Computer>\r\n    <Security />\r\n  </System>\r\n  <EventData>\r\n    <Data>salt-minion</Data>\r\n    <Data>1</Data>\r\n    <Data>Restart</Data>\r\n    <Data>c:\\salt\\salt-minion.exe</Data>\r\n  </EventData>\r\n</Event>\r\nLog Name:      Application\r\nSource:        nssm\r\nDate:          4/17/2014 5:17:47 PM\r\nEvent ID:      1008\r\nTask Category: None\r\nLevel:         Information\r\nKeywords:      Classic\r\nUser:          N/A\r\nComputer:      QTS-MBX1-1.global.local\r\nDescription:\r\nStarted c:\\salt\\salt-minion.exe -c c:\\salt\\conf -l quiet for service salt-minion in c:\\salt.\r\nEvent Xml:\r\n<Event xmlns="http://schemas.microsoft.com/win/2004/08/events/event">\r\n  <System>\r\n    <Provider Name="nssm" />\r\n    <EventID Qualifiers="16384">1008</EventID>\r\n    <Level>4</Level>\r\n    <Task>0</Task>\r\n    <Keywords>0x80000000000000</Keywords>\r\n    <TimeCreated SystemTime="2014-04-17T17:17:47.000Z" />\r\n    <EventRecordID>304248340</EventRecordID>\r\n    <Channel>Application</Channel>\r\n    <Computer>QTS-MBX1-1.global.local</Computer>\r\n    <Security />\r\n  </System>\r\n  <EventData>\r\n    <Data>c:\\salt\\salt-minion.exe</Data>\r\n    <Data>-c c:\\salt\\conf -l quiet</Data>\r\n    <Data>salt-minion</Data>\r\n    <Data>c:\\salt</Data>\r\n  </EventData>\r\n</Event>\r\n```'
12033,'s0undt3ch',"Update to cloud profile/provider extends docs\nIt turns out that extend isn't recursive. So, you can't do:\r\n\r\n```\r\np1:\r\n  blah: thing\r\n\r\np2:\r\n  extend: p1\r\n\r\np3:\r\n  extend: p2\r\n```\r\n\r\nAdditionally, there's no mention of the behaviour around merging lists etc. I believe the behaviour is to override the exiting data, but it's not clear.\r\n\r\nIt's probably worth adding a small note about both of these to the docs.\r\n\r\nhttp://salt-cloud.readthedocs.org/en/latest/topics/config.html#extending-profiles\r\n"
12027,'whiteinge','Switch Sphinx search to Google Site Search\nSwitch the Sphinx JavaScript-based search over to a Google Site Search.\r\n\r\nOr investigate the findanything extension:\r\n\r\nhttps://bitbucket.org/klorenz/sphinxcontrib-findanything\r\n\r\nRefs: #12015'
11934,'techhat','Salt Cloud stacktraces if specified minion name already exists in cloud\nMy salt version is the latest develop branch:\r\n```\r\nroot@boucha:~/salt# salt --version\r\nsalt 2014.1.0-4332-gaa75521\r\n```\r\n\r\nI ran this command the first time successfully. The second time I got this stacktrace. \r\n```\r\nroot@boucha:~/salt# salt-cloud -p fedora_ec2 utahdave1\r\n[WARNING ] The profile \'ubuntu_ec2\' is defining \'class-ec2\' as the provider. Since there\'s no valid configuration for that provider, the profile will be removed from the available listing\r\n[WARNING ] The profile \'aws_micro\' is defining \'demo-aws\' as the provider. Since there\'s no valid configuration for that provider, the profile will be removed from the available listing\r\n[INFO    ] salt-cloud starting\r\n[ERROR   ] Public cloud provider openstack is not available\r\n[INFO    ] Starting new HTTPS connection (1): api.digitalocean.com\r\n[INFO    ] Starting new HTTPS connection (1): ec2.us-east-1.amazonaws.com\r\n[ERROR   ] There was a profile error: \'bool\' object has no attribute \'__getitem__\'\r\nTraceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/dist-packages/salt/cloud/cli.py", line 227, in run\r\n    self.config.get(\'names\')\r\n  File "/usr/local/lib/python2.7/dist-packages/salt/cloud/__init__.py", line 1197, in run_profile\r\n    if name in vms and vms[name][\'state\'].lower() != \'terminated\':\r\nTypeError: \'bool\' object has no attribute \'__getitem__\'\r\n```'
11914,'cachedout','salt-run jobs.active code is not cover full conditions\nSalt Job Management contain the cmd.run Job and other style job, such as state.highstate job. When salt run the active job as following:\r\n\r\nMinion A:\r\n    ----------\r\n    - arg:\r\n    - fun: xx\r\n    - jid: xx\r\n    - pid: xx\r\n    - ret: xx\r\n    - tgt: xx\r\n    - tgt_type: xx\r\n    - user: xx\r\n\r\nMinion B:\r\n    ----------\r\n    - fun:\r\n        state.highstate\r\n    - id:\r\n        Minion B\r\n    - jid:\r\n        20140325184455827609\r\n    - pid:\r\n        22770\r\n\r\nThey are in different style ,and the source code write in only one:\r\n” /usr/lib/python2.6/site-packages/salt/runners/jobs.py “\r\n*************************************************************************************\r\nret[job[\'jid\']] = {\'Running\': [],\r\n                                   \'Returned\': [],\r\n                                   \'Function\': job[\'fun\'],\r\n                                   \'Arguments\': list(job[\'arg\']),\r\n                                   \'Target\': job[\'tgt\'],\r\n                                   \'Target-type\': job[\'tgt_type\'],\r\n                                   \'User\': job.get(\'user\', \'root\')}\r\n*************************************************************************************\r\n\r\nIn this Situation, if I try "salt-run jobs.active", salt will throw an exception in expect:\r\n……\r\n  File "/usr/lib/python2.6/site-packages/salt/runners/jobs.py", line 40, in active\r\n    \'Arguments\': list(job[\'arg\']),\r\nKeyError: \'arg\'\r\n\r\nThks~'
11880,'rallytime','seed.apply throws a stack trace\nWhenever using the `virt.init` runner/module I get the following error from the underlying `seed.apply`:\r\n```python\r\n>>> caller.function(\'seed.apply\', \'images/saltinit-test/rootfs.qcow2\')\r\nTraceback (most recent call last):\r\n  File "<stdin>", line 1, in <module>\r\n  File "/usr/lib/python2.7/dist-packages/salt/client/__init__.py", line 1591, in function\r\n    return func(*args, **kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/seed.py", line 118, in apply_\r\n    res = _check_install(mpt)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/seed.py", line 189, in _check_install\r\n    return not _chroot_exec(root, cmd)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/seed.py", line 218, in _chroot_exec\r\n    pids = _chroot_pids(root)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/seed.py", line 239, in _chroot_pids\r\n    link = os.path.realpath(root)\r\n  File "/usr/lib/python2.7/posixpath.py", line 376, in realpath\r\n    resolved = _resolve_link(component)\r\n  File "/usr/lib/python2.7/posixpath.py", line 399, in _resolve_link\r\n    resolved = os.readlink(path)\r\nOSError: [Errno 2] No such file or directory: \'/proc/12902/root\'\r\n```\r\nThis is completly reproducible even in a fresh install. I am using only the code packaged for Debian, so it could be an issue with that.'
11879,'rallytime','cmd.run: unless/onlyif should show return code in debug loglevel\n'
11877,'cachedout',"Incorrect detection of OpenVZ guest\nI have some debian containers that are not detected as virtual machines. They don't have what grains are looking for to detect an OpenVZ platform:\r\n```\r\nls -laR /proc/vz/\r\n/proc/vz/:\r\ntotal 0\r\ndr-xr-xr-x  3 root root 0 Apr  9 16:53 .\r\ndr-xr-xr-x 84 root root 0 Feb 25  2013 ..\r\ndr-x------  8 root root 0 Apr  9 16:53 vzaquota\r\n\r\n/proc/vz/vzaquota:\r\ntotal 0\r\ndr-x------ 8 root root 0 Apr  9 16:53 .\r\ndr-xr-xr-x 3 root root 0 Apr  9 16:53 ..\r\n```\r\nOn the other hand they have ```/proc/user_beancounters``` - though that file is present both on containers and the host box"
11874,'cachedout','file.directory applies user/group everytime\nmy rendered formula:\r\n<pre>\r\n/data/ftp/test:\r\n  file:\r\n    - directory\r\n    - user: 5034\r\n    - group: 5034\r\n    - mode: 775\r\n    - require:\r\n      - file: /data/ftp\r\n</pre>\r\n\r\ngets applied on every run:\r\n<pre>\r\n          ID: /data/ftp/test\r\n    Function: file.directory\r\n      Result: True\r\n     Comment: Directory /data/ftp/test updated\r\n     Changes:   \r\n              ----------\r\n              group:\r\n                  \r\n              user:\r\n</pre> \r\ndirectory ownership/permissions are fine:\r\n<pre>\r\nls -la /data/ftp/test\r\ndrwxrwxr-x  2 5034 5034   48 Sep 25  2013 .\r\n</pre>\r\n\r\nthis happens with 2014.1.0 and 2014.1.1.\r\nnot tested with prior versions.\r\n\r\nFYI: user/group 5034 does not exist <code>/etc/passwd</code> or <code>/etc/group</code>'
11726,'UtahDave',"Restarting 'salt-minion' service on Windows kills the service\nWhen restarting the `salt-minion` service on Windows clients, the `salt-minion` service will be stopped but won't be started again.\r\n\r\n    salt test-minion-0001 service.restart salt-minion\r\n\r\nWill leave the minion in a dead/unresponsive state, as the service has been shut down.\r\nEven replacing the stop first, then start again approach by an atomic `Restart-Service` PowerShell cmdlet doesn't keep salt-minion from pulling the rug under his own feet.\r\n\r\nA possible approach could be to spawn a subprocess which takes care of restarting the service in case salt-minion itself should be restarted."
11602,'UtahDave','Allow packages on Windows client to be newer than latest in win_repo\nWhen a Windows client updates its packages to a newer version than the latest version available in the win_repo, applying the state will not succeed for this client.\r\n\r\nA way to allow clients to update packages by themselves is required, otherwise a situation like this will occur:\r\n\r\n  * Latest `microsoft.securityessentials` in win_repo is `4.4.304.0`.\r\n  * Client updates through Windows Update to `4.5.216.0`.\r\n  * Applying the state for this client results in:\r\n\r\n```\r\n          ID: microsoft.securityessentials\r\n    Function: pkg.installed\r\n      Result: False\r\n     Comment: The following packages failed to install/update: microsoft.securityessentials=4.4.304.0.\r\n     Changes:\r\n```'
11582,'basepi','Tag filtering usage of SaltEvent eventually uses all memory\n\r\nA few months ago, a change was made to the event handling such that when filtering by tag, non-matching messages were appended to an internal list instead of being discarded (https://github.com/saltstack/salt/issues/8580)\r\n\r\nThis has bitten us, because it turns any usage along these lines into a deadly memory leak:\r\n```\r\nmaster_event = salt.utils.event.MasterEvent(<my config>)\r\nwhile True:\r\n    ev = master_event.get_event(tag=\'interesting/tag\')\r\n```\r\n\r\nEvery event not matching interesting/tag is appended to master_event.pending_events, with no limit.  The system eventually exhausts available memory.\r\n\r\nI consider this a dangerously broken API, because everything appears to work until you go into production and run for several days.\r\n\r\nSome various options to improve the situation:\r\n * Make this behaviour explicit in the API documentation\r\n * Disable the pending_requests behaviour by default so that only people who are aware of the buffering will use it\r\n * Remove tag filtering from the salt API entirely, as it is not doing it especially cleverly and callers can equally just do their own "ev[\'tag\'].startswith()" check instead of having salt do that for them.\r\n * Make the 0MQ-level filtering work, so that non-matching events never make it to the code to begin with and we\'re not worried about them being buffered.\r\n'
11551,'whiteinge','broken links on windows installer page\nGetting 404s trying to download installers from http://docs.saltstack.com/en/latest/topics/installation/windows.html\r\n\r\n'
11539,'UtahDave',"Windows installers not digitally signed and not distributed over HTTPS\nThe official Windows installers (http://docs.saltstack.com/topics/installation/windows.html) and the binaries have no Authenticode signature, and the download isn't even over HTTPS. No checksums are published (for example on the mailing list).\r\n\r\nThere is no way to verify the integrity of the downloaded files.\r\n"
11538,'cachedout',"file.directory recurse operation causes  high memory usage\nHere I  got `/data/logs/` directory, and many big log files  are  under this directory, like `/data/logs/big_log_file1`, `/data/logs/big_log_file2`, etc.\r\n\r\n\r\nwhen I run salt state as below, the state is slowly executed in minion machines, and even causes high memory usage(reach **16GB** memory ), and keep rising !\r\n```\r\n{% for item in ['/data/logs/aaa', '/data/logs/bbb'] %}\r\n{{ item }}:\r\n    file.directory:\r\n        - user: root\r\n        - group: root\r\n        - dir_mode: 755\r\n        - makedirs: True\r\n        - file_mode: 644\r\n        - recurse:\r\n            - user\r\n            - group\r\n            - mode\r\n{% endfor %}\r\n````\r\n\r\n\r\nWhy ?        \r\n\r\n\r\n\r\nsalt --versions-report:\r\n\r\n![image](https://f.cloud.github.com/assets/1198913/2524462/48f003b0-b4e2-11e3-9b40-c8a979ce4e89.png)\r\n\r\n\r\nOS: CentOS 6.4, 64bit"
11536,'s0undt3ch','add grain that detects SSDs\n'
11480,'s0undt3ch','2014.1.0-1 minion connecting to 2014.1.0-1 master fails with Jinja errors about env variable\nI updated our master to 2014.1.0-1.  It went really smooth.  The majority of our minions are running 0.17.x and they all kept working without any problems.  I did notice though that the couple of 2014.0.1-1 minions that we have are failing when running a high state.  They are failing with errors on anything that refers to env, like so:\r\n\r\nRendering SLS "prod:packages.autofs" failed: Jinja variable \'env\' is undefined; line 54\r\n\r\nI can use cmd.run to run commands against these minions from the salt master though, that works without any issues.  I was told on #salt that the env variable would continue to be used for a while but would eventually be deprecated down the road.  Our 0.17.x minions don’t seem to have any issues with it and they are using the same sls files so it seems to only be the 2014.x minions.\r\n\r\nHere’s the full error from a high state:\r\n\r\nhttp://pastebin.com/ksw8gWNi\r\n\r\nThanks,\r\nDan'
11475,'UtahDave','Windows package should be upgraded to latest ZeroMQ\nA serious bug was fixed with TCP keep alive. See #6862 for details. \r\n\r\ncc: @UtahDave '
11471,'UtahDave','KeyError: \'file.replace\' (\'file.blockreplace\') on Salt: 2014.1.0 with Linux Master and Windows Minion\nI\'m consistently reproducing the following issue on multiple hosts (different masters and minions, but the same software versions and Salt states).\r\n\r\n## Problem\r\n\r\nCreate `/srv/salt/replace_a_file.sls` state file (on Linux Salt master - see versions below) with the following content:\r\n```\r\n# replace_a_file.sls\r\n\r\n{% if grains[\'os\'] in [ \'Windows\' ] %}\r\n\r\n{% set file_dir = \'C:\\deleteme\' %}\r\n\r\n\'{{ file_dir }}\\qwer.txt\':\r\n    file.replace:\r\n        - pattern: \'whatever\'\r\n        - repl: \'whatever_replaced\'\r\n        - show_changes: True\r\n\r\n{% endif %}\r\n\r\n```\r\n\r\nCreate `C:\\deleteme\\qwer.txt` sample file (on Windows Salt minion - see versions below) with any random text content.\r\n\r\nPush the state from the master:\r\n```\r\n salt \'winclient\' state.sls replace_a_file\r\nwinclient:\r\n----------\r\n          ID: C:\\deleteme\\qwer.txt\r\n    Function: file.replace\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "salt/state.py", line 1371, in call\r\n                File "salt/states/file.py", line 1963, in replace\r\n              KeyError: \'file.replace\'\r\n     Changes:\r\n```\r\n\r\n## Notes\r\n\r\n* Actually, I don\'t remember cases when any state using `file.replace` function successfully executed from Linux Master on Windows Minion with Salt version 2014.1.0. Please feedback anyone if there are known examples.\r\n* I also tried to modify state code here and there and the only thing I was able to get is another error: #10773 (maybe "`__salt__ is not defined`" problem is completely unrelated here because it appears intermittently and restarting Windows minion usually helps to recover).\r\n* Linux Master to Linux Minion works fine (I have multiple examples in the same state tree which concurrently execute for Linuxes without problems).\r\n\r\n## Versions\r\n\r\n#### Master: Linux F20 64 bit\r\n```\r\n> salt --versions-report\r\n           Salt: 2014.1.0\r\n         Python: 2.7.5 (default, Feb 19 2014, 13:47:28)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.2\r\n            ZMQ: 3.2.4\r\n```\r\n```\r\n> rpm -qf $(which salt-call)\r\nsalt-minion-2014.1.0-1.fc20.noarch\r\n```\r\n\r\n#### Minion: Windows 7 64 bit\r\n```\r\nc:\\salt> salt-minion.exe --versions-report\r\n           Salt: 2014.1.0-5-g32d3463\r\n         Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.4.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.0.1\r\n            ZMQ: 4.0.3\r\n```'
11355,'whiteinge',"Read the Docs not picking up new versions\nhttps://readthedocs.org/projects/salt/ is not showing any versions after v2014.1.0rc1\r\n\r\nAlso up until Today, when I manually initiated a build on RTD, the PDF version of salt documentation was woefully out of date (Release 0.17.5, last generated on February 19, 2014).\r\n\r\nProbable cause of this issue is shown on https://readthedocs.org/builds/salt/1240078/:\r\n```\r\nSphinx Standard Error:\r\n\r\n/var/build/user_builds/salt/checkouts/latest/salt/version.py:434: UserWarning: The parsed version info, `(0, 8, 1)`, is lower than the one defined in the file, `(2014, 1, 0)`.In order to get the proper salt version with the git hash you need to update salt's local git tags. Something like: 'git fetch --tags' or 'git fetch --tags upstream' if you followed salt's contribute documentation. The version string WILL NOT include the git hash.\r\n  __version__, __version_info__ = __get_version(__version__, __version_info__)\r\n```\r\n\r\nI had no such issues when building the Sphinx documentation for Salt on my local machine."
11338,'s0undt3ch','setup.py is broken\nLatest git \r\nOS:  Ubuntu 12.04\r\n\r\n```\r\n[boucha@dasalt salt (develop u=)]$ sudo python setup.py install \r\n2014.1.0-3503-ga5153ba\r\nrunning install\r\nrunning build\r\nrunning build_py\r\npackage init file \'salt/templates/__init__.py\' not found (or not a regular file)\r\nrunning build_scripts\r\nrunning install_lib\r\ncopying build/lib.linux-x86_64-2.7/salt/_version.py -> /usr/local/lib/python2.7/dist-packages/salt\r\ncopying build/lib.linux-x86_64-2.7/salt/_syspaths.py -> /usr/local/lib/python2.7/dist-packages/salt\r\nbyte-compiling /usr/local/lib/python2.7/dist-packages/salt/_version.py to _version.pyc\r\nbyte-compiling /usr/local/lib/python2.7/dist-packages/salt/_syspaths.py to _syspaths.pyc\r\nTraceback (most recent call last):\r\n  File "setup.py", line 648, in <module>\r\n    setup(**SETUP_KWARGS)\r\n  File "/usr/lib/python2.7/distutils/core.py", line 152, in setup\r\n    dist.run_commands()\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 953, in run_commands\r\n    self.run_command(cmd)\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 972, in run_command\r\n    cmd_obj.run()\r\n  File "setup.py", line 381, in run\r\n    install.run(self)\r\n  File "/usr/lib/python2.7/distutils/command/install.py", line 613, in run\r\n    self.run_command(cmd_name)\r\n  File "/usr/lib/python2.7/distutils/cmd.py", line 326, in run_command\r\n    self.distribution.run_command(command)\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 972, in run_command\r\n    cmd_obj.run()\r\n  File "setup.py", line 392, in run\r\n    idx = inp.index(\'build/lib/salt/templates/git/ssh-id-wrapper\')\r\nValueError: \'build/lib/salt/templates/git/ssh-id-wrapper\' is not in list\r\n```'
11327,'cachedout','2014.1.0 denial-of-service when restarting master\nAfter restarting the salt master, all of the minions are attempting to auth with a 3 second timeout, failing, then retrying again in 10 seconds. As far as I can tell all minions are stuck in this loop causing a denial of service.\r\n\r\nThe master is managing ~200 minions running 10 workers on 8 CPUs.'
11293,'cachedout','sys.doc stacktraces on eauth failure\nAt first I thought this was just eauth, now it seems to fail even without passing in eauth arguments.'
11143,'cachedout',"User's with an invalid shell will cause the minion to fail when shelling out\nIf you have a user with an invalid shell - one that is not present in the output of `chsh -l` or something like `nologin` - any shelling out the salt-minion will do on behalf of the user calling `salt-call` will return an error code of 1 but no information on to why that is.\r\n\r\nThis ended up being pretty hard to debug because one user would work, but another user would not. In my mind, since the minion is a daemon, any user calling it should expect exactly the same results if the command and arguments are the same. At the least I think we should potentially check if shell is set to an inactive or invalid shell and log a warning."
11038,'UtahDave','win_servermanager \'KeyError\' in 2014.1.0\nI have 2014.1.0 installed on ubuntu for my master, and a Windows Server 2012 as a minion. When I use win_servermanager, the code fails with a KeyError.\r\n\r\nConsider the following state file:\r\n\r\n```\r\n# telnet.sls\r\nTelnet-Client:\r\n  win_servermanager.installed\r\n```\r\n\r\nThis is what happens when I run this state:\r\n\r\n```\r\n$ sudo salt -G \'os:Windows\' state.sls telnet\r\ncds-vm-146:\r\n----------\r\n          ID: Telnet-Client\r\n    Function: win_servermanager.installed\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "salt/state.py", line 1371, in call\r\n                File "salt/states/win_servermanager.py", line 52, in installed\r\n              KeyError: \'Success\'\r\n     Changes:   \r\n\r\nSummary\r\n------------\r\nSucceeded: 0\r\nFailed:    1\r\n------------\r\nTotal:     1\r\n```\r\n\r\noutput of versions_report:\r\n\r\n```\r\ncds-vm-146:\r\n               Salt: 2014.1.0-5-g32d3463\r\n             Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n             Jinja2: 2.7.1\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.4.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 14.0.1\r\n                ZMQ: 4.0.3\r\nmaster-vm-17:\r\n               Salt: 2014.1.0\r\n             Python: 2.7.5+ (default, Sep 19 2013, 13:48:49)\r\n             Jinja2: 2.7\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.3.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.1.0\r\n                ZMQ: 3.2.3\r\n\r\n```\r\n\r\n'
11003,'s0undt3ch','ImmutableLazyProxy lists breaks file.accumulated having both watch_in and require_in requisites\nHello,\r\n\r\nSo on ``develop`` the new ImmutableLazyProxy (#10766 by @s0undt3ch) has at least one undetected side effect.\r\nOn ``file.accumulated`` there\'s a dependency check which used to add the list of  ``watch_in`` and ``require_in`` states and makes on loop on that. The ``__low__.get()``  results are not lists anymore and this makes a bad crash.\r\n\r\n<pre>\r\n    require_in = __low__.get(\'require_in\', [])\r\n    watch_in = __low__.get(\'watch_in\', [])\r\n    deps = require_in + watch_in  &lt;================== HERE\r\n    if not filter(lambda x: \'file\' in x, deps):\r\n        ret[\'result\'] = False\r\n        ret[\'comment\'] = \'Orphaned accumulator {0} in {1}:{2}\'.format(\r\n            name,\r\n            __low__[\'__sls__\'],\r\n            __low__[\'__id__\']\r\n        )\r\n        return ret\r\n</pre>\r\n\r\nResults in:\r\n\r\n<pre>\r\nAn exception occurred in this state: Traceback (most recent call last):\r\n                File "salt/salt/state.py", line 1372, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "salt/salt/states/file.py", line 2928, in accumulated\r\n                  deps = require_in + watch_in\r\n              TypeError: can only concatenate list (not "ImmutableLazyProxy(list)") to list\r\n</pre>\r\n\r\nFaling Test example (you need both watch_in and require_in):\r\n\r\n<pre>\r\ntest-acc1:\r\n  file.accumulated:\r\n    - require_in:\r\n      - file: final\r\n    - filename: /tmp/foo\r\n    - text: |\r\n            bar\r\ntest-acc2:\r\n  file.accumulated:\r\n    - watch_in:\r\n      - file: final\r\n    - filename: /tmp/foo\r\n    - text: |\r\n            baz\r\nfinal:\r\n  file.blockreplace:\r\n    - name: /tmp/foo\r\n    - marker_start: "#-- start salt managed zoneend -- PLEASE, DO NOT EDIT"\r\n    - marker_end: "#-- end salt managed zoneend --"\r\n    - content: \'\'\r\n    - append_if_not_found: True\r\n    - show_changes: True\r\n</pre>\r\n\r\nAs a workaround I duplicated the loop to check each lists instead of merging lists in ``deps``, but there\'s certainly a more elegant fix.'
10982,'cachedout',"salt-key -l all not implemented\nAccording to `salt-key -h`, `salt-key -L` is deprecated in favor of `salt-key --list all`, however all variation of this (`-lall`, `-l all`, `--list all`) fail with the same error:\r\n\r\n```bash\r\nsalt-key: error: 'all' is not a valid argument to '--list'\r\n```\r\n\r\nI also found no reference to `-L` being deprecated in the code (under salt/key.py), and there's nothing implemented for handling `all` in list_status. I was also unable to find any examples of `--list all` in the documentation, but `-L` is used quite frequently.\r\n\r\n```bash\r\nsalt --versions-report\r\n           Salt: 2014.1.0\r\n         Python: 2.7.3 (default, Jan 13 2013, 11:20:46)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.3\r\n```"
10900,'cachedout',"timeouts when targeting by pillar\nI'm running master 0.17.5 (the latest available for ubuntu via PPA),\r\nand having trouble targeting by pillar, and getting some inconsistent\r\nresults with 3 minions. All minions were installed at the same time,\r\nand all have these versions:\r\n\r\n               Salt: 0.17.5-52-g2d4772c\r\n             Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n             Jinja2: 2.7.1\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.4.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.1.0\r\n                ZMQ: 3.2.2\r\n\r\n## detective work\r\n\r\nSelecting by id works great:\r\n\r\n    $ time sudo salt -E '(ryanwin|strat|voting).*' test.ping -v\r\n    Executing job with jid 20140303094057244900\r\n    -------------------------------------------\r\n    \r\n    voting.acceleration.net:\r\n    True\r\n    stratussync.acceleration.net:\r\n    True\r\n    ryanwin7.acceleration.net:\r\n    True\r\n    \r\n    real    0m1.695s\r\n    user    0m0.253s\r\n    sys     0m0.103s\r\n\r\nWhen I try to select by pillar, the first returns quickly but the\r\nothers fail:\r\n\r\n    $ time sudo salt -I 'nscp:*' test.ping -v                     \r\n    Executing job with jid 20140303094255341834\r\n    -------------------------------------------\r\n    \r\n    ryanwin7.acceleration.net:\r\n        True\r\n    stratussync.acceleration.net:\r\n        Minion did not return\r\n    voting.acceleration.net:\r\n        Minion did not return\r\n    \r\n    real    0m10.525s\r\n    user    0m0.291s\r\n    sys     0m0.072s\r\n\r\nIf I up the timeout, I get the same result, but it takes longer. So it\r\nseems like I'm hitting the timeout somewhere.\r\n\r\n    ryepup@scruffy:~$ time sudo salt -I 'nscp:*' test.ping -v -t 15\r\n    Executing job with jid 20140303094349905445\r\n    -------------------------------------------\r\n    \r\n    ryanwin7.acceleration.net:\r\n        True\r\n    stratussync.acceleration.net:\r\n        Minion did not return\r\n    voting.acceleration.net:\r\n        Minion did not return\r\n    \r\n    real    0m19.523s\r\n    user    0m0.224s\r\n    sys     0m0.136s\r\n    $ time sudo salt -I 'nscp:*' test.ping -v -t 60\r\n    Executing job with jid 20140303094457264656\r\n    -------------------------------------------\r\n    \r\n    ryanwin7.acceleration.net:\r\n        True\r\n    stratussync.acceleration.net:\r\n        Minion did not return\r\n    voting.acceleration.net:\r\n        Minion did not return\r\n    \r\n    real    1m5.702s\r\n    user    0m0.290s\r\n    sys     0m0.098s\r\n    $ time sudo salt -I 'nscp:*' test.ping -v -t 300                \r\n    Executing job with jid 20140303094640755340\r\n    -------------------------------------------\r\n    \r\n    ryanwin7.acceleration.net:\r\n        True\r\n    stratussync.acceleration.net:\r\n        Minion did not return\r\n    voting.acceleration.net:\r\n        Minion did not return\r\n    \r\n    real    5m13.684s\r\n    user    0m0.266s\r\n    sys     0m0.123s\r\n\r\nLooking at `pillar` directly, we see another all three have the right pillar:\r\n\r\n    $ sudo salt -E '(ryanwin|strat|voting).*' pillar.item nscp\r\n    ryanwin7.acceleration.net:\r\n        ----------\r\n        nscp:\r\n            ----------\r\n            ifs:\r\n                ----------\r\n                private:\r\n                    Red Hat VirtIO Ethernet Adapter\r\n    voting.acceleration.net:\r\n        ----------\r\n        nscp:\r\n            ----------\r\n            aspnet:\r\n                True\r\n            ifs:\r\n                ----------\r\n                private:\r\n                    Intel(R) PRO/1000 MT Network Connection #3\r\n                public:\r\n                    Intel(R) PRO/1000 MT Network Connection\r\n            iis:\r\n                True\r\n            mssql:\r\n                MSSQL$SQLEXPRESS\r\n    stratussync.acceleration.net:\r\n        ----------\r\n        nscp:\r\n            ----------\r\n            aspnet:\r\n                True\r\n            ifs:\r\n                ----------\r\n                private:\r\n                    Intel(R) PRO/1000 MT Network Connection #2\r\n                public:\r\n                    Intel(R) PRO/1000 MT Network Connection\r\n            iis:\r\n                True\r\n            mssql:\r\n                MSSQL$SQLEXPRESS\r\n\r\nHowever, if we use `pillar.get` instead of `pillar.item`, then we get different results\r\n\r\n    $ sudo salt -E '(ryanwin|strat|voting).*' pillar.get nscp\r\n    voting.acceleration.net:\r\n        \r\n    stratussync.acceleration.net:\r\n        \r\n    ryanwin7.acceleration.net:\r\n        ----------\r\n        ifs:\r\n            ----------\r\n            private:\r\n                Red Hat VirtIO Ethernet Adapter\r\n    \r\n\r\nWe only see the `nscp` section for the minion that we can successfully\r\nping via pillar targeting. Running `saltutil.refresh_pillar` did not not fix this, but running `saltutil.sync_all` did, and then `pillar.get` matched `pillar.item` as I'd expect.\r\nI was really hopeful with `saltutil.refresh_pillar` would solve my problem, but after that I got the same ping timeouts after all the refreshing/syncing.\r\n\r\nThe failing minions have nothing fishy in their logs, the master has\r\nnothing fishy in it's logs, and all respond nicely if targeted by id.\r\n\r\nI turned up the log verbosity and restarted the minions, and now it\r\nworks as expected:\r\n\r\n    $ time sudo salt -I 'nscp:*' test.ping\r\n    ryanwin7.acceleration.net:\r\n        True\r\n    voting.acceleration.net:\r\n        True\r\n    stratussync.acceleration.net:\r\n        True\r\n    \r\n    real    0m0.459s\r\n    user    0m0.277s\r\n    sys     0m0.097s\r\n\r\n## summary\r\n\r\nIt seems like my minions didn't have the right pillar state. I saw inconsistent results between `pillar.get` and `pillar.item`, and those were not resolved with `saltutil.refresh_pillar`, but were resolved by `saltutil.sync_all`. Even after the `pillar.get` returned the right results, targeting by pillar still timed out.\r\n\r\nMy best guess is there's some cache on the minion side that didn't get cleared enough. When I asked my master to target by pillar, it used its up-to-date cache files to target my three minions, the published the command. All my minions got the message, and looked at the target spec to see if they should reply. One of my minions had up-to-date pillar data, saw it was targeted, and replied. Two of my minions had stale pillar data, and ignored the command. Sometime later my specified timeout was up, and master gave up hearing back from those two.\r\n\r\nAfter rebooting the minions, the bad cache was cleared, and all went well."
10894,'UtahDave','Windows file/repo access broken in 2014.1.0 AMD64?\nWas beating my head against the wall trying to figure out why my windows minions were unable to access anything from the repository.  As a test, on one minion (\'winold\' below), I uninstalled Salt-Minion-2014.1.0-AMD64 and installed Salt-Minion-0.17.5-2-AMD64, and sure enough, it is able to access the repo, install packages, etc..\r\n```\r\n# salt -L \'winold,winnew\' grains.item saltversion os osfullname osrelease\r\nwinnew:\r\n  os: Windows\r\n  osfullname: Microsoft Windows 7 Enterprise\r\n  osrelease: 7\r\n  saltversion: 2014.1.0-5-g32d3463\r\nwinold:\r\n  os: Windows\r\n  osfullname: Microsoft Windows Server 2008 R2 Standard\r\n  osrelease: 2008ServerR2\r\n  saltversion: 0.17.5-52-g2d4772c\r\n\r\n# salt -L \'winold,winnew\' cp.get_file_str salt://filetest\r\nwinold:\r\n    #hello?\r\n\r\nwinnew:\r\n    The minion function caused an exception: Traceback (most recent call last):\r\n      File "salt/minion.py", line 768, in _thread_return\r\n      File "salt/modules/cp.py", line 282, in get_file_str\r\n      File "salt/utils/__init__.py", line 1037, in fopen\r\n    IOError: [Errno 22] invalid mode (\'r\') or filename: \'\'\r\n\r\n# salt -L \'winold,winnew\' pkg.refresh_db \r\nwinnew:\r\n    True\r\nwinold:\r\n    True\r\n```\r\n\'winnew\' C:\\salt\\var\\log\\salt\\minion :\r\n```\r\n2014-03-02 21:22:34,112 [salt.fileclient  ][DEBUG   ] Fetching file from saltenv \'base\', ** attempting ** \'salt://filetest\'\r\n2014-03-02 21:22:34,117 [salt.loaded.int.module.cp][ERROR   ] Unable to cache file \'salt://filetest\' from saltenv \'base\'.\r\n2014-03-02 21:22:34,118 [salt.minion      ][WARNING ] The minion function caused an exception\r\nTraceback (most recent call last):\r\n  File "salt/minion.py", line 768, in _thread_return\r\n  File "salt/modules/cp.py", line 282, in get_file_str\r\n  File "salt/utils/__init__.py", line 1037, in fopen\r\nIOError: [Errno 22] invalid mode (\'r\') or filename: \'\'\r\n....\r\n2014-03-02 21:23:22,134 [salt.fileclient  ][DEBUG   ] Fetching file from saltenv \'base\', ** attempting ** \'salt://win/repo/winrepo.p\'\r\n2014-03-02 21:23:22,149 [salt.loaded.int.module.cp][ERROR   ] Unable to cache file \'salt://win/repo/winrepo.p\' from saltenv \'base\'.\r\n```\r\n'
10773,'UtahDave','On Windows file state throws exception in check_perms: global name \'__salt__\' is not defined\nHi,\r\n\r\nwhen i try to use a file.managed or file.recurse state, very often the following exception will be thrown:\r\n```\r\nAn exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1316, in call\r\n  File "salt/states/file.py", line 1157, in managed\r\n  File "salt/modules/file.py", line 2095, in manage_file\r\n  File "salt/modules/file.py", line 2234, in makedirs\r\n  File "salt/modules/file.py", line 2193, in mkdir\r\n  File "salt/modules/file.py", line 2269, in makedirs_perms\r\n  File "salt/modules/file.py", line 1690, in check_perms\r\nNameError: global name \'__salt__\' is not defined\r\n```\r\n\r\nSometimes it runs without a exception but mostly not.\r\n\r\nMinion (Windows Server 2008 R2):\r\n```\r\nSalt: 0.17.5-52-g2d4772c\r\n             Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n             Jinja2: 2.7.1\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.4.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.1.0\r\n                ZMQ: 3.2.2\r\n```\r\n\r\nMaster (Ubuntu 12.04 LTS AMD64):\r\n```\r\nSalt: 0.17.5\r\n         Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
10733,'s0undt3ch','Major scaling problem\nOkie, so I was trying to implement something and ended up in a position where a `state.sls` call was taking upwards of 15 minutes.  Digging in I found that some 80-90% of the time was being spent in deepcopy and eventually figured out what was wrong.\r\n\r\nThe setup has ~870 states procedurally generated via a for loop, each state had a non trivial chunk of context passed in.  After trimming the context as much as possible, each chunk looks like this:\r\n\r\n```python\r\n {\'__env__\': \'base\',\r\n  \'__id__\': \'hosting_web_file863\',\r\n  \'__sls__\': \'roles.hosting_web\',\r\n  \'context\': {\'aliases\': [],\r\n              \'directories\': [\'/public_html\',\r\n                              \'/public_html/cgi-bin/\',\r\n                              \'/secure\',\r\n                              \'/secure/cgi-bin/\'],\r\n              \'domain\': \'redacted\',\r\n              \'subdomains\': [\'secure\']},\r\n  \'fun\': \'managed\',\r\n  \'group\': \'root\',\r\n  \'makedirs\': False,\r\n  \'mode\': \'0644\',\r\n  \'name\': \'/etc/apache2/new/99_redacted.conf\',\r\n  \'order\': 10864,\r\n  \'source\': \'salt://roles/hosting_web/vhost.conf\',\r\n  \'state\': \'file\',\r\n  \'template\': \'jinja\',\r\n  \'user\': \'root\'},\r\n```\r\n\r\nThe problem is that salt runs in to an O(n^2) style scale on this particular structure.\r\n\r\nUsing salt-call (cause profiling salt-minion is a pain) the call trace looks like this:\r\n```\r\n  File "/usr/bin/salt-call", line 54, in <module>\r\n    exec(data)\r\n  File "<string>", line 11, in <module>\r\n  File "/usr/lib64/python2.7/site-packages/salt/scripts.py", line 84, in salt_call\r\n    client.run()\r\n  File "/usr/lib64/python2.7/site-packages/salt/cli/__init__.py", line 314, in run\r\n    caller.run()\r\n  File "/usr/lib64/python2.7/site-packages/salt/cli/caller.py", line 169, in run\r\n    ret = self.call()\r\n  File "/usr/lib64/python2.7/site-packages/salt/cli/caller.py", line 83, in call\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib64/python2.7/site-packages/salt/modules/state.py", line 385, in sls\r\n    ret = st_.state.call_high(high_)\r\n  File "/usr/lib64/python2.7/site-packages/salt/state.py", line 1711, in call_high\r\n    ret = self.call_chunks(chunks)\r\n  File "/usr/lib64/python2.7/site-packages/salt/state.py", line 1433, in call_chunks\r\n    running = self.call_chunk(low, running, chunks)\r\n  File "/usr/lib64/python2.7/site-packages/salt/state.py", line 1656, in call_chunk\r\n    running[tag] = self.call(low, chunks, running)\r\n  File "/usr/lib64/python2.7/site-packages/line_profiler.py", line 92, in f\r\n    result = func(*args, **kwds)\r\n  File "/usr/lib64/python2.7/site-packages/salt/state.py", line 1301, in call\r\n    raise Exception("Bail!")\r\n```\r\n\r\nThe two frames I need to draw attention to are `call_chunks` and `call` ... Specifically these lines:\r\n```python\r\n    def call_chunks(self, chunks):\r\n        # [...]\r\n        for low in chunks:\r\n            # [...]\r\n                running = self.call_chunk(low, running, chunks)\r\n```\r\n\r\n```python\r\n   def call(self, low, chunks=None, running=None):\r\n        # [...]\r\n        inject_globals = {\r\n            # Pass a copy of the running dictionary, the low state chunks and\r\n            # the current state dictionaries.\r\n            # We pass deep copies here because we don\'t want any misbehaving\r\n            # state module to change these at runtime.\r\n            \'__low__\': copy.deepcopy(low),\r\n            \'__running__\': copy.deepcopy(running) if running else {},\r\n            \'__lowstate__\': copy.deepcopy(chunks) if chunks else {}\r\n        }\r\n```\r\n\r\nIn isolation both are fine and sensible, no problem.  Together they\'re a problem.\r\nFor every chunk passed to call_chunks, it does a deepcopy of all the chunks passed in.  Pass in 870 chunks and you get 870 calls to deepcopy and 756,900 calls to deepcopy a dict like the one above.\r\nTrimming the context helped quite a bit, but that\'s still a crazy number of calls.  With the non-trivial context I initially passed in it was closer to O(2n*n) performance.\r\n\r\nDeepcopy is a rather expensive function and, in vanilla python at least, implemented totally as python an introspection.\r\n\r\nNow that I\'ve finished ranting about that... what to do about it.\r\nAfter some thinking, it seems the best solution would be to convert chunks in to a set of immutable objects and, if necessary, provide an overlay so that states could alter things and then we just throw away the overlay instead of recloning the underlying data.  Does that seem like a solid approach?\r\n\r\nFor the moment I\'m working around this by writing a custom state module that wraps the calls to file.manage_file so that lowstate sees it as just a single state, but the actual issue probably needs fixing rather than just hacking around.  Unless I\'m missing something, this applies a hard limiting factor on how many states you can apply to a single node in a single call.'
10658,'cachedout','closing file handle in file module\nSeems impossible to do a search and replace using the file module since the file handle is not closed after a search is done:\r\n\r\n```\r\nif __salt__[\'file.search\'](filePath, regExp):\r\n\t__salt__[\'file.replace\'](filePath, regExp, newString)\r\nelse:\r\n\t__salt__[\'file.append\'](filePath, newString)\r\n```\r\ngenerates:\r\n```\r\n    __salt__[\'file.replace\'](filePath, regExp, newString)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/file.py", line 971, in replace\r\n    mode=\'rb\'):\r\n  File "/usr/lib64/python2.6/fileinput.py", line 102, in input\r\n    raise RuntimeError, "input() already active"\r\n```'
10654,'cachedout','rpmlint issues 2014-1\nWarning: wrong-file-end-of-line-encoding:\r\n```\r\n/usr/share/doc/packages/salt-doc/html/_static/jquery.js\r\n```\r\nThis file has wrong end-of-line encoding, usually caused by creation or\r\nmodification on a non-Unix system. It could prevent it from being displayed\r\ncorrectly in some circumstances.\r\n\r\nWarning: non-executable-script:\r\n```\r\n/usr/lib/python2.7/site-packages/salt/states/zcbuildout.py\r\n```\r\nshebang: /usr/bin/env\r\n```\r\n/usr/lib/python2.7/site-packages/salt/templates/debian_ip/route_eth.jinja \r\n```\r\nshebang: /bin/sh\r\n```\r\n/usr/lib/python2.7/site-packages/salt/states/dockerio.py \r\n```\r\nshebang: /usr/bin/env\r\n\r\nThis text file contains a shebang or is located in a path dedicated for\r\nexecutables, but lacks the executable bits and cannot thus be executed.  If\r\nthe file is meant to be an executable script, add the executable bits,\r\notherwise remove the shebang or move the file elsewhere.'
10640,'rallytime','file-state: user/group should accept numerical ids\ni want to create a directory using file.directory.\r\nunfortunately the user/group arguments cannot handle numeric IDs (there is no system user for that ID)\r\n\r\nif i pass the ID as a string, i get:\r\n<pre>\r\nUser 5034 is not available Group 5034 is not available\r\n</pre>\r\n\r\nif i pass the ID as an int i get:\r\n<pre>\r\nTypeError: must be string, not int\r\n</pre>\r\n\r\nso i suggest using the latter to skip calling <code>user_to_id</code>\r\n'
10569,'s0undt3ch','salt-cloud not working in develop\nUsing salt-bootstrap for a develop install, the salt-cloud tool does not seem to work:\r\n\r\n(This is a fresh 12.04 instance on EC2)\r\n```\r\n> curl -L http://bootstrap.saltstack.org | sudo sh -s -- git develop\r\n...\r\nthe stuff...\r\n...\r\n> salt-cloud\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-cloud", line 10, in <module>\r\n    salt_cloud()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 120, in salt_cloud\r\n    cloud = salt.cloud.cli.SaltCloud()\r\nAttributeError: \'module\' object has no attribute \'cli\'\r\n```'
10444,'UtahDave',"salt 'minion' pkg.install chrome fails on Windows minion without error\nI followed the instructions on http://docs.saltstack.com/ref/windows-package-manager.html to get Windows packages working, but when I run `salt '*' pkg.install chrome` it fails with no message or error.\r\n\r\nThen I realised I need to put the Chrome installer on the saltmaster at `/srv/salt/win/repo/chrome/GoogleChromeStandaloneEnterprise.msi`.\r\n\r\nIf I then run `pkg.install chrome` from the master, the minions copy the .msi installer to their local cache, but is still not installed.\r\n\r\nI started from a fresh install of Windows Server 2008 R2 and installed Salt 0.17.5-51-g508bed0 straight after that.\r\n\r\nWhat am I doing wrong?"
10438,'rallytime','Suggestions for Requisites doc\nI\'ve commented on the docs twice already (http://pastebin.com/H5qyPPxU http://pastebin.com/WvvPQMKp) but it might work better to do it here.\r\n\r\nRegarding http://docs.saltstack.com/ref/states/requisites.html, as a newbie. The docs are too chatty and lack clear definitions.\r\n\r\nEg: \r\n\r\n>The most basic requisite statement is require. The behavior of require is simple. Make sure that the dependent state is executed before the depending state, and if the dependent state fails, don\'t run the depending state. So in the above examples the file /etc/vimrc will only be applied after the vim package is installed and only if the vim package is installed successfully.\r\n\r\n\r\n(The sentence after "The behaviour of require is simple" is good - an actual definition.)\r\nUnclarities:\r\n- what does "make sure" mean? does require actually trigger the running of that state?\r\n- if the dependent state succeeds but doesn\'t change state, what happens?\r\n\r\n\r\n>The watch statement does everything the require statement does, but with a little more. The watch statement looks into the state modules for a function called mod_watch. If this function is not available in the corresponding state module, then watch does the same thing as require. If the mod_watch function is in the state module, then the watched state is checked to see if it made any changes to the system, if it has, then mod_watch is called.\r\n\r\n\r\nThis jumbled definition is hard to understand, and gets into distracting implementation detail ("mod_watch") too soon for the end user. Suggest a definition that doesn\'t depend on understanding "require".\r\n\r\nUnclarities:\r\n- if A watches B, then does A get called instantly when B happens?\r\n- does it matter in which order A and B are defined?\r\n- if B never changes state (eg, it has an \'unless\'), does A never get called?\r\n\r\n\r\n>The prereq requisite is a powerful requisite added in 0.16.0. This requisite allows for actions to be taken based on the expected results of a state that has not yet been executed. In more practical terms, a service can be shut down because the prereq knows that underlying code is going to be updated and the service should be off-line while the update occurs.\r\n\r\n\r\nAgain, lacking an actual definition. Suggest dropping the chattiness (when it was added etc). Suggested reword:\r\n"Prereq causes an action to be taken in anticipation of another action. It is called just before the second action, but only if that second action is expected to cause a state change."\r\n\r\n\r\n>Require In\r\n>The require_in requisite is the literal reverse of require. If a state declaration needs to be required by another state declaration then require_in can accommodate it, so these two sls files would be the same in the end:\r\n\r\n\r\nIMHO it would be much cleaner and simple to simply generalise for require_in, watch_in and prereq_in:\r\n\r\n"Each of the three requisites - require, watch, prereq - has a reverse counterpart - require_in, watch_in, prereq_in - , allowing you to specify the dependency from the other end." And then examples.\r\n '
10430,'basepi','KeyError: \'mongodb.user_exists\'\n```\r\ngraylog:\r\n----------\r\n    State: - mongodb_user\r\n    Name:      graylog2\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1317, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/mongodb_user.py", line 45, in present\r\n    if __salt__[\'mongodb.user_exists\'](name, user, password, host, port, database):\r\nKeyError: \'mongodb.user_exists\'\r\n```\r\nfor\r\n```\r\nmongodb-db:\r\n  mongodb_user.present:\r\n    - name: graylog2\r\n    - passwd: PASSWRD\r\n    - database: graylog2\r\n    - require:\r\n      - pkg: mongodb-pkgs\r\n```'
10404,'s0undt3ch','ptys not being released\nI\'m using revision 3a894d760699ab155bc63a3e35e6730f87cc7d8c from the develop branch and am running into an issue when launching more than just a few EC2 instances with `salt-cloud`.  In summary it appears that ptys are not being released as instances are launched when using `salt-cloud` with parallel execution enabled (e.g. `salt-cloud -m <our_map_file> -P`) to launch 50+ EC2 instances.\r\n\r\nI\'m on Ubuntu 13.10, where `/proc/sys/kernel/pty/max` defaults to 4096.  In order to get the above to work, I increased it to 16384, and ran the following one-liner to see how many ptys were in use: `while [ 1 ]; do echo "[$(date)] $(cat /proc/sys/kernel/pty/nr)"; sleep 1; done`.  The output of that looked something like this:\r\n\r\n    [Wed Feb 12 22:24:39 UTC 2014] 2\r\n    [Wed Feb 12 22:24:40 UTC 2014] 2\r\n    [Wed Feb 12 22:24:41 UTC 2014] 6\r\n    [Wed Feb 12 22:24:42 UTC 2014] 10\r\n    [Wed Feb 12 22:24:43 UTC 2014] 18\r\n    [Wed Feb 12 22:24:44 UTC 2014] 26\r\n    [Wed Feb 12 22:24:45 UTC 2014] 32\r\n    <snip> continually climbing throughout the launch and not dropping until salt-cloud exited \r\n    [Wed Feb 12 22:42:05 UTC 2014] 9816\r\n    [Wed Feb 12 22:42:06 UTC 2014] 9816\r\n    [Wed Feb 12 22:42:07 UTC 2014] 2\r\n    [Wed Feb 12 22:42:08 UTC 2014] 2\r\n\r\nBefore bumping the pty max I got exceptions like:\r\n\r\n    2014-02-12 13:06:50,022 [salt.utils.cloud ][DEBUG   ] SSH command: "ssh -t -t -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oControlPath=none -oPasswordAuthentication=no -oChallengeResponseAuthentica\r\n    tion=no -oPubkeyAuthentication=yes -oKbdInteractiveAuthentication=no -i /mnt/stackdio_root/stackdio/storage/cloud/aws-qatp/id_rsa root@10.44.13.132 \'/tmp/.saltcloud/deploy.sh -c /tmp/.saltcloud -D git 3a894d76069\r\n    9ab155bc63a3e35e6730f87cc7d8c\'"\r\n    2014-02-12 13:06:50,024 [salt.utils.vt    ][WARNING ] Failed to spawn the VT: out of pty devices\r\n    Traceback (most recent call last):\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/vt.py", line 198, in __init__\r\n        self._spawn()\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/vt.py", line 377, in _spawn\r\n        self.pid, self.child_fd, self.child_fde = self.__fork_ptys()\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/vt.py", line 451, in __fork_ptys\r\n        stderr_parent_fd, stderr_child_fd = pty.openpty()\r\n      File "/usr/lib/python2.7/pty.py", line 29, in openpty\r\n        master_fd, slave_name = _open_terminal()\r\n      File "/usr/lib/python2.7/pty.py", line 70, in _open_terminal\r\n        raise os.error, \'out of pty devices\'\r\n    OSError: out of pty devices\r\n    2014-02-12 13:06:50,025 [salt.utils.cloud ][ERROR   ] Failed to execute command \'/tmp/.saltcloud/deploy.sh -c /tmp/.saltcloud -D git 3a894d760699ab155bc63a3e35e6730f87cc7d8c\': Failed to spawn the VT. Error: out of pty devices\r\n    Traceback (most recent call last):\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/cloud.py", line 1235, in root_cmd\r\n        stream_stderr=kwargs.get(\'display_ssh_output\', True)\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/vt.py", line 207, in __init__\r\n        \'Failed to spawn the VT. Error: {0}\'.format(err)\r\n    TerminalException: Failed to spawn the VT. Error: out of pty devices\r\n    2014-02-12 13:06:50,026 [salt.cloud       ][ERROR   ] Failed to deploy \'test-spot-dn-126\'. Error: Executing the command \'/tmp/.saltcloud/deploy.sh -c /tmp/.saltcloud -D git 3a894d760699ab155bc63a3e35e6730f87cc7d8c\' failed\r\n    Traceback (most recent call last):\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/cloud/__init__.py", line 1699, in create_multiprocessing\r\n        local_master=parallel_data[\'local_master\']\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/cloud/__init__.py", line 854, in create\r\n        output = self.clouds[func](vm_)\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/cloud/clouds/ec2.py", line 1576, in create\r\n        deployed = salt.utils.cloud.deploy_script(**deploy_kwargs)\r\n      File "/home/stackdio/.virtualenvs/stackdio/src/salt/salt/utils/cloud.py", line 864, in deploy_script\r\n        deploy_command\r\n    SaltCloudSystemExit: Executing the command \'/tmp/.saltcloud/deploy.sh -c /tmp/.saltcloud -D git 3a894d760699ab155bc63a3e35e6730f87cc7d8c\' failed\r\n\r\n\r\n\r\n'
10341,'s0undt3ch','salt.utils.cloud.salt_cloud_force_ascii() has no docstring\nAssigning to @s0undt3ch, since the blame bears his name.\r\n\r\nAlso, some docs on the line immediately following this function would be helpful.'
10309,'cachedout','Systemd services broken on Debian Wheezy\nThe systemd module uses the command\r\n```systemctl --full list-unit-files | col -b ``` to find all services.\r\n\r\nThe correct command should be\r\n```systemctl --all --full list-units --type service```'
10248,'cachedout',"sync_grains should force-refresh grains cache\nOtherwise we don't see new grains on sync, which is undesirable. "
10198,'s0undt3ch',"Fix ssh_auth pulling keyfile from correct env\nWhen the ssh_auth state is reading the key from a file on the Salt fileserver, the proper environment isn't passed in. If the key file is in a non-'base' environment it fails.\r\n\r\nLooks like this got broken in fbfd926 and affects the 2014.1.0RC3 branch. It was not broken in v0.17.4. Trying to get this in before the release, since it breaks my state files!\r\n\r\nNot sure if pull requests for bug fixes in 2014.1 go against develop or against 2014.1 branch so either this pull request or https://github.com/saltstack/salt/pull/10197 (which merges into 2014.1) should be closed."
10197,'s0undt3ch',"Fix ssh_auth pulling keyfile from correct env\nWhen the ssh_auth state is reading the key from a file on the Salt fileserver, the proper environment isn't passed in. If the key file is in a non-'base' environment it fails.\r\n\r\nLooks like this got broken in https://github.com/saltstack/salt/commit/fbfd926fdd0c7686b6be4da1dbfe4e9e18bbea60 and affects the 2014.1.0RC3 branch. It was not broken in v0.17.4. Trying to get this in before the release, since it breaks my state files!"
10110,'cro',"2014.1.0-919-ge254cbc cmd.run throws TypeError\nI installed Salt from develop today.  Cmd.run throws this error\r\n\r\n```\r\nTypeError encountered executing cmd.run: 'int' object is not iterable. \r\nSee debug log for more info.  \r\nPossibly a missing arguments issue:  \r\nArgSpec(args=['cmd', 'cwd', 'stdin', 'runas', 'shell', 'python_shell', 'env', 'clean_env', \r\n'template', 'rstrip', 'umask', 'output_loglevel', 'quiet', 'timeout', 'reset_system_locale', 'saltenv'],\r\nvarargs=None, keywords='kwargs', \r\ndefaults=(None, None, None, '/bin/bash', True,\r\n(), False, None, True, None, 'info', False, None, True, 'base'))\r\n```\r\n"
10049,'s0undt3ch','Saltfile Feature\nInitial work with @s0undt3ch, do not merge yet as it is a work in progress.'
10012,'techhat','Add nftables module, for Linux 3.13\nWhen the 3.13 kernel drops, we will need to support nftables, as we currently do with iptables.'
9930,'terminalmage',"gitfs attempts to reinitialize existing git repositories several times\nWhen using gitfs with the following configuration, gitpython is executed every X seconds to update the fileserver backend. However, when gitpython tracing is enabled, I can verify that hundreds of calls to `git init` are executed.\r\n\r\n```\r\nfileserver_backend:\r\n  - git\r\n\r\ngitfs_remotes:\r\n  - file:///srv/salt\r\n  - git+ssh://git@server.com/salt-states.git\r\n```\r\n\r\nSample tracing from gitpython:\r\n\r\n```\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/2069245aa946a53e28df2fdb4c1fb01f/.git/'\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/c1a4682bdee0002ef9e979336910af6b/.git/'\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/2069245aa946a53e28df2fdb4c1fb01f/.git/'\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/c1a4682bdee0002ef9e979336910af6b/.git/'\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/2069245aa946a53e28df2fdb4c1fb01f/.git/'\r\ngit init -> 0; stdout: 'Reinitialized existing Git repository in /var/cache/salt/master/gitfs/c1a4682bdee0002ef9e979336910af6b/.git/'\r\n\r\n... x 100\r\n```\r\n\r\n```\r\nsalt --versions-report\r\n           Salt: 2014.1.0-669-g0f62785\r\n         Python: 2.7.3 (default, Jan  2 2013, 13:56:14)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.3\r\n```"
9923,'cachedout',"pkgrepo comments\nRunning SaltStack 0.17.4 on Debian 7.3 for master and minion, using pkgrepo.managed works as expected, except for the comments attribute which seems to have no effect and no error is generated either. Reference: http://docs.saltstack.com/ref/states/all/salt.states.pkgrepo.html\r\n\r\n```\r\nsalt:\r\n  pkgrepo:\r\n    - managed\r\n    - name: deb http://debian.saltstack.com/debian/ {{salt['grains.get']('oscodename')}}-saltstack main\r\n    - file: /etc/apt/sources.list.d/saltstack.list\r\n    - key_url: http://debian.saltstack.com/debian-salt-team-joehealy.gpg.key\r\n    - comments:\r\n        - '# Managed file'\r\n```\r\ncat /etc/apt/sources.list.d/saltstack.list:\r\n```\r\ndeb http://debian.saltstack.com/debian wheezy-saltstack main\r\n```\r\n\r\nsalt --versions-report:\r\n```\r\nSalt: 0.17.4\r\nPython: 2.7.3 (default, Jan  2 2013, 13:56:14)\r\nJinja2: 2.6\r\nM2Crypto: 0.21.1\r\nmsgpack-python: 0.1.10\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.6\r\nPyYAML: 3.10\r\nPyZMQ: 13.1.0\r\nZMQ: 3.2.3\r\n```"
9904,'s0undt3ch','Always give saltenv argument highest priority\nI was having a problem that when passing saltenv as an argument to the salt.state state function it was being ignored. Passing env worked ok apart from generating a warning. This patch seems to make saltenv work.'
9882,'UtahDave','Permission denied: \'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\highstate.cache.p\'\nI seem to get this error when running this command:\r\nsalt \'admatriusy02c\' state.sls npp\r\n\r\nerror:\r\n[WARNING ] The minion function caused an exception: Traceback (most recent call last):\r\n  File "salt/minion.py", line 722, in _thread_return\r\n  File "salt/modules/state.py", line 382, in sls\r\n  File "salt/utils/__init__.py", line 930, in fopen\r\nIOError: [Errno 13] Permission denied: \'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\highstate.cache.p\''
9880,'UtahDave',"file.managed makes minion hang if drive doesn't exist on Windows\nExecuting the state below on a Windows minion where the D: drive doesn't exist makes the minion process hang. The memory consumed by the process rises indefinitely and the process never returns. It should instead return an error saying this drive doesn't exist. Observed on 0.17.2.\r\n\r\n```\r\nmy_file:\r\n    file.managed:\r\n        - name: D:/MyFolder/my_file.txt\r\n```\r\n\r\n"
9878,'thatch45','salt-ssh jinja TemplateNotFound \nI believe I found an issue with salt-ssh when using jinja map template files.\r\n\r\nPlease take a look at https://github.com/chekolyn/cherokee-formula/blob/master/cherokee/templates/cherokee.conf.jinja\r\n\r\nThis formula renders fine when using the master node configuration but when I try to deploy it using salt-ssh I get the following error:\r\n\r\n    file_|-/etc/cherokee/cherokee.conf_|-/etc/cherokee/cherokee.conf_|-managed:\r\n        ----------\r\n        __run_num__:\r\n            9\r\n        changes:\r\n            ----------\r\n        comment:\r\n            Traceback (most recent call last):\r\n              File "/tmp/.salt/salt/utils/templates.py", line 74, in render_tmpl\r\n                output = render_str(tmplstr, context, tmplpath)\r\n              File "/tmp/.salt/salt/utils/templates.py", line 166, in render_jinja_tmpl\r\n                output = jinja_env.from_string(tmplstr).render(**unicode_context)\r\n              File "/tmp/.salt/jinja2/environment.py", line 969, in render\r\n                return self.environment.handle_exception(exc_info, True)\r\n              File "/tmp/.salt/jinja2/environment.py", line 742, in handle_exception\r\n                reraise(exc_type, exc_value, tb)\r\n              File "<template>", line 1, in top-level template code\r\n              File "/tmp/.salt/salt/utils/jinja.py", line 103, in get_source\r\n                raise TemplateNotFound(template)\r\n            TemplateNotFound: cherokee/map.jinja\r\n            \r\n        name:\r\n            /etc/cherokee/cherokee.conf\r\n        result:\r\n            False'
9874,'terminalmage','salt.states.user generates a traceback when setting passwords on SmartOS\nI\'m getting the following error when creating user accounts on SmartOS with Salt 0.17.4. Although the accounts get created successfully, this traceback is still getting generated. \r\n\r\n```\r\n Name: wheel - Function: group.present - Result: True\r\n Name: py27-pip - Function: pkg.installed - Result: True\r\n Name: foo - Function: group.present - Result: True\r\n Name: bzip2 - Function: pkg.installed - Result: True\r\n Name: autoconf - Function: pkg.installed - Result: True\r\n----------\r\n    State: - user\r\n    Name:     foo\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1307, in call\r\n  File "salt/states/user.py", line 373, in present\r\n  File "salt/modules/solaris_shadow.py", line 208, in set_password\r\n  TypeError: \'NoneType\' object has no attribute \'__getitem__\'\r\n\r\n        Changes:   \r\n```\r\n\r\nLooking at the code here:\r\nhttps://github.com/saltstack/salt/blob/v0.17.4/salt/modules/solaris_shadow.py\r\n\r\nIt appears that the uinfo = info(name) call at line 207 is failing. I think this is triggered by the code here at 118-121 in solaris_shadow.py:\r\n\r\n```\r\n   if len(fields) == 2:\r\n        # For example:\r\n        #   root      NL\r\n        return\r\n```\r\n\r\nThis return should return something, even if it is an empty dict. \r\n\r\nI think this issue is related to the fact that passwd on SmartOS can produce variable output. See this snippet from \'man passwd\' on SmartOS\r\n\r\n```     \r\n     The format of the display is:\r\n\r\n       name status mm/dd/yy min max warn\r\n\r\n     or, if password aging information is not present,\r\n\r\n       name status\r\n```\r\n\r\nBasically, on a new account creation, there may not be password age information present, and thus len==2 and nothing gets returned.'
9782,'techhat','Add ssh gateway option to salt-cloud\nNote: I am working on this, but any help would be much appreciated. I am in IRC as: **bretep**\r\n\r\nMany people use ssh gateways to access machines behind a firewall. Having 8 VPCs in ec2, it does not make sense to deploy 8 different salt-masters to these locations so that salt-cloud can function.\r\n\r\nThe paramiko api request is:\r\n```python\r\nparamiko.ProxyCommand\r\n```\r\nIt would be nice to be able to specify:\r\n```yml\r\nusw2a-web:\r\n  image: ami-fc7712cc\r\n  minion:\r\n    master: salt.server.net\r\n    append_domain: server.net\r\n  subnetid: subnet-888888\r\n  securitygroupid:\r\n    - sg-999999\r\n  ssh_username: ubuntu\r\n  ssh_interface: private_ips\r\n  ssh_gateway: 54.219.266.4\r\n  provider: us-west-2a\r\n  size: m1.small\r\n```\r\n\r\nFor scp file transfer the command would be: (assuming the private ip is 10.2.1.185)\r\n```bash\r\nscp -o "ProxyCommand ssh 54.219.266.4 nc 10.2.1.185 22" scripts/super_cool_bootstrap_script 10.2.1.185:/tmp/bootstrap_script\r\n```\r\n\r\n'
9703,'cachedout','Can no longer load new salt modules into __salt__ from custom grains\n```\r\nTraceback (most recent call last):\r\n  File "/usr/local/salt/virtualenv/lib/python2.7/site-packages/salt/loader.py", line 911, in gen_grains\r\n    ret = fun()\r\n  File "/var/cache/salt/minion/extmods/grains/defaultint.py", line 28, in linux_default_interface\r\n    ip_out = __salt__[\'cmd.run\'](\'ip route show to 0.0.0.0/0\')\r\nKeyError: \'cmd.run\'\r\n```\r\n\r\nreverting this commit fixes the issue:\r\n\r\nhttps://github.com/saltstack/salt/commit/3efb0a867d0596465668300f7b19796e7398e5c2\r\n\r\nIn our custom module we are adding cmd.run by using:\r\n\r\n```\r\n# Solve the Chicken and egg problem where grains need to run before any\r\n# of the modules are loaded and are generally available for any usage.\r\nimport salt.modules.cmdmod\r\n\r\n__salt__ = {\r\n    \'cmd.run\': salt.modules.cmdmod._run_quiet,\r\n    \'cmd.run_all\': salt.modules.cmdmod._run_all_quiet\r\n}\r\n````'
9696,'UtahDave','MSI package fails to install with 0.17.4, extra "None" is inserted into the command\nFrom the salt-users posting:\r\n\r\nIt seems that 0.17.4 adds "None" to the end of the msiexec command:\r\n\r\nFrom a 0.17.4 minion:\r\n```\r\nExecuting command \'msiexec /i "c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extrn_files\\\\base\\\\fileserver.discdrive.bayphoto.com\\\\installs\\\\GPOInstalls\\\\Libreoffice\\\\LibreOffice_4.1.2_Win_x86.msi"  /qn /norestart /l c:\\\\libreoffice_install.log None\' in directory \'C:\\\\Windows\\\\system32\\\\config\\\\systemprofile\'\r\n```\r\n\r\nSo msiexec just hangs out in the background.\r\n\r\nOn a 0.17.2 system, it works as expected:\r\n\r\n```\r\nExecuting command \'msiexec /i "c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extrn_files\\\\base\\\\fileserver.discdrive.bayphoto.com\\\\installs\\\\GPOInstalls\\\\Libreoffice\\\\LibreOffice_4.1.2_Win_x86.msi"  /qn /norestart /l c:\\\\libreoffice_install.log\' in directory \'C:\\\\Windows\\\\system32\\\\config\\\\systemprofile\'\r\n```\r\n\r\nI don\'t know where that None is coming from, but here is our win repo sls for libreoffice:\r\n\r\n```\r\nlibreoffice:\r\n  4.1.2.3:\r\n    installer: \'http://fileserver.discdrive.bayphoto.com/installs/GPOInstalls/Libreoffice/LibreOffice_4.1.2_Win_x86.msi\'\r\n    full_name: \'LibreOffice 4.1.2.3\'\r\n    install_flags: \' /qn /norestart /l c:\\libreoffice_install.log\'\r\n    msiexec: True\r\n    uninstaller: \'http://fileserver.discdrive.bayphoto.com/installs/GPOInstalls/Libreoffice/LibreOffice_4.1.2_Win_x86.msi\'\r\n    uninstall_flags: \' /qn /l c:\\libreoffice_uninstall.log\'\r\n```'
9640,'s0undt3ch',"resource module doesn't exist on Windows platform\nsalt/utils/vt.py imports the resource module without a try/except block.\r\nhttps://github.com/saltstack/salt/blob/develop/salt/utils/vt.py#L33\r\n\r\nThe resource module doesn't exist on Windows systems. \r\nhttp://docs.python.org/2/library/resource.html\r\n\r\n@s0undt3ch, it looks like resource is only used once in vt.py, can this be worked around for Windows?  This stacktrace prevents the minion from starting on Windows in both the RC and develop."
9639,'s0undt3ch',"Saltfile file for managing multiple deployments with salt-ssh, salt-cloud via directories\nI had an idea for salt that I'd like to get some thought and feedback on.\r\n\r\n**Problem**\r\n\r\nI had an idea for salt that I'd like to get some thought and feedback on. One thing we realized quickly when trying to use salt-ssh was that managing mutliple deployments from the same machine requires a lot of verbosity on the commandline. Because salt-ssh, salt-cloud check default locations in /etc/salt for configurations, you commonly have to pass in different cli options for each one.\r\n\r\n**Proposed Feature**\r\n\r\nWhat if like Vagrant, Capistrano, and other tools like them, we allow for a special file that can be checked by salt-ssh, salt-cloud to get these options and set them for you. So while Vagrant has Vagrantfile, Capistrano has Capfile, salt could have Saltfile. The Saltfile could be a simple YAML file that gives instructions to salt-ssh, salt-cloud on what configuration files or directories to use.\r\n\r\nSo instead of:\r\n\r\n```bash\r\nsalt-ssh '*' --config-dir=./saltstack --max-procs=10 sys.doc\r\n```\r\n\r\nWith a Saltfile in the current working directory (or parent directory) it\r\ncould be:\r\n\r\n```bash\r\nsalt-ssh '*' sys.doc\r\n```\r\n\r\nA proposed Saltfile example could look like this:\r\n\r\n```yaml\r\noptions:\r\n    salt-ssh:\r\n        config-dir: ./saltstack\r\n        max-procs: 10\r\n    salt-cloud:\r\n        profile: ./saltstack/profile\r\n        master-config: ./saltstack/master\r\n        cloud-config: ./saltstack/cloud\r\n```\r\n\r\nIf someone passes in a an option in the CLI versus and the Saltfile, the CLI option would override.\r\n\r\nThis makes salt-ssh much more approachable for smaller to medium sized shops that manage multiple deployments and don't want to setup a master for each  one off the bat. The appropriate files for config-dir can be in source control, and a team member can just clone the repository, cd to the correct directory, and immediately start using salt-ssh.\r\n\r\nAny feedback on this would be great since there are probably some scenarios I'm not considering."
9620,'terminalmage','Salt fails with error if no source is matched for file.managed\nI was trying to do something like so in a sls:\r\n\r\n```yaml\r\n/var/spool/cron/root:\r\n  file.managed:\r\n    - source:\r\n      - salt://packages/cron/files/cron.{{ grains[\'host\'] }}/root\r\n      - salt://packages/cron/files/cron.{{ env }}-{{ grains[\'server_role\']|default(\'\') }}/root\r\n      - salt://packages/cron/files/cron.{{ grains[\'server_role\']|default(\'\') }}/root\r\n      - salt://packages/cron/files/cron.{{ env }}/root\r\n    - user: root\r\n    - group: root\r\n```\r\n\r\nand I noticed that if/when the source directory did not exist, salt would exit with an error like so:\r\n\r\n```\r\n    State: - file\r\n    Name:      /var/spool/cron/root\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/state.py", line 1258, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.6/site-packages/salt/states/file.py", line 1091, in managed\r\n    **kwargs\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/file.py", line 1770, in check_managed\r\n    **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/file.py", line 1588, in get_managed\r\n    if salt._compat.urlparse(source).scheme == \'salt\':\r\n  File "/usr/lib64/python2.6/urlparse.py", line 129, in urlparse\r\n    tuple = urlsplit(url, scheme, allow_fragments)\r\n  File "/usr/lib64/python2.6/urlparse.py", line 162, in urlsplit\r\n    cached = _parse_cache.get(key, None)\r\nTypeError: unhashable type: \'list\'\r\n```\r\n\r\nI\'m not 100% sure this is a bug but it seems to me like if the source isn\'t matched then salt would just quietly move along.'
9605,'cachedout','eauth failure with batching\nIf a batch size is set, external auth gives a backtrace, regardless of whether a token is available:\r\n```\r\n$ salt -T -a pam \'*\' test.ping\r\nserver1:\r\n  True\r\nserver2:\r\n  True\r\n...\r\n```\r\n\r\nBut:\r\n```\r\n$ salt -b 2 -T -a pam \'*\' test.ping\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nEauthAuthenticationError: Failed to authenticate, is this user permitted to execute commands?\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt", line 10, in <module>\r\n    salt_main()\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 117, in salt_main\r\n    client.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 61, in run\r\n    batch = salt.cli.batch.Batch(self.config)\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/batch.py", line 24, in __init__\r\n    self.minions = self.__gather_minions()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/batch.py", line 43, in __gather_minions\r\n    for ret in self.local.cmd_iter(*args):\r\n  File "/usr/lib/pymodules/python2.7/salt/client/__init__.py", line 522, in cmd_iter\r\n    **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/client/__init__.py", line 230, in run_job\r\n    return self._check_pub_data(pub_data)\r\n  File "/usr/lib/pymodules/python2.7/salt/client/__init__.py", line 178, in _check_pub_data\r\n    \'Failed to authenticate, is this user permitted to execute \'\r\nEauthAuthenticationError: Failed to authenticate, is this user permitted to execute commands?\r\n```'
9604,'UtahDave','Persission denied on Windows\nI have a Windows 2008 R2 machine and keep getting this error:\r\nadmatriusy02c:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 695, in _thread_return\r\n      File "salt/modules/win_pkg.py", line 457, in refresh_db\r\n      File "salt/modules/cp.py", line 393, in is_cached\r\n      File "salt/modules/cp.py", line 63, in _mk_client\r\n      File "salt/fileclient.py", line 42, in get_file_client\r\n      File "salt/fileclient.py", line 643, in __init__\r\n      File "salt/crypt.py", line 466, in __init__\r\n      File "salt/crypt.py", line 478, in __authenticate\r\n      File "salt/crypt.py", line 320, in sign_in\r\n      File "salt/crypt.py", line 213, in minion_sign_in_payload\r\n      File "M2Crypto/RSA.py", line 157, in save_pub_key\r\n      File "M2Crypto/BIO.py", line 186, in openfile\r\n    IOError: [Errno 13] Permission denied: \'c:\\\\users\\\\exh345\\\\appdata\\\\local\\\\temp\\\\2\\\\tmp3kxie9\'\r\n\r\nSo I assume some of the permissions have changed, since previously on client version 0.17.2, this error wasn\'t present.\r\n\r\nThese people seem to have the same issue:\r\nhttps://groups.google.com/forum/#!topic/salt-users/hGHtl8Xc4m0'
9535,'s0undt3ch','setup.py should not automatically download bootstrap by default\nWhen running setup.py, the current behavior is to download the latest stable bootstrap script, unless explicitly told otherwise. This should be inverted.\r\n\r\nPing @s0undt3ch.'
9531,'UtahDave',"Updated Windows docs to explain the installers provide deps.\nIt isn't very clear in the [Windows documentation](http://docs.saltstack.com/topics/installation/windows.html) whether the installer provides all dependencies or not. A note in that first section would probably resolve this as @UtahDave has confirmed that deps do get installed to provide the full minion setup."
9484,'techhat','SMTP State Module\nLike the XMPP and PagerDuty modules, useful for reactors.'
9468,'terminalmage','Add support to module.git.current_branch for Git < 1.7.8\nThe git.current_branch function doesn\'t work on CentOS 6.x because the version of Git available (1.7.1) does not support the "--list" switch on git branch.\r\n\r\nBased on the git project commit log it looks like support for the switch was added in version 1.7.8.\r\n\r\nCan support be added for the older versions of Git? I haven\'t looked into it very much but  \'git branch\' seems to work the same way as \'git branch --list\'. Although there is additional functionality in some cases: https://github.com/git/git/commit/cddd127b9afe3aa516aafdc38e9a8778f1340e0d\r\n\r\nHere is the minion output:\r\n```\r\n2013-12-27 12:31:30,584 [salt.minion                                 ][INFO    ] User root Executing command git.current_branch with jid 20131227123150160777\r\n2013-12-27 12:31:30,585 [salt.minion                                 ][DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20131227123150160777\', \'tgt\': \'dapp01lxv\', \'ret\': \'\', \'user\': \'root\', \'arg\': [\'/root/test\'], \'fun\': \'git.current_branch\'}\r\n2013-12-27 12:31:30,633 [salt.minion                                 ][DEBUG   ] Parsed args: [\'/root/test\']\r\n2013-12-27 12:31:30,634 [salt.minion                                 ][DEBUG   ] Parsed kwargs: {}\r\n2013-12-27 12:31:30,635 [salt.loaded.int.module.cmdmod               ][INFO    ] Executing command \'git branch --list | grep "^*\\\\ " | cut -d " " -f 2 | grep -v "(detached"\' in directory \'/root/test\'\r\n2013-12-27 12:31:30,664 [salt.loaded.int.module.cmdmod               ][DEBUG   ] stdout: \r\n2013-12-27 12:31:30,665 [salt.minion                                 ][INFO    ] Returning information for job: 20131227123150160777\r\n```\r\nSalt didn\'t return the error, but when you run the command given it returns an unknown option error.\r\n\r\n```\r\n# git branch --list | grep "^*\\\\ " | cut -d " " -f 2 | grep -v "(detached"\r\nerror: unknown option `list\'\r\nusage: git branch [options] [-r | -a] [--merged | --no-merged]\r\n   or: git branch [options] [-l] [-f] <branchname> [<start-point>]\r\n   or: git branch [options] [-r] (-d | -D) <branchname>\r\n   or: git branch [options] (-m | -M) [<oldbranch>] <newbranch>\r\n```'
9454,'techhat','salt-cloud docs should clearly state which versions are compatible with which release of Salt.\nWhile this may soon become irrelevant as salt-cloud will be packaged with Salt, for historical purposes it would be great if both the installation docs, as well as the troubleshooting documentation for salt-cloud included information regarding which versions of salt-cloud is compatible with which version of Salt.'
9440,'cachedout','Wrong master address causes non-informative unhandled exception\nIn /etc/salt/minion:\r\n```\r\nmaster: .5\r\n```\r\n\r\n<b>[root@ip-10-0-0-182 ~]# service salt-minion start</b>\r\n```\r\nStarting salt-minion daemon:                               [  OK  ]\r\n```\r\n\r\n<b>[root@ip-10-0-0-182 ~]# service salt-minion status</b>\r\n```\r\nsalt-minion dead but pid file exists\r\n```\r\n\r\n<b>[root@ip-10-0-0-182 ~]# tail -18 /var/log/salt/minion</b>\r\n```\r\n2013-12-26 09:11:07,656 [salt.log.setup                           ][ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nTypeError: getaddrinfo() argument 1 must be string or None\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-minion", line 14, in <module>\r\n    salt_minion()\r\n  File "/usr/lib/python2.6/site-packages/salt/scripts.py", line 30, in salt_minion\r\n    minion.start()\r\n  File "/usr/lib/python2.6/site-packages/salt/__init__.py", line 219, in start\r\n    self.prepare()\r\n  File "/usr/lib/python2.6/site-packages/salt/__init__.py", line 207, in prepare\r\n    self.minion = salt.minion.Minion(self.config)\r\n  File "/usr/lib/python2.6/site-packages/salt/minion.py", line 501, in __init__\r\n    opts.update(resolve_dns(opts))\r\n  File "/usr/lib/python2.6/site-packages/salt/minion.py", line 79, in resolve_dns\r\n    salt.utils.dns_check(opts[\'master\'], True, opts[\'ipv6\'])\r\n  File "/usr/lib/python2.6/site-packages/salt/utils/__init__.py", line 445, in dns_check\r\n    addr, None, socket.AF_UNSPEC, socket.SOCK_STREAM\r\nTypeError: getaddrinfo() argument 1 must be string or None\r\n```'
9413,'UtahDave','module win_useradd not available in salt-minion 17.2\nAfter upgrading some windows minions to 17.2, I noticed that the user module (and many other win_ modules) were missing:\r\n\r\n```\r\n[root@ip-10-0-2-140 pillar]# salt www9 user.info root\r\nwww9:\r\n    "user.getent" is not available.\r\n[root@ip-10-0-2-140 pillar]# salt www9 user.add test\r\nwww9:\r\n    "user.info" is not available.\r\n[root@ip-10-0-2-140 pillar]# salt www9 group.add testuser\r\nwww9:\r\n    "user.add" is not available.\r\n```\r\n\r\nInvestigating some, I found that the full pywin32 egg is no longer being included in the install of the salt-minion on windows.  When I copied and old version of the egg into C:\\salt\\salt-0.17.2.win-amd64, it resolved the issue and all the modules returned, though the minion complained about a bunch of dupe modules:\r\n\r\n```\r\nC:\\salt\\salt-0.17.2.win-amd64\\msgpack_python-0.3.0-py2.7-win-amd64.egg\\msgpack\\_\r\npacker.py:3: UserWarning: Module win32api was already imported from C:\\salt\\salt\r\n-0.17.2.win-amd64\\win32api.pyd, but c:\\salt\\salt-0.17.2.win-amd64\\pywin32-216-py\r\n2.7-win-amd64.egg is being added to sys.path\r\nC:\\salt\\salt-0.17.2.win-amd64\\msgpack_python-0.3.0-py2.7-win-amd64.egg\\msgpack\\_\r\npacker.py:3: UserWarning: Module pywintypes was already imported from C:\\salt\\sa\r\nlt-0.17.2.win-amd64\\pywintypes27.dll, but c:\\salt\\salt-0.17.2.win-amd64\\pywin32-\r\n216-py2.7-win-amd64.egg is being added to sys.path\r\nC:\\salt\\salt-0.17.2.win-amd64\\msgpack_python-0.3.0-py2.7-win-amd64.egg\\msgpack\\_\r\npacker.py:3: UserWarning: Module winerror was already imported from C:\\salt\\salt\r\n-0.17.2.win-amd64\\library.zip\\winerror.pyc, but c:\\salt\\salt-0.17.2.win-amd64\\py\r\nwin32-216-py2.7-win-amd64.egg is being added to sys.path\r\nC:\\salt\\salt-0.17.2.win-amd64\\msgpack_python-0.3.0-py2.7-win-amd64.egg\\msgpack\\_\r\npacker.py:3: UserWarning: Module win32com was already imported from C:\\salt\\salt\r\n-0.17.2.win-amd64\\library.zip\\win32com\\__init__.pyc, but c:\\salt\\salt-0.17.2.win\r\n-amd64\\pywin32-216-py2.7-win-amd64.egg is being added to sys.path\r\nC:\\salt\\salt-0.17.2.win-amd64\\msgpack_python-0.3.0-py2.7-win-amd64.egg\\msgpack\\_\r\npacker.py:3: UserWarning: Module pythoncom was already imported from C:\\salt\\sal\r\nt-0.17.2.win-amd64\\pythoncom27.dll, but c:\\salt\\salt-0.17.2.win-amd64\\pywin32-21\r\n6-py2.7-win-amd64.egg is being added to sys.path\r\n```\r\n\r\nSo my "solution" seems hacky at best, not sure what the proper fix is, though it did make the needed modules work:\r\n```\r\n[root@ip-10-0-2-140 pillar]# salt www9 user.add testuser\r\nwww9:\r\n    True\r\n```'
9363,'terminalmage','Memcached state/module needs to be rewritten\nMost of the memcache states are just wrappers for their corresponding module execution functions. These are completely unnecessary and just not how Salt does things. There should only be two states: ``managed`` and ``absent``. The others can all be achieved through ``module.run`` or ``module.wait`` states, if one wants to execute these via states instead of via the CLI.'
9341,'terminalmage',"Don't call 'apt-get update' for pkg.installed when the package has been installed\nsalt will call 'apt-get update' every time as long as pkg.installed presents in state file. This makes applying a unchanged state on ubuntu machine costs ~6 seconds which hurts user experience comparing to other similar tools like puppet.\r\n\r\ndetailed discussion see https://groups.google.com/d/msg/salt-users/4izXcmhhAGA/fk9PuZxOf5gJ"
9324,'whiteinge','Inconsistency in master_call interface for Runner/Wheel clients\n* ``salt.runner.RunnerClient().master_call()`` is async and only returns a JID.\r\n* ``salt.wheel.WheelClient().master_call()`` is sync and returns the full return.\r\n\r\nWe should support both event-driven and non-event-driven interfaces, of course!'
9234,'terminalmage','Invalid pkg source results throws an exception\nWhen attempting to use a source for a pkg install, if the file does not exist, an exception is thrown.\r\n\r\n```\r\nfoo:\r\n  pkg.installed:\r\n    - sources:\r\n      - foo: https://www.foo.com/bar_file_missing.deb\r\n```\r\n\r\n```\r\n----------\r\n[ERROR   ] An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1360, in call\r\n    **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.7/dist-packages/salt/states/pkg.py", line 465, in installed\r\n    **kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/apt.py", line 380, in install\r\n    **kwargs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/pkg_resource.py", line 245, in parse_targets\r\n    __salt__[\'cp.cache_file\'](pkg_src, saltenv),\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/cp.py", line 309, in cache_file\r\n    result = __context__[\'cp.fileclient\'].cache_file(path, saltenv)\r\n  File "/usr/lib/python2.7/dist-packages/salt/fileclient.py", line 142, in cache_file\r\n    return self.get_url(path, \'\', True, saltenv)\r\n  File "/usr/lib/python2.7/dist-packages/salt/fileclient.py", line 534, in get_url\r\n    *BaseHTTPServer.BaseHTTPRequestHandler.responses[ex.code]))\r\nMinionError: HTTP error 404 reading https://www.foo.com/bar_file_missing.deb: Nothing matches the given URI\r\n```'
9232,'terminalmage','Invalid URL scheme throws exception on salt.states.file\nThis a very low hanging fruit issue, but I thought I\'d report it anyway. Using an invalid URL scheme on `salt.states.file` source:// results in a traceback.\r\n\r\n```\r\nconfig-file:\r\n  file.managed:\r\n    - name: /etc/somedir/file.yml\r\n    - template: jinja\r\n    - source: source://templates/file.yml\r\n```\r\n\r\n```\r\n----------\r\n    State: - file\r\n    Name:      /etc/somedir/file.yml\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1290, in call\r\n    for low in chunks:\r\n  File "/usr/lib/python2.7/dist-packages/salt/states/file.py", line 1114, in managed\r\n    **kwargs\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/file.py", line 1552, in get_managed\r\n    sfn = __salt__[\'cp.cache_file\'](source, env)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/cp.py", line 241, in cache_file\r\n    result = __context__[\'cp.fileclient\'].cache_file(path, env)\r\n  File "/usr/lib/python2.7/dist-packages/salt/fileclient.py", line 143, in cache_file\r\n    \'\'\'\r\n  File "/usr/lib/python2.7/dist-packages/salt/fileclient.py", line 535, in get_url\r\n    ret[\'hash_type\'] = \'md5\'\r\nMinionError: Error reading source://templates/file.yml: unknown url type: source\r\n```'
9194,'cachedout',"gitfs file hashes\nThe only supported argument for 'hash_type' is 'blob_sha1'. This either needs to be documented or expanded to support other hashes. I've created a test that is currently disabled called 'test_file_hash_md5'."
9193,'cachedout','gitfs file_list_emptydirs is broken\nThe test_file_list_emptydirs() test is failing when it should not. I have disabled the test for now but it should be re-enabled once this is resolved. '
9182,'terminalmage',"Add an option to cmd.run* to specify a loglevel\nRight now, ``cmd.run*`` will log the stdout/stderr at loglevel INFO. However, for things like package management, there are hundreds or sometimes 1000+ lines of output when a command is run. This doesn't need to be logged at loglevel INFO, but is still useful for development purposes.\r\n\r\nWe need an option to be added to ``cmd.run*`` which allows the developer to specify the loglevel for stdout/stderr, so that on a command-by-command basis the loglevel can be chosen. This will allow verbose commands to be logged at debug to aid in development, while hiding this output from the minion log for the majority of people for whom this information is extraneous.\r\n\r\nThis would deprecate the ``quiet`` option, as this could be achieved by using ``loglevel=quiet``."
9161,'terminalmage',"modules.debian_ip._parse_interfaces(): var used before assignment\n```\r\n************* Module salt.modules.debian_ip\r\nsalt/modules/debian_ip.py:358: [E0601(used-before-assignment), _parse_interfaces] Using variable 'cmd_key' before assignment\r\n```"
9150,'cachedout',"Mysql additions\nBig improvments on MySql module internals.\r\nMade some tests, found some edge cases, fixed the code.\r\n\r\nNow I encoutered some problems:\r\n* Connection on database containing non ascci characters, or with users containing non ascii characters are not working in current version of MySQLdb :  https://github.com/farcepest/MySQLdb1/issues/40 . So this part is commented on tests. Maybe we could add a `latin1` decode for a subset of the non-ascii world on theses strings on the connection string.\r\n* Grants queries on MySQL escapes the `%` and `_` on the database name, but only for the `db.*` form, not for `db.table`, it would be too easy.\r\n* MySQLdb needs a `%%` escape for all '%' characters in the whole query to avoid conflicts with escaped arguments... but this escape should not be done on query running in `execute()` without arguments.\r\n* I added `use_unicode` in the connection strings. After some tests this attributes should always be set to False, internally it is set to True when `charset` is used on the connection, but when it is True encoded strings are coming back in `unicode()` and not `str()`\r\n\r\nI made several fixs on the grant queries lexical analysis, and on the mysql_grants state using it. Now this part is quite unsure. It needs some more tests\r\n* does the grants applies on users when they use a password?\r\n* why not using the `table` element on tokens, this makes quite hard to manage table grants with the current states\r\n* why allowing escape=False on the database name? Database escaping rules are complex and ensure security of the query.\r\n\r\nNow, that's already a big peer review stuff for you, I'm open on comments and fixs."
9138,'cro','utils.schedule.clean_proc_dir causes exception on minion startup\nNo idea of what it is supposed to do or why it\'s broken but here is the output from salt log:\r\n\r\n```\r\n2013-12-10 13:28:48,181 [salt             ][INFO    ] Setting up the Salt Minion "task2"\r\n2013-12-10 13:28:48,746 [salt.minion      ][INFO    ] Authentication with master successful!\r\n2013-12-10 13:28:49,801 [salt.log.setup   ][ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nTypeError: argument of type \'NoneType\' is not iterable\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-minion", line 14, in <module>\r\n    salt_minion()\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 30, in salt_minion\r\n    minion.start()\r\n  File "/usr/lib/pymodules/python2.7/salt/__init__.py", line 219, in start\r\n    self.prepare()\r\n  File "/usr/lib/pymodules/python2.7/salt/__init__.py", line 207, in prepare\r\n    self.minion = salt.minion.Minion(self.config)\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 514, in __init__\r\n    self.returners)\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/schedule.py", line 69, in __init__\r\n    clean_proc_dir(opts)\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/schedule.py", line 259, in clean_proc_dir\r\n    if \'pid\' in job:\r\nTypeError: argument of type \'NoneType\' is not iterable\r\n```\r\n\r\n\r\nVersion:\r\n\r\n```\r\n           Salt: 0.17.2\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
9123,'terminalmage','file.replace/blockreplace states return non-dict changes data\nThese states return a diff as the ``changes`` data, resulting in an "Invalid Changes data" notice in the highstate output.\r\n\r\n```\r\nvirtucentos:\r\n----------\r\n          ID: /tmp/foo.txt\r\n    Function: file.replace\r\n      Result: True\r\n     Comment: Changes were made\r\n     Changes:   Invalid Changes data: ---\r\n+++\r\n@@ -1,1 +1,1 @@\r\n-Goodbye world!\r\n+Hello world!\r\n\r\n\r\nSummary\r\n------------\r\nSucceeded: 1\r\nFailed:    0\r\n------------\r\nTotal:     1\r\n```'
9112,'terminalmage',"Replace git-python with dulwich\ngit-python hasn't had a release in two years. It's basically unmaintained. dulwich is actively maintained and has a more complete implementation in python. The only major downside to dulwich is that its documentation is relatively poor."
9099,'basepi','result do not output when specific the minion use syndic\nI have 3 node\r\n\r\nnode1: master\r\nnode2: master + syndic\r\nnode3: minion\r\n\r\n\r\n\r\n\\# salt \'*\' test.ping\r\nnode3:\r\n    True\r\n\\# salt \'node3\' test.ping\r\nnothing is output\r\n\r\ni have trace this the result of "salt \'node3\' test.ping" is returned but do not output\r\n\r\nis there anyone can help me?\r\n\r\nthanks \r\n\r\n\r\n'
9097,'terminalmage',"file.replace does not convert arguments to string, truncates file\nWhen running:\r\n`salt '*' file.replace /root/my_file 'some text' '2'`\r\n\r\nI am getting this error message:\r\n\r\n```\r\nTypeError encountered executing file.replace: object of type 'int' \r\nhas no len(). See debug log for more info.  Possibly a missing arguments \r\nissue:  \r\nArgSpec(args=['path', 'pattern', 'repl', 'count', 'flags', 'bufsize', \r\n'backup', 'dry_run', 'search_only', 'show_changes'], varargs=None, \r\nkeywords=None, defaults=(0, 0, 1, '.bak', False, False, True))\r\n```\r\n\r\nThis is due to the fact that '2' is treated as an int. If I change it to 's2' everything works as expected.\r\n\r\nWhen both arguments are numbers the destination file is truncated.\r\n`salt '*' file.replace /root/my_file '1' '2'`\r\n\r\nThe error message is a bit different:\r\n\r\n```\r\nTypeError encountered executing file.replace: first argument \r\nmust be string or compiled pattern. See debug log for more info.  \r\nPossibly a missing arguments issue:  \r\nArgSpec(args=['path', 'pattern', 'repl', 'count', 'flags', 'bufsize', \r\n'backup', 'dry_run', 'search_only', 'show_changes'], varargs=None, \r\nkeywords=None, defaults=(0, 0, 1, '.bak', False, False, True))\r\n```\r\n\r\nThis is on ubuntu precise, salt version `0.17.2` installed from the ppa."
9093,'terminalmage',"ignored args in memcached module\n```\r\nsalt/modules/memcached.py:126: [W0613(unused-argument), add] Unused argument 'min_compress_len'\r\nsalt/modules/memcached.py:126: [W0613(unused-argument), add] Unused argument 'time'\r\nsalt/modules/memcached.py:188: [W0613(unused-argument), replace] Unused argument 'min_compress_len'\r\nsalt/modules/memcached.py:188: [W0613(unused-argument), replace] Unused argument 'time'\r\n```"
9085,'terminalmage',"user state does not ensure fullname variable is a string.\nOn line 214 of https://github.com/saltstack/salt/blob/develop/salt/states/user.py#L214 it's stated that the following items need to be specified as a string to ensure values are loaded properly, however the fullname variable isn't forced to be a string like the others are on line [230](https://github.com/saltstack/salt/blob/develop/salt/states/user.py#L230). This can create an edge case that generates errors where a user inputs an actual name the first time, but then changes that name to a integer on subsequent runs. Is there any reason that the fullname variable isn't also turned into a string? If not, I'll fix this later."
9052,'UtahDave',"Windows Minions use too much CPU if master has rejected the salt-key\nIf you reject a Windows minion's key with salt-key, the minion will consume high % of cpu. It seems to be spinning on key checks. My workstation was using 50% on two cpus. \r\n\r\nVersion 17.1.1. \r\n\r\nVery repeatable. Install minion on a windows system. Reject the key on the master. Observe the salt process on the windows minion. "
9035,'UtahDave',"Windows packages - node specific installation arguments\nCurrently, install flags are specified only in the windows package repository.  There are cases where these must be machine specific.  For instance, if the installation location changes, or the automated installer requires a different product key.\r\n\r\nIn my instance, I'm automating an InstallShield installer that requires a response file to be supplied.  Puppet / Chef allow the response file to be generated via a template to a location on the local filesystem, where it can be referenced by an argument to the installer.  \r\n"
8979,'terminalmage','pkg.check_db doesn\'t seem to work on Arm architectures\n\r\n\r\nThe following example returns True on CentOS 6 , Fedora 19, Fedora 20 on i686 and x86_64 machines. However it will return False on a Fedora 20 Arm Machine (tested in a Qemu-arm VM and on a BeagleBone Black)\r\n\r\nExample:\r\n\r\n```\r\n# salt \'arm-minion\' pkg.check_db tmux\r\n\r\n\'arm-minion\' :\r\n    ----------\r\n    tmux:\r\n        ----------\r\n        found:\r\n            False\r\n        suggestions:\r\n\r\n```\r\n\r\nLogging into the minion and running a yum install tmux is successful.\r\n\r\nInterestingly enough a noarch package does not return anything and no errors are shown on the minion.\r\n\r\nUsing salt 0.17.2.\r\n\r\nI dug into this when my "pkg.installed" states where failing and tried pkg.check_db.'
8976,'cachedout','Salt disk.usage fails in docker\n```\r\nsalt \'*\' disk.usage\r\nminion:\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.6/site-packages/salt/minion.py", line 659, in _thread_return\r\n        return_data = func(*args, **kwargs)\r\n      File "/usr/lib/python2.6/site-packages/salt/modules/disk.py", line 71, in usage\r\n        while not comps[1].isdigit():\r\n    IndexError: list index out of range\r\n```\r\nThe output of the `df` command is special because I am inside a docker. Consequently inside the container `df` returns `df: cannot read table of mounted file systems: No such file or directory`\r\n\r\nThis is quite a special case but I guess there is a better way to handle such exception.\r\n\r\nOS: CentOS 6.4\r\nBoth minion and master have the same salt version:\r\n```\r\nsalt --versions-report \r\n           Salt: 0.17.1\r\n         Python: 2.6.6 (r266:84292, Feb 22 2013, 00:00:18)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n\r\n```'
8886,'s0undt3ch','Move salt_cloud/tests/ into tests/\n@s0undt3ch, these are left over from the merge. How hard do you think it would be to clean this up?'
8863,'s0undt3ch',"deploy_scripts_search_path ignored\nI tried to set the deploy_scripts_search_path a profile but it doesn't work. Always search the deploy scripts in the default location.\r\n\r\nsalt-cloud 0.8.9"
8845,'cachedout',"test.sleep returns inconsistent results\nHi there, I wonder if this is a bug. \r\n\r\nI have 4 minions, all running the same versions of everything, and the master is running on one of the system that has a minion.\r\n\r\nI ran `test.sleep` a couple of times, and these are the result I get:\r\n``` bash\r\nwari@god:~$ sudo salt '*' test.sleep 10\r\ngitlab:\r\n    True\r\nredmine:\r\n    True\r\nwari@god:~$ sudo salt '*' test.sleep 10\r\ngod:\r\n    True\r\nwari@god:~$ sudo salt '*' test.sleep 5\r\ngod:\r\n    True\r\ndeebeeserver:\r\n    True\r\ngitlab:\r\n    True\r\nredmine:\r\n    True\r\nwari@god:~$ sudo salt '*' test.sleep 6\r\ngod:\r\n    True\r\n```\r\n\r\nThing is anything beyond 6 seconds will return some of the times, and mostly `god` answers.\r\n\r\n``` bash\r\nwari@god:~$ sudo salt '*' cmd.run 'salt-minion --versions'\r\ndeebeeserver:\r\n               Salt: 0.17.2\r\n             Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.4.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.0.0\r\n                ZMQ: 3.2.2\r\n.... (the rest are the repeat of the above)\r\n```\r\n\r\nI'm running on Ubuntu 12.10 LTS Server."
8791,'cachedout','the latest release seems not support Ubuntu apt-get\nI run saltstack v0.17.2 in a fresh Ubuntu 12.04 LTS environment, and set the states providers as below:\r\n    pkg: apt\r\n    service: upstart\r\n\r\nThen I run salt locally and an exception throws:\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/dist-packages/salt/scripts.py", line 77, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/__init__.py", line 293, in run\r\n    caller = salt.cli.caller.Caller(self.config)\r\n  File "/usr/lib/python2.7/dist-packages/salt/cli/caller.py", line 45, in __init__\r\n    self.minion = salt.minion.SMinion(opts)\r\n  File "/usr/lib/python2.7/dist-packages/salt/minion.py", line 246, in __init__\r\n    self.gen_modules()\r\n  File "/usr/lib/python2.7/dist-packages/salt/minion.py", line 256, in gen_modules\r\n    self.opts[\'environment\'],\r\n  File "/usr/lib/python2.7/dist-packages/salt/pillar/__init__.py", line 32, in get_pillar\r\n    }.get(opts[\'file_client\'], Pillar)(opts, grains, id_, env, ext)\r\n  File "/usr/lib/python2.7/dist-packages/salt/pillar/__init__.py", line 79, in __init__\r\n    self.functions = salt.loader.minion_mods(opts)\r\n  File "/usr/lib/python2.7/dist-packages/salt/loader.py", line 101, in minion_mods\r\n    provider_overrides=True\r\n  File "/usr/lib/python2.7/dist-packages/salt/loader.py", line 844, in gen_functions\r\n    newfuncs = raw_mod(self.opts, provider, funcs)\r\n  File "/usr/lib/python2.7/dist-packages/salt/loader.py", line 113, in raw_mod\r\n    return load.gen_module(name, functions)\r\n  File "/usr/lib/python2.7/dist-packages/salt/loader.py", line 512, in gen_module\r\n    mod.__init__(self.opts)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/apt.py", line 62, in __init__\r\n    if __virtual__():\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/apt.py", line 51, in __virtual__\r\n    if __grains__[\'os_family\'] != \'Debian\':\r\nKeyError: \'os_family\''
8781,'cro',"virtualenv misleading error message\nI setup this state in Saltstack 1.16.3 /srv/salt/pypy/init.sls:\r\n```\r\n/opt/pypy:\r\n  file.recurse:\r\n    - source: salt://pypy/pypy-2.2-linux64/\r\n    - include_empty: true\r\n/opt/pypy-virtualenv:\r\n  virtualenv.managed:\r\n    - path: /opt/pypy-virtualenv\r\n    - python: /opt/pypy/bin/pypy\r\n    - pip: True\r\n```\r\nWhen running salt '*' state.sls pypy, the following error occurs on the minion:\r\n```\r\n/bin/sh: 1: /opt/pypy-virtualenv/bin/python: not found\r\n```\r\nI think this is misleading -- the minion log shows:\r\n```\r\n2013-11-24 01:02:27,373 [salt.loaded.int.module.cmdmod][ERROR   ] Command 'virtualenv --python=/opt/pypy/bin/pypy /opt/pypy-virtualenv' failed with return code: 3\r\n2013-11-24 01:02:27,374 [salt.loaded.int.module.cmdmod][ERROR   ] stdout: The executable /opt/pypy/bin/pypy (from --python=/opt/pypy/bin/pypy) is not executable\r\n2013-11-24 01:02:27,378 [salt.state       ][ERROR   ] {'new': '/bin/sh: 1: /opt/pypy-virtualenv/bin/python: not found'}\r\n```\r\nI think it would be better to use\r\n```\r\nThe executable /opt/pypy/bin/pypy (from --python=/opt/pypy/bin/pypy) is not executable\r\n```\r\nas an error message, or to check argument validity in *virtualenv_mod.py* line 165.\r\n\r\nNote that this issue report is not about the actual problem (I just forgot to set the correct file mode) but the misleading error message.\r\n\r\nPlease feel free to share your comments!\r\n\r\nThanks in advance for your support and for this awesome tool!"
8753,'techhat','[salt.cloud] Joyent API change\nIt looks like the Joyent API has changed again:\r\n\r\nhttps://images.joyent.com/docs/\r\n\r\nCurrently `--list-images` is broken. It needs to be updated to use the new API.'
8738,'terminalmage','install() got an unexpected keyword argument \'skip_verify\'\nHello,\r\n\r\nThis is a follow-up of the thread I started on the mailing list: https://groups.google.com/forum/#!topic/salt-users/u8mwQeEkOY8.\r\n\r\nMy environment is FreeBSD 9.2:\r\n```$ uname -a\r\nFreeBSD denica 9.2-RELEASE-p1 FreeBSD 9.2-RELEASE-p1 #0: Fri Nov 22 04:01:22 EET 2013     root@:/usr/obj/usr/src/sys/GENERIC  i386```\r\n\r\nI installed salt 0.17.2 from sysutils/py-salt port. My file_roots reside at /usr/local/etc/salt/states and I have top.sls and common.sls files in it:\r\n\r\n```\r\n# cat top.sls \r\nbase:\r\n  \'*\':\r\n    -  common\r\n# cat common.sls \r\nvim:\r\n  pkg:\r\n    -  installed\r\n```\r\n\r\nI specified the pkg provider in minion\'s configuration file as pkgng:\r\n\r\n```\r\nproviders:\r\n  pkg: pkgng\r\n```\r\n\r\nWhen I run "salt \'*\' state.highstate"I get the following error:\r\n```\r\n# salt \'*\' state.highstate\r\ndenica.lan:\r\n----------\r\n    State: - pkg\r\n    Name:      vim\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/site-packages/salt/state.py", line 1258, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/local/lib/python2.7/site-packages/salt/states/pkg.py", line 459, in installed\r\n    **kwargs)\r\nTypeError: install() got an unexpected keyword argument \'skip_verify\'\r\n```\r\nI also ran\r\n\r\n```\r\n/usr/local/bin/salt-call -l debug state.single pkg.installed name=vim\r\n```\r\n\r\nand the result is the same error.\r\n\r\nI created a gist with the debug logs at https://gist.github.com/rambius/7601185.\r\n\r\nThank you very much for your help.\r\n\r\nRegards\r\nRambius'
8717,'cachedout','postgres_user.present fails but reports success\nThis reports success:\r\n\r\n    auser:\r\n      postgres_user.present:\r\n        - superuser: true\r\n        - createdb: true\r\n        - password: apassword\r\n        - user: postgres\r\n\r\nWhen adding the user has failed with:\r\n`ERROR:  conflicting or redundant options`'
8702,'cachedout',"mysql_user state doesn't honor unix_socket argument\n```\r\nsalt/states/mysql_user.py:64: [W0613(unused-argument), present] Unused argument 'unix_socket'\r\n```"
8701,'UtahDave',"A Minion running on a Syndic blocks lower minions from responding\nI set up a scenario like the image below.\r\n\r\n![syndicerror](https://f.cloud.github.com/assets/306240/1586571/8c2401ec-522a-11e3-98a8-69131c31a314.png)\r\n\r\nEach salt-master has 2 minions below it with properly accepted keys. The syndics also have salt-minion installed and connected to the master directly above it.\r\n\r\nWhen I run 'salt \\* test.ping' from the master master I only get responses from its 2 minions directly below it and the minion running on the syndic below it. If I stop the minion service on the syndic, then the minions below the syndic respond to commands from the mastermaster."
8698,'cro','State ssh_auth.absent does not remove keys is type or options are present in key "name"\nGiven that there is no example for ssh_auth.absent in the documentation, a Salt state writer might write the following state to remove a key by copying a key "name" that was previously specified in a ssh_auth.present state:\r\n\r\n```\r\nsshkeys:\r\n  ssh_auth:\r\n    - absent\r\n    - user: root\r\n    - comment: removing user@email from authorized keys\r\n    - names:\r\n      - ssh-rsa AAAAB3Nza[...]== user@mail\r\n```\r\n\r\nHowever, Salt will falsely return that the key is not present.  This is because the absent function uses:\r\n\r\n```\r\n    # Get just the key\r\n    name = name.split(\' \')[0]\r\n```\r\n\r\nhttps://github.com/saltstack/salt/blob/fbfd926fdd0c7686b6be4da1dbfe4e9e18bbea60/salt/states/ssh_auth.py#L272\r\n\r\nFor the above example, the ssh_auth.absent state will incorrectly check for the key "ssh-rsa" using the ssh.check_key module.\r\n\r\nI\'ve verified that the remove key operation works properly if the "ssh-rsa" prefix is omitted in the example above.'
8690,'cachedout',"Custom grains errors not logging\nI scratched my head for 15 minutes on this one:\r\n\r\nI have a couple of custom grains defined under /etc/salt/grains\r\n\r\nThey wouldn't load for some reason\r\n\r\nIt occurs I had a typo in the file.. something like this:\r\n\r\n```\r\nCOMPANY_GRAIN1: blah\r\nCOMPANY_GRAIN2:bleh\r\nCOMPANY_GRAIN3: lolz\r\n```\r\n\r\nNotice the missing space on COMPANY_GRAIN2?\r\n\r\nThe whole file was discarded silently even when running ```salt-minion -l debug```. I would expect logs to report the syntax error.\r\n\r\n\r\n\r\n"
8648,'terminalmage','KeyError: \'cpuarch\' in salt/modules/yumpkg5.py in _pkg_arch\nWe are doing testing with Salt 0.17.2 on CentOS 6, built with Python 2.7.  We have "providers: pkg: yumpkg5" set in /etc/salt/minion.d/.\r\n\r\nWe see this error for all pkg state calls:\r\n\r\n```\r\n    State: - pkg\r\n    Name:      logrotate\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1278, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.7/site-packages/salt/states/pkg.py", line 593, in latest\r\n    **kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/yumpkg5.py", line 198, in latest_version\r\n    pkgname, pkgarch = _pkg_arch(name)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/yumpkg5.py", line 161, in _pkg_arch\r\n    return name, __grains__[\'cpuarch\']\r\nKeyError: \'cpuarch\'\r\n```\r\n\r\nThis code is part of @terminalmage \'s recent changeset here: a7d880cc456cd688b36b3078d27f87accfcc1993\r\n\r\nThere is "# TODO: Fix __grains__ availability in provider overrides" which sounds like what we are hitting.\r\n\r\nIn this spot, ```__grains__``` appears to be empty, but ```salt["grains.item"]("key")``` is returning proper values. This is tested by putting in a couple print statements:\r\n\r\n```\r\ndef _pkg_arch(name):\r\n    \'\'\'\r\n    Returns a 2-tuple of the name and arch parts of the passed string. Note\r\n    that packages that are for the system architecture should not have the\r\n    architecture specified in the passed string.\r\n    \'\'\'\r\n    # TODO: Fix __grains__ availability in provider overrides\r\n    try:\r\n        pkgname, pkgarch = name.rsplit(\'.\', 1)\r\n    except ValueError:\r\n        print "debug-out1", __grains__\r\n        print "debug-out2", __salt__[\'grains.item\'](\'cpuarch\')\r\n        return name, __grains__[\'cpuarch\']\r\n    if pkgarch in __SUFFIX_NOT_NEEDED:\r\n        pkgname = name\r\n    return pkgname, pkgarch\r\n```\r\n\r\nand then running:\r\n\r\n```\r\n$ sudo -i salt-call state.highstate |grep debug-out\r\n...\r\ndebug-out1 {}\r\ndebug-out2 {\'cpuarch\': \'x86_64\'}\r\n```'
8635,'s0undt3ch','logstash_zmq: not working properly (un-handled exception)\nWhen using the new logstash_zmq_handler (from 0.17)\r\n\r\nOn Salt side, running a simple salt command such as `salt \'*\' test.ping` will display the following error:\r\n\r\n```\r\n@arch-dev-vbox ~/projects/jules/halite:G:v0.17.2 > salt \'*\' test.ping\r\nlocaldev:\r\n    True\r\nError in atexit._run_exitfuncs:\r\nTraceback (most recent call last):\r\n  File "/usr/lib64/python2.7/atexit.py", line 24, in _run_exitfuncs\r\n    func(*targs, **kargs)\r\n  File "/usr/lib64/python2.7/logging/__init__.py", line 1651, in shutdown\r\n    h.close()\r\n  File "/media/sf_shared/jules/salt/salt/log/handlers/logstash_mod.py", line 323, in close\r\n    self._context.destroy(1 * 1000)\r\nAttributeError: \'NoneType\' object has no attribute \'destroy\'\r\nError in sys.exitfunc:\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nAttributeError: \'NoneType\' object has no attribute \'destroy\'\r\nTraceback (most recent call last):\r\n  File "/usr/lib64/python2.7/atexit.py", line 24, in _run_exitfuncs\r\n    func(*targs, **kargs)\r\n  File "/usr/lib64/python2.7/logging/__init__.py", line 1651, in shutdown\r\n    h.close()\r\n  File "/media/sf_shared/jules/salt/salt/log/handlers/logstash_mod.py", line 323, in close\r\n    self._context.destroy(1 * 1000)\r\nAttributeError: \'NoneType\' object has no attribute \'destroy\'\r\n```\r\n\r\nVersion info:\r\n\r\n```\r\nsalt --versions-report\r\n           Salt: 0.17.2\r\n         Python: 2.7.5 (default, Sep  6 2013, 09:55:21)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.4.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.0.0\r\n            ZMQ: 3.2.3\r\n``` '
8590,'cachedout',"managed files are initially copied/created with 0644 permissions irrespective of mode.\nGiven a state to manage a file by salt, the resulting file that is copied to it's ultimate destination on the filesystem initially (albeit briefly) has 644 permissions allowing a window of opportunity for a local user to gain undesired access to the file's contents. Despite the state's mode specifying 600, for example.\r\n\r\nI first spotted this on CentOS 6.4 with Salt 0.16.4 on both master and minion (EPEL RPMs) but testing shows the same behaviour on the develop branch - last pull a couple of hours ago.\r\n\r\nSteps to demonstrate (on Gentoo here, in this case that's important):\r\n\r\nLogs, files, configuration are all running in a virtual environment created by pip.\r\n\r\n```\r\n$ cat srv/salt/files/secure.sls\r\nsecure:\r\n  file.managed:\r\n    - name: /home/grant/secure.txt\r\n    - mode: 600\r\n    - source: salt://files/secure.txt\r\n```\r\n\r\nRunning stat in another terminal (whilst calling ```salt-call state.sls files.secure``` in the virtual environment):\r\n\r\n```\r\n$ while true; do stat /home/grant/secure.txt 2>/dev/null --format=%a; done | grep -v '60'\r\n644\r\n```\r\n\r\nBecause the operation can be quick, it's not always easy to catch but on a slower system and with many more salt states I have successfully copied /etc/sudoers as a regular user using cat in a while true loop.\r\n\r\nThe source file srv/salt/files/secure.txt is just a file many lines of X chars.\r\n\r\n```\r\n$ ls -lh /home/grant/secure.txt\r\n-rw------- 1 grant grant 445K Nov 16 18:52 /home/grant/secure.txt\r\n$ wc /home/grant/virtualenv/salt-env/srv/salt/files/secure.txt\r\n  5758   5758 454882 /home/grant/virtualenv/salt-env/srv/salt/files/secure.txt\r\n```\r\n\r\n```\r\n$ pip list\r\napache-libcloud (0.13.2)\r\nJinja2 (2.7.1)\r\nM2Crypto (0.21.1)\r\nMarkupSafe (0.18)\r\nmock (1.0.1)\r\nmsgpack-python (0.4.0)\r\npsutil (1.1.1)\r\npycrypto (2.6.1)\r\nPyYAML (3.10)\r\npyzmq (14.0.0)\r\nsalt (0.17.0-4694-gae7e813, /home/grant/git/remote/salt-fork)\r\nwsgiref (0.1.2)\r\n```\r\n\r\nI believe files should be created with, at most, 0600 permissions and then the state mode (or internal default) applied once the file is successfully copied.\r\n\r\n---\r\n\r\nLooking through code I see some uses of shutil module copy functions (salt.utils also makes use of shutil). shutil.py uses open() on new files that seems to honour umasks, in isolated testing at least, and will create files with 644 permissions. So I have been experimenting with umasks in various places (primarily ```salt/utils/__init__.py```) like daemonize(), copyfile() without success so far. Maybe ```pip uninstall salt; pip install -e salt-fork;``` isn't the best way to test or I am doing something wrong.\r\n\r\nSince ```salt/modules/file.py:psed()``` uses shutil.copy2 to create a backup file before editing, ```file.replace``` can also expose file content.\r\n\r\nI am still wrapping my head around the code and thought I would post this issue for review.\r\n\r\nBest regards,\r\nGrant\r\n\r\n"
8570,'basepi','Fixing pylint errors\n'
8557,'s0undt3ch','Sentry Logging Handler requires Raven\nDocumentation should reflect that Raven needs to be installed to use Sentry logging.'
8556,'s0undt3ch','Sentry Logging Handler errors out\nWhen configured with either\r\n```\r\nsentry_handler:\r\n  dsn: http://dead:beef@saltmaster:9000/2\r\n  log_level: debug\r\n```\r\nor\r\n```\r\nsentry_handler:\r\n  servers:\r\n    - \'http://saltmaster:9000\'\r\n  project: 2\r\n  public_key: dead\r\n  secret_key: beef\r\n  log_level: debug\r\n```\r\nsalt fails to start with following error:\r\n```\r\n# service salt-master start\r\nStarting salt master control daemon: salt-master[ERROR   ] \'SentryHandler\' object is not iterable\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/utils/parsers.py", line 158, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/usr/lib/python2.7/dist-packages/salt/utils/parsers.py", line 556, in __setup_extended_logging\r\n    log.setup_extended_logging(self.config)\r\n  File "/usr/lib/python2.7/dist-packages/salt/log/setup.py", line 528, in setup_extended_logging\r\n    for handler in handlers:\r\nTypeError: \'SentryHandler\' object is not iterable\r\nUsage: salt-master\r\n\r\nsalt-master: error: Error while processing <bound method Master.__setup_extended_logging of <salt.Master object at 0x2c52c10>>: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/utils/parsers.py", line 158, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/usr/lib/python2.7/dist-packages/salt/utils/parsers.py", line 556, in __setup_extended_logging\r\n    log.setup_extended_logging(self.config)\r\n  File "/usr/lib/python2.7/dist-packages/salt/log/setup.py", line 528, in setup_extended_logging\r\n    for handler in handlers:\r\nTypeError: \'SentryHandler\' object is not iterable\r\n\r\n failed!\r\n```'
8447,'cachedout','Salt version 17.1 does not respect timeout in /etc/salt/master file\nAfter upgrading from 16.4 to 17.1, most of our servers stopped running state.highstate commands. We set a 60 second timeout in /etc/salt/master (timeout: 60). Most minions return with the failure error "Minion did not return" after 8-10 seconds, well short of the timeout. \r\n\r\nRunning grains.item saltversion or test.ping to a select server works, running a highstate fails. Using time in front of the command shows that the command returns after only a few seconds. \r\n\r\nRunning salt -v -t 60 HOST state.highstate fixes the problem. \r\n\r\n'
8419,'techhat','iptables module not working?\nAttempting to do this in an sls file using salt v 0.17.1:\r\n\r\n````\r\n  iptables.append:\r\n    - table: filter\r\n    - chain: INPUT\r\n    - proto: tcp\r\n    - match: tcp\r\n    - source: 10.0.0.0/24\r\n    - destination: {{ private_ip }}\r\n    - destination-port: 3306\r\n    - jump: ACCEPT\r\n````\r\n\r\nUsing salt-call on the minion (Ubuntu 12.04.3 LTS) shows:\r\n\r\n````\r\n[INFO    ] Executing state iptables.append for mysql-server\r\n[INFO    ] Executing command \'iptables -t filter -C INPUT -m tcp --destination-port 3306 --proto tcp --destination 10.xxx.xxx.xxx --jump ACCEPT --source 10.0.0.0/24 --__env__ base --__sls__ mysql-server --order 10002 \' in directory \'/root\'\r\n[INFO    ] Executing command \'iptables -t filter -A INPUT -m tcp --destination-port 3306 --proto tcp --destination 10.xxx.xxx.xxx --jump ACCEPT --source 10.0.0.0/24 --__env__ base --__sls__ mysql-server --order 10002 \' in directory \'/root\'\r\n[ERROR   ] Failed to set iptables rule for mysql-server\r\n````\r\n\r\nManually running the rules generated:\r\n\r\n````\r\niptables -t filter -C INPUT -m tcp --destination-port 3306 --proto tcp --destination 10.xxx.xxx.xxx --jump ACCEPT --source 10.0.0.0/24 --__env__ base --__sls__ mysql-server --order 10002\r\n\r\niptables v1.4.12: unknown option "--__env__"\r\nTry `iptables -h\' or \'iptables --help\' for more information.\r\n````\r\n\r\nAm I doing it wrong?'
8418,'terminalmage','jinja renderer reports wrong line number\nIf a jinja template references a subkey of a non existent jinja variable (e.g., ``{{ foo.bar }}`` when foo itself is undefined), Salt reports:\r\n\r\n```\r\n        Comment:   Undefined jinja variable; line 375 in template\r\n```\r\n\r\nBut line 375 is a line number from ``jinja2/environment.py``'
8404,'rallytime','Fix salt-cloud map format and documentation\nIn the documentation there is an example of cloud map that doesn\'t work as expected:\r\n\r\n```\r\nfedora_small:\r\n    - web1:\r\n        minion:\r\n            log_level: debug\r\n            grains:\r\n                cheese: tasty\r\n                omelet: du fromage\r\n    - web2:\r\n        minion:\r\n            log_level: warn\r\n            grains:\r\n                cheese: more tasty\r\n                omelet: with peppers\r\n```\r\n\r\nIt works with the following modifications:\r\n\r\n```\r\nfedora_small:\r\n    web1:\r\n        minion:\r\n            log_level: debug\r\n            grains:\r\n                cheese: tasty\r\n                omelet: du fromage\r\n    web2:\r\n        minion:\r\n            log_level: warn\r\n            grains:\r\n                cheese: more tasty\r\n                omelet: with peppers\r\n```\r\n\r\nThe documentation and/or the parser should be fixed.\r\n\r\nIn the first example, when you run "salt-cloud -m ...", it tries try created web1, web2 and minion instances (!?).'
8399,'terminalmage','salt.states.file.replace should honor user and group\nCurrently, `salt.states.file.replace` changes the ownership of the file that is being updated. I believe this state should honor the original user/group and additionally support the `user` and `group` parameters as most of the other `file` states.\r\n\r\nI worked around this issue by using a `file.managed` state later on the process to manage the ownership.'
8386,'techhat',"Add execution and state modules for salt.cloud\nThis would effectively replace salt.cloud's existing map functionality. Refs saltstack/salt-cloud#220."
8351,'whiteinge','Fix "What is Salt Stack?" appended to every title in the docs\nReported on salt-users.  If you search for something salt-related, any docs entries in the results will have "What is Salt Stack?" appended on the end of the title.'
8343,'s0undt3ch','salt.states.file.accumulated throws KeyError: \'__sls__\'\nOn git develop `file.accumulated` now throws an exception when using the `require_in` clause. I should be receiving an "Orphaned accumulated" exception message ([here](https://github.com/saltstack/salt/blob/78662d270b53150573e431a2c019068d22ff0874/salt/states/file.py#L2753)). Possibly related to commit cecd105, although I haven\'t bisected it.\r\n\r\n```\r\ntest:\r\n  file:\r\n    - accumulated\r\n    - name: sections\r\n    - filename: /etc/test\r\n    - require_in:\r\n      - file: some-config-file\r\n    - text: "myconfig value"\r\n\r\n\r\nState: - file\r\n    Name:      sections\r\n    Function:  accumulated\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1290, in call\r\n    **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.7/dist-packages/salt/states/file.py", line 2755, in accumulated\r\n    kwargs[\'__sls__\'],\r\nKeyError: \'__sls__\'\r\n```\r\n'
8336,'cachedout','salt-ssh broken on FreeBSD\nI received the below bug report at the FreeBSD bug tracker this afternoon. salt-ssh calls ``md5sum``, but on FreeBSD the command is simply ``md5``. \r\n\r\nMore details and a suggested patch are available at the link below.\r\n\r\nhttp://www.freebsd.org/cgi/query-pr.cgi?pr=ports/183766'
8333,'cro',"global whitespace removal in .jinja files\nI would like to globally remove whitespace created by jinja tags from all config files that i deploy with salt.   ie salt://myconfig.jinja\r\n\r\nThe jinja docs mention that this is possible in the master jinja config but is this something we can add to the salt master config?\r\n\r\ntrim_blocks and lstrip_blocks are the relevant options found here: http://jinja.pocoo.org/docs/templates/#whitespace-control\r\n\r\nI'm also aware that you can do {%- some test -%} in salt jinja files but I would rather not do this for all tags."
8321,'terminalmage','pkgrepo.managed in ubuntu 13.10 not behaving as expected\nI am seeing this since uprgrading to Ubuntu Server 13.10:\r\n\r\n```\r\n[INFO    ] Executing state pkgrepo.managed for base\r\n[WARNING ] Unable to use functions from "python-software-properties" package, making best guess at ppa format: ppa:rethinkdb/ppa\r\n[INFO    ] Executing command \'apt-key export BBBB206CC673114771F12CC0135D945A11D62AD6\' in directory \'/root\'\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/root\'\r\n[INFO    ] Configured package repo base\r\n[INFO    ] Executing state pkg.installed for rethinkdb\r\n[INFO    ] Package rethinkdb is already installed\r\n```\r\n\r\nEverything appears to be working properly, but this message makes me think something is not working properly.\r\n\r\nI\'m using salt-bootstrap to get salt onto this instance, and running with the following as root:\r\n\r\nsalt-call --local state.highstate\r\n\r\nEverything else seems to be working just fine except this.'
8296,'s0undt3ch','Memoize deprecation warning still mentions 0.19\n'
8279,'terminalmage','file.replace with test=true deletes file\n```\r\nsalt my-minion file.replace /etc/environment \'^no_proxy=.*$\\n\' \'\' flags="[MULTILINE]" test=true\r\n```\r\nResults in the target file becoming empty and produces the following error:\r\n\r\n>TypeError encountered executing file.replace: an integer is required. See debug log for more info.  Possibly a missing arguments issue:  ArgSpec(args=[\'path\', \'pattern\', \'repl\', \'count\', \'flags\', \'bufsize\', \'backup\', \'dry_run\', \'search_only\', \'show_changes\'], varargs=None, keywords=None, defaults=(0, 0, 1, \'.bak\', False, False, True))\r\n\r\nThe same command without ``test=true`` has the desired effect.'
8271,'cachedout',"`salt-ssh` doesn't honor `file_roots` configuration option \nIn the debug log, I can see that `salt-ssh` reads the master configuration file located in the directory passed via `--config-dir`. However, `salt-ssh` doesn't seem to honor the `file_roots` configuration option, and instead only looks in the default location `/srv/salt` (which isn't even listed in my `file_roots`). As a result, it can't find any states passed to `state.sls`. As soon as I copy state files into `/srv/salt`, things start to work. Mac OS 10.9, Salt 0.17.1.\r\n\r\nTo troubleshoot this problem, it would help if Salt not only logged `[ERROR   ] No matching sls found for 'foo' in env 'base'`, but also the locations it searched."
8264,'cro',"upgrade to 0.17.1\nAfter I updated the salt-minion from 0.16 to 0.17.1, there were two issues:\r\n1.  some warning message output like as\r\n```\r\n/var/cache/salt/minion/extmods/modules/circus.py:12: DeprecationWarning: The 'memoize' decorator was mo\r\nved to 'salt.utils.decorators', please start importing it from there. This warning and wrapper will be removed on salt > 0.19.0.\r\n  @salt.utils.memoize\r\n```\r\n\r\n2. after throwing the above message, I tried to logout the server, but failed. I had to kill the ssh process.\r\n\r\n==OUTPUT==\r\n```\r\nUpdated: salt.noarch 0:0.17.1-1.el5 salt-minion.noarch 0:0.17.1-1.el5\r\nComplete!\r\n[root@ip-10-166-12-212 ~]# kill -9 2344\r\n[root@ip-10-166-12-212 ~]# /etc/init.d/salt-minion restart\r\nStopping salt-minion daemon:                               [确定]\r\nStarting salt-minion daemon:                               [确定]\r\n[root@ip-10-166-12-212 ~]# /var/cache/salt/minion/extmods/modules/circus.py:12: DeprecationWarning: The 'memoize' decorator was mo\r\nved to 'salt.utils.decorators', please start importing it from there. This warning and wrapper will be removed on salt > 0.19.0.\r\n  @salt.utils.memoize\r\n\r\n[root@ip-10-166-12-212 ~]#\r\n[root@ip-10-166-12-212 ~]#\r\n[root@ip-10-166-12-212 ~]#\r\n[root@ip-10-166-12-212 ~]#\r\n[root@ip-10-166-12-212 ~]# logout\r\n\r\nKilled\r\n[root@manage01 ~]#\r\n```"
8261,'terminalmage','Disabling an upstart service and subsequently enabling it again leaves .override file\nI just ran into a problem where I had a state like this:\r\n\r\n```\r\nlxc-net:\r\n  service.dead:\r\n    - enable: false\r\n```\r\n\r\nWhich I changed to:\r\n\r\n```\r\nlxc-net:\r\n  service.running:\r\n    - enable: true\r\n```\r\n\r\nThe above did not enable the service because `lxc-net.override` was not cleaned up.  I would expect `- enable: true` to remove the `lxc-net.override` file that was written by `- enable: false`.\r\n\r\nIn order to enable the service I also had to add the following, which should not be necessary:\r\n\r\n```\r\n/etc/init/lxc-net.override:\r\n  file:\r\n    - absent\r\n```'
8247,'terminalmage',"pkgrepo, pkg.latest upgrade is incomplete with test=True\ni've noticed an incompleteness of output using test=True, when migrating from a distribution package repo to a backport-repo providing newer versions.\r\n\r\nlet's assume htop is installed from ubuntu-precise.\r\nrunning a highstate using test=True will just show that pkgrepo.managed is executed. it won't detect that the package needs to be upgraded, as applying highstate normally does:\r\n\r\nstate.highstate:\r\n<pre>\r\n----------\r\n    State: - pkgrepo\r\n    Name:      precise-bleed\r\n    Function:  managed\r\n        Result:    True\r\n        Comment:   Configured package repo precise-bleed\r\n        Changes:   repo: ppa:precisebleed/ppa\r\n                   \r\n----------\r\n    State: - pkg\r\n    Name:      htop\r\n    Function:  latest\r\n        Result:    True\r\n        Comment:   The following packages were successfully installed/upgraded: htop.\r\n        Changes:   htop: { new : 1.0.2-2~ppa1~precise1\r\nold : 1.0.1-1\r\n}\r\n                   \r\n----------\r\n</pre>\r\n\r\ni can understand why this probably happens.\r\nsince the pkgrepo is not added for read during simulation, there's no package-information-change available and salt cannot see the available upgrade.\r\nsince the simulation diverges from reality, please at least include a hint in such situations, that additional packages may be changed.\r\n\r\ni guess that's all you can do without running a real simulation."
8222,'thatch45',"Track mine processes, fixes #5729\nThis pull request adds support for scheduled jobs being added to the /var/cache/salt/minion/procs directory so we can keep track of them and prevent too many of the same process from being started at the same time.  It also includes a small performance optimization for listing processes--in one case in the code we don't need to grab a list of all the processes and check against each one--we only need to know if the process we care about is still running.\r\n@thatch : you probably want to look at this somewhat closely.\r\n\r\nThis fixes #5729."
8221,'s0undt3ch','DeprecationWarning: with htpasswd => webutil when running salt from git\nFound this issue in my salt master log file:\r\n```\r\n: DeprecationWarning: The \'salt.loaded.int.module.htpasswd\' module is renaming itself in it\'s __virtual__() function (htpasswd => webutil). Please set it\'s virtual name as the \'__virtualname__\' module attribute. Example: "__virtualname__ = \'webutil\'"\r\n  virtual\r\n```\r\n\r\nversions:\r\n```\r\nSalt: 0.17.1709.gd14aa58-1\r\nhalite:0.1.04.2.gf88316a-1\r\nPython: 2.7.5 (default, Sep  6 2013, 09:55:21)\r\nJinja2: 2.7.1\r\nM2Crypto: 0.21.1\r\nmsgpack-python: 0.3.0\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.6\r\nPyYAML: 3.10\r\nPyZMQ: 13.1.0\r\nZMQ: 4.0.1\r\n```'
8218,'UtahDave','user.present will not create windows account (version 17.1.1)\n```\r\nC:\\salt>salt --versions-report\r\n           Salt: 0.17.1\r\n         Python: 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Intel)]\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\n```\r\nC:\\salt>salt-call state.sls usermgmt\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state user.present for Jenkins\r\n[INFO    ] Executing command \'net localgroup JenkinsGroup\' in directory \'C:\\\\Users\\\\<snipped>\'\r\n[ERROR   ] An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1260, in call\r\n  File "salt/states/user.py", line 277, in present\r\n  File "salt/states/user.py", line 67, in _changes\r\nKeyError: \'user.info\'\r\n\r\n[INFO    ] Executing command \'attrib -R "c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\sls\r\n.p"\' in directory \'C:\\\\Users\\\\<snipped>\'\r\n←[0;31mlocal:←[0m\r\n←[0;31m----------\r\n    State: - user←[0m\r\n    ←[0;31mName:      Jenkins←[0m\r\n    ←[0;31mFunction:  present←[0m\r\n        ←[0;31mResult:    False←[0m\r\n        ←[0;31mComment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1260, in call\r\n  File "salt/states/user.py", line 277, in present\r\n  File "salt/states/user.py", line 67, in _changes\r\nKeyError: \'user.info\'\r\n←[0m\r\n←[0;31m        Changes:\r\n                   ←[0;36m----------←[0;36m←[0m\r\n←[0;36m\r\nSummary\r\n------------←[0m\r\n←[0;32mSucceeded: 0←[0m\r\n←[0;31mFailed:    1←[0m\r\n←[0;36m------------\r\nTotal:     1←[0m\r\n```\r\n\r\n\r\n_____________________________________________________________________\r\nHere is the usermgmt.sls\r\n\r\n```\r\nJenkins:\r\n  user.present:\r\n    - fullname: \'Continuous Integration Account\'\r\n    - groups:\r\n      - JenkinsGroup\r\n```'
8216,'cro',"salt-ssh: wrong user owner for some files in /tmp/.salt on remote machine\nWhen using salt-ssh some files which salt internally copies to remote host to /tmp/.salt have wrong ownership, actually the owner id is the id of the user who runs the salt-ssh on his laptop. This may probably cause problems if different users will use the same remote user to control remote host. Example:\r\n\r\n501 and staff is my id and group on my laptop, user with id 501 doesn't exist on target host. \r\n\r\n```\r\n# ls -la /tmp/.salt/\r\ntotal 1080\r\ndrwx------  6 vagrant vagrant    4096 Nov  1 17:23 .\r\ndrwxrwxrwt  4 root    root       4096 Nov  1 17:23 ..\r\ndrwxr-xr-x  3 root    root       4096 Nov  1 17:23 jinja2\r\ndrwxr-xr-x  2 root    root       4096 Nov  1 17:23 markupsafe\r\ndrwxr-xr-x 21 root    root       4096 Nov  1 17:23 salt\r\n-rw-r--r--  1     501 staff        79 Nov  1 16:53 salt-call\r\n-rw-r--r--  1 vagrant vagrant 1072629 Nov  1 17:23 salt-thin.tgz\r\n-rw-r--r--  1     501 staff         6 Nov  1 16:53 version\r\ndrwxr-xr-x  2 root    root       4096 Nov  1 17:23 yaml \r\n```"
8205,'terminalmage',"Only one return is written to the specified file.\nI need to parse output from salt. But when I run command like this: `salt -L node1,node2 cmd.run_all 'ls' --output=yaml --output-file=tmp.output`, I only got return of one node in tmp.output file.\r\n\r\nI don't think it's correct. What I expected, it should be same as output in stdout when I run without '--output-file' option.\r\n\r\nI think salt should append output to file here: https://github.com/saltstack/salt/blob/develop/salt/output/__init__.py#L25"
8201,'terminalmage',"yumpkg.py: pkg.remove on a package required by other packages adds the parent packages to the transaction and removes them\nI discovered this with my development VM today. This is actually a bug in the yum python API, where ``yum.YumBase.processTransaction()`` proceeds with the transaction even when there were errors. Scarier still, it adds the packages which require the package you are trying to remove to the transaction. So, run ``pkg.remove python`` and yum, rpm, etc. are all removed and your minion is toast.\r\n\r\nI've got a fix for this mostly done."
8196,'s0undt3ch','salt.loaded.int.module.cp not respecting environments\nBecause of this guy:\r\nhttps://github.com/saltstack/salt/issues/8078\r\n\r\nI\'m running on the development branch right now, using a custom build package here:\r\n\r\n```\r\ndeployment@linuxslave01:~/salt$ git show\r\ncommit b821f6a174e67a3e1def1ba7fa16885cd985bb0c\r\nMerge: b4abfc3 9291c36\r\nAuthor: Thomas S Hatch <thatch45@gmail.com>\r\n```\r\n\r\non salt master in base/states/top.sls\r\n\r\n```\r\nprod:\r\n  \'G@env:prod and G@roles:pgpool\':\r\n      - match : compound\r\n      - prodpgpool\r\n```\r\n\r\nMinion\'s salt/minion file:\r\n\r\n```\r\nmaster: saltmaster01.ourdomain.com\r\ngrains:\r\n  roles:\r\n    - pgpool\r\n  app:\r\n    - someapp1\r\n  env:\r\n    - prod\r\nmine_functions:\r\n  network.interfaces: []\r\n  test.ping: []\r\n  grains.item:\r\n    - app\r\n    - roles\r\n    - env\r\n    - fqdn\r\n```\r\n\r\non salt master in init.sls for prodpgpool:\r\n\r\n```\r\n/etc/pgpool2/pgpool.conf:\r\n  file.managed:\r\n    - source: salt://prodpgpool/pgpool.conf.jinja\r\n    - template: jinja\r\n    - user: postgres\r\n    - group: www-data\r\n    - mode: 664\r\n    - require:\r\n      - pkg: pgpool2-pkgs\r\n    - defaults:\r\n{% if \'app\' in grains and \'someapp1\' in grains[\'app\'] %}\r\n      pg_md5: {{ pillar[\'prod.someapp1.pg.md5\'] }}\r\n{% elif \'app\' in grains and \'someapp2\' in grains[\'app\'] %}\r\n      pg_md5: {{ pillar[\'prod.someapp2.pg.md5\'] }}\r\n{% endif %}\r\n```\r\n\r\n/var/log/salt/minion:\r\n\r\n```\r\n2013-10-31 15:36:40,949 [salt.loaded.int.module.cp][ERROR   ] Unable to cache file "salt://prodpgpool/pgpool.conf.jinja" from env "base".\r\n2013-10-31 15:36:40,950 [salt.state       ][ERROR   ] Source file salt://prodpgpool/pgpool.conf.jinja not found\r\n```\r\n\r\n'
8190,'s0undt3ch','pip deprecation warning when using virtualenv\nHi there, when running the state:\r\n\r\n```yaml\r\n/home/vagrant/venv:\r\n  virtualenv.managed:\r\n    - system_site_packages: False\r\n    - requirements: salt://python-virtualenv/requirements.txt\r\n    - user: vagrant\r\n```\r\n\r\nI get the following deprecation warning from modules.pip: \r\n\r\n```\r\n[WARNING ] /usr/lib/pymodules/python2.7/salt/modules/pip.py:252: DeprecationWarning: The \'runas\' argument to pip.install is deprecated, and will be removed in 0.18.0. Please use \'user\' instead.\r\n[DEBUG   ] Changing ownership of requirements file \'/tmp/tmpznJT2R\' to user \'vagrant\'\r\n[INFO    ] Executing command "/home/vagrant/venv/bin/pip install --requirement=\'/tmp/tmpznJT2R\'" as user \'vagrant\' in directory \'/home/vagrant\'\r\n```\r\n\r\nSince I\'m new to salt, I can\'t seem to find the code in virtualenv.py that calls this.\r\n\r\nPS: Used `- runas: vagrant` at first, then when seeing the deprecation warning, I used `user`. But the `user` param is not really accepted by `virtualenv_mod.py`, but the command seems to work fine anyway, with the same warning of course.'
8142,'UtahDave',"If the win_repo .p files haven't been compiled we get a stacktrace.\nJust log that the repo is unavailable and return that the package isn't available"
8140,'UtahDave','Default Windows cache path wrong\nOn Windows, a clean install using the 0.17.1 installer and the default configuration, I see the following:\r\n```\r\nC:\\salt>salt-call.exe test.ping\r\n[ERROR   ] Could not cache minion ID: [Errno 2] No such file or directory: \'c:\\\\\r\nsalt\\\\onf\\\\minion_id\'\r\n?[0;36mlocal?[0m:\r\n    ?[1;33mTrue?[0m\r\n```\r\n\r\nPresumably "onf" should be "conf"?'
8119,'UtahDave','cmd.run priviliges on windows minion\nI\'d like to control the insertion and removal of DNS records and DHCP leases/reservations in Active Directory for all of our Linux hosts/VMs. These hosts/VMs can easily have at least 4 IPs each depending which VLANs they have access to and it\'s really hard to maintain this manually and I\'d prefer not to have to write powershell to do all of this if possible (I\'ve written some of it with WMI and powershell and I can\'t say I enjoy it).\r\n\r\nSo before we get too far, I\'d just like to be able to get basic command execution returning results.\r\n\r\nI\'m finding cmd.run isn\'t executing my calls to dnscmd with sufficient privileges (not surprising):\r\n\r\nwhen run in a command prompt with my A/D Domain admin user:\r\n\r\n```\r\n\r\nC:\\>dnscmd dc-name-goes-here /enumzones\r\n\r\nEnumerated zone list:\r\n        Zone count = 17\r\n\r\n Zone name                      Type       Storage         Properties\r\n\r\n .                              Cache      AD-Domain\r\n _msdcs.domain.org            Primary    AD-Forest       Secure Aging\r\n a.b.in-addr.arpa              Primary    AD-Forest       Update Rev Aging\r\n c.d.e.in-addr.arpa          Primary    AD-Domain       Secure Rev Aging\r\n <snip>\r\n domain2.org                  Forwarder  AD-Forest\r\n domain.org                   Primary    AD-Domain       Update Aging\r\n TrustAnchors                   Primary    AD-Forest       Aging\r\n\r\n```\r\n\r\nHere is when I try running the same command from the same host via salt\'s cmd.run module on the salt master:\r\n\r\n```\r\n\r\n[root@salt-master ~]# salt -v "win-dev*" cmd.run "dnscmd dc-name-goes-here /enumzones"\r\nExecuting job with jid 20131026090518282166\r\n-------------------------------------------\r\n\r\nwin-dev:\r\n    \r\n    Zone enumeration failed\r\n        status = 5 (0x00000005)\r\n    Command failed:  ERROR_ACCESS_DENIED     5    0x5\r\n\r\n```\r\n\r\nThe salt-minion is set to run as a "Local System" service. Do I just need to replace the "Local System" start as privileges with an A/D privileged service account that has a tailored permission set for the service access we need?\r\n\r\nI tried to in fact do this but I can\'t get the salt-minion service to start when I change the "Log On As" user to a privileged A/D account. \r\n\r\nThe error is as follows:\r\n\r\n```\r\n\r\nWindows could not start the salt-minion service on Local Computer.\r\n\r\nError 5: Access is Denied.\r\n\r\n```\r\n\r\nI try the same thing with shelled out calls to netsh dhcp server and here it errors out but with a cryptic error that suggests something else may be in play (although permissions could be playing a role).\r\n\r\nadd a dhcp reservation from the command prompt:\r\n\r\n```\r\nnetsh dhcp server <dhcp-server-ip-goes-here> scope <scope-ip-goes-here> add reservedip a.b.c.d abc123abc123 test123.domain.org added-by-script\r\n\r\nChanged the current scope context to <scope-ip-goes-here> scope.\r\n\r\nCommand completed successfully.\r\n```\r\n\r\nwhen I try this from the salt-master via cmd-run:\r\n\r\n```\r\n\r\n[root@salt-master ~]# salt -v \'win-dev*\' cmd.run "netsh dhcp server <dhcp-server-ip-goes-here> scope <scope-ip-goes-here> add reservedip a.b.c.d abc123abc123 test123.domain.org added-by-script"\r\n\r\nExecuting job with jid 20131026155559514704\r\n-------------------------------------------\r\n\r\nwin-dev.trailmix.net:\r\n    \r\n    The command needs a valid Scope IP Address.\r\n\r\n```\r\n\r\nNormally I would think I\'ve entered something in wrong, but I\'m literally copying and pasting commands from the command prompt and what I\'m trying to execute via cmd.run via salt. Is this just a poorly worded error message associated with salt not having access to the referenced scope in DHCP? I can\'t tell.'
8102,'terminalmage','quota.set is broken: set_() takes exactly 1 argument (10 given)\nRunning the module function quota.set with the syntax describe in http://docs.saltstack.com/ref/modules/all/salt.modules.quota.html#salt.modules.quota.set does not work:\r\n```\r\n[INFO    ] User root Executing command quota.set with jid 20131025205234376365\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20131025205234376365\', \'tgt\': \'<MYHOSTNAME>\', \'ret\': \'\', \'user\': \'root\', \'arg\': [\'/\', \'user=u98779975\', \'block-hard-limit=524288000\'], \'fun\': \'quota.set\'}\r\n[WARNING ] TypeError encountered executing quota.set: set_() takes exactly 1 argument (10 given). See debug log for more info.  Possibly a missing arguments issue:  ArgSpec(args=[\'device\'], varargs=None, keywords=\'kwargs\', defaults=None)\r\n[DEBUG   ] TypeError intercepted: set_() takes exactly 1 argument (10 given)\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/minion.py", line 659, in _thread_return\r\n    return_data = func(*args, **kwargs)\r\nTypeError: set_() takes exactly 1 argument (10 given)\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/minion.py", line 659, in _thread_return\r\n    return_data = func(*args, **kwargs)\r\nTypeError: set_() takes exactly 1 argument (10 given)\r\n[INFO    ] Returning information for job: 20131025205234376365\r\n```\r\nThe master and the minion run Debian Wheezy 7.2 with the latest packages (salt 0.17.1+dfsg-1~bpo70+1~dst.1, etc.).\r\n\r\nSome unit tests might be useful.\r\n\r\nSee also http://irclog.perlgeek.de/salt/2013-10-25 (18:28 - 18:49)'
8100,'UtahDave','0.17.1 Windows AMD64 stacktraces on the minion during state.highstate and state.top with "ImportError: No module named audio"\nWe\'re using the 0.17.1 AMD64 installer. We can run a statefile via `state.sls`. But when we create a topfile and include only that one statefile (which itself has only one state), the minion stacktraces:\r\n\r\n```\r\n[WARNING ] The minion function caused an exception: Traceback (most recent call last):\r\n  File "salt/minion.py", line 659, in _thread_return\r\n  File "salt/modules/state.py", line 257, in highstate\r\n  File "salt/state.py", line 2304, in call_highstate\r\n  File "salt/state.py", line 1927, in load_dynamic\r\n  File "salt/state.py", line 553, in module_refresh\r\n  File "site.py", line 548, in <module>\r\n  File "site.py", line 526, in main\r\n  File "site.py", line 94, in abs__file__\r\n  File "email/__init__.py", line 79, in __getattr__\r\nImportError: No module named audio\r\n```\r\n\r\nOutput of `versions-report` on the Minion:\r\n\r\n```\r\nSalt: 0.17.1\r\nPython: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\nJinja2: 2.7.1\r\nM2Crypto: 0.21.1\r\nmsgpack-python: 0.3.0\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.6\r\nPyYAML: 3.10\r\nPyZMQ: 13.1.0\r\nZMQ: 3.2.2\r\n```\r\n\r\nThis is the case with both `state.highstate` and `state.top`'
8096,'s0undt3ch',"possible regression : __virtual__ vs. __virtualname__\nUsing \\__virtual__ allows to merge custom modules to existing modules, while \\__virtualname__ erases previously defined\r\n\r\nfor example:\r\n\r\n```python\r\ndef __virtual__():\r\n   return 'event'\r\n\r\ndef foo(): pass\r\n```\r\n\r\nWill be rendered as\r\n\r\n```sh\r\nsalt-call sys.doc event.\r\n\r\n    event.foo:\r\n        ...\r\n    event.fire:\r\n        ...\r\n    event.fire_master:\r\n        ...\r\n```\r\n\r\nBut\r\n\r\n```python\r\n__virtualname__ = 'event'\r\n\r\ndef foo(): pass\r\n```\r\n\r\nWill be rendered as\r\n\r\n```sh\r\nsalt-call sys.doc event.\r\n\r\n    event.foo:\r\n        ...\r\n```"
8087,'cro',"Simple module calls take a long time in 0.17.1\n```bash\r\n root@sockpuppet:~# salt '*' test.ping --out raw |wc\r\n     406     812   11808\r\n root@sockpuppet:~# salt -t 10 '*' test.ping --out raw |wc\r\n     768    1536   22597\r\n root@sockpuppet:~# salt -t 15 '*' test.ping --out raw |wc\r\n     950    1900   27910\r\n root@sockpuppet:~# salt -t 20 '*' test.ping --out raw |wc\r\n     950    1900   27910\r\n```\r\n\r\n950 is indeed the correct number of minions to return. In 0.15.3 this returned all 950 minions within the default timeout and in general within 1-2 seconds."
8083,'cachedout',"Only the first set grains item has new value in the same state run\nOther grains contain old values until next run. For example:\r\n\r\ntest.sls\r\n```\r\ngrains-test1:\r\n  grains.present:\r\n    - value: value1\r\n    - order: 1\r\n  custom.show:\r\n    - order: 2\r\n\r\ngrains-test2:\r\n  grains.present:\r\n    - value: value2\r\n    - order: 3\r\n  custom.show:\r\n    - order: 4\r\n```\r\n_states/custom.py\r\n```Python\r\ndef show(name):\r\n    return {'name': name,\r\n           'changes': {},\r\n           'result': True,\r\n           'comment': __grains__.get(name)}\r\n```\r\nResult of the first test.sls run will be:\r\n```\r\n    State: - custom\r\n    Name:      grains-test1\r\n    Function:  show\r\n        Result:    True\r\n        Comment:   value1\r\n...\r\n    State: - custom\r\n    Name:      grains-test2\r\n    Function:  show\r\n        Result:    True\r\n        Comment:   None\r\n```\r\nNote the value of grains-test2 grain in comment."
8068,'terminalmage',"JInja template rendering issue\nAfter upgrading to 0.17.1, my jinja templating rendering got screwed when a pillar is being passed from sls state to a template file.\r\n\r\nsls:\r\n\r\n```\r\n{% set config_nfs = pillar['config_nfs'] %}\r\n/etc/exports:\r\n  file.managed:\r\n    - source: salt://templates/etc/exports\r\n    - user: root\r\n    - group: root\r\n    - mode: 0644\r\n    - template: jinja\r\n    - context:\r\n      config_nfs: {{ config_nfs }}\r\n    - require:\r\n      - pkg: nfs-kernel-server\r\n```\r\n\r\nTemplate file:\r\n\r\n```\r\n{% for export, export_values in config_nfs['exports'].items() -%}\r\n{{ export }} {{ export_values['address'] }}({{ export_values['permissions'] }})\r\n{% endfor -%}\r\n```\r\n\r\nThe above doesnt work but if I do it like below:\r\n\r\nsls:\r\n\r\n```\r\n/etc/exports:\r\n  file.managed:\r\n    - source: salt://templates/etc/exports\r\n    - user: root\r\n    - group: root\r\n    - mode: 0644\r\n    - template: jinja\r\n    - require:\r\n      - pkg: nfs-kernel-server\r\n```\r\n\r\nTemplate file:\r\n\r\n```\r\n{% set config_nfs = pillar['config_nfs'] %}\r\n{% for export, export_values in config_nfs['exports'].items() -%}\r\n{{ export }} {{ export_values['address'] }}({{ export_values['permissions'] }})\r\n{% endfor -%}\r\n```\r\n\r\nI have so many templates so I had to downgrade back to 0.17.0-2."
8016,'cachedout','salt -v with no additional commands throws traceback\n(master_env)root@master:~# /salt_mount/scripts/salt -v\r\nUsage: salt [options] \'<target>\' <function> [arguments]\r\n\r\nOptions:\r\n  --version             show program\'s version number and exit\r\n  --versions-report     show program\'s dependencies version number and exit\r\n  -h, --help            show this help message and exit\r\n  -c CONFIG_DIR, --config-dir=CONFIG_DIR\r\n                        Pass in an alternative configuration directory.\r\n                        Default: /etc/salt\r\n  -t TIMEOUT, --timeout=TIMEOUT\r\n                        Change the timeout, if applicable, for the running\r\n                        command; default=5\r\n  -s, --static          Return the data from minions as a group after they all\r\n                        return.\r\n  --async               Run the salt command but don\'t wait for a reply\r\n  --state-output=STATE_OUTPUT, --state_output=STATE_OUTPUT\r\n                        Override the configured state_output value for minion\r\n                        output. Default: full\r\n  --subset=SUBSET       Execute the routine on a random subset of the targeted\r\n                        minions. The minions will be verified that they have\r\n                        the named function before executing\r\n  -v, --verbose         Turn on command verbosity, display jid and active job\r\n                        queries\r\n  --show-timeout        Display minions that timeout\r\n  -b BATCH, --batch=BATCH, --batch-size=BATCH\r\n                        Execute the salt job in batch mode, pass either the\r\n                        number of minions to batch at a time, or the\r\n                        percentage of minions to have running\r\n  -a EAUTH, --auth=EAUTH, --eauth=EAUTH, --external-auth=EAUTH\r\n                        Specify an external authentication system to use.\r\n  -T, --make-token      Generate and save an authentication token for re-use.\r\n                        Thetoken is generated and made available for the\r\n                        period defined in the Salt Master.\r\n  --return=RETURNER     Set an alternative return method. By default salt will\r\n                        send the return data from the command back to the\r\n                        master, but the return data can be redirected into any\r\n                        number of systems, databases or applications.\r\n  -d, --doc, --documentation\r\n                        Return the documentation for the specified module or\r\n                        for all modules if none are specified.\r\n  --args-separator=ARGS_SEPARATOR\r\n                        Set the special argument used as a delimiter between\r\n                        command arguments of compound commands. This is useful\r\n                        when one wants to pass commas as arguments to some of\r\n                        the commands in a compound command.\r\n\r\n  Logging Options:\r\n    Logging options which override any settings defined on the\r\n    configuration files.\r\n\r\n    -l LOG_LEVEL, --log-level=LOG_LEVEL\r\n                        Console logging log level. One of \'all\', \'garbage\',\r\n                        \'trace\', \'debug\', \'info\', \'warning\', \'error\', \'quiet\'.\r\n                        Default: \'warning\'.\r\n    --log-file=LOG_FILE\r\n                        Log file path. Default: /var/log/salt/master.\r\n    --log-file-level=LOG_LEVEL_LOGFILE\r\n                        Logfile logging log level. One of \'all\', \'garbage\',\r\n                        \'trace\', \'debug\', \'info\', \'warning\', \'error\', \'quiet\'.\r\n                        Default: \'warning\'.\r\n\r\n  Target Options:\r\n    Target Selection Options\r\n\r\n    -E, --pcre          Instead of using shell globs to evaluate the target\r\n                        servers, use pcre regular expressions\r\n    -L, --list          Instead of using shell globs to evaluate the target\r\n                        servers, take a comma or space delimited list of\r\n                        servers.\r\n    -G, --grain         Instead of using shell globs to evaluate the target\r\n                        use a grain value to identify targets, the syntax for\r\n                        the target is the grain key followed by a\r\n                        globexpression: "os:Arch*"\r\n    --grain-pcre        Instead of using shell globs to evaluate the target\r\n                        use a grain value to identify targets, the syntax for\r\n                        the target is the grain key followed by a pcre regular\r\n                        expression: "os:Arch.*"\r\n    -N, --nodegroup     Instead of using shell globs to evaluate the target\r\n                        use one of the predefined nodegroups to identify a\r\n                        list of targets.\r\n    -R, --range         Instead of using shell globs to evaluate the target\r\n                        use a range expression to identify targets. Range\r\n                        expressions look like %cluster\r\n    -C, --compound      The compound target option allows for multiple target\r\n                        types to be evaluated, allowing for greater\r\n                        granularity in target matching. The compound target is\r\n                        space delimited, targets other than globs are preceded\r\n                        with an identifier matching the specific targets\r\n                        argument type: salt \'G@os:RedHat and webser* or\r\n                        E@database.*\'\r\n    -X, --exsel         Instead of using shell globs use the return code of a\r\n                        function.\r\n    -I, --pillar        Instead of using shell globs to evaluate the target\r\n                        use a pillar value to identify targets, the syntax for\r\n                        the target is the pillar key followed by a\r\n                        globexpression: "role:production*"\r\n    -S, --ipcidr        Match based on Subnet (CIDR notation) or IPv4 address.\r\n\r\n  Output Options:\r\n    Configure your preferred output format\r\n\r\n    --out=OUTPUT, --output=OUTPUT\r\n                        Print the output from the \'salt\' command using the\r\n                        specified outputter. The builtins are \'no_return\',\r\n                        \'grains\', \'yaml\', \'overstatestage\', \'json\', \'pprint\',\r\n                        \'nested\', \'raw\', \'highstate\', \'quiet\', \'key\', \'txt\',\r\n                        \'virt_query\'.\r\n    --out-indent=OUTPUT_INDENT, --output-indent=OUTPUT_INDENT\r\n                        Print the output indented by the provided value in\r\n                        spaces. Negative values disables indentation. Only\r\n                        applicable in outputters that support indentation.\r\n    --out-file=OUTPUT_FILE, --output-file=OUTPUT_FILE\r\n                        Write the output to the specified file\r\n    --no-color, --no-colour\r\n                        Disable all colored output\r\n    --force-color, --force-colour\r\n                        Force colored output\r\n\r\nYou can find additional help about salt issuing "man salt" or on\r\nhttp://docs.saltstack.org\r\n[ERROR   ] list index out of range\r\nTraceback (most recent call last):\r\n  File "/salt_mount/salt/utils/parsers.py", line 158, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/salt_mount/salt/utils/parsers.py", line 1111, in _mixin_after_parsed\r\n    self.config[\'tgt\'] = self.args[0]\r\nIndexError: list index out of range\r\nUsage: salt [options] \'<target>\' <function> [arguments]\r\n\r\nsalt: error: Error while processing <unbound method SaltCMDOptionParser._mixin_after_parsed>: Traceback (most recent call last):\r\n  File "/salt_mount/salt/utils/parsers.py", line 158, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/salt_mount/salt/utils/parsers.py", line 1111, in _mixin_after_parsed\r\n    self.config[\'tgt\'] = self.args[0]\r\nIndexError: list index out of range\r\n'
8010,'whiteinge',"unused icons in doc/_static\nThe following icons appear to be unused (and probably more importantly, not copyright saltstack community, licenced apache v2):\r\n\r\nbook_open.png\r\npage_white_acrobat.png\r\nfilm_link.png\r\n\r\nThey appear to be from the silk set and although available under CCA v3, should probably not be distributed unless they are used (ie ok to be in git, but would be good to remove from .tar.gz).\r\n\r\nIf they are used, they should be documented (I'm happy to do this, as I'll be doing it for debian shortly).\r\n\r\nJoe"
8000,'cachedout','--out=quiet not actually quiet\nwhen I run `salt-call state.highstate --out=quiet -l warning` on a minion, I still get a newline of output. This causes my cronjob to email me (since I have configured any cron output to be sent via email). Is there a way to make salt actually run in quiet mode that only outputs actual errors? This would make running via cronjobs cleaner.\r\n\r\nI am using salt version 0.17.0'
7996,'cachedout','Better error message for lack of mysql.default_file in mysql_user.present\nstate similar to example, that worked in the past leads to:\r\n</pre>\r\n----------\r\n    State: - mysql_user\r\n    Name:      staging\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   State mysql_user.present found in sls appstack.staging is unavailable\r\n\r\n        Changes:   \r\n----------\r\n</pre>'
7995,'techhat','ipables state parameter order won\'t allow to add a rule on Ubuntu 12.04\nI am trying to use the iptables.append state to add some firewall rules. The iptables command that is being built by the iptables state however, has the arguments in the wrong order and hence won\'t add a rule.\r\nIf I switch the arguments and run the same command from the command line, the rule is being added.\r\n\r\nI have the kwargs in pillar like this:\r\n```\r\niptables:\r\n  redis-port:\r\n    table: filter\r\n    chain: INPUT\r\n    jump: ACCEPT\r\n    proto: tcp\r\n    match: state\r\n    connstate: NEW\r\n    dport: 6379\r\n    source: 127.0.0.1/32\r\n```\r\n\r\nMy iptables/init.sls loops through the arguments for each rule:\r\n```\r\n{%- if salt[\'pillar.get\'](\'iptables\') %}\r\n{%- for rule in pillar[\'iptables\'] %}\r\n{{ grains[\'id\'] }}-{{ rule }}:\r\n  iptables.append:\r\n{%- for parameter, value in pillar[\'iptables\'][rule].items() %}\r\n    - {{ parameter }}: {{ value }}\r\n{%- endfor %}\r\n{%- endfor %}\r\n{%- endif %}\r\n```\r\n\r\nThe iptables command that is being built, is the the following in the debug log:\r\n```\r\n2013-10-21 06:15:18,751 [salt.loaded.int.module.cmdmod               ][INFO    ] Executing command \'iptables -t filter -A INPUT -m state --dport 6379 --proto tcp --jump ACCEPT --source 127.0.0.1/32 --state NEW --__env__ base --__sls__ iptables --order 10405 \' in directory \'/root\'\r\n2013-10-21 06:15:18,762 [salt.state                                  ][ERROR   ] Failed to set iptables rule for minion1-redis-port\r\n```\r\n\r\nThat command being run from the command line, produces the following error:\r\n```\r\nroot@minion1:/$ iptables -t filter -A INPUT -m state --dport 6379 --proto tcp --jump ACCEPT --source 127.0.0.1/32 --state NEW\r\niptables v1.4.12: unknown option "--dport"\r\nTry `iptables -h\' or \'iptables --help\' for more information.\r\n```\r\n\r\nOnce I put the proto argument before dport, the rule is being added:\r\n```\r\nroot@minion1:/$ iptables -t filter -A INPUT -m state --proto tcp --dport 6379 --jump ACCEPT --source 127.0.0.1/32 --state NEW\r\nroot@minion1:/$ iptables -L INPUT\r\nChain INPUT (policy ACCEPT)\r\ntarget     prot opt source               destination\r\nACCEPT     tcp  --  localhost            anywhere             state NEW tcp dpt:6379\r\n```\r\n\r\nAm I missing something?\r\n\r\nThe exact same error happens, when I hardcode the parameter values in the init.sls like this:\r\n```\r\n  iptables.append:\r\n    - table: filter\r\n    - chain: INPUT\r\n    - jump: ACCEPT\r\n    - proto: tcp\r\n    - match: state\r\n    - connstate: NEW\r\n    - dport: 6379\r\n    - source: 127.0.0.1/32\r\n```\r\n\r\ndport is being put before proto which makes the command fail.\r\n\r\nsalt-version report:\r\n```\r\n           Salt: 0.16.0-3479-g3d9351b\r\n         Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
7984,'cachedout','oddly named image in docs\ndoc/_themes/saltstack/static/images/cloudOrchestration2.png is a jpeg not a png\r\n\r\nadditionally, it does not seem to be used in the documentation theme. Same goes for most the other images in that directory...'
7983,'cachedout',"font licencing\nI'm not sure you have the permission to redistribute the font files in:\r\n\r\ndoc/_themes/saltstack/static/fonts/\r\n\r\nMost appear to be in under the youworkforthem licence which appears not to permit free redistribution.\r\n\r\nhttp://www.youworkforthem.com/font-license"
7977,'s0undt3ch',"Fix pkg.installed regression and deadlock on subprocess.Popen.poll()\n- I was trying to install aaa.noarch on an x86_64 system.\r\nExecution of `salt \\* pkg.check_db aaa` will result in the following:\r\n`_pkg_arch('aaa')` returns `['aaa', 'x86_64']`; therefore, in `check_db` (line 396) `x.arch == pkgarch` will be false\r\n\r\nExecution of `salt \\* pkg.check_db aaa.noarch` will result in the following:\r\n`_pkg_arch('aaa.noarch')` returns `['aaa.noarch', 'noarch']`; therefore, in `check_db` (line 395) `yumbase.searchPackages(('name', 'arch'), ('aaa.noarch',))` will be an empty dictionary\r\n\r\nConsequently, I could not install aaa.noarch!\r\n\r\n- According to [doc. of Popen](http://docs.python.org/2/library/subprocess.html#subprocess.Popen.poll) a deadlock is produces when there is enough output to the pipe, which was happening when I was bootstrapping a minion from git using Salt-Cloud, which uses salt.utils.nb_popen to execute SSH coomands:\r\n`def root_cmd(command, tty, sudo, **kwargs)` in `saltcloud/utils/__init__.py` uses `proc = NonBlockingPopen(...` which is defined at the bottom of `saltcloud/utils/nb_popen.py`: `from salt.utils.nb_popen import NonBlockingPopen as SaltNonBlockingPopen`\r\n"
7967,'UtahDave','file.recurse does not work on 0.17.0 or 0.17.1, but works on 0.16.3\nI have written a state:\r\n\r\n```\r\n"{{maindir}}/Repo":\r\n    file.recurse:\r\n        - source: salt://nodes/Stack/Repo\r\n```\r\n\r\nand this works fine on a minion of version 0.16.3 running under Windows.  Under /srv/salt I have a nodes directory that contains the directories Stack and Repo under that.\r\n\r\nIf I run the same thing on a minion of version 0.17.0 or greater under Windows I get this error:\r\n\r\n```\r\nminion2:\r\n----------\r\n    State: - file\r\n    Name:      C:\\Stack2\\Repo\r\n    Function:  recurse\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1256, in call\r\n  File "salt/states/file.py", line 1621, in recurse\r\n  File "salt/states/file.py", line 1544, in manage_file\r\n  File "salt/states/file.py", line 1131, in managed\r\n  File "salt/modules/file.py", line 2048, in manage_file\r\n  File "salt/modules/cp.py", line 241, in cache_file\r\n  File "salt/fileclient.py", line 117, in cache_file\r\n  File "salt/fileclient.py", line 332, in get_url\r\n  File "salt/fileclient.py", line 613, in get_file\r\n  File "contextlib.py", line 17, in __enter__\r\n  File "salt/fileclient.py", line 95, in _cache_loc\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 157, in makedirs\r\nWindowsError: [Error 123] The filename, directory name, or volume label syntax is incorrect: \'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\files\\\\base\\\\|nodes\'\r\n```\r\n\r\nNow the first thing I notice is that we have a \'|\' character in the path of a file being created in the minion\'s cache directory.  This is an invalid path name.\r\n\r\nInitially \'nodes\' was a symbolic link on the master\'s file server, but I have since changed it to a real directory with the same results.\r\n\r\nThis seems like a bug to me since it works on 0.16.3 without issue.'
7954,'UtahDave','msgpack broken in Windows minion since 0.17.0\nSteps to reproduce:\r\n\r\n- Install Windows minion version 0.17.0 or 0.17.1, 32bit or 64 bit\r\n- On master, create a state file with this content:\r\n\r\n```\r\nmy_file:\r\n  file.managed:\r\n    - name: \'C:\\my_file.txt\'\r\n```\r\n\r\n- Issue the following command on the master:\r\n\r\n```\r\n    salt \'*\' state.sls my_file \r\n```\r\n\r\n- You get the following output in the minion debug log\r\n\r\n```\r\n[INFO    ] User ############ Executing command state.sls with jid 20131016144156828643\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20131016144156828643\', \'tgt\': \'*\', \'ret\': \'\', \'user\': \'############\', \'arg\': [\'my_file\'], \'fun\': \'state.sls\'}\r\n[DEBUG   ] Initializing COM library\r\n[DEBUG   ] Uninitializing COM library\r\n[WARNING ] The minion function caused an exception: Traceback (most recent call last):\r\n  File "salt/minion.py", line 657, in _thread_return\r\n  File "salt/modules/state.py", line 300, in sls\r\n  File "salt/modules/state.py", line 86, in running\r\n  File "salt/modules/saltutil.py", line 363, in is_running\r\n  File "salt/modules/saltutil.py", line 391, in running\r\n  File "salt/payload.py", line 95, in loads\r\n  File "_unpacker.pyx", line 119, in msgpack._unpacker.unpackb (msgpack/_unpacker.cpp:119)\r\nUnpackValueError\r\n[INFO    ] Returning information for job: 20131016144156828643\r\n```'
7928,'cachedout','Salt-ssh bug parsing command arguments\nI discovered this while trying to use cmd.run via salt-ssh. Using zmq, I do this:\r\n\r\n```\r\nroot@salt~:$ salt salt cmd.run "echo \'hi\'"\r\nsalt:\r\n    hi\r\n```\r\n\r\nBut when I use salt-ssh this happens\r\n\r\n```\r\nroot@salt~:$ salt-ssh salt cmd.run "echo \'hi\'"\r\nsalt:\r\n    Pseudo-terminal will not be allocated because stdin is not a terminal.\r\n    stdin: is not a tty\r\n    Error running \'cmd.run\': Specified cwd \'hi\' either not absolute or does not exist\r\n```\r\n\r\nThis happens because the second positional argument to cmd.run is the cwd. I assume that it\'s probably the argument parser that\'s broken, though I suppose it could also be the salt ssh client?\r\n\r\nversion report:\r\n```\r\n           Salt: 0.17.0\r\n         Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
7919,'cachedout',"The --state-output flag only seems to output 'full'\nI've been trying to use the new `--state-output` flag to the salt command on deploys, but no matter which option I pass to it (full, terse, mixed), I always get full output."
7904,'s0undt3ch','function which does not work correctly on windows machines\nProblem\r\n------------\r\nsalt version: 0.17.0\r\n\r\nsalt/utils/\\__init\\__.py has a defined function \r\n```\r\ndef which(exe=None):\r\n```\r\nthis method returns only a path for full name (including extension) (e.g. for unzip.exe but not for unzip)\r\n\r\nthis is used in salt/modules/archive to determine if a specific command (e.g. unzip) is available. e.g. Even if unzip is available in the path and can be executed, the executable is not detected on windows. Hence, command will not work.\r\n\r\nReproduce\r\n---------------\r\nOS: Windows\r\nInstall: http://gnuwin32.sourceforge.net/packages/unzip.htm or git (which includes unix tools into path)\r\n\r\n__execute:__ salt \'*\' archive.unzip /path/zipfile.zip\r\n\r\nThis will not extract the zip file even if unzip is available in path. Message "archive.unzip" is not available. will be printed.\r\n\r\nExpected behavior\r\n-------------------------\r\nwhich also detects windows executables in the path which have the passed name when ommiting extension'
7887,'cro',"archive.tar doesn't parse kwarg for 'dest' properly\nUsing v0.17.0 (git), I can't untar a tar.gz:\r\n\r\n```sh\r\n$ salt-call archive.tar xvzf archive.tar.gz dest=/tmp/\r\n[INFO    ] Executing command 'tar -xvzf archive.tar.gz dest=/tmp/' in directory '/root'\r\nlocal:\r\n    - tar: dest=/tmp: Not found in archive\r\n    - tar: Exiting with failure status due to previous errors\r\n```\r\n\r\n`dest=/tmp/` is being passed unaltered to `tar -xvzf`, rather than being parsed into the kwarg."
7872,'cachedout','grains.delval doesn\'t work as advertised/expected\nI spoke with Seth and he confirmed it was a bug and asked that I file it as an issue. Without further ado...\r\n\r\nDeleting grains doesn\'t seem to work like it should in v.17. I run grains.delval but it doesn\'t actually delete the grain but rather only it’s value. This matters since I have code that not only looks at the value of a grain but acts differently if the grain is present vs. not present (I\'m using the grains to encode minion state in a state machine sense, and I need to be able to discretely move between states).\r\n\r\nHere’s an example:\r\n\r\n```\r\n[root@salt-master base]# salt \'salt-minion-00*\' -v grains.setval foo bar --out json\r\nExecuting job with jid 20131009201650172154\r\n-------------------------------------------\r\n\r\n{\r\n    "salt-minion-00": {\r\n        "foo": "bar"\r\n    }\r\n}\r\n\r\n[root@salt-master base]# salt \'salt-minion-00*\' -v grains.delval foo --out json\r\nExecuting job with jid 20131009201720910261\r\n-------------------------------------------\r\n\r\n{\r\n    "salt-minion-00": null\r\n}\r\n\r\n[root@salt-master base]# salt \'salt-minion-00*\' -v cmd.run "cat /etc/salt/grains"\r\nExecuting job with jid 20131009201748806515\r\n-------------------------------------------\r\n\r\nsalt-minion-00:\r\n    foo: null\r\n```\r\n\r\nAlthough not related to this issue, I\'m a big fan of the new grains functions introduced in v.17 (delval, remove, append). Will these functions be made available in the grains states api as well? At present only "present" is available which reproduces the grains.setval functionality.\r\n\r\nThanks.'
7860,'cachedout',"file.managed: support dir_mode\nIn general it seems like anything that allows for makedirs should also expose dir_mode, otherwise doing something like\r\n\r\n```\r\n  file.managed:\r\n    - name: /home/ubuntu/.ssh/deploy_key\r\n    - user: ubuntu\r\n    - group: ubuntu\r\n    - mode: 600\r\n    - makedirs: True\r\n    - contents_pillar: 'deploy_key:ec2'\r\n```\r\n\r\nrenders the key unusable since .ssh is created with mode 600.\r\n"
7842,'cachedout',"Salt tar.gz contains code with multiple licences\nThe LICENSE file in the salt contains the licence for the salt code. However, other code is distributed with salt with a variety of licences. A quick survey of these includes:\r\n\r\ndoc/_themes/saltstack/static/css/bootstrap-responsive.css\r\njQuery - MIT licence\r\ndoc/_themes/saltstack/static/js/vendor/modernizr-2.6.2-respond-1.1.0.min.js - MIT i believe\r\nsalt/utils/ipaddr.py - Apache licence (c) Google\r\nsalt/auth/pam.py - MIT, Chris Atlee\r\n\r\nThere may be others...\r\n\r\nIs it possible to get this included in a COPYING file in the top level [1]?\r\n\r\nThis would really aid downstream packaging and people checking what is being distributed in the tar.gz.\r\n\r\nI'm happy enough to do it, but wish to ensure there are no objections before doing so. It would be great if there was a check of files added in a release and an subsequent update before the actual release.\r\n\r\nThanks very much,\r\n\r\nJoe\r\n[1] https://wiki.debian.org/UpstreamGuide#Licenses\r\n\r\n"
7835,'cachedout','0.17 git module user option not working\nAccording to the docs, starting with 0.17 the git module specifies to use "user" instead of "runas". The user option did not work for me, but the runas worked as expected. '
7833,'cachedout','system.shutdown unusable\nThe system.shutdown execution module looks unusable in its current form on probably any GNU/Linux distribution.\r\n\r\n```\r\nroot@master:~# salt \'*\' system.shutdown\r\nminion01.example.com:\r\n    shutdown: time expected\r\n    Try `shutdown --help\' for more information.\r\n```\r\n\r\nLooking at salt/modules/system.py, the \'shutdown\' command is called without any provision for user-specified parameters. It doesn\'t have \'-h now\' hard-coded either.\r\n\r\nHappy to supply a patch (which would likely be trivial), but I\'m not sure which approach was intended. I see we have support for \'halt\' (which is generally effectively the same as "shutdown -h now", so I guess supporting parameters is the way to go.\r\n\r\nHowever, if adding parameters for shutdown, why not add them to halt, reboot, etc as well? Having them only for shutdown would seem inconsistent. Then again, if you\'re accepting parameters, is there actually any advantage to calling:\r\n\r\n```salt \'some-instance\' system.shutdown -h now```\r\n\r\nover\r\n\r\n```salt \'some-instance\' cmd.run shutdown -h now```\r\n\r\n(other than having to type one character less)? Maybe it can all disappear?\r\n\r\nIn short, it doesn\'t work currently, but I don\'t know how you would like to see it fixed. :)'
7813,'terminalmage','0.17 pkg.instaled 32bit package on 64bit regression with yum\nHi,\r\n\r\nupgraded from 0.16.4. to 0.17.0 and got this on a x86_64 Amazon Linux:\r\n```\r\n[INFO    ] Executing state pkg.installed for jre.i586\r\nLoaded plugins: priorities, update-motd, upgrade-helper, versionlock\r\nLoaded plugins: priorities, update-motd, upgrade-helper, versionlock\r\n652 packages excluded due to repository priority protections\r\n[ERROR   ] The following package(s) were not found, and no possible matches were found in the package db: jre.i586\r\n```\r\nyum -y install jre.i586 works just fine.'
7806,'UtahDave',"Feature request: Bootstrapping for Windows minions\nMy employer is interested in a mostly-foolproof way of rolling out a Salt minion release to a list of domain-managed Windows machines, assuming they had near-vanilla installs. I prototyped a method that succeeded at this for XP, 2003 and 7Pro targets. The one prerequisite is that the calling machine must have _psexec_ from [Sysinternals PsTools](http://technet.microsoft.com/en-au/sysinternals/bb896649.aspx) installed in the path.\r\n\r\nIn a nutshell, manage.bootstrap_psexec does the following:\r\n- Compile batch file on host\r\n- Use PsExec to transmit and execute batch file over SMB with appropriate admin credentials\r\n- Batch file terminates salt-minion service if present, removes any cached master public key for an existing Salt install\r\n- Batch file downloads two installer payloads via HTTP (VSCRT 2008 installer and latest Salt Installer) thanks to liberal abuse of the Windows Script Host and the IE6 AJAX module\r\n- Batch file executes both installers in silent mode\r\n\r\nI'm not sure what the rule is re. non-master machines calling _salt-run_, but I've tried it on a few Windows client installs and there doesn't seem to be any issue. Let me know if this is the appropriate place to have this functionality, or if there is somewhere else better (e.g. split out into a module)"
7795,'s0undt3ch','New Documentation Language Dutch\nPlease create a new documentation language for Dutch'
7788,'cachedout','State stuck in test mode\nHere are my versions-report values:\r\n\r\nMaster:\r\n```\r\n           Salt: 0.17.0rc1\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\nMinion:\r\n```\r\n           Salt: 0.17.0\r\n         Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\nIn my minion configuration I have "test: True" and when I run the following on my master: salt "*" state.highstate test=False mystate\r\n\r\nI get results in yellow counted under "Not run" like so:\r\n\r\n```\r\nSummary\r\n-------------\r\nSucceeded: 20\r\nFailed:     0\r\nNot Run:    2\r\n-------------\r\nTotal:     22\r\n```\r\n\r\nAm I doing something wrong?'
7755,'cachedout','Error in function _file_hash since upgrading to 0.17.0\nHi,\r\n\r\n```\r\n# salt-call --versions-report\r\n           Salt: 0.17.0\r\n         Python: 2.6.8 (unknown, Mar 14 2013, 09:31:22)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n\r\n# cat /etc/salt/master\r\nuser: salt\r\nopen_mode: true\r\nlog_file: file:///dev/log/LOG_LOCAL3\r\nlog_level_logfile: warning\r\nlog_level: quiet\r\n\r\nfileserver_backend:\r\n  - roots\r\n\r\nfile_roots:\r\n  base:\r\n    - /var/data/salt/srv/salt\r\nfile_ignore_regex:\r\n  - \'/\\.svn($|/)\'\r\n  - \'/\\.git($|/)\'\r\nfile_ignore_glob:\r\n  - \'*.pyc\'\r\n\r\npillar_roots:\r\n  base:\r\n    - /var/data/salt/srv/pillar\r\n\r\nclient_acl:\r\n  jenkins:\r\n    - .*\r\n```\r\n\r\nThe following error keeps popping up:\r\n```\r\n[ERROR   ] Error in function _file_hash:\r\n#012Traceback (most recent call last):\r\n#012  File "/usr/lib/python2.6/site-packages/salt/master.py", line 1444, in run_func\r\n#012    ret = getattr(self, func)(load)\r\n#012  File "/usr/lib/python2.6/site-packages/salt/fileserver/__init__.py", line 233, in file_hash\r\n#012    return self.servers[fstr](load, fnd)\r\n#012  File "/usr/lib/python2.6/site-packages/salt/fileserver/roots.py", line 153, in file_hash\r\n#012    hsum, mtime = fp_.read().split(\':\')\r\n#012ValueError: need more than 1 value to unpack\r\n\r\n```\r\nHowever, it doesn\'t seem to affect the functionality of this test setup.'
7754,'s0undt3ch',"log_file: file:///dev/log bug\nHi,\r\n\r\n```\r\n# salt-call --versions-report\r\n           Salt: 0.17.0\r\n         Python: 2.6.8 (unknown, Mar 14 2013, 09:31:22)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n\r\n# grep log /etc/salt/master\r\n#log_file: file:///dev/log/LOG_LOCAL3\r\nlog_file: file:///dev/log\r\nlog_level_logfile: warning\r\nlog_level: quiet\r\n```\r\nIn this setup, calling salt creates this path 'file:/dev/log' in the current directory (or file:/dev/log/LOG_LOCAL3):\r\n\r\n```\r\n# salt '*' test.ping\r\nip-10-45-8-94:\r\n    True\r\nip-10-60-0-50:\r\n    True\r\n# ls -l file\\:/dev/log/LOG_LOCAL3\r\n-rw-r--r-- 1 salt root 0 Oct 11 09:44 file:/dev/log/LOG_LOCAL3\r\n```\r\n"
7752,'cachedout','salt-master warning with hidepid=2 /proc\nHi,\r\n```\r\n# salt-call --versions-report\r\n           Salt: 0.17.0\r\n         Python: 2.6.8 (unknown, Mar 14 2013, 09:31:22)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n\r\n# mount | grep ^proc\r\nproc on /proc type proc (rw,noexec,nosuid,nodev,hidepid=2)\r\n```\r\nStarting the salt-master as user salt (user: salt in /etc/salt/master) in this environment leads to a lot of:\r\n\r\nwarning: Failed to read auxiliary vector, /proc not mounted?\r\n'
7748,'cachedout','Minified version of js library included without source\nHi,\r\n\r\nThe minified version of  modernizr-2.6.2-respond-1.1.0.min.js is included, but not the original library.\r\n\r\nCould the original library be included as well so that we can ship the source as well as the minified library.\r\n\r\nThanks,\r\n\r\nJoe'
7730,'UtahDave','Create better Windows installer upgrade logic\nThe current Salt Minion Windows installer is oblivious of any existing installs. We need add logic to deal properly with existing Salt-Minion installs.'
7718,'cro','0.17: ssh_auth adds ssh-rsa key to authorized_keys even though type is set to ecdsa\nThis is from state.show_highstate():\r\n```\r\n sshpubkey-mailbackup-root@backup-2:\r\n        ----------\r\n        __env__:\r\n            base\r\n        __sls__:\r\n            virtual-users\r\n        ssh_auth:\r\n            ----------\r\n            - name:\r\n                AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbm...\r\n            ----------\r\n            - enc:\r\n                dsa-sha2-nistp256\r\n            ----------\r\n            - comment:\r\n                root@backup-2\r\n            ----------\r\n            - user:\r\n                mailbackup\r\n[...]\r\n```\r\n\r\nIt shows `enc` to be of type `ecdsa`. However, it blindly puts `ssh-rsa` in the authorized_keys file...\r\n\r\n`no-agent-forwarding,no-port-forwarding,no-pty,no-user-rc,no-X11-forwarding ssh-rsa AAAA...`\r\n\r\n'
7715,'UtahDave',"Windows: inherited file attributes result in read-only files\nDue to inherited file attributes, when the minion initially writes C:\\salt\\var\\cache\\salt\\minion\\sls.p, it is created read-only, meaning that future write attempts fail.\r\n\r\nI temporarily fixed this by adding a ``salt.utils.is_windows()`` check to both ``state.highstate`` and ``state.sls``, right before these functions attempt to write to the cache file, and if the minion is Windows then ``attrib -R filename`` is run, removing the read-only attribute.\r\n\r\nHowever, this is not a long-term solution, as 1) this can't be the only affected file write, and 2) the read-only attribute shows up again every time the file is written.\r\n\r\nIt seems like a more long-term solution would be to add some sanity checks to ``salt.utils.fopen()``, but my Windows-fu is not what it should be, so if anyone else has an idea, I'm all ears."
7712,'UtahDave',"recurse is recursing through '..'\nThe following directive is making salt 0.17.0.1-1quantal on ubuntu recurse through the '..' directory which in turn creates a ton of useless directories in /etc for me:\r\n\r\n```\r\n/etc/diamond/collectors:\r\n  file.recurse:\r\n    - source: salt://graphite/collectors\r\n    - exclude_pat: ..\r\n    - clean: True\r\n    - include_empty: True\r\n    - user: root\r\n    - group: root\r\n    - dir_mode: 0755\r\n    - file_mode: 0644\r\n    - require:\r\n      - pkg: diamond\r\n    - watch_in:\r\n      - service: diamond\r\n```\r\n\r\n```\r\n[INFO    ] {'/etc/diamond/collectors/../../_grains': {'/etc/diamond/collectors/../../_grains': 'New Dir'},  ....\r\n```\r\n\r\nIt appears to only do it for directories, and not for files as its not copying new files and such."
7707,'UtahDave','Windows: Import and unpack errors after upgrade to 0.17.0\nJust upgraded my minion from 0.16.3 to 0.17.0\r\nWindows 2008 R2 64bit\r\n\r\n```\r\nC:\\salt>salt-call.exe state.single file.touch name="C:\\test.salt" --no-color\r\n[DEBUG   ] Configuration file path: c:\\salt\\conf\\minion\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[DEBUG   ] loading grain in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\grain\r\ns\', \'C:\\\\salt\\\\salt-0.17.0.win-amd64\\\\salt-0.17.0-py2.7.egg\\\\salt\\\\grains\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\grains, it is not a di\r\nrectory\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] loading module in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\modu\r\nles\', \'C:\\\\salt\\\\salt-0.17.0.win-amd64\\\\salt-0.17.0-py2.7.egg\\\\salt\\\\modules\']\r\n[DEBUG   ] Failed to import module activemq, this is most likely NOT a problem:\r\nTraceback (most recent call last):\r\n  File "salt/loader.py", line 641, in gen_functions\r\n  File "c:\\salt\\var\\cache\\salt\\minion\\extmods\\modules\\activemq.py", line 13, in\r\n<module>\r\n    from HTMLParser import HTMLParser\r\nImportError: No module named HTMLParser\r\n[DEBUG   ] Loaded win_shadow as virtual shadow\r\n[DEBUG   ] Loaded win_disk as virtual disk\r\n[DEBUG   ] Loaded win_status as virtual status\r\n[DEBUG   ] Loaded win_network as virtual network\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded win_autoruns as virtual autoruns\r\n[DEBUG   ] Loaded win_system as virtual system\r\n[DEBUG   ] Loaded win_firewall as virtual firewall\r\n[DEBUG   ] Loaded win_pkg as virtual pkg\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded win_file as virtual file\r\n[DEBUG   ] Loaded virtualenv_mod as virtual virtualenv\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] Loaded win_service as virtual service\r\n[DEBUG   ] Loaded win_timezone as virtual timezone\r\n[DEBUG   ] Loaded win_repo as virtual winrepo\r\n[DEBUG   ] Loaded win_groupadd as virtual group\r\n[DEBUG   ] loading returner in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\re\r\nturners\', \'C:\\\\salt\\\\salt-0.17.0.win-amd64\\\\salt-0.17.0-py2.7.egg\\\\salt\\\\returne\r\nrs\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\returners, it is not a\r\n directory\r\n[DEBUG   ] Loaded couchdb_return as virtual couchdb\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] loading states in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\stat\r\nes\', \'C:\\\\salt\\\\salt-0.17.0.win-amd64\\\\salt-0.17.0-py2.7.egg\\\\salt\\\\states\']\r\n[DEBUG   ] Loaded saltmod as virtual salt\r\n[DEBUG   ] Loaded virtualenv_mod as virtual virtualenv\r\n[DEBUG   ] loading render in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\rend\r\nerers\', \'C:\\\\salt\\\\salt-0.17.0.win-amd64\\\\salt-0.17.0-py2.7.egg\\\\salt\\\\renderers\r\n\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\renderers, it is not a\r\n directory\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler\r\n:\r\nUnpackValueError:\r\nTraceback (most recent call last):\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chainload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-call__.py", line 14, in <module>\r\n  File "salt/scripts.py", line 77, in salt_call\r\n  File "salt/cli/__init__.py", line 303, in run\r\n  File "salt/cli/caller.py", line 137, in run\r\n  File "salt/cli/caller.py", line 78, in call\r\n  File "salt/modules/state.py", line 571, in single\r\n  File "salt/modules/state.py", line 86, in running\r\n  File "salt/modules/saltutil.py", line 363, in is_running\r\n  File "salt/modules/saltutil.py", line 391, in running\r\n  File "salt/payload.py", line 95, in loads\r\n  File "_unpacker.pyx", line 119, in msgpack._unpacker.unpackb (msgpack/_unpacke\r\nr.cpp:119)\r\nUnpackValueError\r\nTraceback (most recent call last):\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chainload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-call__.py", line 14, in <module>\r\n  File "salt/scripts.py", line 77, in salt_call\r\n  File "salt/cli/__init__.py", line 303, in run\r\n  File "salt/cli/caller.py", line 137, in run\r\n  File "salt/cli/caller.py", line 78, in call\r\n  File "salt/modules/state.py", line 571, in single\r\n  File "salt/modules/state.py", line 86, in running\r\n  File "salt/modules/saltutil.py", line 363, in is_running\r\n  File "salt/modules/saltutil.py", line 391, in running\r\n  File "salt/payload.py", line 95, in loads\r\n  File "_unpacker.pyx", line 119, in msgpack._unpacker.unpackb (msgpack/_unpacke\r\nr.cpp:119)\r\nmsgpack.exceptions.UnpackValueError\r\n```\r\n\r\n```\r\nC:\\salt>salt --versions-report\r\n           Salt: 0.17.0\r\n         Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.2\r\n```'
7703,'cachedout',"state.sls runner doesn't load custom modules\nSalt 0.17.0 64bit"
7695,'cachedout','postgres.version might return None\nI just fought against a strange error:\r\n\r\nFinal diagnosis : postgres.version might return `None`, so `LooseVersion` does not have a `version` attribute:\r\n```bash\r\nsalt \'*\' postgres.version\r\nminion-1:\r\n    None\r\nminion-2:\r\n    9.2.4\r\n```\r\n\r\nInitial error:\r\nException raised when calling other postgres functions like `salt \'*\' postgres.user_list`:\r\n```python\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/pymodules/python2.7/salt/minion.py", line 657, in _thread_return\r\n        return_data = func(*args, **kwargs)\r\n      File "/usr/lib/pymodules/python2.7/salt/modules/postgres.py", line 409, in user_list\r\n        if ver >= distutils.version.LooseVersion(\'9.1\'):\r\n      File "/usr/lib/python2.7/distutils/version.py", line 296, in __cmp__\r\n        return cmp(self.version, other.version)\r\n    AttributeError: LooseVersion instance has no attribute \'version\'\r\n```\r\n\r\nThe fix is to restart the postgresql server.\r\n\r\nIt might be a good idea to check for `None` in  https://github.com/saltstack/salt/blob/develop/salt/modules/postgres.py#L114 and provide a clearer error message.\r\n\r\nThanks !\r\n\r\n-- \r\nEdit:\r\nThe real reason is that the server was not found in the first place...\r\n\r\nlog was:\r\n```\r\n2013-10-09 13:01:48,145 [salt.loaded.int.module.cmdmod][ERROR   ] Command \'/usr/bin/psql --no-align --no-readline --dbname postgres -c \'SELECT setting FROM pg_catalog.pg_settings WHERE name = \'"\'"\'server_version\'"\'"\'\' -t\' failed with return code: 2\r\n2013-10-09 13:01:48,146 [salt.loaded.int.module.cmdmod][ERROR   ] stderr: psql: could not connect to server: No such file or directory\r\n        Is the server running locally and accepting\r\n        connections on Unix domain socket "/var/run/postgresql/.s.PGSQL.5432"?\r\n```\r\n\r\nbut the suggestion to provide a clearer error message stands !'
7691,'cachedout','saltuitl.sync_modules is not a sync it is a copy\nI tried running sync_modules to get rid of an old module that I had used to override a salt module using the same name. The saltutil.sync_modules command happily syncs changes in existing files and new files, however it does not delete files that should not be there.\r\n\r\nIn my opinion, either the documentation should mention this, since a sync would mean to bring the directory exactly in line with what the salt fileserver has in that directory, or preferrably the sync operation should actually delete old/missing files.'
7690,'cachedout','Salt 0.17.0 - User creation state breaks when specifying existing group\nSalt 0.17.0 changed the way that calls are made to create users.  The new mechanism no longer passes in the \'-g\' option to __useradd__ on Debian based systems which can lead to a failure in creating the user account.\r\n\r\n* salt-call --versions-report\r\n\r\n````\r\n           Salt: 0.17.0\r\n         Python: 2.7.3 (default, Apr 20 2012, 22:39:59)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n````\r\n\r\n* salt-call state.sls states.users.test (salt 0.17.0)\r\n\r\n````\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Fetching file ** skipped **, latest already in cache \'salt://states/users/test.sls\'\r\n[INFO    ] Executing state group.absent for puppet\r\n[INFO    ] Group not present\r\n[INFO    ] Executing state group.absent for vboxsf\r\n[INFO    ] Group not present\r\n[INFO    ] Executing state group.present for testers\r\n[INFO    ] Executing command \'groupadd testers\' in directory \'/root\'\r\n[INFO    ] {\'passwd\': \'x\', \'gid\': 1001, \'name\': \'testers\', \'members\': []}\r\n[INFO    ] Executing state group.present for jdoe\r\n[INFO    ] Executing command \'groupadd jdoe\' in directory \'/root\'\r\n[INFO    ] {\'passwd\': \'x\', \'gid\': 1002, \'name\': \'jdoe\', \'members\': []}\r\n[INFO    ] Executing state user.present for jdoe\r\n[INFO    ] Executing command \'useradd -s /bin/bash -u 5000 -m -d /home/jdoe jdoe\' in directory \'/root\'\r\n[ERROR   ] Command \'useradd -s /bin/bash -u 5000 -m -d /home/jdoe jdoe\' failed with return code: 9\r\n[ERROR   ] stderr: useradd: group jdoe exists - if you want to add this user to that group, use -g.\r\n[ERROR   ] Failed to create new user jdoe\r\n````\r\n\r\n* salt-call state.sls states.users.test (salt 0.16.4)\r\n\r\n````\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Fetching file \'salt://states/users/test.sls\'\r\n[INFO    ] Executing state group.present for testers\r\n[INFO    ] Executing command \'groupadd testers\' in directory \'/root\'\r\n[INFO    ] {\'passwd\': \'x\', \'gid\': 1001, \'name\': \'testers\', \'members\': []}\r\n[INFO    ] Executing state group.present for jdoe\r\n[INFO    ] Executing command \'groupadd jdoe\' in directory \'/root\'\r\n[INFO    ] {\'passwd\': \'x\', \'gid\': 1002, \'name\': \'jdoe\', \'members\': []}\r\n[INFO    ] Executing state group.absent for puppet\r\n[INFO    ] No changes made for puppet\r\n[INFO    ] Executing state group.absent for vboxsf\r\n[INFO    ] No changes made for vboxsf\r\n[INFO    ] Executing state user.present for jdoe\r\n[INFO    ] Executing command \'useradd -s /bin/bash -u 5000 -g 1002 -m -d /home/jdoe jdoe\' in directory \'/root\'\r\n[INFO    ] Executing command \'usermod -G "jdoe,testers" jdoe\' in directory \'/root\'\r\n[INFO    ] Executing command \'usermod -c "John Doe,,," jdoe\' in directory \'/root\'\r\n[INFO    ] {\'shell\': \'/bin/bash\', \'workphone\': \'\', \'uid\': 5000, \'passwd\': \'x\', \'roomnumber\': \'\', \'groups\': [\'jdoe\', \'testers\'], \'home\': \'/home/jdoe\', \'name\': \'jdoe\', \'gid\': 1002, \'fullname\': \'John Doe\', \'homephone\': \'\'}\r\n[INFO    ] Executing state file.directory for /home/jdoe/.ssh\r\n[INFO    ] {\'/home/jdoe/.ssh\': \'New Dir\'}\r\n````\r\n\r\n* cat states/users/test.sls\r\n\r\n````\r\npuppet:\r\n    group.absent\r\n\r\nvboxsf:\r\n    group.absent\r\n\r\ntesters:\r\n    group:\r\n        - present\r\n\r\njdoe:\r\n    file:\r\n        - directory\r\n        - name: /home/jdoe/.ssh\r\n        - mode: \'700\'\r\n        - user: jdoe\r\n        - group: jdoe\r\n        - require:\r\n            - user: jdoe\r\n    group:\r\n        - present\r\n    user:\r\n        - present\r\n        - name: jdoe\r\n        - shell: /bin/bash\r\n        - fullname: John Doe\r\n        - home: /home/jdoe\r\n        - uid: 5000\r\n        - groups:\r\n            - jdoe\r\n            - testers\r\n        - require:\r\n            - group: testers\r\n            - group: jdoe\r\n            - group: puppet\r\n            - group: vboxsf\r\n````'
7649,'cro','"-names" argument in state not retaining order (turned into set())\nIn states, "-names" argument is a set, not like list, so it will reorder, not the state order. This discussion url is https://groups.google.com/forum/#!topic/salt-users/DYTvPYHUveg'
7647,'cachedout',"[Bug] 0.17.0 Minion defines nsca service state wrong\nDebian GNU/Linux 7.1 (wheezy)\r\n\r\n```\r\nsalt --versions-report\r\n Salt: 0.17.0\r\n Python: 2.7.3 (default, Jan  2 2013, 13:56:14)\r\n Jinja2: 2.6\r\n M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n msgpack-pure: Not Installed\r\n pycrypto: 2.6\r\n PyYAML: 3.10\r\n PyZMQ: 13.1.0\r\n ZMQ: 3.2.3\r\n```\r\n\r\nProblem:\r\n\r\nSalt minion doesn't handle service state right. Package nsca.\r\n\r\nReproducing:\r\n\r\nMaster sls file listing\r\n\r\n```\r\nnsca:\r\n  pkg:\r\n   - installed\r\n  service:\r\n    - running\r\n    - enable: True\r\n```\r\n\r\n```\r\nuser@host:~$ sudo service nsca stop\r\n[ ok ] Stopping Nagios Service Check Acceptor: nsca.\r\nuser@host:~$ sudo service nsca status\r\n[FAIL] nsca is not running ... failed!\r\nuser@host:~$ sudo salt-call state.highstate\r\n....\r\n        service_|-nsca_|-nsca_|-running:\r\n            ----------\r\n            __run_num__:\r\n                2\r\n            changes:\r\n                ----------\r\n            comment:\r\n                Service nsca is already enabled, and is in the desired state\r\n            name:\r\n                nsca\r\n            result:\r\n                True\r\n\r\nuser@host:~$ sudo service nsca status\r\n[FAIL] nsca is not running ... failed!\r\n```\r\n"
7615,'cachedout','salt-ssh: list index out of range with no command line args\nShould print help like salt & salt-call'
7604,'thatch45','Provide a passthrough external auth for testing and development \nAs discussed with @thatch45 yesterday at the SaltStack Meetup in Paris, it would be nice to have an external auth that bypasses authentication and returns "True" whatever password is given. '
7598,'thatch45','salt.modules.state.pkg Links in tarfiles could write outside of designated directories\n@ze42 pointed this issue out in #7577:\r\n\r\nNow, an other bug would be if we had something like :\r\n\r\n```\r\n$ tar tvf /tmp/piou.tar \r\nlrwxrwxrwx ze/ze             0 2013-10-03 11:15 link -> /tmp\r\n-rw-r--r-- ze/ze             3 2013-10-03 11:15 link/target\r\n```\r\n\r\nExtracting that anywhere with tarfile would write a /tmp/target file.\r\nUsing the tar command on my Debian would reject the link/target and not\r\nwrite in /tmp.'
7597,'thatch45',"salt-ssh on archlinux (from develop)  error: Pseudo-terminal will not be allocated because stdin is not a terminal.\nSalt-ssh (packaged from develop) version  0.17.0-488-g3942b87\r\ngives the following error:\r\n\r\nmy root user has /bin/zsh as shell\r\n\r\n```\r\nspectre ~ # salt-ssh '*' test.ping\r\nspectre:\r\n    Pseudo-terminal will not be allocated because stdin is not a terminal.\r\n    zsh: condition expected: not\r\n    zsh: condition expected: is\r\n    zsh: condition expected: not\r\n    zsh: condition expected: not\r\n    zsh: condition expected: is\r\n    zsh: condition expected: is\r\n    zsh: toegang geweigerd: /tmp/.salt/salt-call\r\n    \r\nroot@spectre ~ # salt-ssh '*' test.ping -l all\r\n[DEBUG   ] Reading configuration from /etc/salt/master\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/auth.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/auth.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/gitfs.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/gitfs.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/halite.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/halite.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/output.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/output.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/peer.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/peer.conf\r\n[DEBUG   ] loading log_handlers in ['/var/cache/salt/master/extmods/log_handlers', '/usr/lib/python2.7/site-packages/salt/log/handlers']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/log_handlers, it is not a directory\r\n[DEBUG   ] None of the required configuration sections, 'logstash_udp_handler' and 'logstash_zmq_handler', were found the in the configuration. Not loading the Logstash logging handlers module.\r\n[DEBUG   ] Configuration file path: /etc/salt/master\r\n[DEBUG   ] loading roster in ['/var/cache/salt/master/extmods/roster', '/usr/lib/python2.7/site-packages/salt/roster']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/roster, it is not a directory\r\n[TRACE   ] Added flat.RosterMatcher to roster\r\n[TRACE   ] Added flat.compile_template to roster\r\n[TRACE   ] Added flat.targets to roster\r\n[TRACE   ] Added scan.RosterMatcher to roster\r\n[TRACE   ] Added scan.targets to roster\r\n[DEBUG   ] loading render in ['/var/cache/salt/master/extmods/renderers', '/usr/lib/python2.7/site-packages/salt/renderers']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/renderers, it is not a directory\r\n[TRACE   ] Added mako.StringIO to render\r\n[TRACE   ] Added mako.render to render\r\n[TRACE   ] Added pydsl.render to render\r\n[TRACE   ] Added stateconf.Bunch to render\r\n[TRACE   ] Added stateconf.StringIO to render\r\n[TRACE   ] Added stateconf.add_goal_state to render\r\n[TRACE   ] Added stateconf.add_implicit_requires to render\r\n[TRACE   ] Added stateconf.add_start_state to render\r\n[TRACE   ] Added stateconf.chain to render\r\n[TRACE   ] Added stateconf.extract_state_confs to render\r\n[TRACE   ] Added stateconf.has_names_decls to render\r\n[TRACE   ] Added stateconf.nvlist to render\r\n[TRACE   ] Added stateconf.nvlist2 to render\r\n[TRACE   ] Added stateconf.rename_state_ids to render\r\n[TRACE   ] Added stateconf.render to render\r\n[TRACE   ] Added stateconf.rewrite_single_shorthand_state_decl to render\r\n[TRACE   ] Added stateconf.rewrite_sls_includes_excludes to render\r\n[TRACE   ] Added stateconf.state_name to render\r\n[TRACE   ] Added stateconf.statelist to render\r\n[TRACE   ] Added jinja.StringIO to render\r\n[TRACE   ] Added jinja.render to render\r\n[TRACE   ] Added py.render to render\r\n[TRACE   ] Added yaml.CustomLoader to render\r\n[TRACE   ] Added yaml.OrderedDict to render\r\n[TRACE   ] Added yaml.get_yaml_loader to render\r\n[TRACE   ] Added yaml.load to render\r\n[TRACE   ] Added yaml.render to render\r\n[TRACE   ] Added json.render to render\r\n[TRACE   ] Added wempy.StringIO to render\r\n[TRACE   ] Added wempy.render to render\r\n[DEBUG   ] Rendered data from file: /etc/salt/roster:\r\nspectre:\r\n  host: spectre\r\n  user: root\r\n  passwd: redacted\r\n  sudo: False\r\n\r\n[DEBUG   ] Results of YAML rendering: \r\nOrderedDict([('spectre', OrderedDict([('host', 'spectre'), ('user', 'root'), ('passwd', 'al2in1'), ('sudo', False)]))])\r\n[DEBUG   ] loading wrapper in ['/var/cache/salt/master/extmods/wrapper', '/usr/lib/python2.7/site-packages/salt/client/ssh/wrapper']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/wrapper, it is not a directory\r\n[TRACE   ] Added state.high to wrapper\r\n[TRACE   ] Added state.highstate to wrapper\r\n[TRACE   ] Added state.low to wrapper\r\n[TRACE   ] Added state.show_highstate to wrapper\r\n[TRACE   ] Added state.show_lowstate to wrapper\r\n[TRACE   ] Added state.show_sls to wrapper\r\n[TRACE   ] Added state.show_top to wrapper\r\n[TRACE   ] Added state.sls to wrapper\r\n[TRACE   ] Added state.top to wrapper\r\n[TRACE   ] Added config.backup_mode to wrapper\r\n[TRACE   ] Added config.dot_vals to wrapper\r\n[TRACE   ] Added config.get to wrapper\r\n[TRACE   ] Added config.manage_mode to wrapper\r\n[TRACE   ] Added config.merge to wrapper\r\n[TRACE   ] Added config.option to wrapper\r\n[TRACE   ] Added config.valid_fileproto to wrapper\r\n[TRACE   ] Added pillar.data to wrapper\r\n[TRACE   ] Added pillar.get to wrapper\r\n[TRACE   ] Added pillar.item to wrapper\r\n[TRACE   ] Added pillar.items to wrapper\r\n[TRACE   ] Added pillar.raw to wrapper\r\n[TRACE   ] Added grains.filter_by to wrapper\r\n[TRACE   ] Added grains.get to wrapper\r\n[TRACE   ] Added grains.item to wrapper\r\n[TRACE   ] Added grains.items to wrapper\r\n[TRACE   ] Added grains.ls to wrapper\r\n[DEBUG   ] loading output in ['/var/cache/salt/master/extmods/output', '/usr/lib/python2.7/site-packages/salt/output']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/output, it is not a directory\r\n[TRACE   ] Added txt.output to output\r\n[TRACE   ] Added grains.output to output\r\n[TRACE   ] Added overstatestage.output to output\r\n[DEBUG   ] Loaded no_out as virtual quiet\r\n[TRACE   ] Added quiet.output to output\r\n[DEBUG   ] Loaded json_out as virtual json\r\n[TRACE   ] Added json.output to output\r\n[TRACE   ] Added nested.NestDisplay to output\r\n[TRACE   ] Added nested.Number to output\r\n[TRACE   ] Added nested.output to output\r\n[TRACE   ] Added raw.output to output\r\n[TRACE   ] Added highstate.output to output\r\n[DEBUG   ] Loaded yaml_out as virtual yaml\r\n[TRACE   ] Added yaml.output to output\r\n[TRACE   ] Added virt_query.output to output\r\n[TRACE   ] Added key.output to output\r\n[TRACE   ] Added no_return.NestDisplay to output\r\n[TRACE   ] Added no_return.output to output\r\n[DEBUG   ] Loaded pprint_out as virtual pprint\r\n[TRACE   ] Added pprint.output to output\r\nspectre:\r\n    Pseudo-terminal will not be allocated because stdin is not a terminal.\r\n    zsh: condition expected: not\r\n    zsh: condition expected: is\r\n    zsh: condition expected: not\r\n    zsh: condition expected: not\r\n    zsh: condition expected: is\r\n    zsh: condition expected: is\r\n    zsh: toegang geweigerd: /tmp/.salt/salt-call\r\n```\r\n\r\nrunning salt from pypi 0.17.0 on the same machine with same config:\r\n```\r\nroot@spectre ~ # salt-ssh '*' test.ping -l all\r\n[DEBUG   ] Reading configuration from /etc/salt/master\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/auth.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/auth.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/gitfs.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/gitfs.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/halite.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/halite.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/output.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/output.conf\r\n[DEBUG   ] Including configuration from '/etc/salt/master.d/peer.conf'\r\n[DEBUG   ] Reading configuration from /etc/salt/master.d/peer.conf\r\n[DEBUG   ] loading log_handlers in ['/var/cache/salt/master/extmods/log_handlers', '/usr/lib/python2.7/site-packages/salt/log/handlers']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/log_handlers, it is not a directory\r\n[DEBUG   ] None of the required configuration sections, 'logstash_udp_handler' and 'logstash_zmq_handler', were found the in the configuration. Not loading the Logstash logging handlers module.\r\n[DEBUG   ] Configuration file path: /etc/salt/master\r\n[DEBUG   ] loading roster in ['/var/cache/salt/master/extmods/roster', '/usr/lib/python2.7/site-packages/salt/roster']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/roster, it is not a directory\r\n[TRACE   ] Added flat.RosterMatcher to roster\r\n[TRACE   ] Added flat.compile_template to roster\r\n[TRACE   ] Added flat.targets to roster\r\n[TRACE   ] Added scan.RosterMatcher to roster\r\n[TRACE   ] Added scan.targets to roster\r\n[DEBUG   ] loading render in ['/var/cache/salt/master/extmods/renderers', '/usr/lib/python2.7/site-packages/salt/renderers']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/renderers, it is not a directory\r\n[TRACE   ] Added mako.StringIO to render\r\n[TRACE   ] Added mako.render to render\r\n[TRACE   ] Added pydsl.render to render\r\n[TRACE   ] Added stateconf.Bunch to render\r\n[TRACE   ] Added stateconf.StringIO to render\r\n[TRACE   ] Added stateconf.add_goal_state to render\r\n[TRACE   ] Added stateconf.add_implicit_requires to render\r\n[TRACE   ] Added stateconf.add_start_state to render\r\n[TRACE   ] Added stateconf.chain to render\r\n[TRACE   ] Added stateconf.extract_state_confs to render\r\n[TRACE   ] Added stateconf.has_names_decls to render\r\n[TRACE   ] Added stateconf.nvlist to render\r\n[TRACE   ] Added stateconf.nvlist2 to render\r\n[TRACE   ] Added stateconf.rename_state_ids to render\r\n[TRACE   ] Added stateconf.render to render\r\n[TRACE   ] Added stateconf.rewrite_single_shorthand_state_decl to render\r\n[TRACE   ] Added stateconf.rewrite_sls_includes_excludes to render\r\n[TRACE   ] Added stateconf.state_name to render\r\n[TRACE   ] Added stateconf.statelist to render\r\n[TRACE   ] Added jinja.StringIO to render\r\n[TRACE   ] Added jinja.render to render\r\n[TRACE   ] Added py.render to render\r\n[TRACE   ] Added yaml.CustomLoader to render\r\n[TRACE   ] Added yaml.OrderedDict to render\r\n[TRACE   ] Added yaml.get_yaml_loader to render\r\n[TRACE   ] Added yaml.load to render\r\n[TRACE   ] Added yaml.render to render\r\n[TRACE   ] Added json.render to render\r\n[TRACE   ] Added wempy.StringIO to render\r\n[TRACE   ] Added wempy.render to render\r\n[DEBUG   ] Rendered data from file: /etc/salt/roster:\r\nspectre:\r\n  host: spectre\r\n  user: root\r\n  passwd: redacted\r\n  sudo: False\r\n\r\n[DEBUG   ] Results of YAML rendering: \r\nOrderedDict([('spectre', OrderedDict([('host', 'spectre'), ('user', 'root'), ('passwd', 'al2in1'), ('sudo', False)]))])\r\n[DEBUG   ] loading wrapper in ['/var/cache/salt/master/extmods/wrapper', '/usr/lib/python2.7/site-packages/salt/client/ssh/wrapper']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/wrapper, it is not a directory\r\n[TRACE   ] Added state.high to wrapper\r\n[TRACE   ] Added state.highstate to wrapper\r\n[TRACE   ] Added state.low to wrapper\r\n[TRACE   ] Added state.show_highstate to wrapper\r\n[TRACE   ] Added state.show_lowstate to wrapper\r\n[TRACE   ] Added state.show_sls to wrapper\r\n[TRACE   ] Added state.show_top to wrapper\r\n[TRACE   ] Added state.sls to wrapper\r\n[TRACE   ] Added state.top to wrapper\r\n[TRACE   ] Added config.backup_mode to wrapper\r\n[TRACE   ] Added config.dot_vals to wrapper\r\n[TRACE   ] Added config.get to wrapper\r\n[TRACE   ] Added config.manage_mode to wrapper\r\n[TRACE   ] Added config.merge to wrapper\r\n[TRACE   ] Added config.option to wrapper\r\n[TRACE   ] Added config.valid_fileproto to wrapper\r\n[TRACE   ] Added pillar.data to wrapper\r\n[TRACE   ] Added pillar.get to wrapper\r\n[TRACE   ] Added pillar.item to wrapper\r\n[TRACE   ] Added pillar.items to wrapper\r\n[TRACE   ] Added pillar.raw to wrapper\r\n[TRACE   ] Added grains.filter_by to wrapper\r\n[TRACE   ] Added grains.get to wrapper\r\n[TRACE   ] Added grains.item to wrapper\r\n[TRACE   ] Added grains.items to wrapper\r\n[TRACE   ] Added grains.ls to wrapper\r\n[DEBUG   ] loading output in ['/var/cache/salt/master/extmods/output', '/usr/lib/python2.7/site-packages/salt/output']\r\n[DEBUG   ] Skipping /var/cache/salt/master/extmods/output, it is not a directory\r\n[TRACE   ] Added txt.output to output\r\n[TRACE   ] Added grains.output to output\r\n[TRACE   ] Added overstatestage.output to output\r\n[DEBUG   ] Loaded no_out as virtual quiet\r\n[TRACE   ] Added quiet.output to output\r\n[DEBUG   ] Loaded json_out as virtual json\r\n[TRACE   ] Added json.output to output\r\n[TRACE   ] Added nested.NestDisplay to output\r\n[TRACE   ] Added nested.Number to output\r\n[TRACE   ] Added nested.output to output\r\n[TRACE   ] Added raw.output to output\r\n[TRACE   ] Added highstate.output to output\r\n[DEBUG   ] Loaded yaml_out as virtual yaml\r\n[TRACE   ] Added yaml.output to output\r\n[TRACE   ] Added virt_query.output to output\r\n[TRACE   ] Added key.output to output\r\n[TRACE   ] Added no_return.NestDisplay to output\r\n[TRACE   ] Added no_return.output to output\r\n[DEBUG   ] Loaded pprint_out as virtual pprint\r\n[TRACE   ] Added pprint.output to output\r\nspectre:\r\n    True\r\n```\r\n"
7558,'basepi',"Minion id changes from 0.16.4 to 0.17.0, looses domain name part\nThis patch (https://github.com/saltstack/salt/pull/6734) seems to break the id grain on debian systems (and maybe many others)!\r\n\r\nWe just got an update from 0.16.4 to 0.17.0 through unattended upgrades (maybe a bad idea for salt).\r\nNow all our minions are gone, because the id changed and new minion keys are generated, which of course are unknown to the master.\r\nThese ids do not include the domain part anymore.\r\nThis is really bad. It's not the first time the id generation changes and breaks everything. With such a crucial part of the system, i think there should be taken more care.\r\n\r\nSorry for the angry tone, but this is a bit frustrating."
7546,'thatch45','SaltCloud API in Salt\nProvide API for SaltCloud in Salt and associated events so that SaltCloud can be controlled from the UI.'
7531,'thatch45','salt-ssh writes to /etc\nsalt-ssh wants to create the file /etc/salt/pki/minion\r\n\r\nThis seems to be unnecessary for salt-ssh.'
7524,'UtahDave','connection is lost windows minions in AWS\nHi guys,\r\n\r\nIn a continue to this [thread](https://groups.google.com/forum/#!topic/salt-users/EmdTp1qh0Ho)\r\nWhen executing state.sls or state.single from the master on a windows minion the state doesn\'t apply on the minion and it losses connectivity to the master\r\nIf salt-call is used from the minion it-self it doesn\'t loss the connection but the state isn\'t enforced and the salt-call command hangs\r\n```\r\nC:\\>c:\\salt\\salt-call state.single file.touch name="c:\\test.txt" --no-color -l debug\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[DEBUG   ] Configuration file path: c:\\salt\\conf\\minion\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[DEBUG   ] loading grain in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\grains\', \'c:\\\\salt\\\\salt-0.16.3.win-amd64\\\\salt-0.16.3-py2.7.egg\\\\salt\\\\gra\r\nins\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\grains, it is not a directory\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] loading module in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\modules\', \'c:\\\\salt\\\\salt-0.16.3.win-amd64\\\\salt-0.16.3-py2.7.egg\\\\salt\\\\m\r\nodules\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\modules, it is not a directory\r\n[DEBUG   ] Failed to import module postgres, this is most likely NOT a problem:\r\nTraceback (most recent call last):\r\n  File "salt/loader.py", line 605, in gen_functions\r\n  File "salt/modules/postgres.py", line 23, in <module>\r\nImportError: No module named pipes\r\n[DEBUG   ] Failed to import module git, this is most likely NOT a problem:\r\nTraceback (most recent call last):\r\n  File "salt/loader.py", line 605, in gen_functions\r\n  File "salt/modules/git.py", line 8, in <module>\r\nImportError: No module named pipes\r\n[DEBUG   ] Failed to import module virt, this is most likely NOT a problem:\r\nTraceback (most recent call last):\r\n  File "salt/loader.py", line 605, in gen_functions\r\n  File "salt/modules/virt.py", line 14, in <module>\r\nImportError: No module named dom\r\n[DEBUG   ] Loaded win_shadow as virtual shadow\r\n[DEBUG   ] Loaded win_disk as virtual disk\r\n[DEBUG   ] Loaded win_status as virtual status\r\n[DEBUG   ] Loaded win_network as virtual network\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded win_system as virtual system\r\n[DEBUG   ] Loaded win_pkg as virtual pkg\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded win_file as virtual file\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] Loaded win_service as virtual service\r\n[DEBUG   ] Loaded win_useradd as virtual user\r\n[DEBUG   ] Loaded win_groupadd as virtual group\r\n[DEBUG   ] loading returner in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\returners\', \'c:\\\\salt\\\\salt-0.16.3.win-amd64\\\\salt-0.16.3-py2.7.egg\\\\sal\r\nt\\\\returners\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\returners, it is not a directory\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] loading states in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\states\', \'c:\\\\salt\\\\salt-0.16.3.win-amd64\\\\salt-0.16.3-py2.7.egg\\\\salt\\\\st\r\nates\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\states, it is not a directory\r\n[DEBUG   ] loading render in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\renderers\', \'c:\\\\salt\\\\salt-0.16.3.win-amd64\\\\salt-0.16.3-py2.7.egg\\\\salt\\\r\n\\renderers\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\renderers, it is not a directory\r\n\r\nExiting gracefully on Ctrl-c\r\n```\r\n\r\nThe same configuration on a non-AWS machine works so I suspect AWS\r\nand this happens on all the windows minion in AWS\r\n\r\nminion: Windows 2008 R2 64bit Salt 0.16.3\r\nmaster: RHEL 6.4 64bit 0.16.4\r\n\r\nThanks !\r\nRan'
7516,'thatch45',"salt-ssh copy wrong salt-call to target machine\nsalt-ssh generated a salt-thin package with a wrong salt-call file. \r\n- salt 0.17.0 installed via pip [the following result is from a Mac OS X 10.8 env, but I've tested on a Ubuntu 12.04 with the same result] \r\n- minion box1 is a Ubuntu 12.04 \r\n- ssh auth used public key\r\n- I can run raw command with salt-ssh on box1 without any problem\r\n\r\n`sudo salt-ssh -l debug box1 test.ping`\r\nhttps://gist.github.com/number5/02a32f1ceceb92bb613b#file-salt-ssh-debug-log\r\n`box1 output`\r\nhttps://gist.github.com/number5/02a32f1ceceb92bb613b#file-test-ping-traceback\r\n`/tmp/.salt/salt-call` on box1\r\nhttps://gist.github.com/number5/02a32f1ceceb92bb613b#file-salt-call-on-target-machine\r\n`salt-ssh --versions-report`\r\nhttps://gist.github.com/number5/02a32f1ceceb92bb613b#file-salt-ssh-versions-report"
7482,'thatch45','salt-ssh fails when salt .py files are behind symbolic links\nOn debian, python modules that are able to be shared between python versions are symbolic linked into the relevant site(dist)-packages directory.\r\n\r\nWhen salt-ssh copies the files across, it preserves these links which means it doesn\'t work. Errors like:\r\n\r\n```\r\nunstable-sshmin:\r\n    Pseudo-terminal will not be allocated because stdin is not a terminal.\r\n    stdin: is not a tty\r\n    Traceback (most recent call last):\r\n      File "/tmp/.salt/salt-call", line 7, in <module>\r\n        from salt.scripts import salt_call\r\n    ImportError: No module named salt.scripts\r\n    \r\n```'
7474,'thatch45','salt-ssh stopped working with salt develop 0.17.0-253-g89be674\nI think with commit 089207094a7c375ffab6039e84e9f28754057321 \r\nsalt-ssh stopped working it just hangs...\r\n\r\nstacktrace:\r\n```\r\nsalt-ssh \'*\' test.ping    \r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 114, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 234, in handle_routine\r\n    stdout, stderr = single.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 462, in run\r\n    return self.cmd_block()\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 500, in cmd_block\r\n    self.deploy()\r\n  File "/usr/lib/python2.7/site-packages/salt/client/ssh/__init__.py", line 390, in deploy\r\n    thin = salt.utils.thin.gen_thin(self.opts[\'cachedir\'])\r\n  File "/usr/lib/python2.7/site-packages/salt/utils/thin.py", line 41, in gen_thin\r\n    os.path.dirname(yaml.yaml.__file__),\r\nAttributeError: \'module\' object has no attribute \'yaml\'\r\n^C\r\nExiting gracefully on Ctrl-c\r\nsalt-ssh \'*\' test.ping  42,65s user 2,15s system 100% cpu 44,785 total\r\n```'
7425,'s0undt3ch','setup.py gets broken\nIn recent updates, setup.py is broken because salt.version call salt._compat which in turn load salt.__init__ which in turn load utils which fails without jinja2.\r\n\r\nThis make salt not usable as a develop package without buildout as you can\'t fiddle in packages without having to install them to your site-packages, and this defeat the purpose of buildout ;)\r\n\r\nNormally a setup.py should not rely on extra packages more that we have in stdlib.\r\nYou must can do python setup.py --anypackaging command without any issue.\r\n\r\n\r\n```\r\npython setup.py --version\r\n[CRITICAL] Unable to import msgpack or msgpack_pure python modules\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nImportError: No module named jinja2\r\nTraceback (most recent call last):\r\n  File "setup.py", line 77, in <module>\r\n    exec(compile(open(SALT_VERSION).read(), SALT_VERSION, \'exec\'))\r\n  File "/srv/salt/makina-states/src/salt/salt/version.py", line 11, in <module>\r\n    import salt._compat\r\n  File "/srv/salt/makina-states/src/salt/salt/__init__.py", line 28, in <module>\r\n    from salt.utils import migrations\r\n  File "/srv/salt/makina-states/src/salt/salt/utils/__init__.py", line 67, in <module>\r\n    import salt.minion\r\n  File "/srv/salt/makina-states/src/salt/salt/minion.py", line 43, in <module>\r\n    import salt.client\r\n  File "/srv/salt/makina-states/src/salt/salt/client/__init__.py", line 40, in <module>\r\n    import salt.config\r\n  File "/srv/salt/makina-states/src/salt/salt/config.py", line 27, in <module>\r\n    import salt.pillar\r\n  File "/srv/salt/makina-states/src/salt/salt/pillar/__init__.py", line 13, in <module>\r\n    import salt.fileclient\r\n  File "/srv/salt/makina-states/src/salt/salt/fileclient.py", line 25, in <module>\r\n    import salt.utils.templates\r\n  File "/srv/salt/makina-states/src/salt/salt/utils/templates.py", line 18, in <module>\r\n    import jinja2\r\nImportError: No module named jinja2\r\nTraceback (most recent call last):\r\n  File "setup.py", line 77, in <module>\r\n    exec(compile(open(SALT_VERSION).read(), SALT_VERSION, \'exec\'))\r\n  File "/srv/salt/makina-states/src/salt/salt/version.py", line 11, in <module>\r\n    import salt._compat\r\n  File "/srv/salt/makina-states/src/salt/salt/__init__.py", line 28, in <module>\r\n    from salt.utils import migrations\r\n  File "/srv/salt/makina-states/src/salt/salt/utils/__init__.py", line 67, in <module>\r\n    import salt.minion\r\n  File "/srv/salt/makina-states/src/salt/salt/minion.py", line 43, in <module>\r\n    import salt.client\r\n  File "/srv/salt/makina-states/src/salt/salt/client/__init__.py", line 40, in <module>\r\n    import salt.config\r\n  File "/srv/salt/makina-states/src/salt/salt/config.py", line 27, in <module>\r\n    import salt.pillar\r\n  File "/srv/salt/makina-states/src/salt/salt/pillar/__init__.py", line 13, in <module>\r\n    import salt.fileclient\r\n  File "/srv/salt/makina-states/src/salt/salt/fileclient.py", line 25, in <module>\r\n    import salt.utils.templates\r\n  File "/srv/salt/makina-states/src/salt/salt/utils/templates.py", line 18, in <module>\r\n    import jinja2\r\nImportError: No module named jinja2\r\n```'
7422,'cachedout',"Scheduler and ps.cpu_percent function\nI'm not sure if this is a bug or I'm doing something wrong, but I was directed here from the salt-users mailing list. [original discussion](https://groups.google.com/forum/#!topic/salt-users/vynCM-cUeDE)\r\n\r\nI have a bunch of tasks running via the scheduler. They seem to be working fine except for ps.cpu_percent. Here's the snippet of my schedule:\r\n\r\n```\r\nschedule: \r\n  stat_cpu_percent:\r\n    function: ps.cpu_percent \r\n      kwargs: \r\n        interval: 5 \r\n        per_cpu: True \r\n    minutes: 5 \r\n    returner: librato\r\n```\r\n\r\nWhen I run `salt-call --return librato ps.cpu_percent interval=5 per_cpu=True` it works as expected. The scheduler never returns anything as far as I can tell. I've refreshed my pillars, done a config.merge schedule, and even restarted the minion. I did notice that this is the only function that returns a list (instead of a dict) and it also runs longer than any of the others, maybe that could be causing issues? Any thoughts?"
7409,'s0undt3ch',"Adding key to ssh, adds in the same line as last entry\nI am trying out salt and have come across a weird problem:\r\n\r\nWhen I try to add an ssh key using the ssh_auth state, I see that there the key gets added in the same line as the last entry of the file. I am using EC2 and salt-cloud -  the authorized_keys file already has an entry upon installation like:\r\n\r\n```\r\n$ cat ~/.ssh/authorized_keys\r\n\r\n--\r\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCQDdsYiJdChkf006T6qkbKYXukyPmNNhktfyLFNhpwDnLJP3dFtDAuGAnl3g0NsZ5wd7Q9L6mGtOcScw1wUyKZgGJxnA1Z27brNTsTssssssssssssssssssssssssssssssszAjDlFPT5yrC1MFdkS3ILzY0RsPIgCX/7Pnb6SBTgi5mL23WsqL+mRLKhxEeWrIGFZi8kFFcqw7o7R7oclF+Weh3GiIxQW9MXXerZshUQDGF1+yCM9aF8+13oFFajmvcnzmNdQbY9qpIfPMseVVHypgBZpkwfwAe2xxZdMJnHo0jCfkHsV4FPVMJnK6lp4eUV9/VV3RXOe5akLrSGUN+/ test_someting\r\n```\r\n\r\n**This file did not have a newline at the end upon launch of the EC2 instance.**\r\n\r\nAdding a key to this user using states with something like:\r\n\r\n```\r\nsalt '*' state.sls ssh_auth.sls\r\n```\r\n\r\nsucceeds, but the key is added at the end of the last line and looks like:\r\n\r\n```\r\n$ cat ~/.ssh/authorized_keys\r\n\r\n--\r\nssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCQDdsYiJdChkf006T6qkbKYXukyPmNNhktfyLFNhpwDnLJP3dFtDAuGAnl3g0NsZ5wd7Q9L6mGtOcScw1wUyKZgGJxnA1Z27brNTsTssssssssssssssssssssssssssssssszAjDlFPT5yrC1MFdkS3ILzY0RsPIgCX/7Pnb6SBTgi5mL23WsqL+mRLKhxEeWrIGFZi8kFFcqw7o7R7oclF+Weh3GiIxQW9MXXerZshUQDGF1+yCM9aF8+13oFFajmvcnzmNdQbY9qpIfPMseVVHypgBZpkwfwAe2xxZdMJnHo0jCfkHsV4FPVMJnK6lp4eUV9/VV3RXOe5akLrSGUN+/ test_sometingssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCQDdsYiJdChkf006T6qkbKYXukyPmNNhktfyLFNhpwDnLJP3dFtDAuGAnl3g0NsZwd7Q9L6mGtOcScw1wUyKZgGJxnA1Z27brNTsTssssssssssssssssssssssssssssssszAjDlFPT5yrC1MFdkS3ILzY0RsPIgCX/7Pnb6SBTgi5mL23WsqL+mRLKhxEeWrIGFZi8kFFcqw7o7R7oclF+Weh3GiIxQW9MXXerZshUQDaaaaaaaaaaaaaaaaaaaaaaaaamvcnzmNdQbY9qpIfPMseVVHypgBZpkwfwAe2xxZdMJnHo0jCfkHsV4FPVMJnK6lp4eUV9/VV3RXOe5akLrSGUN+/ test_someting2\r\n```\r\n\r\nThis ends up invalidating both keys and disabling access to the server.\r\n\r\nIs there a work around for this, until the bug is fixed? I am using Debian 7 and the debian repository for saltstack/wheezy. My saltstack version is 0.16.4"
7371,'terminalmage','Salt fails to detect Rasbian as a Debian derivative\nHello,\r\n\r\nTwo errors seem to keep persisting across later versions of salt for some reason.\r\nThis is causing me a traceback error on pkg and service states.\r\n\r\nThe first error is this:\r\n```\r\nState: - pkg\r\n    Name:      unison-pkg\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1255, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 412, in installed\r\n    fromrepo=fromrepo, **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 99, in _find_install_targets\r\n    cur_pkgs = __salt__[\'pkg.list_pkgs\'](versions_as_list=True)\r\nKeyError: \'pkg.list_pkgs\'\r\n```\r\n\r\nAnd the second one is this:\r\n```\r\nState: - service\r\n    Name:      freepbx\r\n    Function:  running\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1255, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/service.py", line 273, in running\r\n    ret = _available(name, ret)\r\n  File "/usr/lib/pymodules/python2.7/salt/states/service.py", line 239, in _available\r\n    ret[\'available\'] = __salt__[\'service.available\'](name)\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/service.py", line 169, in available\r\n    return name in get_all()\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/service.py", line 154, in get_all\r\n    if not os.path.isdir(GRAINMAP[__grains__[\'os\']]):\r\nKeyError: \'Raspbian GNU/\'\r\n```\r\n\r\nCan someone help out? I\'m pretty sure my config is ok as it worked before.\r\nLet me know if you want to see my config and I can post it..\r\n\r\nEdit: I just noticed that the second one might have failed because it\'s a Raspbian GNU system?\r\n\r\n\\# salt --version\r\nsalt 0.16.0-3514-g7063499\r\n\r\nThanks!'
7367,'pass-by-value',"scheduler either not running or not returning anything via returner\nI have a schedule configured via pillar to have my minions run ``status.uptime`` every few minutes. I have this configured to return via the mysql returner, but nothing is added to the database. Either the schedule isn't running, or the returner isn't working (although it works manually).\r\n\r\n```shell\r\n[root@summer ~]# salt --versions-report\r\n           Salt: 0.16.3\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n\r\n[root@summer pillar]# tree\r\n.\r\n├── nginx.sls\r\n├── top.sls\r\n└── uptime.sls\r\n\r\n[root@summer pillar]# cat uptime.sls \r\nschedule:\r\n  uptime:\r\n    function: status.uptime\r\n    minutes: 5\r\n    returner: mysql"
7361,'terminalmage',"daemontools service provider not working\nI have a daemontools service that I am managing using salt. the service state does not recognize the service, even though the daemontools module does.\r\n\r\nStarting the service:\r\n```\r\n[INFO    ] Executing command 'svc -u /service/trafficsac' in directory '/root'\r\nlocal:\r\n    True\r\n```\r\n\r\nStopping the service:\r\n```\r\n[root@saltqa /]# salt-call daemontools.stop trafficsac\r\n[INFO    ] Executing command 'svc -d /service/trafficsac' in directory '/root'\r\nlocal:\r\n    True\r\n```\r\n\r\nManaging the service with the service state:\r\n```\r\n[root@saltqa /]# salt-call state.sls trafficserver.collector\r\n...\r\n----------\r\n    State: - service\r\n    Name:      trafficsac\r\n    Function:  running\r\n        Result:    False\r\n        Comment:   The named service trafficsac is not available\r\n        Changes: \r\n...\r\n```\r\n\r\nThe relevant stanza in the SLS file is:\r\n```\r\ntrafficsac:\r\n  service.running:\r\n    - provider: daemontools\r\n```\r\n\r\nFrom the documentation and issue #4397, my understanding is that this syntax should work.\r\n\r\nI'm running Salt 0.16.4 from EPEL:\r\n```\r\n[root@saltqa /]# salt --versions\r\n           Salt: 0.16.4\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 06:42:56)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n```"
7360,'thatch45','salt-ssh 0.17rc1 only outputs motd with salt functions\nIn the documentation of salt-ssh it says:\r\n  The standard salt functions are available! \r\n  The output is the same as salt and many of     the same flags are available.\r\n\r\ncmd:\r\n```\r\nsalt-ssh \'*\' test.version \r\nsalt.example.com\r\n    The operating system is Archlinux\r\n    The domain is example.com\r\n\r\n```\r\nand another:\r\n```\r\nsalt-ssh \'*\' test.ping\r\nsalt.example.com\r\n    The operating system is Archlinux\r\n    The domain is example.com\r\n\r\n```\r\nThat\'s my MOTD if this machine....\r\n\r\nbut the raw shell is working\r\n```\r\nsalt-ssh \'*\' -r \'echo "salt is cool"\'\r\nsalt.example.com:\r\n    salt is cool\r\n    \r\n````'
7357,'thatch45','salt-ssh: Values instance has no attribute \'list\'\nI have a single minion in my roster: minion1.\r\n\r\nWhen I run:\r\n\r\n```bash\r\n$ salt-ssh minion1 test.ping\r\n```\r\nI get this traceback:\r\n\r\n```pytb\r\nError while processing <unbound method SaltSSHOptionParser._mixin_after_parsed>: Traceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/dist-packages/salt/utils/parsers.py", line 156, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/usr/local/lib/python2.7/dist-packages/salt/utils/parsers.py", line 1547, in _mixin_after_parsed\r\n    if self.options.list:\r\nAttributeError: Values instance has no attribute \'list\'\r\n```\r\nTested against [latest develop - commit 606d](https://github.com/saltstack/salt/tree/606d).'
7351,'terminalmage','sysctl.conf changes in Archlinux\nJust a heads up, I think the sysctl module is going to need to be updated for this:\r\n\r\nhttps://www.archlinux.org/news/deprecation-of-etcsysctlconf/'
7328,'thatch45',"ignored arguments in saltmod\n```\r\nsalt/states/saltmod.py:50: [W0613(unused-argument), state] Unused argument 'allow_fail'\r\nsalt/states/saltmod.py:148: [W0613(unused-argument), function] Unused argument 'arg'\r\n```\r\n\r\nAlso, `allow_fail` isn't documented."
7306,'terminalmage','Specifying architecture in a state causes failure\nSalt 0.16.3 on CentOS 6 (package from EPEL), following state fails:\r\n\r\n```\r\njava_packages:\r\n  pkg.installed:\r\n    - pkgs:\r\n      - java-1.7.0-openjdk.x86_64\r\n      - java-1.7.0-openjdk-devel.x86_64\r\n```\r\n\r\nWhere as the state works as expected when defined as:\r\n\r\n\r\n```\r\njava_packages:\r\n  pkg.installed:\r\n    - pkgs:\r\n      - java-1.7.0-openjdk\r\n      - java-1.7.0-openjdk-devel\r\n```'
7294,'s0undt3ch',"PEP 263 file encodings\nI've experienced when neglecting to specify UTF-8 file encodings have resulted in problems when a non-ASCII char sneeks in through big patches. All the sudden encoding havoc breaks loose.\r\n\r\nI propose to add a PEP 263 [0] compliant encoding to all Python files in the repository. And an entry in the Salt Coding Style document.\r\n\r\n [0] http://www.python.org/dev/peps/pep-0263/\r\n\r\nThoughts on this? I can send a PR if there is consensus."
7279,'techhat',"nova.{list_,show}() ignore profile\nLooks like some arguments weren't passed along in #7272.\r\n```\r\n************* Module salt.modules.nova\r\nsalt/modules/nova.py:355: [W0613(unused-argument), list_] Unused argument 'profile'\r\nsalt/modules/nova.py:384: [W0613(unused-argument), show] Unused argument 'profile'\r\n```"
7215,'terminalmage','modules.pkgng.latest_version seems to have incorrect signature\nAll the other `latest_version()` implementations accept `*names`, whereas `pkgng.latest_version()` only accepts `pkg_name`.'
7213,'terminalmage',"freebsdservice.available() should not take **kwargs\n[According to my research](https://docs.google.com/spreadsheet/ccc?key=0AtlcnvZE4HSKdDMzZGRxWEdVMzBkSVFvRWZwOWM4akE&usp=sharing), the `available()` function is defined by only 3 service modules, with a slightly inconsistent signature:\r\n* `systemd`, as: `def available(name):`\r\n* `rh_service`, as : `def available(name, limit=''):`\r\n* `freebsdservice`: `def available(name, **kwargs):`\r\n\r\n`freebsdservice.available()` ignores its `**kwargs`. I propose deprecating/removing this instance of `**kwargs`."
7206,'terminalmage',"some_service_module.reload() should not take **kwargs\nThe `reload()` functions in `netbsdservice` & `rh_service` accept `**kwargs`, *but ignore them*.\r\n[According to my research](https://docs.google.com/spreadsheet/ccc?key=0AtlcnvZE4HSKdDMzZGRxWEdVMzBkSVFvRWZwOWM4akE&usp=sharing) (and feel free to double-check me), no other service module accepts any additional args besides the required `name`, so accepting `**kwargs` isn't necessary for interface compliance.\r\n\r\nSince they are ignored and aren't needed for API compatibility, do these `**kwargs` parameters need to go through a deprecation process, or can they be removed immediately?"
7156,'terminalmage',"git module lacks --global flag support\nThe git module has support for the `git config` command, but not for the `--global` flag to said command, meaning you can't set server-wide git config options with Salt Stack at the moment without resorting to running a\r\n\r\n    cmd.run 'git config --global user.email foo@bar.com'\r\n\r\nfor example."
7144,'techhat','Feature Request: sqlite3 returner\nI did not see a sqlite3 returner in the code base. Although it is recommended to not use  sqlite3 for medium to large scale deployments, it can be useful for small deployments and testing/prototyping efforts.'
7030,'UtahDave','[salt-minion] [Windows] MSI installer\nHi,\r\n\r\nOn windows 2003 server, if salt-minion isn\'t returning all installed softwares, you need to install a windows component: "wmi windows installer provider".\r\n\r\nThis is not a bug but perhaps it could be written in the doc.\r\n\r\nRegards,\r\nPierre'
7024,'terminalmage','Salt FAQ\nHaving been hit by a couple of "obvious" gotchas in the last few days, I wonder if it is worth adding a FAQ or troubleshooting page to the docs.\r\n\r\nInitial things to add would be:\r\n\r\n- minion name guessing and /etc/hosts\r\n- cmd.wait vs cmd.run\r\n- windows package names and versions\r\n\r\nmaybe there is one, but if there is, I couldn\'t find it.'
7003,'s0undt3ch','salt integration test opensuse fails on test_user_present_git_from_default\nOn jenkins.saltstack.com the opensuse test fails with this error:\r\n```\r\n======================================================================\r\nFAIL: test_user_present_gid_from_name_default (integration.states.user.UserTest)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File "/testing/tests/saltunittest.py", line 54, in wrap\r\n    return func(cls)\r\n  File "/testing/tests/integration/states/user.py", line 80, in test_user_present_gid_from_name_default\r\n    self.assertEqual(group_name, \'salt_test\')\r\nAssertionError: \'users\' != \'salt_test\'\r\n```\r\nThis is because on a vanilla opensuse system a default user will get gid \'users\' not the gid from it\'s name.\r\n\r\nIn the following file it says:\r\n```python\r\ndef test_user_present_gid_from_name_default(self):\r\n        \'\'\'\r\n        This is a DESTRUCTIVE TEST. It creates a new user on the on the minion.\r\n        This is an integration test. Not all systems will automatically create\r\n        a group of the same name as the user, but I don\'t have access to any.\r\n        If you run the test and it fails, please fix the code it\'s testing to\r\n        work on your operating system.\r\n        \'\'\'\r\n        ret = self.run_state(\'user.present\', name=\'salt_test\',\r\n                             gid_from_name=True, home=\'/var/lib/salt_test\')\r\n        self.assertSaltTrueReturn(ret)\r\n\r\n        ret = self.run_function(\'user.info\', [\'salt_test\'])\r\n        self.assertReturnNonEmptySaltType(ret)\r\n        group_name = grp.getgrgid(ret[\'gid\']).gr_name\r\n\r\n        self.assertTrue(os.path.isdir(\'/var/lib/salt_test\'))\r\n        self.assertEqual(group_name, \'salt_test\')\r\n\r\n        ret = self.run_state(\'user.absent\', name=\'salt_test\')\r\n        self.assertSaltTrueReturn(ret)\r\n```\r\nOpensuse is such a system but I don\'t know how to fix this code...'
7001,'cachedout','salt-call does not detect failure to contact master\nIf I run `salt-call` while the master is not running, `salt-call` assembles all grains and then stops with the debug message\r\n\r\n    [DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n\r\nand after one minute, says\r\n\r\n    Minion failed to authenticate with the master, has the minion key been accepted\r\n\r\nwhen in fact it should be able to determine that the connection to the master socket has been refused, and bail out much earlier.'
6994,'terminalmage',"augeas state returns invalid change data\nThe augeas state returns invalid change data. This causes problems when you watch the state. In the below example postfix is restarted every time it is run, even when no changes are made to main.cf as confirmed by file timestamps. A warning is also displayed.\r\n\r\n```yaml\r\npostfix:\r\n  pkg.installed:\r\n    - name: postfix\r\n  service.running:\r\n    - enable: True\r\n    - reload: True\r\n    - watch:\r\n      - pkg: postfix\r\n      - augeas: /etc/postfix/main.cf\r\n\r\n/etc/postfix/main.cf:\r\n  augeas.setvalue:\r\n    - prefix: /files/etc/postfix/main.cf\r\n    - changes:\r\n      - myhostname {{ grains['fqdn'] }}\r\n```\r\n\r\nMessage displayed when running the above sls:\r\n\r\n```\r\n----------\r\n    State: - augeas\r\n    Name:      /etc/postfix/main.cf\r\n    Function:  setvalue\r\n        Result:    True\r\n        Comment:   Success\r\n        Changes:   Invalid Changes data: ['myhostname hostnamehere']\r\n----------\r\n    State: - pkg\r\n    Name:      postfix\r\n    Function:  installed\r\n        Result:    True\r\n        Comment:   Package postfix is already installed\r\n        Changes:\r\n----------\r\n    State: - service\r\n    Name:      postfix\r\n    Function:  running\r\n        Result:    True\r\n        Comment:   Service restarted\r\n        Changes:   postfix: True\r\n```"
6961,'s0undt3ch','User as oposed to runas on all states\nNeed to deprecate **all** usage of `runas` on all state modules, properly deprecating it and still maintaining backwards compatibility until 0.18.x'
6954,'terminalmage','[FreeBSD] pkg installation in states broken: "ValueError: too many values to unpack"\nsalt 0.16.0-2695-g94133c1\r\nFreeBSD 9.1-RELEASE-p6\r\n\r\n```python\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      vim-lite\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/site-packages/salt/state.py", line 1252, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/local/lib/python2.7/site-packages/salt/states/pkg.py", line 546, in latest\r\n    **kwargs)\r\n  File "/usr/local/lib/python2.7/site-packages/salt/modules/freebsdpkg.py", line 88, in latest_version\r\n    _, pkg, _, _, ver = line.split()\r\nValueError: too many values to unpack\r\n```'
6945,'terminalmage',"GPU not found for Radeon HD 3450\nsalt-call grains.items is returning no GPUs on a system with a Radeon HD 3450 installed. I am currently on salt 0.16.3 running SL 6.4 x86_64.\r\n\r\n```\r\n# lspci -vmm\r\nSlot:   01:00.0\r\nClass:  VGA compatible controller\r\nVendor: Advanced Micro Devices [AMD] nee ATI\r\nDevice: RV620 LE [Radeon HD 3450]\r\nSVendor:        Dell\r\nSDevice:        OptiPlex 980\r\n```\r\n```\r\n# salt-call grains.items\r\n...\r\ngpus:                               <-- I get an empty list.\r\n...\r\nnum_gpus:\r\n    0\r\n```\r\n\r\nExpected output should be:\r\n\r\n```\r\ngpus:\r\n    ----------\r\n    - model:\r\n        RV620 LE [Radeon HD 3450]\r\n    - vendor:\r\n        ati\r\n...\r\nnum_gpus:\r\n    1\r\n```\r\n\r\nI took a quick glance at the code, but didn't see anything obvious that was preventing it from parsing it properly..."
6932,'UtahDave','[salt-minion] [Windows] invalid grain osrelease for Windows server 2012\nHi,\r\n\r\nUnder Windows server 2012, the grain osrelease is reported as "post2008Server" instead of 2012server\r\n\r\nnot a shame but, to be perfect ;)'
6929,'terminalmage',"mine.send and network.ip_addrs\nHere is the problem: i have one minion with eth0 interface, and one minion with eth1 interface. I'm trying to retrieve ip addrs of both.\r\n```\r\nsalt -G 'roles:webnode' mine.send network.ip_addrs\r\n```\r\n\r\nThere is 'all True' output. But in minion's log i see:\r\n```\r\n2013-08-28 13:50:53,675 [salt.utils.network                          ][ERROR   ] Interface False not found.\r\n```\r\n\r\nIf i call mine.send with specified interface, it works.\r\nIt looks like mine.send's arguments lookup is broken (instead of interface=False it should be interface=None). \r\n\r\n```\r\nsalt-(minion|master) --versions-report\r\n           Salt: 0.16.3\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n```"
6928,'cachedout',"Salt master stating every single file in files root and follows symlinks\nIt seems my salt master is stating every single file in my files root even when nothing is referencing them. It does this for every highstate and every also for sync_all. It also follows symlinks, going into recursive hell. I can't run a highstate on more than a few nodes at a time without consuming all CPU on the master and even simple calls take 10-15 minutes."
6920,'s0undt3ch',"Remove user account is wrongly failing on debian based distros\nWhile debugging why removing a user account was failing I saw on the logs:\r\n\r\n```\r\nCommand 'userdel -r -f issue-6912' failed with return code: 12\r\n```\r\n\r\nThis is a bug on debian based distros(apparently), which is fixed upstream, [see here](https://bugs.launchpad.net/ubuntu/+source/shadow/+bug/1023509).\r\n\r\nShould we treat 12 as a good exit code?\r\nOnly for debian based distributions?"
6917,'whiteinge','bad link in docs\nIt may be a temporary/transient thing, but the link to the cmd state goes to the python docs rather than the salt stack docs at:\r\n\r\nhttps://salt.readthedocs.org/en/latest/ref/states/all/index.html\r\n\r\n'
6912,'s0undt3ch',"pip runas creates files as root\nAlthough it's being deprecated, the pip state's `runas` option is still creating files as root.\r\n\r\nhttps://gist.github.com/Caustic/6347483\r\n\r\n    $ salt-call --version\r\n    salt-call 0.16.3\r\n\r\nFiles used:\r\nhttps://gist.github.com/Caustic/6347551"
6908,'thatch45',"Minions with multiple masters auth with masters in serial\nIf a new unauthenticated minion starts up, it will ping the masters in serial to auth, like:\r\n\r\n```\r\nStarting the salt minion\r\nWaiting for key to be accepted on master-1...\r\n```\r\n\r\nSo I have to accept the key on `master-1` before I can on `master-2`, if for whatever reason I can not access `master-1` then I can't accept the minion on `master-2`. As am I using EC2 for auto starting the minions with a `cloud-init` script, I don't have access to the minion at this point to edit the `minion.conf` to remove the `master-1` and also I think needing to do that would be somewhat annoying. \r\n\r\nI really have `master-1` as redundancy so don't use it on a day to day basis, so would be good if `salt-minion` were able to continue just fine with only one master accepting the key.\r\n\r\nMight be worth mentioning: the first master it tried to auth with was actually the _second_ in masters link in my `minion.conf`"
6906,'s0undt3ch','AttributeError is raised when installing from develop\nInstalling from develop (SHA: 45f816c0829061073be58208e89af09d5e9e407d) causes the following stacktrace:\r\n\r\n```\r\nroot@sgtpepper:/usr/local/src/salt# python setup.py install\r\n0.16.0-2614-g45f816c\r\nrunning install\r\nrunning build\r\nrunning build_py\r\nrunning build_scripts\r\nTraceback (most recent call last):\r\n  File "setup.py", line 408, in <module>\r\n    setup(**SETUP_KWARGS)\r\n  File "/usr/lib/python2.7/distutils/core.py", line 152, in setup\r\n    dist.run_commands()\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 953, in run_commands\r\n    self.run_command(cmd)\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 972, in run_command\r\n    cmd_obj.run()\r\n  File "setup.py", line 248, in run\r\n    install.run(self)\r\n  File "/usr/lib/python2.7/distutils/command/install.py", line 601, in run\r\n    self.run_command(\'build\')\r\n  File "/usr/lib/python2.7/distutils/cmd.py", line 326, in run_command\r\n    self.distribution.run_command(command)\r\n  File "/usr/lib/python2.7/distutils/dist.py", line 972, in run_command\r\n    cmd_obj.run()\r\n  File "setup.py", line 180, in run\r\n    root_dir=self.salt_root_dir,\r\n  File "/usr/lib/python2.7/distutils/cmd.py", line 105, in __getattr__\r\n    raise AttributeError, attr\r\nAttributeError: salt_root_dir\r\n\r\n```'
6904,'basepi','Merge network require decorator to 0.16\n'
6894,'UtahDave',"Can't list or install packages on Windows minions\nThis is probably related to <https://github.com/saltstack/salt/issues/4630>, as I'm experiencing the same problem.\r\n\r\nI can't list available package and I can't install anything on Windows x64 minions using salt 0.16.3 on both minions and master (CentOS).\r\n\r\nI run the following commands on the master:\r\n\r\n    [root]# salt-run winrepo.genrepo => lists a lot of packages\r\n    [root]# salt '*' pkg.refresh_db => True\r\n    [root]# salt '*' pkg.available_version firefox -v -t 200\r\n    Executing job with jid 20130826102307928029\r\n    -------------------------------------------\r\n\r\n    myhost:\r\n \r\n    [root]# \r\n\r\nOnly empty response.\r\n\r\nNot sure that this is a bug, or if I'm doing something wrong here."
6893,'UtahDave',' I found a bug\nhello\r\n    In windows2003 server x86 installed on salt-minion (0.16.0), but the state of access to services, and found always error: Minion did not return\r\nI found from the log, sc showsid / start / stop can not run\r\n    My English is poor, please forgive me\r\n    thx'
6854,'thatch45','While handling `iorder` on states, skip non string entries.\n@kjkuan this fixes the issue which I emailed you about, ie, the failing pydsl tests. Can you confirm this is the best approach to the problem please?'
6843,'UtahDave',"Windows user.present doesn't set Full Name on first run\n"
6830,'UtahDave',"Setting groups in user.present on Windows requires two executions\nRight now if you're setting the group in user.present on Windows, it successfully creates the user and returns success, but the group option doesn't actually get set until a second execution of the state."
6824,'UtahDave','When using user.present state, setting the password fails\nWhen using the user.present state, if you set a password, the password creation fails'
6817,'basepi','file.contains data type error\nseems forget to change the input data type from int to string.\r\n\r\n\r\nroot@TestServer-0:/srv/salt/files# salt \'m0\' file.contains /etc/c1.conf "a"\r\nm0:\r\n    True\r\n\r\nroot@TestServer-0:/srv/salt/files# salt \'m0\' file.contains /etc/c1.conf "1"\r\nm0:\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/pymodules/python2.7/salt/minion.py", line 635, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "/usr/lib/pymodules/python2.7/salt/modules/file.py", line 790, in contains\r\n        stripped_text = text.strip()\r\n    AttributeError: \'int\' object has no attribute \'strip\'\r\n    \r\nroot@TestServer-0:/srv/salt/files# \r\n\r\n\r\n\r\nm0 is the master itself\r\n\r\nroot@TestServer-0:/srv/salt/files# salt --versions-report\r\n           Salt: 0.16.2\r\n         Python: 2.7.3 (default, Apr 10 2013, 05:09:49)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\nroot@TestServer-0:/srv/salt/files# \r\nroot@TestServer-0:/srv/salt/files# salt \'m0\' test.versions_report\r\nm0:\r\n               Salt: 0.16.2\r\n             Python: 2.7.3 (default, Apr 10 2013, 05:09:49)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.0.0\r\n                ZMQ: 3.2.2\r\nroot@TestServer-0:/srv/salt/files# \r\n\r\n'
6800,'terminalmage','git.latest "unless" does not work \n    An exception occurred in this state: Traceback (most recent call last):\r\n      File "/usr/lib/python2.6/site-packages/salt/state.py", line 1236, in call\r\n        ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n      File "/usr/lib/python2.6/site-packages/salt/states/git.py", line 99, in latest\r\n        run_check_cmd_kwargs, onlyif, unless\r\n      File "/usr/lib/python2.6/site-packages/salt/states/git.py", line 336, in _run_check\r\n        if __salt__[\'cmd.retcode\'](unless, **cmd_kwargs) == 0:\r\n      File "/usr/lib/python2.6/site-packages/salt/modules/cmdmod.py", line 708, in retcode\r\n        timeout=timeout)[\'retcode\']\r\n      File "/usr/lib/python2.6/site-packages/salt/modules/cmdmod.py", line 323, in _run\r\n        proc = salt.utils.timed_subprocess.TimedProc(cmd, **kwargs)\r\n      File "/usr/lib/python2.6/site-packages/salt/utils/timed_subprocess.py", line 20, in __init__\r\n        self.process = subprocess.Popen(args, stdin=subprocess.PIPE, **kwargs)\r\n      File "/usr/lib64/python2.6/subprocess.py", line 642, in __init__\r\n        errread, errwrite)\r\n      File "/usr/lib64/python2.6/subprocess.py", line 1234, in _execute_child\r\n        raise child_exception\r\n    OSError: [Errno 2] No such file or directory: \'/srv/cms/instances/openair\'\r\n\r\nThis is because the command is run with the target directory as the cwd, which does not exist yet.'
6789,'terminalmage',"pkg.installed using sources syntax - differences due to underlying OS/pkg manager\nFor a CentOS package, where the pkg arch is i586 and host arch is x86_64, a state currently has to look like this:\r\n```bash\r\ninstall-java:\r\n  pkg.installed:\r\n    - sources:\r\n      - jre.i586: salt://jre-version-i586.rpm\r\n```\r\nFor an Ubuntu package, where the pkg arch is i386 and the host arch is x86_64, a state currently has to look like this:\r\n```bash\r\ninstall-av:\r\n  pkg.installed:\r\n    - sources:\r\n      - kaspersky:i386: salt://kaspersky_version_i386.deb\r\n```\r\nNotice the CentOS state has a '.' period between the pkg and arch, while the Ubuntu state has a ':' colon there.\r\n\r\nIs there a way to make the syntax common for all OS's that use states like these?\r\n\r\nI ran this using today's develop branch.\r\n\r\nThanks!"
6777,'terminalmage','for pip state and module, add "--use-wheel" option\npip version 1.4(.1) is out and is finally supporting long-awaited wheel format (not to be confused with Salt wheel).\r\nSome c-based packages, like lxml, can be now installed in fraction of second.\r\n\r\ncurrent documentation for 0.16.3 does not show that an option "--use-wheel" is supported.\r\n\r\nSuch a support would help a lot. Larger python applications could be installed from wheel directory within seconds, regardless if it contains packages, requiring compilation.'
6764,'cachedout','Global readable highstate.cache\nHi together,\r\n\r\n-rw-r--r-- 1 root root 1,1K Aug 17 17:21 highstate.cache.p\r\n\r\nin /var/cache/salt/minion is global readable. The problem is if there are any security related informations (e.g. password from a pillar) in a state file, any user on the system can see this.\r\n\r\nMaybe the complete /var/cache/salt/minion directory could be 700 by default. Are there any problems with this rights? \r\n\r\ncheers,\r\n\r\nSebastian'
6756,'terminalmage','Debconfmod fails to read data\nI\'m trying to use debconfmod state in a fairly standard way:\r\n\r\n```\r\nmariadb-server:\r\n  debconf.set:\r\n    - name: mariadb-server-10.0\r\n    - data:\r\n      \'mysql-server/root_password\': {\'type\': \'password\', \'value\': \'...\'}\r\n      \'mysql-server/root_password_again\': {\'type\': \'password\', \'value\': \'...\'}\r\n```\r\n\r\nBut unfortunately this doesn\'t work in 0.16.3. It looks like `data` isn\'t passed to the state:\r\n\r\n```\r\nAn exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1237, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.7/dist-packages/salt/states/debconfmod.py", line 125, in set\r\n    for (key, args) in data.iteritems():\r\nAttributeError: \'NoneType\' object has no attribute \'iteritems\'\r\n```\r\n\r\nIf I add `**kwargs` to this function signature, then all of the data\'s elements can be found in kwargs itself.\r\n\r\n```\r\nversions:\r\n           Salt: 0.16.3\r\n         Python: 2.7.4 (default, Apr 19 2013, 18:28:01)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.2.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
6751,'terminalmage','PDF doc file at http://docs.saltstack.com/ is broken \nAfter downloading today https://media.readthedocs.org/pdf/salt/latest/salt.pdf and trying to open it with poppler-based linux viewer, I get error "The document contains no pages". I tried to download several times. File header and tailer look like PDF.\r\n\r\n    $ ls -l salt.pdf \r\n    -rw-rw-r-- 1 pfalcon pfalcon 3045766 Aug 16 22:32 salt.pdf\r\n    $ md5sum salt.pdf \r\n    80bf509ee25deedb44bd6024609ef598  salt.pdf\r\n'
6746,'herlo',"RHEL/CentOS package for salt-master still runs as root by default\nIn response to issue \\#5249, I would like to note that the CentOS/RHEL package that's currently available in the [EPEL repo](https://fedoraproject.org/wiki/EPEL) is still configured to run salt-master as root.\r\n\r\nThe only fixes necessary to the spec file should be to add a system user to the system credential databases, then change the `/etc/salt/master` file to use that new user via: `user: salt`\r\n\r\nI didn't want this packaging bug to be forgotten.\r\n\r\n_sidenote_: as of this writing, it's also 3 point revs behind stable."
6737,'s0undt3ch','Allow network related tests to be skipped if no network is available\n'
6733,'terminalmage','FTP authentication is broken since 0.16.1\nBefore the commit 3e6833ff0517259d939090a8a906295c539cacab Salt was able to get files like \r\n\r\n    salt --local --log-level=debug cp.get_url ftp://name:password@example.com/example.tar.gz /tmp/example.tar.gz\r\n\r\nNow it breaks with \r\n\r\n    salt.exceptions.MinionError: Error reading ftp://name:password@example.com/example.tar.gz: ftp error: 530 Anonymous access not allowed\r\n\r\nIt is because FTPHandler need the full url, ftp://name:password@example.com/example.tar.gz, not the stripped one (see fixed_url in fileclient.py). See also #5641.\r\n\r\nPlease fix it.'
6724,'thatch45',"salt-minion not tending to it's children\nI was trying to refine and test the https://github.com/saltstack-formulas/salt-formula repo and keep running into this issue.\r\n\r\nIn my setup I have a Debian 7 box setup as my test master, with just the salt-formula repo via gitfs_remotes for state file definitions.\r\n\r\nIn this test I have the following servers launched via salt-cloud:\r\ncentos-kw-test: CentOS 6.3\r\ndebian-kw-test: Debian 7 (Wheezy)\r\nfedora-kw-test: Fedora 18 (Spherical Cow)\r\nubuntu-kw-test: Ubuntu 12.04 LTS\r\ngentoo-kw-test: Gentoo 13.3\r\n\r\nWhen I tried to run `salt \\*test state.sls salt.master` the processes seem to have all completed correctly, but only the ubuntu and fedora boxes returned, the other left defunct children as seen at https://gist.github.com/KennethWilke/6242422\r\n\r\nI was able to fix this by killing the parent salt-minion processes and restarting them, re-running the state command returned sucessfully the second time. After this I tested `salt \\*test service.restart salt-master` and the issue recurred, only the fedora and ubuntu boxes returned successfully and the centos, debian and gentoo boxes have the defunct pids under salt-minion procs."
6714,'s0undt3ch','complete config ignore upon parse error\ni made a mistake in /etc/salt/minion by forgetting a blank after the colon of:\r\n<pre>\r\nappend_domain:example.com\r\n</pre>\r\n\r\nsalt was not connecting to master.\r\nthe message in /var/log/minion was completely unhelpful:\r\n<pre>\r\n2013-08-15 10:41:18,066 [salt.utils       ][ERROR   ] This master address: \'salt\' was previously resolvable but now fails to resolve! The previously resolved ip addr will continue to be used\r\n2013-08-15 10:41:18,066 [salt.minion      ][WARNING ] Master hostname: salt not found. Retrying in 30 seconds\r\n</pre>\r\nthis indiates that the config is ignored completely, as salt is the default master (but not in my setup).\r\n\r\nrunning:\r\n<pre>\r\nappstack1 cache # salt-call test.ping\r\nError parsing configuration file: /etc/salt/minion - while scanning a simple key\r\n  in "<string>", line 58, column 1:\r\n    append_domain:example.com\r\n    ^\r\ncould not found expected \':\'\r\n  in "<string>", line 60, column 1:\r\n    # Custom static grains for this  ... \r\n    ^\r\n[WARNING ] Error parsing configuration file: /etc/salt/minion - while scanning a simple key\r\n  in "<string>", line 58, column 1:\r\n    append_domain:example.com\r\n    ^\r\ncould not found expected \':\'\r\n  in "<string>", line 60, column 1:\r\n    # Custom static grains for this  ... \r\n<pre>\r\n\r\nthis message actually was helpful and should go into the log.\r\nsalt-minion-0.16.3'
6710,'s0undt3ch','ERROR: test_verify_socket (unit.utils.verify_test.TestVerify) on opensuse build server no Network\nI want to add unit testing to the package building process of suse but sockets verify will fail because the opensuse build server doesn\'t have network access.\r\n\r\n```\r\n----------------------------------------------------------------------\r\n[   58s] Traceback (most recent call last):\r\n[   58s]   File "/home/abuild/rpmbuild/BUILD/salt-0.16.3/tests/unit/utils/verify_test.py", line 78, in test_verify_socket\r\n[   58s]     self.assertTrue(verify_socket(\'\', 18000, 18001))\r\n[   58s]   File "/home/abuild/rpmbuild/BUILD/salt-0.16.3/salt/utils/verify.py", line 108, in verify_socket\r\n[   58s]     addr_family = lookup_family(interface)\r\n[   58s]   File "/home/abuild/rpmbuild/BUILD/salt-0.16.3/salt/utils/verify.py", line 95, in lookup_family\r\n[   58s]     socket.SOCK_STREAM)\r\n[   58s] error: [Errno 111] Connection refused\r\n[   58s] \r\n[   58s] ----------------------------------------------------------------------\r\n[   58s] Ran 121 tests in 5.459s\r\n[   58s] \r\n[   58s] FAILED (errors=1, skipped=1)\r\n[   58s] default\r\n```\r\n\r\nCould this test be made to skip with a option -n (--nonetwork) ?\r\n'
6705,'whiteinge','Salt client API docs are missing some parts\nThe client api page (http://docs.saltstack.com/ref/python-api.html) is missing quite a few of the calls that the client supports (such as cmd_async). '
6697,'UtahDave','Windows file.managed broken\nSeems to have reappeared on Windows 7 with version 0.16.3 when trying to apply a small test state \'motd\' (C:\\MOTD directory already exists):\r\n\r\nsalt master is a linux box, salt minion is win 7\r\n\r\n```\r\nsalt \'*\' state.highstate test=True\r\nmachine-name-redacted:\r\n    State: - file\r\n    Name:      c:\\motd\\motd\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1237, in call\r\n  File "salt/states/file.py", line 828, in managed\r\n  File "salt/modules/file.py", line 1397, in check_managed\r\n  File "salt/modules/file.py", line 1187, in get_managed\r\nNameError: global name \'__salt__\' is not defined\r\n```'
6690,'terminalmage',"match.grain does not handle lists and dicts\nIf I have the following grains file.\r\n\r\n```\r\nrole:\r\n  - deployment\r\n  - web\r\n```\r\n\r\nand do a `salt \\* match.grain 'role:web'` it will never match. It only seems to successfully match against simple `key:value` pairs."
6670,'terminalmage','Error: grains.append is not available.\nHello, \r\nwhen i execute command:\r\n salt \'test\' grains.append roles db\r\n i receive error\r\n"grains.append" is not available.\r\n\r\nMy OS: Ubuntu 12.04.2 LTS\r\n # salt-master --version\r\nsalt-master 0.16.3\r\n # salt-minion --version\r\nsalt-minion 0.16.3\r\n\r\nfile /etc/salt/grains :\r\n```\r\nroles: \r\n  - test\r\n  - web\r\n```\r\n\r\nCommand \'salt \'test\' grains.item roles\' return:\r\n```\r\ntest:\r\n  roles:\r\n    - test\r\n    - web\r\n```\r\n\r\nWhat a error is in the salt or system configuration? '
6666,'UtahDave','Service status stalls on Windows Server 2003\nWhen running a state which checks that a service is running, salt calls `sc showsid <service>` (from `getsid()` in `salt/modules/win_service.py`). For example:\r\n```\r\nC:\\salt>salt-call service.status nxlog\r\n[INFO    ] Executing command \'sc query "nxlog"\' in directory \'C:\\\\Documents and Settings\\\\djs-admin\'\r\n[INFO    ] Executing command \'sc showsid "nxlog"\' in directory \'C:\\\\Documents and Settings\\\\djs-admin\'\r\n```\r\nThis stalls. Pressing enter continues the execution. Running `sc showsid nxlog` at the command line produces the following:\r\n```\r\nC:\\>sc showsid nxlog\r\n\r\nERROR:  Unrecognized command\r\n\r\nDESCRIPTION:\r\n        SC is a command line program used for communicating with the\r\n        Service Control Manager and services.\r\nUSAGE:\r\n        sc <server> [command] [service name] <option1> <option2>...\r\n\r\n\r\n        The option <server> has the form "\\\\ServerName"\r\n        Further help on commands can be obtained by typing: "sc [command]"\r\n        Commands:\r\n          query-----------Queries the status for a service, or\r\n                          enumerates the status for types of services.\r\n          queryex---------Queries the extended status for a service, or\r\n                          enumerates the status for types of services.\r\n          start-----------Starts a service.\r\n          pause-----------Sends a PAUSE control request to a service.\r\n          interrogate-----Sends an INTERROGATE control request to a service.\r\n          continue--------Sends a CONTINUE control request to a service.\r\n          stop------------Sends a STOP request to a service.\r\n          config----------Changes the configuration of a service (persistent).\r\n          description-----Changes the description of a service.\r\n          failure---------Changes the actions taken by a service upon failure.\r\n          qc--------------Queries the configuration information for a service.\r\n          qdescription----Queries the description for a service.\r\n          qfailure--------Queries the actions taken by a service upon failure.\r\n          delete----------Deletes a service (from the registry).\r\n          create----------Creates a service. (adds it to the registry).\r\n          control---------Sends a control to a service.\r\n          sdshow----------Displays a service\'s security descriptor.\r\n          sdset-----------Sets a service\'s security descriptor.\r\n          GetDisplayName--Gets the DisplayName for a service.\r\n          GetKeyName------Gets the ServiceKeyName for a service.\r\n          EnumDepend------Enumerates Service Dependencies.\r\n\r\n        The following commands don\'t require a service name:\r\n        sc <server> <command> <option>\r\n          boot------------(ok | bad) Indicates whether the last boot should\r\n                          be saved as the last-known-good boot configuration\r\n          Lock------------Locks the Service Database\r\n          QueryLock-------Queries the LockStatus for the SCManager Database\r\nEXAMPLE:\r\n        sc start MyService\r\n\r\nWould you like to see help for the QUERY and QUERYEX commands? [ y | n ]:\r\n```\r\nThe SID doesn\'t seem to be used for anything. I guess the call should just not be made on Windows Server 2003. \r\n'
6661,'basepi','bug in hg state deletes directories\nin _handle_existing, the code checks to see if the target directory is empty [1]. This check is faulty and will cause salt to delete (shutil.rmtree) anything in the target directory.\r\n\r\n[1] https://github.com/saltstack/salt/blob/develop/salt/states/hg.py#L137'
6656,'terminalmage',"grains['ipv4'] only returning two ip addresses\nI have several hosts setup with multiple IP addresses and as of the update to 0.16.2 I have noticed that grains['ipv4'] is only returning two ip addresses."
6653,'terminalmage','Issue with salt.states.user on OSX 10.8.4 (need user module for OSX)\nTrying to make an account using salt.states.user, but we get the following Python error:\r\n\r\n```\r\n----------\r\n    State: - user\r\n    Name:      fred\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/Library/Python/2.7/site-packages/salt/state.py", line 1239, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "/Library/Python/2.7/site-packages/salt/states/user.py", line 238, in present\r\n    missing_groups = [x for x in groups if not __salt__[\'group.info\'](x)]\r\nKeyError: \'group.info\'\r\n\r\n        Changes:\r\n```\r\nI just copied the example from http://docs.saltstack.com/ref/states/all/salt.states.user.html and got rid of the groups that aren\'t present on my system. Here\'s my SLS:\r\n\r\n```\r\nfred:\r\n  user.present:\r\n    - fullname: Fred Jones\r\n    - shell: /bin/zsh\r\n    - home: /home/fred\r\n    - uid: 4000\r\n    - gid: 4000\r\n    - groups:\r\n      - wheel\r\n```\r\nConfirmed wheel exists:\r\n```\r\n$ grep wheel /etc/group\r\nwheel:*:0:root\r\n```'
6646,'UtahDave','Missing requisites produce exceptions\nThis is using the 0.16.3 package on Windows, with a 0.16.2 server, but I don\'t think it makes a difference.\r\n\r\nIf a require or watch clause in a state has a spelling error, the state run on the minion produces an exception:\r\n```\r\nTraceback (most recent call last):\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chainload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-call__.py", line 14, in <module>\r\n  File "salt/scripts.py", line 76, in salt_call\r\n  File "salt/cli/__init__.py", line 265, in run\r\n  File "salt/cli/caller.py", line 132, in run\r\n  File "salt/cli/caller.py", line 73, in call\r\n  File "salt/modules/state.py", line 288, in sls\r\n  File "salt/state.py", line 1532, in call_high\r\n  File "salt/state.py", line 1275, in call_chunks\r\n  File "salt/state.py", line 1387, in call_chunk\r\n  File "salt/state.py", line 1319, in check_requisite\r\nTypeError: \'NoneType\' object is not iterable\r\n```'
6645,'terminalmage','On Archlinux, salt is not getting correct hostname\nAccording to official Arch installation notes, a servers hostname should be stored in /etc/hostname (not /etc/hosts).\r\n\r\nThis causes issues when generating new salt keys, and when grains refresh for the fqdn grain.  Salt is pulling out localhost from the /etc/hosts file instead of the actual hostname from /etc/hosts.'
6644,'terminalmage','Get the package name from `egg=<package-name>`. Refs #6643.\n'
6643,'terminalmage',"pip.installed state does not report VCS packages as properly installed\n```\r\nlocal:\r\n----------\r\n    State: - pip                                                                                                                                                                    \r\n    Name:      git+https://github.com/saltstack/salt-testing.git#egg=SaltTesting\r\n    Function:  installed\r\n        Result:    True\r\n        Comment:   There was no error installing package 'git+https://github.com/saltstack/salt-testing.git#egg=SaltTesting' although it does not show when calling 'pip.freeze'.\r\n        Changes:   git+https://github.com/saltstack/salt-testing.git#egg=SaltTesting==???: Installed\r\n```"
6641,'UtahDave','\'user.present\' on Windows Server 2012 don\'t create a new user \nuser.present doesn\'t create user on Windows Server 2012.\r\n\r\nInterestingly enough, I\'ve seen two different ways saltstack reports the issue. I experiment on a VM installation, so can do a fresh saltstack assisted setup, so originally I got on salt-master the error below:\r\n```\r\n    State: - user\r\n    Name:      testuser01\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n    File "salt/state.py", line 1239, in call\r\n    File "salt/states/user.py", line 367, in present\r\n    TypeError: add() got an unexpected keyword argument \'createhome\'\r\n```\r\n\r\nAnd now, after few changes all over the place, on fresh windows installation the salt-master doesn\'t even report the error, the only trace about attempt to create the user I get from salt-minion logs:\r\n```\r\n    ...\r\n    2013-08-12 15:56:06,252 [salt.state       ][ERROR   ] No changes made for far\r\n    2013-08-12 15:56:08,748 [salt.state       ][ERROR   ] No changes made for winscp\r\n    2013-08-12 15:56:08,780 [salt.state       ][ERROR   ] No changes made for testuser01\r\n    ...\r\n```\r\n\r\nwhere the first two entries are not really errors even though reported as ERROR (to my understanding, as those already installed on the box), and the third one states that no changes done to testuser01, and indeed, it wasn\'t created, which is a problem.\r\n\r\nBelow are init.sls with regards to user creating\r\n```\r\n    testuser01:\r\n      user.present:\r\n        - name: testuser01\r\n        - fullname: Test User 01\r\n        - password: ABC123321ABC\r\n        - groups:\r\n          - administrators\r\n```\r\n\r\n```\r\nVersion info:\r\n__c:\\salt\\salt --versions-report\r\n           Salt: 0.16.2\r\n         Python: 2.7.3 (default, Apr 10 2012, 23:24:47) [MSC v.1500 64 bit (AMD64)]\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.12\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.3\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.2__\r\n```'
6637,'UtahDave','Windows minion re-installs packages everytime state.highstate is invoked\nSalt minion on Windows Server 2012 exhibits a bit strange behavior - when ``salt \'*\' state.highstate`` invoked it calls package installation regardless without considering whether it has been installed already or not. And it shows no output on the salt-master, just quietly finishes.\r\n\r\nAlso, may be helpfull in understanding what the issue is - invoking ``salt \'*win*\' pkg.list_pkgs`` results in the output where packages names are not the full name as specified in repo/.../init.sls, but the topmost name of the package, e.g. below\r\n\r\n```\r\n$ cat /srv/salt/os/windows/repo/chrome/init.sls\r\nchrome:\r\n  28.0.1500.95:\r\n    installer: \'salt://os/windows/repo/chrome/ChromeSetup.exe\'\r\n    full_name: Google Chrome\r\n    locale: en_US\r\n    reboot: False\r\n    install_flags: \'\'\r\n    uninstaller: \'C:\\Program Files (x86)\\Google\\Chrome\\Application\\28.0.1500.95\\Installer\\setup.exe\'\r\n    uninstall_flags: \' --uninstall -force-uninstall\'\r\n```\r\n\r\n```\r\n$ salt \'*win*\' pkg.list_pkgs\r\nwin2012.example.com:\r\n    ----------\r\n    7z:\r\n        9.20.00.0\r\n    Google Update Helper:\r\n        1.3.21.153\r\n    Microsoft Visual C++ 2008 Redistributable - x64 9.0.30729.6161:\r\n        9.0.30729.6161\r\n    Microsoft Visual C++ 2008 Redistributable - x86 9.0.30729.4148:\r\n        9.0.30729.4148\r\n    Python 2.7.5 (64-bit):\r\n        2.7.5150\r\n    Salt Minion 0.16.2:\r\n        0.16.2\r\n    VMware Tools:\r\n        8.6.11.20852\r\n    chrome:\r\n        28.0.1500.95\r\n    far:\r\n        3.0.3525\r\n    winscp:\r\n        5.2.2 beta\r\n```\r\n\r\nAnother thing, which may help to understand the nature of the problem - windows minion log, where is seems like few problems reported as errors (e.g. \'Not able to read repo file\'), but few other things are mistakenly reported as problems (e.g. \'[ERROR   ] No changes made for 7z\').\r\n\r\n```\r\ncat C:\\salt\\var\\log\\salt-minion.log\r\n...\r\n2013-08-12 12:36:30,604 [salt.crypt       ][ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\r\n2013-08-12 12:36:59,979 [salt.loaded.int.module.win_pkg][ERROR   ] Not able to read repo file\r\n2013-08-12 12:36:59,979 [salt.loaded.int.module.win_pkg][ERROR   ] [Errno 22] invalid mode (\'r\') or filename: \'\'\r\nTraceback (most recent call last):\r\n  File "salt/modules/win_pkg.py", line 664, in get_repo_data\r\n  File "salt/utils/__init__.py", line 825, in fopen\r\nIOError: [Errno 22] invalid mode (\'r\') or filename: \'\'\r\n2013-08-12 12:37:02,225 [salt.state       ][ERROR   ] Installed Packages:\r\n7z changed from absent to 9.20.00.0\r\n\r\n2013-08-12 12:37:32,958 [salt.state       ][ERROR   ] Installed Packages:\r\nchrome changed from absent to 28.0.1500.95\r\n\r\n2013-08-12 12:37:36,577 [salt.state       ][ERROR   ] Installed Packages:\r\nfar changed from absent to 3.0.3525\r\n\r\n2013-08-12 12:37:43,277 [salt.state       ][ERROR   ] Installed Packages:\r\nwinscp changed from absent to 5.2.2 beta\r\n\r\n2013-08-12 12:41:39,526 [salt.state       ][ERROR   ] No changes made for 7z\r\n2013-08-12 12:41:47,950 [salt.state       ][ERROR   ] No changes made for chrome\r\n2013-08-12 12:41:48,860 [salt.state       ][ERROR   ] No changes made for far\r\n2013-08-12 12:41:51,149 [salt.state       ][ERROR   ] No changes made for winscp\r\n```\r\n\r\n\r\nOutput below will help to understand the version situation \r\n\r\n```\r\nc:\\salt\\salt --versions-report\r\n           Salt: 0.16.2\r\n         Python: 2.7.3 (default, Apr 10 2012, 23:24:47) [MSC v.1500 64 bit (AMD64)]\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.12\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.3\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.2\r\n```'
6607,'terminalmage','32bits binary packages "fail" to install on debian32 on x64\nHello,\r\n\r\nSince last "fix" from https://github.com/saltstack/salt/issues/6303, a new issue arises.\r\n\r\n```\r\nze@zetest-32:~$ uname -a\r\nLinux zetest-32 3.2.0-4-amd64 #1 SMP Debian 3.2.32-1 x86_64 GNU/Linux\r\nze@zetest-32:~$ dpkg --print-architecture\r\ni386\r\n\r\nArch: amd64 / x86_64\r\nOS: i386\r\n```\r\n\r\n`apt.list_pkgs` shows all packages with an `:i386` suffix, thought it\'s the default architecture for packages.\r\n\r\n`pkg.install` state "fails" as it can\'t find it in the newly installed packages (but is correctly installed).\r\n\r\nRemoving a package fails as it can\'t find it to start with.\r\n\r\nResponsible code:\r\n```python\r\n# file: salt/modules/apt.py - lines 538-540\r\n\r\n        if __grains__.get(\'cpuarch\', \'\') == \'x86_64\' \\\r\n                and re.match(r\'i\\d86\', arch):\r\n            name += \':{0}\'.format(arch)\r\n```\r\n\r\nIt mislead the module into thinking that a x86_64 CPU means i386 is not the default architecture.\r\n\r\nGuess it should try to check with something like `dpkg --print-architecture`, but not sure what would the properway to do it. (calling each time, grain, cache, ...)\r\n\r\nAny suggestion on a proper way to fix it ?\r\n\r\n(adding `:i386` to the package would work, but would also require the state file to be aware of the host arch)\r\n'
6606,'cachedout','Problem downgrading MySQL grants from ALL PRIVILEGES\nUnfortunately we\'re still on v0.15.3 and I don\'t have time to dig in further right now, but I skimmed the relevant code in git and it still doesn\'t seem like there\'s any explicit logic for this case.\r\n\r\nTo repro:\r\n1. Create a MySQL user and database.\r\n2. Manually grant the user `ALL PRIVILEGES` to the database.\r\n3. Attempt to execute a state that specifies more limited privileges:\r\n```yaml\r\nwhatever_identifier:\r\n  mysql_grants.present:\r\n    - database: "your_db_here.*"\r\n    - user: your_username_here\r\n    - grant: SELECT\r\n```\r\n\r\nResults (names Find/Replace\'d for privacy):\r\n```\r\n----------\r\n    State: - mysql_grants\r\n    Name:      whatever_identifier\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   Failed to execute: "GRANT SELECT ON your_db_here.* TO your_username_here@localhost"\r\n        Changes:   \r\n----------\r\n```\r\n```\r\n2013-08-10 02:47:36,901 [salt.state       ][INFO    ] Executing state mysql_grants.present for whatever_identifier\r\n2013-08-10 02:47:36,901 [salt.loaded.int.module.mysql][DEBUG   ] Query generated: GRANT SELECT ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,903 [salt.loaded.int.module.mysql][DEBUG   ] Doing query: SELECT User,Host FROM mysql.user WHERE User = \'your_username_here\' AND Host = \'localhost\'\r\n2013-08-10 02:47:36,905 [salt.loaded.int.module.mysql][DEBUG   ] Doing query: SHOW GRANTS FOR \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,905 [salt.loaded.int.module.mysql][DEBUG   ] ["GRANT USAGE ON *.* TO \'your_username_here\'@\'localhost\'", "GRANT ALL PRIVILEGES ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'"]\r\n2013-08-10 02:47:36,906 [salt.loaded.int.module.mysql][DEBUG   ] Grant does not exist, or is perhaps not ordered properly?\r\n2013-08-10 02:47:36,907 [salt.loaded.int.module.mysql][DEBUG   ] Query generated: GRANT SELECT ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,907 [salt.loaded.int.module.mysql][DEBUG   ] Query: GRANT SELECT ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,908 [salt.loaded.int.module.mysql][DEBUG   ] Query generated: GRANT SELECT ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,909 [salt.loaded.int.module.mysql][DEBUG   ] Doing query: SELECT User,Host FROM mysql.user WHERE User = \'your_username_here\' AND Host = \'localhost\'\r\n2013-08-10 02:47:36,911 [salt.loaded.int.module.mysql][DEBUG   ] Doing query: SHOW GRANTS FOR \'your_username_here\'@\'localhost\'\r\n2013-08-10 02:47:36,911 [salt.loaded.int.module.mysql][DEBUG   ] ["GRANT USAGE ON *.* TO \'your_username_here\'@\'localhost\'", "GRANT ALL PRIVILEGES ON `your_db_here`.* TO \'your_username_here\'@\'localhost\'"]\r\n2013-08-10 02:47:36,912 [salt.loaded.int.module.mysql][DEBUG   ] Grant does not exist, or is perhaps not ordered properly?\r\n2013-08-10 02:47:36,912 [salt.loaded.int.module.mysql][INFO    ] Grant \'SELECT\' on \'your_db_here.*\' for user \'your_username_here\' has NOT been added\r\n```\r\nI would hope for either a more helpful+specific error message, or for the user to end up with just `SELECT` privileges, as desired.'
6581,'terminalmage','file.symlink ignores user\nIf a state invokes file.symlink and explicitly specifies user: foo, the symlink is still created as root:root\r\nFurther re-runs of the state still ignores the user setting\r\n\r\nLooking at https://github.com/saltstack/salt/blob/develop/salt/states/file.py#L493 , the actual symlink creation is delegated to os.symlink(), and the "user" argument is discarded.'
6563,'terminalmage','installing custom kernel package broke in 0.16.2\nI have the following sls:\r\n\r\n```\r\nkernelpkgs:\r\n  pkg.installed:\r\n    - sources:\r\n      - linux-image-3.9.4-custom: salt://kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\r\n      - linux-headers-3.9.4-custom: salt://kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\r\n```\r\n\r\nTo install some custom kernel packages on Ubuntu. This worked well in 0.16.0, but seems to have broken in 0.16.2.\r\n\r\n```\r\n[INFO    ] Executing command \'grep-available -F Provides -s Package,Provides -e "^.+$"\' in directory \'/root\'\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/root\'\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/root\'\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/root\'\r\n[ERROR   ] Package file salt://kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb (Name: linux-headers-3.9.4-custom:amd64) does not match the specified package name (linux-headers-3.9.4-custom).\r\n[ERROR   ] Package file salt://kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb (Name: linux-image-3.9.4-custom:amd64) does not match the specified package name (linux-image-3.9.4-custom).\r\n[ERROR   ] No changes made for kernelpkgs\r\n```\r\n\r\nSo, I noticed the ":amd64" at the end of the package name and changed my sls to look like this:\r\n\r\n```\r\nkernelpkgs:\r\n  pkg.installed:\r\n    - sources:\r\n      - "linux-image-3.9.4-custom:amd64": salt://kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\r\n      - "linux-headers-3.9.4-custom:amd64": salt://kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\r\n```\r\n\r\nThat seems to have installed them (yay?):\r\n\r\n```\r\n[INFO    ] Executing state pkg.installed for kernelpkgs\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version} ${Architecture}\\n\' -W" in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'grep-available -F Provides -s Package,Provides -e "^.+$"\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'dpkg -i --force-confold  /vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb /vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version} ${Architecture}\\n\' -W" in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'grep-available -F Provides -s Package,Provides -e "^.+$"\' in directory \'/home/vagrant\'\r\n[ERROR   ] Installed Packages:\r\nlinux-headers-3.9.4-custom changed from absent to 3.9.4-custom-10.00.Custom\r\nlinux-headers-3.9 changed from absent to 1\r\nlinux-image-3.9 changed from absent to 1\r\nlinux-image-3.9.4-custom changed from absent to 3.9.4-custom-10.00.Custom\r\n```\r\n\r\n```\r\nvagrant@reports01:~$ dpkg --list|grep linux-|grep 3.9\r\nii  linux-headers-3.9.4-custom      3.9.4-custom-10.00.Custom  Header files related to Linux kernel, specifically,\r\nii  linux-image-3.9.4-custom        3.9.4-custom-10.00.Custom  Linux kernel binary image for version 3.9.4-custom\r\n```\r\n\r\nBut, \r\nWhen the highstate runs the next time, it breaks the package install:\r\n\r\n```\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'dpkg-deb -I "/vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb"\' in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'dpkg -i --force-confold  /vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb /vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\' in directory \'/home/vagrant\'\r\n[ERROR   ] Command \'dpkg -i --force-confold  /vagrant/salt-master-etc/srv/kernel/linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb /vagrant/salt-master-etc/srv/kernel/linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb\' failed with return code: 1\r\n[ERROR   ] stdout: (Reading database ... 71251 files and directories currently installed.)\r\nPreparing to replace linux-headers-3.9.4-custom 3.9.4-custom-10.00.Custom (using .../linux-headers-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb) ...\r\nUnpacking replacement linux-headers-3.9.4-custom ...\r\nPreparing to replace linux-image-3.9.4-custom 3.9.4-custom-10.00.Custom (using .../linux-image-3.9.4-custom_3.9.4-custom-10.00.Custom_amd64.deb) ...\r\nUnpacking replacement linux-image-3.9.4-custom ...\r\nSetting up linux-headers-3.9.4-custom (3.9.4-custom-10.00.Custom) ...\r\nSetting up linux-image-3.9.4-custom (3.9.4-custom-10.00.Custom) ...\r\n[ERROR   ] stderr: Examining /etc/kernel/preinst.d/\r\nDone.\r\nExamining /etc/kernel/postrm.d .\r\nrun-parts: executing /etc/kernel/postrm.d/initramfs-tools 3.9.4-custom /boot/vmlinuz-3.9.4-custom\r\nupdate-initramfs: Deleting /boot/initrd.img-3.9.4-custom\r\nrun-parts: executing /etc/kernel/postrm.d/zz-update-grub 3.9.4-custom /boot/vmlinuz-3.9.4-custom\r\nGenerating grub.cfg ...\r\nFound linux image: /boot/vmlinuz-3.9.4-custom\r\nFound linux image: /boot/vmlinuz-3.2.0-23-generic\r\nFound initrd image: /boot/initrd.img-3.2.0-23-generic\r\nFound memtest86+ image: /memtest86+.bin\r\ndone\r\nExamining /etc/kernel/header_postinst.d.\r\nRunning depmod.\r\nupdate-initramfs: deferring update (hook will be called later)\r\nFailed to symbolic-link /boot/initrd.img-3.9.4-custom to initrd.img: File exists\r\ndpkg: error processing linux-image-3.9.4-custom (--install):\r\n subprocess installed post-installation script returned error exit status 17\r\nErrors were encountered while processing:\r\n linux-image-3.9.4-custom\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version} ${Architecture}\\n\' -W" in directory \'/home/vagrant\'\r\n[INFO    ] Executing command \'grep-available -F Provides -s Package,Provides -e "^.+$"\' in directory \'/home/vagrant\'\r\n[ERROR   ] Installed Packages:\r\nlinux-image-3.9.4-custom changed from 3.9.4-custom-10.00.Custom to absent\r\nlinux-image-3.9 changed from 1 to absent\r\n\r\n    State: - pkg\r\n    Name:      kernelpkgs\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: linux-headers-3.9.4-custom:amd64, linux-image-3.9.4-custom:amd64.\r\n        Changes:   linux-image-3.9.4-custom: { new : \r\nold : 3.9.4-custom-10.00.Custom\r\n}\r\n                   linux-image-3.9: { new : \r\nold : 1\r\n}\r\n```\r\n\r\nWhat am I missing here?'
6545,'basepi','0.16.2 regression: mount\n```\r\n    State: - mount\r\n    Name:      /run/hugepages\r\n    Function:  mounted\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1239, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/mount.py", line 109, in mounted\r\n    if ret[\'changes\'][\'mount\']:\r\nKeyError: \'mount\'\r\n\r\n        Changes:\r\n```'
6544,'basepi','commenting a section breaks salt without a proper error message\nAdding:\r\n```\r\n  \'something\':\r\n# nothing\r\n```\r\nto your top.sls will break salt. Running salt commands will give you no output (not even warnings or errors!). Running the minion with -l debug will bring up:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/site-packages/salt/minion.py", line 635, in _thread_return\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/state.py", line 201, in highstate\r\n    cache_name=kwargs.get(\'cache_name\', \'highstate\')\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 2173, in call_highstate\r\n    err += self.verify_tops(top)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1789, in verify_tops\r\n    for slsmod in slsmods:\r\nTypeError: \'NoneType\' object is not iterable\r\n```\r\n'
6542,'terminalmage','pkg.latest not finding available packages in 0.16.2 [FreeBSD]\n```python\r\nroot@ariel:/root # salt-call state.sls bash\r\n[INFO    ] Executing command \'/sbin/zfs help || :\' in directory \'/root\'\r\n[INFO    ] Executing command \'ps auxwww\' in directory \'/root\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing command \'/sbin/zfs help || :\' in directory \'/root\'\r\n[INFO    ] Executing command \'/sbin/zfs help || :\' in directory \'/root\'\r\n[INFO    ] Fetching file \'salt://bash.sls\'\r\n[INFO    ] Fetching file \'salt://bash/init.sls\'\r\n[INFO    ] Executing state pkg.latest for bash\r\n[INFO    ] Executing command \'/usr/sbin/pkg info\' in directory \'/root\'\r\n[INFO    ] Executing command \'/usr/sbin/pkg update\' in directory \'/root\'\r\n[INFO    ] Executing command \'/usr/sbin/pkg upgrade -nq\' in directory \'/root\'\r\n[INFO    ] Executing command \'/usr/sbin/pkg search -fe bash\' in directory \'/root\'\r\n[ERROR   ] No information found for "bash".\r\n[ERROR   ] No changes made for bash\r\n[INFO    ] Executing state pkg.latest for bash-completion\r\n[INFO    ] Executing command \'/usr/sbin/pkg upgrade -nq\' in directory \'/root\'\r\n[INFO    ] Executing command \'/usr/sbin/pkg search -fe bash-completion\' in directory \'/root\'\r\n[ERROR   ] No information found for "bash-completion".\r\n[ERROR   ] No changes made for bash-completion\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      bash\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   No information found for "bash".\r\n        Changes:\r\n----------\r\n    State: - cmd\r\n    Name:      chsh -s /usr/local/bin/bash\r\n    Function:  run\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:\r\n----------\r\n    State: - pkg\r\n    Name:      bash-completion\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   No information found for "bash-completion".\r\n        Changes:\r\n----------\r\n    State: - file\r\n    Name:      /root/.bash_profile\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:\r\n----------\r\n    State: - file\r\n    Name:      /root/.bashrc\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:\r\nroot@ariel:/root # pkg search bash\r\nbash-4.2.45\r\nbash-completion-2.1,1\r\n```'
6538,'cachedout','IOError traceback using malformed commandline\n```salt \\(xxx|yyy\\)* test.ping```\r\n\r\n```\r\nbash: yyy)*: command not found\r\n[ERROR   ] [Errno 32] Broken pipe\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/parsers.py", line 156, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/parsers.py", line 1048, in _mixin_after_parsed\r\n    self.print_help()\r\n  File "/usr/lib/python2.7/optparse.py", line 1669, in print_help\r\n    file.write(self.format_help().encode(encoding, "replace"))\r\nIOError: [Errno 32] Broken pipe\r\nUsage: salt [options] \'<target>\' <function> [arguments]\r\nsalt: error: Error while processing <unbound method SaltCMDOptionParser._mixin_after_parsed>: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/parsers.py", line 156, in parse_args\r\n    mixin_after_parsed_func(self)\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/parsers.py", line 1048, in _mixin_after_parsed\r\n    self.print_help()\r\n  File "/usr/lib/python2.7/optparse.py", line 1669, in print_help\r\n    file.write(self.format_help().encode(encoding, "replace"))\r\nIOError: [Errno 32] Broken pipe\r\n```'
6527,'terminalmage','tracebacks from gains in _grains prevent the minion from recovering\nI\'m running Salt 0.16.0-1precise from the PPA.\r\n\r\nI tried using ec2-tags.py from @saltstack/salt-contrib . After that had synced to all the minions, they stopped responding. Any command with salt-call gave a traceback:\r\n\r\n```\r\n$ sudo salt-call grains.get tags \r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[WARNING ] The function \'grains()\' defined in \'/usr/lib/pymodules/python2.7/salt/loader.py\' is not yet using the new \'default_path\' argument to `salt.config.load_config()`. As such, the \'SALT_MINION_CONFIG\' environment variable will be ignored\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 255, in run\r\n    caller = salt.cli.caller.Caller(self.config)\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/caller.py", line 47, in __init__\r\n    self.minion = salt.minion.SMinion(opts)\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 205, in __init__\r\n    self.gen_modules()\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 217, in gen_modules\r\n    self.opts[\'environment\'],\r\n  File "/usr/lib/pymodules/python2.7/salt/pillar/__init__.py", line 57, in compile_pillar\r\n    ret = self.sreq.send(\'aes\', self.auth.crypticle.dumps(load), 3, 7200)\r\n  File "/usr/lib/pymodules/python2.7/salt/crypt.py", line 407, in dumps\r\n    return self.encrypt(self.PICKLE_PAD + self.serial.dumps(obj))\r\n  File "/usr/lib/pymodules/python2.7/salt/payload.py", line 136, in dumps\r\n    return msgpack.dumps(msg)\r\n  File "_msgpack.pyx", line 169, in msgpack._msgpack.packb (msgpack/_msgpack.c:2384)\r\n  File "_msgpack.pyx", line 153, in msgpack._msgpack.Packer.pack (msgpack/_msgpack.c:2020)\r\n  File "_msgpack.pyx", line 136, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1761)\r\n  File "_msgpack.pyx", line 136, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1761)\r\n  File "_msgpack.pyx", line 130, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1611)\r\nTypeError: Expected dict, got TagSet\r\n```\r\n\r\nI had to delete the script from /var/cache/salt/minions/extmod/grains from each host with ssh before they would respond again.'
6523,'terminalmage',"cmd.script not picking up environment\nThe bellow is an example of passing the environment like I normaly do for most cmd functions.\r\n\r\n```yaml\r\n# salt 'ps-aw2az2-dbcpu0001*' cmd.script salt://openstack/scripts/testing.sh dbaas_aw2_az2\r\nps-aw2az2-dbcpu0001.uswest.hpcloud.net:\r\n    ----------\r\n    pid:\r\n        11079\r\n    retcode:\r\n        0\r\n    stderr:\r\n\r\n    stdout:\r\n\r\n::master\r\n```\r\nIt is missing the output I expected. I need to pass  the environment like so.\r\n\r\n```yaml\r\n# salt 'ps-aw2az2-dbcpu0001*' cmd.script salt://openstack/scripts/testing.sh env=dbaas_aw2_az2\r\nps-aw2az2-dbcpu0001.uswest.hpcloud.net:\r\n    ----------\r\n    pid:\r\n        10177\r\n    retcode:\r\n        0\r\n    stderr:\r\n\r\n    stdout:\r\n        [paas-deploy]\r\n```\r\n"
6522,'terminalmage','Problems with mounting\nI have a fairly simple sls to mount a NFS volume from a NetApp:\r\n\r\n    nfs-utils:\r\n       pkg.installed\r\n     \r\n     /srv:\r\n       mount.mounted:\r\n        - device: 10.6.16.72:/vol/Web09\r\n        - fstype: nfs\r\n        - opts: vers=3\r\n        - persist: True\r\n        - require:\r\n           - pkg: nfs-utils\r\n\r\nThis worked fine until 0.16.2, now I get:\r\n\r\n    ----------\r\n        State: - mount\r\n        Name:      /srv\r\n        Function:  mounted\r\n            Result:    False\r\n            Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n      File "/usr/lib/python2.7/site-packages/salt/state.py", line 1239, in call\r\n        ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n       File "/usr/lib/python2.7/site-packages/salt/states/mount.py", line 109, in mounted\r\n        if ret[\'changes\'][\'mount\']:\r\n    KeyError: \'mount\'\r\n    \r\n            Changes:   \r\n\r\nSeems likely that this is related to commit 0c9bcab35507f4307126322a86fdcbb6bccd425b\r\n'
6521,'terminalmage','fnmatch traceback\nGetting the following traceback on 0.16.0 and 0.16.2.  Not really sure where to begin to debug this.\r\n\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 265, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 129, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 70, in call\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/state.py", line 283, in sls\r\n    ret = st_.state.call_high(high_)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1508, in call_high\r\n    ret = self.call_chunks(chunks)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1275, in call_chunks\r\n    running = self.call_chunk(low, running, chunks)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1445, in call_chunk\r\n    running = self.call_chunk(chunk, running, chunks)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1385, in call_chunk\r\n    status = self.check_requisite(low, running, chunks, True)\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1325, in check_requisite\r\n    if (fnmatch.fnmatch(chunk[\'name\'], req_val) or\r\n  File "/usr/lib64/python2.7/fnmatch.py", line 43, in fnmatch\r\n    return fnmatchcase(name, pat)\r\n  File "/usr/lib64/python2.7/fnmatch.py", line 75, in fnmatchcase\r\n    res = translate(pat)\r\n  File "/usr/lib64/python2.7/fnmatch.py", line 87, in translate\r\n    i, n = 0, len(pat)\r\nTypeError: object of type \'NoneType\' has no len()\r\n'
6503,'terminalmage',"salt.states.file documentation should include information regarding setuid, setgid, and sticky bit.\nDuring a discussion in the IRC today it was noted that to set <code>-mode: 0755</code>, it would need to be expressed as <code>-mode: '0755'</code> due to the way the 0 would be interpreted. It would be nice if some of the examples included this, as well as an explanation as to why it needs to be that way over at: https://github.com/saltstack/salt/blob/46700b2e6c462e6254d8b97e2421851fbd8b9556/salt/states/file.py"
6502,'terminalmage','An error is produced when trying to run \'salt-call --local state.sls example\' using 0.16.2 on CentOS 6.4 x86_64.\nBug Desc: \r\n\r\nAn error is produced when trying to run \'salt-call --local state.sls example\' using 0.16.2 on CentOS 6.4 x86_64.\r\n\r\nVMWare Workstation 9.0.2\r\nOperating System: CentOS 6.4 x86_64\r\nSalt Version 0.16.2 and the latest development\r\nVersions affected: 0.16.2 and development repo\r\n\r\nSteps to repeat:\r\n\r\n 1) Fresh install of CentOS 6.4 x86_64\r\n 2) Run all updates via yum update.\r\n 3) Install salt 0.16.2 -> curl -L http://bootstrap.saltstack.org | sh -s -- git v0.16.2\r\n 4) Create any example salt state in /srv/salt/example.sls for testing.\r\n 5) Try to run the state you created with -> salt-call --local state.sls example\r\n 6) The error below will be present. The same example runs fine using 0.16.0.\r\n\r\nThe Error: \r\n\r\n[root@salty-centos ~]# salt-call --local state.sls lnpp\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.6/site-packages/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/__init__.py", line 300, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/caller.py", line 133, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/caller.py", line 68, in call\r\n    with salt.utils.fopen(proc_fn, \'w+\') as fp_:\r\n  File "/usr/lib/python2.6/site-packages/salt/utils/__init__.py", line 849, in fopen\r\n    fhandle = open(*args, **kwargs)\r\nIOError: [Errno 2] No such file or directory: \'/var/cache/salt/minion/proc/20130802092631486864\'\r\n\r\nLatest Stable Versions Report: (error present on this version)\r\nInstalled with - curl -L http://bootstrap.saltstack.org | sh -s -- git v0.16.2\r\n           Salt: 0.16.2\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n\t\t\t\r\nDevelopment Versions Report: (error present on this version)\r\nInstalled with - curl -L http://bootstrap.saltstack.org | sh -s -- git develop\r\n           Salt: 0.16.0-1669-g2824746\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n\r\n0.16.1 Versions Report: (no error on this version)\r\nInstalled with - curl -L http://bootstrap.saltstack.org | sh -s -- git v0.16.1\r\n           Salt: 0.16.1\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n\r\n0.16.0 Versions Report: (no error on this version)\r\nInstalled with - curl -L http://bootstrap.saltstack.org | sh -s -- git v0.16.0\r\n           Salt: 0.16.0\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3'
6472,'thatch45','stateconf Travis err: TypeError: list indices must be integers, not str\nhttps://travis-ci.org/saltstack/salt/jobs/9721259 :\r\n```\r\nTraceback (most recent call last):\r\n  File "/home/travis/build/saltstack/salt/tests/unit/stateconf_test.py", line 97, in test_adding_state_name_arg_for_dot_state_id\r\n    \'\'\', sls=\'test\')\r\n  File "/home/travis/build/saltstack/salt/tests/unit/stateconf_test.py", line 30, in render_sls\r\n    **kws\r\n  File "/home/travis/build/saltstack/salt/salt/renderers/stateconf.py", line 229, in render\r\n    data = process_sls_data(sls_templ, tmplctx)\r\n  File "/home/travis/build/saltstack/salt/salt/renderers/stateconf.py", line 114, in process_sls_data\r\n    argline=rt_argline.strip(), **kws\r\n  File "/home/travis/build/saltstack/salt/salt/renderers/jinja.py", line 41, in render\r\n    tmp_data.get(\'data\', \'Unknown render error in jinja renderer\')\r\nSaltRenderError: Traceback (most recent call last):\r\n  File "/home/travis/build/saltstack/salt/salt/utils/templates.py", line 64, in render_tmpl\r\n    output = render_str(tmplstr, context, tmplpath)\r\n  File "/home/travis/build/saltstack/salt/salt/utils/templates.py", line 102, in render_jinja_tmpl\r\n    loader = JinjaSaltCacheLoader(opts, context[\'env\'])\r\n  File "/home/travis/build/saltstack/salt/salt/utils/jinja.py", line 42, in __init__\r\n    self.searchpath = opts[\'file_roots\'][env]\r\nTypeError: list indices must be integers, not str\r\n```'
6471,'terminalmage',"require_in and yum\nIt looks like require_in clause is broken when works with yum.\r\n\r\nRelated topic: https://groups.google.com/forum/#!topic/salt-users/oLEWrFOpOaE\r\n\r\nFor example, my states are:\r\n```\r\n#my-repo.sls\r\nmy-repo:\r\n  pkgrepo.managed:\r\n    - humanname: #...\r\n    - name: #...\r\n    - baseurl: #...\r\n    - gpgkey: #...\r\n \r\n#my-cool-pkg.sls\r\ninclude:\r\n  - long.so.long.path.to.my-repo\r\n \r\nextend:\r\n  my-repo:\r\n    pkgrepo.managed:\r\n      - require_in:\r\n        - pkg: my-cool-pkg\r\n \r\nmy-cool-pkg:\r\n  pkg.latest\r\n```\r\n\r\nAfter calling\r\n```\r\nsalt 'node' state.sls my-cool-pkg\r\n```\r\non CentOS 6.4 (0.16.0 salt) i have the repo file contains\r\n```\r\nrequire_in=[{'pkg': 'my-cool-pkg'}]\r\n```\r\nline"
6470,'terminalmage',"Update docstring and behavior or name of minion.get_proc_dir()\nWhile investigating issue #6238, I found that get_proc_dir() deletes all the contents of the passed proc directory if it exists.  This differs from its docstring, which reads:\r\n\r\n```\r\nReturn the directory that process data is stored in\r\n```\r\n\r\nI see three possibilities:\r\n\r\n1. If the function should clear the directory, it should get a name similar to empty_proc_dir() to ensure that it doesn't get used when a nondestructive return of the proc directory is desired.\r\n\r\n2. If it should be less destructive, it should check for whether processes exist for each entry in the directory prior to deleting them for being old.\r\n\r\n3. If it should be non-destructive, the deletion behavior should be moved elsewhere.\r\n\r\nThe docstring should be updated for whichever case to reflect the corrected behavior."
6467,'terminalmage','Moving packages from one SLS file to another causes `Conflicting SLS IDs` error\nHi,\r\n\r\nI had a top.sls file which referenced a webserver.sls file:\r\n\r\n```\r\nbase:\r\n  \'*\':\r\n    - webserver\r\n```\r\n\r\nI then added essentials.sls, and moved all the packages from the webserver.sls file to the new essentials.sls file.\r\n\r\n```\r\nbase:\r\n  \'*\':\r\n    - essentials\r\n    - webserver\r\n```\r\n\r\nThe contents of essentials.sls (previously this was in webserver.sls):\r\n\r\n```\r\ntmux:\r\n  pkg:\r\n    - installed\r\nvim:\r\n  pkg:\r\n    - installed\r\npython-pip:\r\n  pkg:\r\n    - installed\r\nvirtualenv:\r\n  pip.installed:\r\n    - require:\r\n      - pkg: python-pip\r\nvirtualenvwrapper:\r\n  pip.installed:\r\n    - require:\r\n      - pkg: python-pip\r\n```\r\n\r\nThen, when I ran SaltStack, I got the error:\r\n\r\n```\r\nvictor@discussion:/srv/salt$ sudo salt \'*\' state.highstate                                                                                                                                                                                                                      \r\ndiscussion:\r\n    Data failed to compile:\r\n----------\r\n    Detected conflicting IDs, SLS IDs need to be globally unique.\r\n    The conflicting ID is "python-pip" and is found in SLS "base:essentials" and SLS "base:webserver"\r\n----------\r\n    Detected conflicting IDs, SLS IDs need to be globally unique.\r\n    The conflicting ID is "virtualenvwrapper" and is found in SLS "base:essentials" and SLS "base:webserver"\r\n----------\r\n    Detected conflicting IDs, SLS IDs need to be globally unique.\r\n    The conflicting ID is "tmux" and is found in SLS "base:essentials" and SLS "base:webserver"\r\n----------\r\n    Detected conflicting IDs, SLS IDs need to be globally unique.\r\n    The conflicting ID is "vim" and is found in SLS "base:essentials" and SLS "base:webserver"\r\n```\r\n\r\nThere\'s a Google thread with more background information here:\r\n\r\nhttps://groups.google.com/d/topic/salt-users/OfAJlZPt3k8/discussion\r\n\r\ncheers,\r\nVictor'
6462,'terminalmage','Cannot format a partition/disk with extfs.mkfs\nUsing salt-master/salt-minion 0.16.0 from your repo on Debian:\r\n\r\nI am running the following command from the salt-master:\r\n```\r\nsalt \'client01\' extfs.mkfs /dev/mapper/vol1-new fs_type=ext4 test=True (or anything else as the third argument)\r\n```\r\nand getting this error:\r\n```\r\nclient01:\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.6/dist-packages/salt/minion.py", line 626, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "/usr/lib/python2.6/dist-packages/salt/modules/extfs.py", line 89, in mkfs\r\n        opt = kwarg_map[key]\r\n    KeyError: \'__pub_user\'\r\n```\r\nCommenting this block of code (obviously) in extfs.py fixes the issue and allows me to at least format the partition in ext4.\r\n```\r\n    opts = \'\'\r\n    for key in kwargs.keys():\r\n        opt = kwarg_map[key]\r\n        if kwargs[key] == \'True\':\r\n            opts += \'-{0} \'.format(opt)\r\n        else:\r\n            opts += \'-{0} {1} \'.format(opt, kwargs[key])\r\n```\r\n\r\nThe problem is also present for extfs.tune since it\'s using the same code.'
6459,'terminalmage','supervisord.state.running: adding a program does not guarantee that it is running\nhttps://github.com/saltstack/salt/blob/3368a3d27a19412c4fe5272e66db8b6c32af2be6/salt/states/supervisord.py#L126\r\n\r\nAdding a program (via supervisorctl update) will only start it if autostart=true. The running state must handle the case where it is not.'
6440,'UtahDave','Add file.copy to file module\nThere is no file.copy function. This would be analogous to file.rename'
6427,'terminalmage','Salt 0.16.0 (aeb20a4, PR#6426): KeyError When Installing Packages With Yum Through Salt\nReceiving this error when referencing any packages available through yum. This example is when installing rpm-build, but I receive this error when trying to install git, tree, vim-enhanced, or any package like that. No errors when installing new directories, files from templates, or anything like that.\r\n\r\nhttps://gist.github.com/tsheibar/6110103'
6424,'cachedout','Minions lose connection with master after IPTables loaded\nHowdy,\r\n\r\nThis is an issue that is driving me crazy.  I am using iptables-persistent to load very simple iptables rules on reboot of my Ubuntu 12.04 nodes.  As soon as the rules are loaded; the minions will not talk Salt for about 5 minutes.  This happened in Salt 0.16.0 nd 0.16.1 / Ubuntu 12.04 and 13.04.\r\n\r\n#### SLS\r\n```\r\niptables-persistent_install:\r\n  pkg.installed:\r\n    - name: iptables-persistent\r\n  file.managed:\r\n    - user: root\r\n    - group: root\r\n    - mode: 0640\r\n    - source: salt://packages/iptables/{{ grains.node_type }}/rules.v4\r\n    - name: /etc/iptables/rules.v4\r\n    - require:\r\n      - pkg: iptables-persistent_install\r\n  service.running:\r\n    - name: iptables-persistent\r\n    - enable: True\r\n    - reload: True\r\n    - require:\r\n      - file: iptables-persistent_install\r\n    - watch: \r\n      - file: /etc/iptables/rules.v4\r\n```\r\n\r\n#### rules.v4\r\n```\r\n# Generated by iptables-save v1.4.12 on Wed Jul 24 12:52:31 2013\r\n*filter\r\n:INPUT ACCEPT [0:0]\r\n:FORWARD ACCEPT [0:0]\r\n:OUTPUT ACCEPT [0:0]\r\n-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT\r\n-A INPUT -p icmp -j ACCEPT\r\n-A INPUT -i lo -j ACCEPT\r\n-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT\r\n-A INPUT -p tcp -m tcp --dport 6379 -j ACCEPT\r\n-A INPUT -m state --state new -m tcp -p tcp --dport 4505 -j ACCEPT\r\n-A INPUT -m state --state new -m tcp -p tcp --dport 4506 -j ACCEPT\r\n-A INPUT -j REJECT --reject-with icmp-host-prohibited\r\n-A FORWARD -j REJECT --reject-with icmp-host-prohibited\r\nCOMMIT\r\n# Completed on Wed Jul 24 12:52:31 2013\r\n```\r\n\r\n#### Salt Master\r\n```\r\nalt --versions-report\r\n           Salt: 0.16.1\r\n         Python: 2.7.3 (default, Apr 20 2012, 22:39:59)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\n#### Salt Minion\r\n```\r\nsalt-call --versions-report\r\n           Salt: 0.16.0\r\n         Python: 2.7.3 (default, Apr 10 2013, 06:20:15)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\n~Jason Swindle\r\n\r\n'
6418,'terminalmage','0.16.0 caching -- not reading updated sls\nError message:\r\n\r\n```\r\nsalt \'web01-region-0-0-www-prod.mine.net\' state.sls common.roles.nodejs.deploy-config\r\nweb01-region-0-0-www-prod.mine.net:\r\n    Data failed to compile:\r\n----------\r\n    Pillar failed to render with the following messages:\r\n----------\r\n    Rendering SLS staging.groups.www.custom failed, render error:\r\n\r\n\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/templates.py", line 151, in render_mako_tmpl\r\n    lookup=lookup\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/template.py", line 273, in __init__\r\n    (code, module) = _compile_text(self, text, filename)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/template.py", line 615, in _compile_text\r\n    generate_magic_comment=template.disable_unicode)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/template.py", line 597, in _compile\r\n    node = lexer.parse()\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/lexer.py", line 241, in parse\r\n    if self.match_python_block():\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/lexer.py", line 376, in match_python_block\r\n    match.group(1)==\'!\', lineno=line, pos=pos)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/lexer.py", line 131, in append_node\r\n    node = nodecls(*args, **kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/parsetree.py", line 139, in __init__\r\n    self.code = ast.PythonCode(text, **self.exception_kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/ast.py", line 37, in __init__\r\n    expr = pyparser.parse(code.lstrip(), "exec", **exception_kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/mako/pyparser.py", line 60, in parse\r\n    ), **exception_kwargs)\r\nSyntaxException: (SyntaxError) invalid syntax (<unknown>, line 1) (u"% if ${environment} == \'staging\':\\n    siteUrl = \'h") at line: 29 char: 1\r\n```\r\n\r\nrelevant custom.sls\r\n\r\n```\r\n% if environment==\'staging\':\r\n        siteUrl = \'http://staging.domain.com\'\r\n        apiUrl = \'https://stagingapi.domain.com/api2\'\r\n        appUrl = \'https://stagingapi.domain.com/app2\'\r\n% else:\r\n        siteUrl = \'http://domain.com\'\r\n        apiUrl = \'https://api.domain.com/api2\'\r\n        appUrl = \'https://api.domain.com/app2\'\r\n% endif\r\n```\r\n\r\nIt **used** to be % if ${environment} == \'staging\': as you can see, it is no longer.  '
6384,'terminalmage','Summary on finish of Highstate\nHowdy,\r\n\r\nCurrently when a highstate finishes, you have to scroll up through your list of states to see if any fail.  It would be nice to have a summary at the end like:\r\n\r\n\r\n```\r\n----------\r\n    State: - service\r\n    Name:      iptables-persistent\r\n    Function:  running\r\n        Result:    True\r\n        Comment:   Service restarted\r\n        Changes:   iptables-persistent: True\r\n\r\n----------\r\n    State: - service\r\n    Name:      newrelic_plugin_agent\r\n    Function:  running\r\n        Result:    True\r\n        Comment:   Service newrelic_plugin_agent has been enabled, and is running\r\n        Changes:   newrelic_plugin_agent: True\r\n\r\n----------\r\n    State: - service\r\n    Name:      postgresql\r\n    Function:  running\r\n        Result:    True\r\n        Comment:   Service postgresql is already enabled, and is in the desired state\r\n        Changes:\r\n\r\n==============\r\n   Summary\r\n==============\r\n* Total States: 52\r\n* States True: 50\r\n* States False: 2\r\n```\r\n\r\nThanks,\r\n\r\nJason Swindle '
6370,'terminalmage',"modules.rabbitmq.status: Unused argument `name`\n```\r\n************* Module salt.modules.rabbitmq\r\nW0613:218,11:status: Unused argument 'name'\r\n```"
6361,'UtahDave',"Windows Privilege Escalation\nWhen using the cmd.script to upload and execute a script hosted on the salt-master, an attacker is able to overwrite the minion's copy of the script before execution.  This leads to a race condition where the attacker may be able to run malicious code as system.\r\n\r\nSample script:\r\n"
6352,'s0undt3ch','Salt \'*\' postgres.user_list ValueError with PostgreSql version 9.3beta2\nWhen using PostgreSql version 9.3beta2\r\ncommand\r\n```\r\nsalt \'*\' postgres.user_list\r\n```\r\nreturns following error\r\n```\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.7/dist-packages/salt/minion.py", line 639, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "/usr/lib/python2.7/dist-packages/salt/modules/postgres.py", line 371, in user_list\r\n        if len(ver) >= 2 and int(ver[0]) >= 9 and int(ver[1]) >= 1:\r\n    ValueError: invalid literal for int() with base 10: \'3beta2\'\r\n```\r\nDirty fix is just convert first character from ver[1] to int. With that modification line 371 in salt/modules/postgres.py is:\r\n```\r\n        if len(ver) >= 2 and int(ver[0]) >= 9 and int(ver[1][0]) >= 1:\r\n```\r\n'
6338,'terminalmage','rabbitmq_user set password with $$ in it fails\nIf the password for a rabbitmq_user contains the characters \'$$\', proper shell escaping is not occurring and $$ is replaced by the shell with the PID of the current process. For example, the password \'wh$$\' may become \'wh22456\'. rabbitmqctl is actually performing that substitution but salt is preventing us from properly escaping the $$ combination.\r\n\r\nSteps to reproduce -\r\n\r\n- add to config --\r\nmy_user:\r\n  rabbitmq_user.present:\r\n    - password: fancypa$$\r\n    - force: True\r\n    - runas: root\r\n\r\n- execute the salt state.\r\n- observe that rabbitmqctl is called\r\n- try to log in with specified password (failure)\r\n\r\nWe have tried quoting with \' and \'" but salt removes those when executing rabbitmqctl.\r\n\r\nWe have tried escaping the quotes but they end up double-escaped (i.e. \\\\\\\'fancypa$$\\\\\\\', ending up, presumable, with literal \' as part of the password and PID as well\r\n \r\nWe have tried escaping the $$ but salt further escapes the escapes (i.e. fancypa\\$\\$ becomes fancypa\\\\\\$\\\\\\$).\r\n\r\nWe\'ve tried all combinations of the above both directly in the state definition and in a pillar definition to no avail.\r\n\r\nThe correct invocation of rabbitmqctl that works is --\r\n    rabbitmqctl change_password my_user \'fancypa\\$\\$\'\r\n\r\nWe don\'t see how we can get salt to achieve this construct.\r\n'
6323,'terminalmage','pip: runas argument is ignored, pip installs as root\nhaving a pip.installed state with "runas" set to user "vagrant", python packages are installed as user root and not as user vagrant.\r\n\r\nOS: Ubuntu 12.04 64 bit\r\nversion: salt-call 0.16.0-1335-g5de7cd5\r\ncall:\r\n```\r\n  $ sudo salt-call state.sls pypackages\r\n```\r\n\r\nAfter installation, virtualenv for my python app seems to be installed as user vagrant (log file reports running virtualenv command as user vagrant and basic python files for virtualenv report user and group "vagrant").\r\nWith packages being installed via pip I have problems:\r\n\r\n* log file does not report running pip command as a user, so it seems to run as root\r\n* pip installed packages (in virtualenv) show being owned by root:root and not as vagrant:vagrant.\r\n\r\nI was willing to have an installation of python application, which is easy to maintain from a command line by pip, add and remove python packages - with packages being owned by a root it is not possible.\r\n\r\nHere is my sls file:\r\n```yaml\r\n{% set app_dir="/opt/cet-jinja" %}\r\n{% set user="vagrant" %}\r\n{{ app_dir }}:\r\n  file.recurse:\r\n    - source: salt://{{ sls }}/app\r\n    - include_pat: "*"\r\n    - user: {{ user }}\r\n    - group: {{ user }}\r\n  virtualenv.managed:\r\n    - system_site_package: False\r\n    - runas: {{ user }}\r\n    - require:\r\n      - file: {{ app_dir }}\r\n  pip.installed:\r\n    - bin_env: {{ app_dir }}\r\n    - requirements: {{ app_dir }}/requirements.txt\r\n    - find_links: "file://{{ app_dir }}/cache"\r\n    - no_index: True\r\n    - runas: {{ user }}\r\n    - no_chown: True\r\n    - require:\r\n      - virtualenv: {{ app_dir }}\r\n      - pkg: python-dev\r\n      - pkg: lib32z1-dev\r\n      - pkg: libxml2-dev\r\n      - pkg: libxslt1-dev\r\n    - watch:\r\n      - file: {{ app_dir }}/requirements.txt\r\n\r\nlxml_deps:\r\n  pkg.installed:\r\n    - names:\r\n      - python-dev\r\n      - lib32z1-dev\r\n      - libxml2-dev\r\n      - libxslt1-dev\r\n\r\n{{ app_dir }}/requirements.txt:\r\n  file.managed:\r\n    - source: salt://{{ sls }}/app/requirements.txt\r\n    - user: {{ user }}\r\n    - group: {{ user }}\r\n    - require:\r\n      - file: {{ app_dir }}\r\n```'
6322,'terminalmage',"feature request: rollback a file managed by Saltstack\nThanks for your attention.\r\n\r\nI use file.managed(with backup=True) to copy a file to the minions, this works very well. \r\nBut when I want to rollback the file to previous version, I can't find how to do this.\r\n\r\nPlease add this feature. Thanks."
6314,'basepi','mysql_user not using correct connection host\nI have an sls which looks like such:\r\n\r\n    mysql-nagios-user:\r\n      mysql_user.present:\r\n        - host: localhost\r\n        - name: newuser\r\n        - password: newpassword\r\n        - connection_host: 127.0.0.1\r\n        - connection_user: user\r\n        - connection_password: password\r\n\r\nWhen trying to highstate this, I get the following error:\r\n\r\n    State: - mysql_user\r\n    Name:      nagios\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n               File "/usr/lib/python2.6/site-packages/salt/state.py", line 1210, in call\r\n                   ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n               File "/usr/lib/python2.6/site-packages/salt/states/mysql_user.py", line 52, in present\r\n                   if __salt__[\'mysql.user_exists\'](name, host, password, password_hash):\r\n               File "/usr/lib/python2.6/site-packages/salt/modules/mysql.py", line 440, in user_exists\r\n                   dbc = _connect()\r\n               File "/usr/lib/python2.6/site-packages/salt/modules/mysql.py", line 115, in _connect\r\n                   dbc = MySQLdb.connect(**connargs)\r\n               File "/usr/lib64/python2.6/site-packages/MySQLdb/__init__.py", line 81, in Connect\r\n                   return Connection(*args, **kwargs)\r\n               File "/usr/lib64/python2.6/site-packages/MySQLdb/connections.py", line 187, in __init__\r\n                   super(Connection, self).__init__(*args, **kwargs2)\r\n           OperationalError: (2003, "Can\'t connect to MySQL server on \'10.19.0.4\' (110)")\r\n\r\nI\'m wondering why it is trying to connect to mysql on 10.19.0.4. I don\'t have anything mysql related in the salt/minion config. This is running on CentOS 6.4 with MySQL-python.x86_64 1.2.3-0.3.c1.1.el6 installed from the repos.\r\n\r\n    salt --versions-report\r\n    Salt: 0.15.1\r\n    Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n    Jinja2: unknown\r\n    M2Crypto: 0.20.2\r\n    msgpack-python: 0.1.13\r\n    msgpack-pure: Not Installed\r\n    pycrypto: 2.0.1\r\n    PyYAML: 3.10\r\n    PyZMQ: 2.2.0.1\r\n    ZMQ: 3.2.3\r\n\r\n\r\nThanks.\r\n'
6306,'thatch45','ZMQError: Operation cannot be accomplished in current state\nI got the following error messages from the salt-minion:\r\n```\r\n[INFO    ] Authentication with master successful!\r\nProcess Process-10:\r\nTraceback (most recent call last):\r\n  File "/usr/lib64/python2.6/multiprocessing/process.py", line 232, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib64/python2.6/multiprocessing/process.py", line 88, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/minion.py", line 484, in _thread_return\r\n    minion_instance._return_pub(ret)\r\n  File "/usr/lib/python2.6/site-packages/salt/minion.py", line 606, in _return_pub\r\n    ret_val = sreq.send(\'aes\', self.crypticle.dumps(load))\r\n  File "/usr/lib/python2.6/site-packages/salt/payload.py", line 176, in send\r\n    self.socket.send(pkg)\r\n  File "socket.pyx", line 499, in zmq.core.socket.Socket.send (zmq/core/socket.c:5381)\r\n  File "socket.pyx", line 546, in zmq.core.socket.Socket.send (zmq/core/socket.c:5143)\r\n  File "socket.pyx", line 175, in zmq.core.socket._send_copy (zmq/core/socket.c:2139)\r\nZMQError: Operation cannot be accomplished in current state\r\n```\r\n```\r\n ###environment\r\nrpm -qa |grep salt\r\nsalt-0.15.3-1.el6.noarch\r\nsalt-minion-0.15.3-1.el6.noarch\r\n```'
6303,'terminalmage',"32-bit binary package installs fail on 64-bit OS\nThis is due to the fact that RPMs and DEBs have a metadata check done in ``salt.modules.pkg_resource._parse_pkg_meta()``, which doesn't return the arch. Therefore, packages submitted in the format ``pkgname.arch`` in CentOS fail the metadata check, because the name returned is just ``pkgname`` (no arch)."
6300,'thatch45','Ugly multiprocessing bug\nThe code below causes the minion to timeout. It should not. If run from an interpreter (or script via the Linux command line), the code returns correctly.\r\n\r\nIf run via the salt command, it will timeout on the res.get() call.\r\n\r\nIf I run it via salt-call I get some additional error info:\r\n```\r\nException in thread Thread-2:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/threading.py", line 552, in __bootstrap_inner\r\n    self.run()\r\n  File "/usr/lib/python2.7/threading.py", line 505, in run\r\n    self.__target(*self.__args, **self.__kwargs)\r\n  File "/usr/lib/python2.7/multiprocessing/pool.py", line 342, in _handle_tasks\r\n    put(task)\r\nPicklingError: Can\'t pickle <type \'function\'>: attribute lookup __builtin__.function failed\r\n```\r\n\r\n\r\nSince this *only* happens when run via salt and runs flawlessly outside of it, this signifies to me a deep and un-pretty bug in Salt. I\'m not quite sure where in the code to look to find this, so I am hoping someone with deep knowledge of salt\'s internals can track it down.\r\n\r\nIf it is the case that salt modules can not use multiprocessing, this needs to be documented and called out specifically so devs and engineers know not to expect it.\r\n\r\nCode follows:\r\n\r\n```\r\n"""\r\nshould return as follows:\r\n>>> bugcase.parallelHostBenchmark()\r\n{0: {\'full\': True, \'port\': 0}, 1: {\'full\': True, \'port\': 1}, 2: {\'full\': True, \'port\': 2}, 3: {\'full\': True, \'port\': 3}, 4: {\'full\': True, \'port\': 4}, 5: {\'full\': True, \'port\': 5}, 6: {\'full\': True, \'port\': 6}, 7: {\'full\': True, \'port\': 7}, 8: {\'full\': True, \'port\': 8}, 9: {\'full\': True, \'port\': 9}}\r\n>>> \r\ninstead it times out on the get(), never having run the runBenchmark function at all.\r\n"""\r\nimport multiprocessing\r\ndef runBenchmark(port=6379,full=True):\r\n    return {\'port\':port,\'full\':full }\r\n\r\ndef parallelHostBenchmark(full=True):\r\n    ports = sorted( [x for x in xrange(10)] )\r\n    workers = []\r\n    print >>open("/tmp/bug.log",\'a\'), "ports = {}".format(str(ports))\r\n\r\n    benchmarkpool = multiprocessing.Pool( processes=len(ports) )\r\n    results = [benchmarkpool.apply_async(runBenchmark, (port,full) ) for port in ports]\r\n\r\n    retdata = {}\r\n    for res in results:\r\n        print >>open("/tmp/bug.log",\'a\'), "getting result"\r\n        data = res.get()\r\n        retdata[data[\'port\']] = data \r\n        print >>open("/tmp/bug.log",\'a\'), "result={}".format( str(data) )\r\n    return retdata\r\n```\r\n'
6298,'terminalmage','Try to guess when "virtual" package is being used, warn the user\nOn fedora, I have\r\n\r\n```yaml\r\nvim:\r\n  pkg:\r\n    - installed\r\n```\r\n\r\nWhich salt reports as failed, but yum correctly matches this to `vim-enhanced` and proceeds to install and returns `0` as the error code.'
6292,'terminalmage',"pkg.latest refreshes on every run\nThis was brought up on the mailing list. The reason for this is that ``pkg.latest_version`` is run each time, and does not pass through the ``refresh`` value. As ``latest_version`` defaults to ``refresh=True``, this ensures a refresh every time.\r\n\r\nThe intended behavior is for a refresh tag to be generated once per salt run (in ``mod_init()``), and the existence of this file is used to limit only one refresh per salt run, unless ``refresh=True`` is used.\r\n\r\nThe solution here is to move the rtag check to the ``latest_version`` call, and pass ``refresh=False`` to ``__salt__['pkg.install']()``.\r\n\r\nWorking on a fix now."
6284,'terminalmage','Document how to inherit files from different environments in file_roots\nReferences #693. Perhaps put something like [my gist](https://gist.github.com/SEJeff/5257789) into the docs'
6283,'terminalmage','Ubuntu 13.04 and service.running failure\nHowdy,\r\n\r\nWhen using service.running enable: True in Ubuntu 13.04, the correct links in /etc/rc*.d are made but the service never starts on reboot.  The init.d script does work because I can do ```service <<script-name>> start``` and it comes right up.  I have ran into this on two services; NGINX and REDIS-SERVER.  Below is the SLS files, and my ENV.\r\n\r\n#### NGINX SLS\r\n```\r\nnginx_install:\r\n  pkgrepo.managed:\r\n    - ppa: nginx/stable\r\n    - require:\r\n      - pkg: python-software-properties_install\r\n  pkg.installed:\r\n    - name: nginx-extras\r\n    - require:\r\n      - pkgrepo: nginx_install\r\n  service.running:\r\n    - name: nginx\r\n    - enable: True\r\n    - require:\r\n      - pkg: nginx_install\r\n    - watch:\r\n      - file: /etc/nginx/sites-enabled/default\r\n```\r\n\r\n#### REDIS-SERVER SLS\r\n```\r\nredis_install:\r\n    pkg.installed:\r\n        - name: redis-server\r\n    file.managed:\r\n        - name: /etc/redis/redis.conf\r\n        - source: salt://packages/redis/redis.conf\r\n        - user: root\r\n        - group: root\r\n        - mode: 0644\r\n        - require:\r\n          - pkg: redis_install\r\n    service.running:\r\n        - name: redis-server\r\n        - enable: True\r\n        - watch: \r\n          - file: /etc/redis/redis.conf\r\n        - require:\r\n          - file: redis_install\r\n```\r\n\r\n#### runlevel\r\n```\r\n# runlevel\r\nN 2\r\n```\r\n\r\n#### Links are there for startup\r\n```\r\n# ls -hal /etc/rc2.d/\r\ntotal 12K\r\ndrwxr-xr-x  2 root root 4.0K Jul 23 13:01 .\r\ndrwxr-xr-x 98 root root 4.0K Jul 23 13:02 ..\r\n-rw-r--r--  1 root root  677 Jan 30 06:58 README\r\nlrwxrwxrwx  1 root root   31 Jul 23 13:01 S20newrelic_plugin_agent -> ../init.d/newrelic_plugin_agent\r\nlrwxrwxrwx  1 root root   26 Jul 23 12:59 S20newrelic-sysmond -> ../init.d/newrelic-sysmond\r\nlrwxrwxrwx  1 root root   22 Jul 23 13:01 S20redis-server -> ../init.d/redis-server\r\nlrwxrwxrwx  1 root root   32 Jun  3 23:05 S20virtualbox-guest-utils -> ../init.d/virtualbox-guest-utils\r\nlrwxrwxrwx  1 root root   16 Jun  3 23:05 S21puppet -> ../init.d/puppet\r\nlrwxrwxrwx  1 root root   13 Jul 23 13:01 S23ntp -> ../init.d/ntp\r\nlrwxrwxrwx  1 root root   29 Jul 23 13:01 S37iptables-persistent -> ../init.d/iptables-persistent\r\nlrwxrwxrwx  1 root root   26 Jun  3 22:36 S45landscape-client -> ../init.d/landscape-client\r\nlrwxrwxrwx  1 root root   15 Jun  3 22:36 S50rsync -> ../init.d/rsync\r\nlrwxrwxrwx  1 root root   19 Jun  3 22:36 S70dns-clean -> ../init.d/dns-clean\r\nlrwxrwxrwx  1 root root   18 Jun  3 22:36 S70pppd-dns -> ../init.d/pppd-dns\r\nlrwxrwxrwx  1 root root   14 Jun  3 22:35 S75sudo -> ../init.d/sudo\r\nlrwxrwxrwx  1 root root   21 Jun  3 23:05 S99chef-client -> ../init.d/chef-client\r\nlrwxrwxrwx  1 root root   21 Jun  3 22:36 S99grub-common -> ../init.d/grub-common\r\nlrwxrwxrwx  1 root root   18 Jun  3 22:35 S99ondemand -> ../init.d/ondemand\r\nlrwxrwxrwx  1 root root   18 Jun  3 22:35 S99rc.local -> ../init.d/rc.local\r\n```\r\n\r\n#### Ubuntu 13.04\r\n```\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 13.04\r\nRelease:\t13.04\r\nCodename:\traring\r\n```\r\n\r\n#### Salt in masterless setup\r\n```\r\nsalt-call --versions-report\r\n           Salt: 0.16.0\r\n         Python: 2.7.4 (default, Apr 19 2013, 18:28:01)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.2.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
6277,'terminalmage','Cannot setup ssh public key\nHey,\r\n\r\nI\'m trying to install a ssh public key on a ubuntu 12.04 machine for a user unison. The keys are generated with PuTtyGen on a windows machine. The home directory of user unison has a .ssh-dir with nothing inside. When I run bighstate by \r\n\r\nsalt-call --local state.highstate -l debug \r\n\r\nwith the following state:\r\n----------\r\n    ssh-key-unison: \r\n      ssh_auth:\r\n        - present\r\n        - user: unison\r\n        - source: salt://unison/public_key_unison.id_rsa.pub\r\n        - enc: ssh-rsa\r\n----------\r\n\r\nant this contents of public_key_unison.id_rsa.pub:\r\n\r\n---- BEGIN SSH2 PUBLIC KEY ----\r\nComment: "rsa-key-20130723"\r\nAAAAB3NzaC1yc2EAAAABJQAAAIB+QjbJjsAWPcGqPNfj5ZVt8BKwaSI+BsFxKwo+\r\n9dVMmKNJir4vF6ETqM6PFvQcw5eMj8lbbjzMOhvFkzTr0NEGWcboozMEs+k64vwt\r\n7XqHDzRU02vKgrpa8XtEnXhVDOgd/2taKH4gm0QHkYy+/ImExJ1329bN+lUMwmR8\r\nz7++5w==\r\n---- END SSH2 PUBLIC KEY ----\r\n\r\n\r\nthe output of salt is: \r\n\r\n----------\r\n    State: - ssh_auth\r\n    Name:      ssh-key-unison\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   The authorized host key ssh-key-unison is already present for user unison\r\n        Changes:\r\n----------\r\n\r\n\r\nThat is not true. The ~/.ssh of user unison is empty! \r\n\r\nBest regards,\r\nmajo83'
6274,'UtahDave','Windows is missing the virtual grain key\nWindows 2008 R2 64bit\r\nSalt 0.16.0\r\nVMware vSphere Guest\r\n\r\n```yaml\r\nsalt win grains.items --out yaml\r\nwin:\r\n  biosversion: PhoenixBIOS 4.0 Release 6.0\r\n  cpu_model: Intel64 Family 6 Model 37 Stepping 1, GenuineIntel\r\n  cpuarch: AMD64\r\n  defaultencoding: cp1252\r\n  defaultlanguage: en_US\r\n  domain: rnd.confidela\r\n  fqdn: WATCHDO-52A7LNT.rnd.confidela\r\n  gpus: []\r\n  host: WATCHDO-52A7LNT\r\n  id: win\r\n  ip_interfaces:\r\n    Intel(R) PRO/1000 MT Network Connection:\r\n    - 192.168.0.12\r\n  ipv4:\r\n  - 192.168.0.12\r\n  kernel: Windows\r\n  kernelrelease: 6.1.7601\r\n  localhost: WATCHDO-52A7LNT\r\n  manufacturer: VMware, Inc.\r\n  master: 192.168.0.13\r\n  mem_total: 8192\r\n  nodename: WATCHDO-52A7LNT\r\n  num_cpus: 4\r\n  num_gpus: 0\r\n  os: Windows\r\n  os_family: Windows\r\n  osfullname: Microsoft Windows Server 2008 R2 Standard\r\n  osmanufacturer: Microsoft Corporation\r\n  osrelease: 2008ServerR2\r\n  osversion: 6.1.7601\r\n  path: c:\\salt\\salt-0.16.0.win-amd64;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;\r\n  productname: VMware Virtual Platform\r\n  ps: tasklist.exe\r\n  pythonpath:\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\library.zip\r\n  - c:\\salt\\salt-0.16.0.win-amd64\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\cython-0.15.1-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\distribute-0.6.28-py2.7.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\esky-0.9.7-py2.7.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\jinja2-2.6-py2.7.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\m2crypto-0.21.1-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\msgpack_python-0.1.12-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\pycrypto-2.3-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\pywin32-216-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\pyzmq-13.1.0-py2.7-win-amd64.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\salt-0.16.0-py2.7.egg\r\n  - c:\\salt\\salt-0.16.0.win-amd64\\wmi-1.4.9-py2.7.egg\r\n  pythonversion:\r\n  - 2\r\n  - 7\r\n  - 3\r\n  - final\r\n  - 0\r\n  roles:\r\n  - weca\r\n  saltpath: c:\\salt\\salt-0.16.0.win-amd64\\salt-0.16.0-py2.7.egg\\salt\r\n  saltversion: 0.16.0\r\n  server_id: 454459733\r\n  shell: /bin/sh\r\n  timezone: (UTC-08:00) Pacific Time (US & Canada)\r\n  windowsdomain: WORKGROUP\r\n```'
6253,'terminalmage',"mention managed file templating in tutorials\nWhen reading through the salt/doc/topics/tutorials/starting_states.rst I came across the 'template: jinja' argument things to a file.managed for the first time, in the moosefs/chunk.sls example.\r\n\r\nAs a newcomer to salt (coming from puppet) being able to use a template for a file content is something I was looking for. With further reading I found more detail on this at http://docs.saltstack.com/ref/states/all/salt.states.file.html#module-salt.states.file.\r\n\r\nBut I wonder if this could be mentioned briefly in the tutorials. Perhaps by adding an explanatory note just after this moosefs/chunk.sls example saying:\r\n\r\n  Files don't just have to be served up as static files. As you can see in the above '/etc/mfschunkserver.cfg' file you can use the jinja templating system to render the file. More detail can be round at :doc:`the state file reference. </ref/states/all/salt.states.file.html>`\r\n"
6246,'terminalmage','Debconf module is not listed in fresh installed 0.16 in debian\nI just installed a debian stable server with salt-minion 0.16 and updated my master to have 0.16 and there is no debconf module on it, I added the providers with apt in /etc/salt/minion to be able to use apt as backend, but I have no idea what is happening with the debconf module.\r\n\r\nIt seem that __grains__[\'os_family\'] is returning a string with "" included that is \'"Debian"\' then the comparison should be (in modules/debconfmod.py function __virtual__):\r\n    if __grains__[\'os_family\'] != \'"Debian"\':\r\n\r\nBut I imagine the error is in the way we\'re getting the os family.'
6245,'terminalmage',"walkthrough add minion documentation on viewing key\nIn the walkthrough ( salt / doc / topics / tutorials / walkthrough.rst ) it says keys should be verified, and how to display the minions keys on the master by running 'salt-key -P'.\r\n\r\nThe documentation should also then say (or link to) how to display the public key on the minion."
6240,'UtahDave','pillar data in winrepo for templating installs\nIt maybe desirable to be able to use pillar data to do templated installs in winrepo.  \r\n\r\nFor example, the winrepo package definition:\r\n<pre><code>\r\nexampleapp:\r\n  0.1.2:\r\n    installer: {{ pillar[\'exampleapp\'][\'installer] }}\r\n    full_name: {{ pillar[\'exampleapp\'][\'full_name] }}\r\n    locale: en_US\r\n    reboot: False\r\n    install_flags: {{ pillar[\'exampleapp\'][\'install_flags] }}\r\n    uninstaller: {{ pillar[\'exampleapp\'][\'uninstaller] }}\r\n    uninstall_flags: \' /qn\'\r\n</code>\r\n</pre>\r\n\r\nAnd the pillar file:\r\n<pre><code>\r\nexampleapp:\r\n  install_flags: \' /q HOSTNAME="host.dom" USERNAME="admin" PASSWORD="ApkGdtpkpFvN" PORT="12345"\'\r\n  {% if grains[\'cpuarch\'] == \'AMD64\' %}\r\n    installer: \'salt://win/repo/exampleapp/exampleapp-x64.msi\'\r\n    full_name: ExampleApp 0.1.2 (x64)\r\n    uninstaller: \'salt://win/repo/exampleapp/exampleapp-x64.msi\'\r\n  {% elif grains[\'cpuarch\'] == \'x86\' %}\r\n    installer: \'salt://win/repo/exampleapp/exampleapp-x86.msi\'\r\n    full_name: ExampleApp 0.1.2 (x86)\r\n    uninstaller: \'salt://win/repo/exampleapp/exampleapp-x86.msi\'\r\n  {% endif %}\r\n</code></pre>'
6234,'terminalmage','cron state and module do not support removing \'*/n\' timing notation\nThe cron state and module fail to match a cron job like: "*/5 * * * * script.sh" when stepped times are specified. \r\n\r\nAdding works, and so does removing when not specifying the times.\r\n```\r\nroot@salt:/srv/pillar# salt minion1 cron.set_job root \'*/5\' \'*\' \'*\' \'*\' \'*\' \'echo crontest\'\r\nminion1:\r\n    new\r\nroot@salt:/srv/pillar# salt minion1 cron.rm_job root \'echo crontest\'\r\nminion1:\r\n    removed\r\n```\r\n\r\nBut if you try to remove with a stepped time set:\r\n```\r\nroot@salt:/srv/pillar# salt minion1 cron.set_job root \'*/5\' \'*\' \'*\' \'*\' \'*\' \'echo crontest\'\r\nminion1:\r\n    new\r\nroot@salt:/srv/pillar# salt minion1 cron.rm_job root \'*/5\' \'*\' \'*\' \'*\' \'*\' \'echo crontest\'\r\nminion1:\r\n    absent\r\n```\r\nI verified the job failed to be removed on the minion. This also appears to be an issue with the cron state. Tested with 16.0.'
6230,'terminalmage',"cmd.script doesn't complain if script path is invalid\nI am not sure if this is a feature or bug. I call it a bug.\r\n\r\nRunning a cmd.script with a bad/invalid path to the script simply returns ok with a retcode of 0 etc. I would have expected some sort of error that the script doesn't exist and hence was not run.\r\n\r\nver: 0.15.3\r\nOS: Ubuntu 12.04.2"
6220,'UtahDave','Running a "salt \'*win*\' \'powershell.exe\' cmd to windows minions renders them unresponsive until salt-minion service restarted\nTesting a bunch of commands on windows 2008 servers (0.16.0 minions, 0.16.0 master). Running 8 or so Windows minions and 2 centos. Everything was working great until i ran a glob "salt \'*win*\' cmd.run \'powershell.exe \'" (yes, not something you should really ever run...powershell with no cmdlets/params) and then after a bit I had to CTRL + C\r\n\r\nExiting on Ctrl-C\r\nThis job\'s jid is:\r\n20130717223346282287\r\nThe minions may not have all finished running and any remaining minions will return upon completion. To look up the return data for this job later run:\r\nsalt-run jobs.lookup_jid 20130717223346282287\r\n\r\n\r\nAfter that the minions would not respond, even to test.ping, until the minion service was manually restarted. \r\n\r\nI was able to recreate this issue after all the minions were restarted. \r\n\r\n[root@saltpoc201 salt]#  salt \'*win*\' cmd.run test.ping\r\n[root@saltpoc201 salt]#\r\n\r\n\r\nRestarting individual minion services brought them back to life, even when others were still unresponsive. \r\n'
6219,'thatch45',"Salt as Cloud Controller documentation cuts off mid-sentence\nThe docs at:\r\nhttp://salt.readthedocs.org/en/v0.16/topics/tutorials/cloud_controller.html#using-salt-virt\r\n\r\nare cut off mid-sentence, and don't actually show anything about how to use salt-virt."
6213,'UtahDave','Windows state.highstate returns False despite successful package installation\nSalt master is 0.16.0 on Debian Wheezy (install from deb http://debian.saltstack.com/debian wheezy-saltstack main).  The test minions are XP & Win2k8R2 running 0.16.0 (installed from http://saltstack.com/downloads/Salt-Minion-0.16.0-win32-Setup.exe & AMD64 version, respectively).\r\n\r\n<pre><code>\r\nroot@salt:/srv/salt# salt -G \'cpuarch:x86\' pkg.list_pkgs |egrep -a -A1 "7-Zip|Notepad"\r\n\r\nroot@salt:/srv/salt# salt -G \'cpuarch:x86\' state.highstate -t 60\r\n\r\nroot@salt:/srv/salt# salt -G \'cpuarch:x86\' pkg.list_pkgs |egrep -a -A1 "7-Zip|Notepad"\r\n    7-Zip 9.20:\r\n        9.20.00.0\r\n--\r\n    Notepad++:\r\n        6.4.2\r\n\r\nroot@salt:/srv/salt# salt -G \'cpuarch:x86\' state.highstate -t 60\r\nroot@salt:/srv/salt# salt -G \'cpuarch:x86\' state.highstate -t 120\r\nxp_host:\r\n----------\r\n    State: - pkg\r\n    Name:      7zip.x86\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: 7zip.x86=9.22.\r\n        Changes:   \r\n----------\r\n    State: - pkg\r\n    Name:      notepad++.x86\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: notepad++.x86=6.4.2.\r\n\r\n</code></pre>\r\n\r\n<pre><code>\r\nroot@salt:/srv/salt# salt -G \'cpuarch:AMD64\' pkg.list_pkgs |egrep -A1 "7-Zip|Notepad"\r\n\r\nroot@salt:/srv/salt# salt -G \'cpuarch:AMD64\' state.highstate -t 60\r\nwin2k8r2_host.domain:\r\n----------\r\n    State: - pkg\r\n    Name:      7zip\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: 7zip=9.22.\r\n        Changes:   7-Zip 9.20 (x64 edition): { new : 9.20.00.0\r\nold : \r\n}\r\n                   \r\n----------\r\n    State: - pkg\r\n    Name:      notepad++\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: notepad++=6.4.2.\r\n        Changes:   Notepad++: { new : 6.4.2\r\nold : \r\n}\r\n\r\n\r\nroot@salt:/srv/salt# salt -G \'cpuarch:AMD64\' pkg.list_pkgs |egrep -A1 "7-Zip|Notepad"\r\n    7-Zip 9.20 (x64 edition):\r\n        9.20.00.0\r\n--\r\n    Notepad++:\r\n        6.4.2\r\n</code></pre>'
6211,'terminalmage',"solarispkg.install: Unused argument 'refresh'\nOther than the presumably still work-in-progress `win_pkg`, none of the other package management modules seem to have unused `refresh` parameters, making `solarispkg` the odd man out here.\r\n```\r\n************* Module salt.modules.solarispkg\r\nW0613:166,23:install: Unused argument 'refresh'\r\n```"
6209,'basepi','NameError: global name \'itertools\' is not defined\nIn the Current build #6204 (worked 1-2 days ago) the following error shows up with my Ubuntu Master speaking to Windows 2008R2 minion.  Both Master/Minion have just been updated.\r\n\r\n```\r\nError:\r\n\r\n    State: - file\r\n    Name:      c:/jboss-eap-5.1/jboss-as/bin/service.bat\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\state.py", line 1237, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\states\\file.py", line 872, in managed\r\n    contents)\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\modules\\file.py", line 1757, in manage_file\r\n    __clean_tmp(sfn)\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\modules\\file.py", line 57, in __clean_tmp\r\n    all_roots = itertools.chain.from_iterable(__opts__[\'file_roots\'].itervalues())\r\nNameError: global name \'itertools\' is not defined\r\n```'
6200,'terminalmage',"Empty managed file doesn't overwrite cached version\nWhen trying to deploy an empty managed file, which was previously not empty, the salt minion prefers to use the old cached version instead.\r\n\r\nSteps to reproduce : \r\n- Deploy a non-empty managed file\r\n\r\n```yaml\r\n# /srv/salt/test.sls\r\n/tmp/testfile:\r\n  file.managed:\r\n    - source: salt://testfile\r\n    - template: jinja\r\n```\r\n/srv/salt/testfile:\r\n```text\r\nNot empty\r\n```\r\n```shell\r\n# salt-call state.sls test\r\n[... snip debug messages ...]\r\nlocal:\r\n----------\r\n    State: - file\r\n    Name:      /tmp/testfile\r\n    Function:  managed\r\n        Result:    True\r\n        Comment:   File /tmp/testfile updated\r\n        Changes:   diff: New file\r\n# cat /tmp/testfile\r\nNot empty\r\n```\r\n\r\n- Empty that file \r\n\r\n```shell\r\n# echo -n > /srv/salt/testfile\r\n# cat /srv/salt/testfile\r\n# du /srv/salt/testfile\r\n0\t/srv/salt/testfile\r\n```\r\nThe /srv/salt/testfile is now empty\r\n\r\n- Try to re-deploy that file, the salt minion will deploy the old (non-empty) cached version instead\r\n\r\n```shell\r\n# salt-call state.sls test\r\n[... snip debug messages ...]\r\nlocal:\r\n----------\r\n    State: - file\r\n    Name:      /tmp/testfile\r\n    Function:  managed\r\n        Result:    True\r\n        Comment:   File /tmp/testfile is in the correct state\r\n        Changes:   \r\n# cat /tmp/testfile\r\nNot empty\r\n# cat /var/cache/salt/minion/files/base/testfile \r\nNot empty\r\n```\r\nSo the minion seems to prefer the cached version when the /srv/salt one is empty?\r\n\r\nVersion information : \r\nDistribution: Debian sid, salt packages from http://debian.saltstack.com/debian\r\n```shell\r\n# salt --versions-report\r\n           Salt: 0.16.0\r\n         Python: 2.7.3 (default, Sep  9 2012, 17:41:34)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.2.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.3\r\n```"
6194,'terminalmage','Error Reading php extensions file: AttributeError: \'str\' object has no attribute \'read\'\nMaster Version: 0.16.0\r\nMinion Version: 0.16.0\r\n\r\nHighstate output:\r\n\r\n```\r\n    State: - file\r\n    Name:      /usr/local/etc/php/extensions.ini\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/site-packages/salt/state.py", line 1201, in call\r\n  File "/usr/local/lib/python2.7/site-packages/salt/states/file.py", line 854, in managed\r\n    contents)\r\n  File "/usr/local/lib/python2.7/site-packages/salt/modules/file.py", line 1554, in manage_file\r\n    if not salt.utils.istextfile(sfn) \\\r\n  File "/usr/local/lib/python2.7/site-packages/salt/utils/__init__.py", line 717, in istextfile\r\n    by reading a single block of bytes from the file.\r\nAttributeError: \'str\' object has no attribute \'read\'\r\n\r\n        Changes:   \r\n```\r\n\r\n[root@salt /usr/local/etc/salt/states]# cat php5/etc/php/extensions.ini \r\nextension=xml.so\r\nextension=pdo.so\r\nextension=session.so\r\nextension=dom.so\r\nextension=mysql.so\r\nextension=gd.so\r\nextension=hash.so\r\nextension=zlib.so\r\nextension=mysqli.so\r\nextension=odbc.so\r\nextension=pdo_odbc.so\r\nextension=bz2.so\r\nextension=phar.so\r\nextension=zip.so\r\nextension=iconv.so\r\nextension=mbstring.so\r\nextension=pdo_mysql.so\r\nextension=pdo_sqlite.so\r\nextension=sqlite3.so\r\nextension=gettext.so\r\nextension=json.so\r\nextension=pcntl.so\r\nextension=readline.so\r\nextension=shmop.so\r\nextension=sysvmsg.so\r\nextension=sysvsem.so\r\nextension=sysvshm.so\r\nextension=tokenizer.so\r\nextension=curl.so\r\nextension=exif.so\r\nextension=bcmath.so\r\nextension=calendar.so\r\nextension=snmp.so\r\nextension=ldap.so\r\nextension=sockets.so\r\nextension=pdf.so\r\nextension=filter.so\r\nextension=mcrypt.so\r\nextension=openssl.so\r\nextension=fileinfo.so\r\nextension=posix.so\r\nextension=ctype.so\r\nextension=pspell.so\r\nextension=simplexml.so\r\nextension=xmlreader.so\r\nextension=xmlwriter.so\r\nextension=xsl.so\r\nextension=soap.so\r\nextension=magickwand.so\r\nextension=memcache.so\r\n'
6185,'whiteinge',"Restore legacy documentation on readthedocs.org\nHey guys,\r\n\r\nIt looks like I'm going to need to upgrade my existing infrastructure from 0.11.1 to something newer. But in order to start planning for the inevitable SLS and config changes I need to be able to review legacy documentation. The documentation on http://salt.readthedocs.org only shows the latest revision, whereas it used to have all revisions. Would you please post the legacy documentation?\r\n\r\nIn addition to that, I'm struggling to find a changelog among the documentation. If it's available, can future changlogs be linked / included on the index for the documentation?\r\n\r\nThanks"
6184,'UtahDave','Salt-Minion on Windows fails with "-" in minion name\nversion 0.16.0\r\n\r\nIn using the Windows silent installer I was running the command below on a Windows 2008 R2 vm and noticed salt-minion wasn\'t running and determined that the hostname/minion name was \'saltpoc-0\'. Running the command again with the minion name \'saltpoc0\', everything worked as expected. \r\n\r\nWas using this as a silent installer:\r\nSalt-Minion-0.16.0-AMD64-Setup.exe /S /master=10.4.167.xx /minion-name=%COMPUTERNAME%\r\n'
6180,'UtahDave','Does saltstack support a masterless windows minion.\nI have repeatedly tried, without success, to use saltstack in my *nix-less, git-less windows environment. Has anyone been successful in running a windows standalone minion?   I\'ve even updated to the latest development version and like the others it complains about the repo:\r\n\r\nIOError: [Errno 22] invalid mode (\'r\') or filename: \'\'\r\n[ERROR   ] Unable to cache file "salt://win/repo/winrepo.p" from env "base".\r\n[ERROR   ] Not able to read repo file\r\n[ERROR   ] [Errno 22] invalid mode (\'r\') or filename: \'\'\r\nTraceback (most recent call last):\r\n  File "C:\\salt\\salt-0.16.0.win32\\salt-0.16.0-py2.7.egg\\salt\\modules\\win_pkg.py"\r\n, line 671, in get_repo_data\r\n    with salt.utils.fopen(cached_repo, \'r\') as repofile:\r\n  File "C:\\salt\\salt-0.16.0.win32\\salt-0.16.0-py2.7.egg\\salt\\utils\\__init__.py",\r\n line 822, in fopen\r\n    fhandle = open(*args, **kwargs)\r\nIOError: [Errno 22] invalid mode (\'r\') or filename: \'\'\r\n\r\nHow may I configure my windows minion to use its local filesystem as its repo for packages and whatnot?  \r\n\r\nShould I even try to use saltstack for my all-windows server environment?'
6179,'terminalmage',"a salt-minion deamon reboot should not be needed to make mysql config loaded\nI think it's not a good idea that the mysql module use the grains file of the minion to load the configuration. It implies a reboot of the minion to take any configuration changes into account.\r\n\r\nMaybe add a `load_config` function into the mysql states or add some documentation about the mysql module should be a good idea.\r\n\r\n "
6177,'terminalmage','cmd full path support\nWhy I can not call a full path command while using cwd?\r\n\r\nI have to type ../env/bin/activate instead of /var/www/bin/activate\r\n\r\nIf I input full path, it adds up to cwd and results: /var/www/myproject/var/www/bin/activate\r\n\r\n```\r\nI would like to have:\r\ndjango-migrate:\r\n    cmd.run:\r\n        - user: vagrant\r\n        - name: ". /var/www/env/bin/activate && python manage.py syncdb --noinput"\r\n        - cwd: /var/www/myproject/\r\n```\r\n\r\nBut only this works:\r\n```\r\ndjango-migrate:\r\n    cmd.run:\r\n        - user: vagrant\r\n        - name: ". ../env/bin/activate && python manage.py syncdb --noinput"\r\n        - cwd: /var/www/myproject/\r\n```\r\n\r\n\r\nPS: I don\'t want to use full path for manage.py btw\r\n\r\n\r\n'
6175,'terminalmage','AttributeError: \'NoneType\' object has no attribute \'group\'\nI use 0.15.3 for both minion and master, and I want to do some benchmark, so I install sysbench(0.4.12-5.el6) in minion. When I exec in the master it says:\r\n<pre>\r\n # salt \'xxx\' sysbench.cpu\r\nxxx:\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.6/site-packages/salt/minion.py", line 443, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "/usr/lib/python2.6/site-packages/salt/modules/sysbench.py", line 78, in cpu\r\n        ret_val[key] = _parser(result)\r\n      File "/usr/lib/python2.6/site-packages/salt/modules/sysbench.py", line 37, in _parser\r\n        total_execution = re.search(_total_execution, result).group(1)\r\n    AttributeError: \'NoneType\' object has no attribute \'group\'\r\n</pre>'
6165,'basepi',"[ERROR   ] stderr: useradd: group larry exists - if you want to add this user to that group, use -g.\nmy salt-minion  and salt-master are all the same version:\r\n<pre>\r\n# salt-minion  --version\r\nsalt-minion 0.15.3\r\n\r\n# salt-master  --version\r\nsalt-master 0.15.3\r\n</pre>\r\n\r\nWhen I create the three users curly, larry, moe according the manual:\r\nhttp://docs.saltstack.com/topics/tutorials/states_pt3.html\r\n\r\n<pre>\r\n{% for usr in 'moe','larry','curly' %}\r\n{{ usr }}:\r\n  group:\r\n    - present\r\n  user:\r\n    - present\r\n    - gid: {{ salt['file.group_to_gid'](usr) }}\r\n    - require:\r\n      - group: {{ usr }}\r\n{% endfor %}\r\n</pre>\r\n\r\nThen I exec the state file, the master says:\r\n<pre>\r\n----------\r\n    State: - group\r\n    Name:      curly\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Added group curly\r\n        Changes:   passwd: x\r\n                   gid: 503\r\n                   name: curly\r\n                   members: []\r\n                   \r\n----------\r\n    State: - group\r\n    Name:      larry\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Added group larry\r\n        Changes:   passwd: x\r\n                   gid: 504\r\n                   name: larry\r\n                   members: []\r\n                   \r\n----------\r\n    State: - group\r\n    Name:      moe\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Added group moe\r\n        Changes:   passwd: x\r\n                   gid: 505\r\n                   name: moe\r\n                   members: []\r\n                   \r\n----------\r\n    State: - user\r\n    Name:      curly\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   Failed to create new user curly\r\n        Changes:   \r\n----------\r\n    State: - user\r\n    Name:      larry\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   Failed to create new user larry\r\n        Changes:   \r\n----------\r\n    State: - user\r\n    Name:      moe\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   Failed to create new user moe\r\n        Changes:   \r\n</pre>\r\n\r\nAnd the debug info of minion shows:\r\n<pre>\r\n[INFO    ] Executing state user.present for curly\r\n[INFO    ] Executing command 'useradd -m curly' in directory '/root'\r\n[ERROR   ] Command 'useradd -m curly' failed with return code: 9\r\n[ERROR   ] stderr: useradd: group curly exists - if you want to add this user to that group, use -g.\r\n[ERROR   ] No changes made for curly\r\n[INFO    ] Executing state user.present for larry\r\n[INFO    ] Executing command 'useradd -m larry' in directory '/root'\r\n[ERROR   ] Command 'useradd -m larry' failed with return code: 9\r\n[ERROR   ] stderr: useradd: group larry exists - if you want to add this user to that group, use -g.\r\n[ERROR   ] No changes made for larry\r\n[INFO    ] Executing state user.present for moe\r\n[INFO    ] Executing command 'useradd -m moe' in directory '/root'\r\n[ERROR   ] Command 'useradd -m moe' failed with return code: 9\r\n[ERROR   ] stderr: useradd: group moe exists - if you want to add this user to that group, use -g.\r\n</pre>\r\n\r\nWhen I exec the ` salt 'xxx' state.highstate` again, it goes well again.\r\nThe master says:\r\n<pre>\r\n----------\r\n    State: - group\r\n    Name:      curly\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Group curly is already present\r\n        Changes:   \r\n----------\r\n    State: - group\r\n    Name:      larry\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Group larry is already present\r\n        Changes:   \r\n----------\r\n    State: - group\r\n    Name:      moe\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   Group moe is already present\r\n        Changes:   \r\n----------\r\n    State: - user\r\n    Name:      curly\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   New user curly created\r\n        Changes:   shell: /bin/bash\r\n                   workphone: \r\n                   uid: 502\r\n                   passwd: x\r\n                   roomnumber: \r\n                   gid: 503\r\n                   groups: ['curly']\r\n                   home: /home/curly\r\n                   fullname: \r\n                   homephone: \r\n                   name: curly\r\n                   \r\n----------\r\n    State: - user\r\n    Name:      larry\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   New user larry created\r\n        Changes:   shell: /bin/bash\r\n                   workphone: \r\n                   uid: 503\r\n                   passwd: x\r\n                   roomnumber: \r\n                   gid: 504\r\n                   groups: ['larry']\r\n                   home: /home/larry\r\n                   fullname: \r\n                   homephone: \r\n                   name: larry\r\n                   \r\n----------\r\n    State: - user\r\n    Name:      moe\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   New user moe created\r\n        Changes:   shell: /bin/bash\r\n                   workphone: \r\n                   uid: 504\r\n                   passwd: x\r\n                   roomnumber: \r\n                   gid: 505\r\n                   groups: ['moe']\r\n                   home: /home/moe\r\n                   fullname: \r\n                   homephone: \r\n                   name: moe\r\n</pre>\r\n\r\nAnd the minion says:\r\n<pre>\r\n[INFO    ] Executing state user.present for curly\r\n[INFO    ] Executing command 'useradd -g 503 -m curly' in directory '/root'\r\n[INFO    ] {'shell': '/bin/bash', 'workphone': '', 'uid': 502, 'passwd': 'x', 'roomnumber': '', 'groups': ['curly'], 'home': '/home/curly', 'name': 'curly', 'gid': 503, 'fullname': '', 'homephone': ''}\r\n[INFO    ] Executing state user.present for larry\r\n[INFO    ] Executing command 'useradd -g 504 -m larry' in directory '/root'\r\n[INFO    ] {'shell': '/bin/bash', 'workphone': '', 'uid': 503, 'passwd': 'x', 'roomnumber': '', 'groups': ['larry'], 'home': '/home/larry', 'name': 'larry', 'gid': 504, 'fullname': '', 'homephone': ''}\r\n[INFO    ] Executing state user.present for moe\r\n[INFO    ] Executing command 'useradd -g 505 -m moe' in directory '/root'\r\n[INFO    ] {'shell': '/bin/bash', 'workphone': '', 'uid': 504, 'passwd': 'x', 'roomnumber': '', 'groups': ['moe'], 'home': '/home/moe', 'name': 'moe', 'gid': 505, 'fullname': '', 'homephone': ''}\r\n[INFO    ] Returning information for job: 20130715222110438308\r\n</pre>\r\n"
6162,'terminalmage','[patch] alternatives.show_current and alternatives.check_installed may return wrong data\nalternatives.show_current and alternatives.check_installed use os.path.realpath, which will normally recursively follow symlinks until something that isn\'t a symlink is found. With that in mind, take this state example:\r\n\r\n<pre>\r\nemacs23-nox:\r\n  pkg.installed\r\n\r\n{% if salt[\'alternatives.show_current\'](\'editor\') != \'/usr/bin/emacs23\' %}\r\nupdate-editor:\r\n  cmd.run:\r\n    - name: update-alternatives --set editor /usr/bin/emacs23\r\n    - require:\r\n      - pkg: emacs23-nox\r\n{% endif %}\r\n</pre>\r\n\r\nWe should update this state to to use the new built-in alternatives module, but regardless this should work. However, here\'s the thing:\r\n\r\n<pre>\r\n$ python -c \'import os ; print os.path.realpath("/etc/alternatives/editor")\'\r\n/usr/bin/emacs23-nox\r\n$ python -c \'import os ; print os.readlink("/etc/alternatives/editor")\'\r\n/usr/bin/emacs23\r\n$ update-alternatives --list editor | grep emacs\r\n/usr/bin/emacs23\r\n$ \r\n</pre>\r\n\r\nThe salt module will return /usr/bin/emacs23-nox (since that\'s the binary), but there is no actual "alternative" option by that name - it has to be /usr/bin/emacs23 to be valid.\r\n\r\nAs such, my state example will always run the update-alternatives command even when it is set correctly, causing my state.highstate runs to incorrectly indicate that things have changed unexpectedly.'
6136,'terminalmage','Documentation: incorrect description of freebsdpkg.refresh_db\nhttps://salt.readthedocs.org/en/latest/ref/modules/all/salt.modules.freebsdpkg.html#salt.modules.freebsdpkg.refresh_db\r\n\r\nThe description for freebsdpkg.refresh_db reads "Use pkg update to get latest repo.txz when using pkgng, else update the ports tree with portsnap otherwise. If the ports tree does not exist it will be downloaded and set up", which comes from the docstring in freebsdpkg.py.\r\n\r\nHowever, the code in freebsdpkg.py does not actually do anything with portsnap, and supports pkgng only.\r\n\r\nThis is probably non-trivial to fix, because "portsnap fetch" actually checks if it is running on a TTY and will refuse to run in other cases, suggesting to use "portsnap cron" (which introduces a random delay) to avoid overloading the servers. I am currently using a patched version of portsnap from cmd.run for this reason.\r\n\r\nPending a better solution for actually invoking portsnap, the docstring should be corrected.'
6127,'terminalmage','Documentation : Inconsistent methods between grains and pillar module\nhttp://docs.saltstack.com/ref/modules/all/salt.modules.pillar.html\r\nhttp://docs.saltstack.com/ref/modules/all/salt.modules.grains.html\r\n\r\nThese modules interface are inconsistent and they make learning Salt a little bit harder (when you are a newbie). I know that there\'s a static/dynamic difference but could it be merge under the same interface?\r\n\r\npillar.items is not documented but working\r\ngrains.item seem more like pillar.get?\r\n\r\nCan we do something like "grains.item os osrelease oscodename" with pillar?\r\n'
6118,'basepi','File state removes sources files if you have file_root in /tmp\nWhen using file states if your source files happen to be located in a subdirectory of  tempfile.gettempdir() then salt will delete them after running the state.\r\n\r\nHere is an example:\r\n\r\n    # Create some directories\r\n    $ mkdir -p /tmp/scripts/salt/{conf,states}\r\n\r\n    # Our conf file\r\n    $ echo -e "file_roots:\\n  base:\\n    -  /tmp/scripts/salt/states" > /tmp/scripts/salt/conf/minion\r\n\r\n    # Our source file\r\n    echo dont_remove_me > /tmp/scripts/salt/states/myfile\r\n\r\n    # Run salt\r\n    $ salt-call --local -c /tmp/scripts/salt/conf state.single file.managed name=/tmp/test source=salt://myfile\r\n\r\n    # Look at your missing file:\r\n    $ cat /tmp/scripts/salt/states/myfile\r\n    cat: /tmp/scripts/salt/states/myfile: No such file or directory\r\n\r\nI dug into the could and it looks like salt.modules.file.__clean_tmp is the culprit:  \r\n\r\n```python\r\ndef __clean_tmp(sfn):\r\n\'\'\'\r\nClean out a template temp file\r\n\'\'\'\r\nif sfn.startswith(tempfile.gettempdir()):\r\n    # Only clean up files that exist\r\n    if os.path.exists(sfn):\r\n        os.remove(sfn)\r\n```\r\n\r\nThe salt.modules.file.manage_file function calls on __clean_tmp any source file it\'s given.  It should really only try to clean *actual* temporary files.  A stricter check would probably be good enough.\r\n\r\nVersion information:\r\n\r\n    Ubuntu 12.04 - 32 bit (inside a chroot)\r\n\r\n               Salt: 0.16.0\r\n             Python: 2.7.3 (default, Aug  1 2012, 05:16:07)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.4.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.0.0\r\n                ZMQ: 3.2.2\r\n\r\n'
6116,'UtahDave','File > Manage seems to be broken.\nRunning an Ubuntu Master and Windows2008R2 Minion, and during the execution of the file - managed, the error "NameError: global name \'_binary_replace\' is not defined" is returned.\r\n\r\n(http://pastebin.com/REDLpwAR) \r\n\r\n```\r\n--- Example .sls entry\r\n# Configure the jmx-console-users.properties\r\n/jboss-eap-5.1/jboss-as/server/default/conf/props/jmx-console-users.properties:\r\n  file:\r\n    - managed\r\n    - template: jinja\r\n    - name: \'c:/jboss-eap-5.1/jboss-as/server/default/conf/props/jmx-console-users.properties\'\r\n    - source: \'salt://win/repo/jboss-as/jmx-console-users.properties.jinja2\'\r\n    - require:\r\n      - cmd: \'extract-jboss-as\'\r\n```\r\n\r\n```\r\n---Error Message\r\n    State: - file\r\n    Name:      c:/jboss-eap-5.1/jboss-as/server/default/conf/props/jmx-console-users.properties\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\state.py", line 1237, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\states\\file.py", line 868, in managed\r\n    contents)\r\n  File "c:\\salt\\python27\\lib\\site-packages\\salt\\modules\\file.py", line 1600, in manage_file\r\n    bdiff = _binary_replace(name, sfn)\r\nNameError: global name \'_binary_replace\' is not defined\r\n```\r\n'
6105,'UtahDave','Windows Software Repo\nI have a 0.16.0 master running on Ubuntu 12.04, and a 0.16.0 minion running on Windows 2008 Server R2, installed with the Windows installer.  I followed the documentation in setting up a Windows software repo.  When I issue the command salt \'minion\' pkg.install package, I get the following:\r\n\r\nTraceback (most recent call last):\r\n  FIle "salt/minion.py", line 626, in _thread_return\r\n  File "salt/modules/win_pkg.py", line 477, in install\r\n  File "salt/modules/win_pkg.py", line 240, in list_pkgs\r\n  File "salt/modules/win_pkg.py", line 324, in _get_reg_software\r\nAttributeError: \'module\' object has no attribute \'getpreferredencoding\'\r\n'
6102,'terminalmage',"Multiple environments support documentation\nAs a new user to Salt, I have found the [top file documentation](http://docs.saltstack.com/ref/states/top.html) to be a little confusing in terms of multiple environments best practices. I am submitting this issue in the spirit of improving documentation (if necessary).\r\n\r\nGiven this setup:\r\n\r\n```\r\nfile_roots:\r\n  base:\r\n    - /srv/salt/base\r\n  dev:\r\n    - /srv/salt/dev\r\n  qa:\r\n    - /srv/salt/qa\r\n  prod:\r\n    - /srv/salt/prod\r\n```\r\n\r\nAs far as I understood, a single `top.sls` file is generated from aggregating each file root. This means that each file root should include its own `top.sls` file. As such, the following files are expected:\r\n\r\n* /srv/salt/base/top.sls\r\n\r\n```\r\nbase:\r\n  '*':\r\n    - global\r\n```\r\n\r\n* /srv/salt/dev/top.sls\r\n\r\n```\r\ndev:\r\n  'webserver*dev*':\r\n    - webserver\r\n  'db*dev*':\r\n    - db\r\n```\r\n\r\n* /srv/salt/qa/top.sls\r\n\r\n```\r\nqa:\r\n  'webserver*qa*':\r\n    - webserver\r\n  'db*qa*':\r\n    - db\r\n```\r\n\r\n* /srv/salt/prod/top.sls\r\n\r\n```\r\nprod:\r\n  'webserver*prod*':\r\n    - webserver\r\n  'db*prod*':\r\n    - db\r\n````\r\n\r\nInitially I thought this content should be placed inside a single `top.sls` under `/srv/salt`, but since it's not listed as a file root in this setup, it makes sense that Salt does not pick it up. Is this assumption correct?\r\n\r\nAdditionally, it would be very helpful to have a `tree` output next to each example. If anyone can contribute that, I would be truly thankful."
6097,'terminalmage','`pkg.installed` state always evaluates to `error`\nI have the following state:\r\n\r\n```\r\nlibxslt-dev:\r\n  pkg:\r\n    - installed\r\n```\r\n\r\nWhen running `state.highstate` I get the following:\r\n\r\n```\r\n2013-07-10 16:36:40,245 [salt.state       ][INFO    ] Executing state pkg.installed for libxslt-dev\r\n2013-07-10 16:36:40,249 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'apt-get -q update\' in directory \'/home/saltuser\'\r\n2013-07-10 16:36:51,596 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'apt-get -q -y  -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-c\r\nonfdef   install libxslt-dev\' in directory \'/home/saltuser\'\r\n2013-07-10 16:36:52,669 [salt.loaded.int.module.cmdmod][INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'\r\n/home/saltuser\'\r\n2013-07-10 16:36:52,748 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'grep-available -F Provides -s Package,Provides -e "^.+$"\' in directory \'/home/\r\nsaltuser\'\r\n2013-07-10 16:36:52,779 [salt.state       ][ERROR   ] No changes made for libxslt-dev\r\n```\r\n\r\nVersions report:\r\n```\r\n           Salt: 0.16.0\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```'
6090,'terminalmage','Cannot install anything?!\nsalt-call 0.16.0-748-g74a834d\r\n\r\n````\r\n\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      sudo\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: sudo.\r\n        Changes:   : { new : 0.0.7,0.02-5,0.04-2+b1,0.04-7+b1,0.06-7,0.08-2,0.1.10-1,0.1.21,0.1.4-2,0.109.1,0.12-3,0             .12.0-4,0.13+nmu2,0.13.7,0.14-2,0.140-5,0.15-1,0.17-1,0.17-27,0.17-36,0.18.1.1-9,0.18.1.1-9,0.2.0-8,0.2.2-5,0.21.1-2             ,0.23.1-1,0.25-4,0.36-3,0.4-2,0.4.1+nmu1,0.4.1+nmu1,0.4.2-1,0.5.16-3.4,0.5.3-8,0.5.7-3,0.51.0-1,0.52.14-11.1,0.52.14             -11.1,0.6.24-1,0.6.24-1,0.6.8.2-1,0.6.8.2-1,0.60.7~20110707-1,0.60.7~20110707-1,0.7.2-5,0.7.8,0.8.8.2,0.8.8.2,0.9-4,             0.9.7.9,0.9.7.9,0.9.7.9,0.9.7.9,1.0.0-1.1,1.0.1-1,1.0.15,1.0.1e-2,1.0.1e-2,1.0.4-3,1.0.6-4,1.0.6-4,1.0.9,1.05-7+b1,1             .06.95-2+b1,1.06.95-2+b1,1.09-5,1.09-5,1.1-3,1.1.3-7.1,1.1.3-7.1,1.1.3-7.1,1.1.3-7.1,1.10-3.1,1.10-40,1.10.1+dfsg-5+             deb7u1,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+deb7u1,1.11,1.12.11,1.13.4-3,1.1             4.0-5,1.15.3-9,1.16-7,1.16.10,1.16.10,1.16.10,1.18.4-2,1.19.02-2,1.2.0-1.4,1.2.1-1,1.2.12-2,1.20.4-6,1.2000-1,1.21-9             ,1.25-2,1.26+dfsg-0.1,1.26+nmu4,1.3.3-17,1.4.12-7,1.4.12-7,1.4.14-3.1,1.4.16-3,1.4.2-1.1,1.4.47-2,1.4.8+dfsg-3,1.42.             5-1.1,1.42.5-1.1,1.42.5-1.1,1.42.5-1.1,1.49.0-3.2,1.5-1.1,1.5.0-5,1.5.21-6.2,1.5.49,1.5.49,1.5.5-3,1.58,1.6-1,1.60-2             4.2,1.7-24,1.7-5,1.8.1-2+deb7u1,1.8.3-11,1.8.5p2-1+nmu1,1.88,1.88,1.88,1.99-27+deb7u1,1.99-27+deb7u1,1.99-27+deb7u1,             1.99-27+deb7u1,11.5,13.1.0-1~bpo70~dst+1,175-7.2,175-7.2,1:005-3,1:1.0.7-1,1:1.0.7-1,1:1.1.1-1,1:1.2.6-4,1:1.2.7.dfs             g-13,1:1.20.0-7,1:1.7.10.4-1+wheezy1,1:1.7.10.4-1+wheezy1,1:2.0-1,1:2.0.16-1+deb7u1,1:2.0.18-3,1:2.20.1-5.3,1:2.22-1             .2,1:2.4.46-8,1:3.1.9-6,1:3.1.9-6,1:3.2-6,1:3.3.3-3,1:3.3.3-3,1:4.1.5.1-1,1:4.1.5.1-1,1:4.7.2-5,1:6.0p1-4,1:6.0p1-4,             1:7.1-9.1,1:8.30-5,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8             .4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,2.0.1-             2,2.0.1-3.1,2.0.1-3.1,2.0.19-stable-3,2.0.7-16,2.1.0-1,2.1.2-5.2,2.1.2-5.2,2.1.25.dfsg1-6+deb7u1,2.1.25.dfsg1-6+deb7             u1,2.1.4-3,2.1.5+deb1+cvs20081104-13,2.1.6-6,2.1.6-6,2.1.9-5,2.11+dfsg-0.1,2.11-20080614-5,2.11-9,2.12-2,2.12.20-7,2             .13-2,2.13-38,2.13-38,2.13-38,2.13-38,2.13-38,2.13-38,2.16-2,2.2.10-0.2,2.2.4-15,2.2.51-8,2.2.6-1+b1,2.20.1-5.3,2.20             .1-5.3,2.20.1-5.3,2.20.1-5.3,2.20.1-5.3,2.20.1-5.3,2.2010.10.18,2.22-8,2.4+20111222.git4e06e21-1,2.4.31-1+nmu2,2.4.9             -1.1,2.49,2.5.1-3,2.6-1,2.6-4,2.6.1-3,2.6.2-1,2.6.8-1.1,2.6.8-1.1,2.7.3-4,2.7.3-4,2.7.3-6,2.7.3-6,2.8.0+dfsg1-7+nmu1             ,2.85.11,2.88dsf-41,2.88dsf-41,2.88dsf-41,2.88dsf-41,2.9.0-2+deb7u1,2012.4,20120521-3+b3,20130119,20130213-1,2013c-0             wheezy1,22.19-1+deb7u1,2:0.1.12-20+nmu1,2:1.0.11-1,2:1.02.74-7,2:1.02.74-7,2:1.1.1-1,2:1.3.1-2+deb7u1,2:1.5.0-1+deb7             u1,2:1.5.0-1+deb7u1,2:5.0.5+dfsg-2,2:7.3.547-7,2:7.3.547-7,2:7.3.547-7,2:7.3.547-7,3.0.9-4,3.0025+nmu3,3.0pl1-124,3.             1.0-5,3.1.13-2,3.10-4,3.11,3.113+nmu3,3.14.1,3.14.1,3.14.1,3.2+46,3.2.3+dfsg-1~bpo70~dst+1,3.2.46-1,3.2.46-1,3.22-20             ,3.41-1,3.44-1,3.44-1,3.5,3.5.26,3.52-1,3.7.13-1+deb7u1,3.8.1-4,3.81-8.2,3:20101006-1+b1,4.1+Debian8+deb7u1,4.1+Debi             an8+deb7u1,4.13a.dfsg.1-10,4.13a.dfsg.1-10,4.13a.dfsg.1-10,4.2+dfsg-0.1,4.2.1-10,4.2.2.dfsg.1-5+deb70u6,4.2.2.dfsg.1             -5+deb70u6,4.3.2,4.4.2-4,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.80-7,4.80-7,4.80-             7,4.80-7,4.86+dfsg-1,444-4,4:4.7.2-1,4:4.7.2-1,4:4.7.2-1,5.0,5.0.1,5.0.23,5.1.118-1~dfsg-0.1,5.1.1alpha+20120614-2,5             .1.1alpha+20120614-2,5.1.29-5,5.1.29-5,5.11-2,5.11-2,5.14.2-21,5.14.2-21,5.14.2-21,5.8.11-3,5.9-10,5.9-10,5.9-10,5.9             -10,5.9-10,5.9-10,6.1,6.2+dfsg-0.1,6.2+dfsg-0.1,6.4.4,6.4.4,7.1-1,7.1wheezy1,7.26.0-1+wheezy3,7.26.0-1+wheezy3,7.26.             0-1+wheezy3,7.6.q-24,7.6.q-24,8.1.2-0.20111106cvs-1,8.13-3.5,9-3,9-3,9-3,9.0.3\r\nold : 0.0.7,0.02-5,0.04-2+b1,0.04-7+b1,0.06-7,0.08-2,0.1.10-1,0.1.21,0.1.4-2,0.109.1,0.12-3,0.12.0-4,0.13+nmu2,0.13.             7,0.14-2,0.140-5,0.15-1,0.17-1,0.17-27,0.17-36,0.18.1.1-9,0.18.1.1-9,0.2.0-8,0.2.2-5,0.21.1-2,0.23.1-1,0.25-4,0.36-3             ,0.4-2,0.4.1+nmu1,0.4.1+nmu1,0.4.2-1,0.5.16-3.4,0.5.3-8,0.5.7-3,0.51.0-1,0.52.14-11.1,0.52.14-11.1,0.6.24-1,0.6.24-1             ,0.6.8.2-1,0.6.8.2-1,0.60.7~20110707-1,0.60.7~20110707-1,0.7.2-5,0.7.8,0.8.8.2,0.8.8.2,0.9-4,0.9.7.9,0.9.7.9,0.9.7.9             ,0.9.7.9,1.0.0-1.1,1.0.1-1,1.0.15,1.0.1e-2,1.0.1e-2,1.0.4-3,1.0.6-4,1.0.6-4,1.0.9,1.05-7+b1,1.06.95-2+b1,1.06.95-2+b             1,1.09-5,1.09-5,1.1-3,1.1.3-7.1,1.1.3-7.1,1.1.3-7.1,1.1.3-7.1,1.10-3.1,1.10-40,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+de             b7u1,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+deb7u1,1.10.1+dfsg-5+deb7u1,1.11,1.12.11,1.13.4-3,1.14.0-5,1.15.3-9,1.16-7,1             .16.10,1.16.10,1.16.10,1.18.4-2,1.19.02-2,1.2.0-1.4,1.2.1-1,1.2.12-2,1.20.4-6,1.2000-1,1.21-9,1.25-2,1.26+dfsg-0.1,1             .26+nmu4,1.3.3-17,1.4.12-7,1.4.12-7,1.4.14-3.1,1.4.16-3,1.4.2-1.1,1.4.47-2,1.4.8+dfsg-3,1.42.5-1.1,1.42.5-1.1,1.42.5             -1.1,1.42.5-1.1,1.49.0-3.2,1.5-1.1,1.5.0-5,1.5.21-6.2,1.5.49,1.5.49,1.5.5-3,1.58,1.6-1,1.60-24.2,1.7-24,1.7-5,1.8.1-             2+deb7u1,1.8.3-11,1.88,1.88,1.88,1.99-27+deb7u1,1.99-27+deb7u1,1.99-27+deb7u1,1.99-27+deb7u1,11.5,13.1.0-1~bpo70~dst             +1,175-7.2,175-7.2,1:005-3,1:1.0.7-1,1:1.0.7-1,1:1.1.1-1,1:1.2.6-4,1:1.2.7.dfsg-13,1:1.20.0-7,1:1.7.10.4-1+wheezy1,1             :1.7.10.4-1+wheezy1,1:2.0-1,1:2.0.16-1+deb7u1,1:2.0.18-3,1:2.20.1-5.3,1:2.22-1.2,1:2.4.46-8,1:3.1.9-6,1:3.1.9-6,1:3.             2-6,1:3.3.3-3,1:3.3.3-3,1:4.1.5.1-1,1:4.1.5.1-1,1:4.7.2-5,1:6.0p1-4,1:6.0p1-4,1:7.1-9.1,1:8.30-5,1:9.8.4.dfsg.P1-6+n             mu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nm             u2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,1:9.8.4.dfsg.P1-6+nmu2,2.0.1-2,2.0.1-3.1,2.0.1-3.1,2.0.19-stable-3,             2.0.7-16,2.1.0-1,2.1.2-5.2,2.1.2-5.2,2.1.25.dfsg1-6+deb7u1,2.1.25.dfsg1-6+deb7u1,2.1.4-3,2.1.5+deb1+cvs20081104-13,2             .1.6-6,2.1.6-6,2.1.9-5,2.11+dfsg-0.1,2.11-20080614-5,2.11-9,2.12-2,2.12.20-7,2.13-2,2.13-38,2.13-38,2.13-38,2.13-38,             2.13-38,2.13-38,2.16-2,2.2.10-0.2,2.2.4-15,2.2.51-8,2.2.6-1+b1,2.20.1-5.3,2.20.1-5.3,2.20.1-5.3,2.20.1-5.3,2.20.1-5.             3,2.20.1-5.3,2.2010.10.18,2.22-8,2.4+20111222.git4e06e21-1,2.4.31-1+nmu2,2.4.9-1.1,2.49,2.5.1-3,2.6-1,2.6-4,2.6.1-3,             2.6.2-1,2.6.8-1.1,2.6.8-1.1,2.7.3-4,2.7.3-4,2.7.3-6,2.7.3-6,2.8.0+dfsg1-7+nmu1,2.85.11,2.88dsf-41,2.88dsf-41,2.88dsf             -41,2.88dsf-41,2.9.0-2+deb7u1,2012.4,20120521-3+b3,20130119,20130213-1,2013c-0wheezy1,22.19-1+deb7u1,2:0.1.12-20+nmu             1,2:1.0.11-1,2:1.02.74-7,2:1.02.74-7,2:1.1.1-1,2:1.3.1-2+deb7u1,2:1.5.0-1+deb7u1,2:1.5.0-1+deb7u1,2:5.0.5+dfsg-2,2:7             .3.547-7,2:7.3.547-7,2:7.3.547-7,2:7.3.547-7,3.0.9-4,3.0025+nmu3,3.0pl1-124,3.1.0-5,3.1.13-2,3.10-4,3.11,3.113+nmu3,             3.14.1,3.14.1,3.14.1,3.2+46,3.2.3+dfsg-1~bpo70~dst+1,3.2.46-1,3.2.46-1,3.22-20,3.41-1,3.44-1,3.44-1,3.5,3.5.26,3.52-             1,3.7.13-1+deb7u1,3.8.1-4,3.81-8.2,3:20101006-1+b1,4.1+Debian8+deb7u1,4.1+Debian8+deb7u1,4.13a.dfsg.1-10,4.13a.dfsg.             1-10,4.13a.dfsg.1-10,4.2+dfsg-0.1,4.2.1-10,4.2.2.dfsg.1-5+deb70u6,4.2.2.dfsg.1-5+deb70u6,4.3.2,4.4.2-4,4.7.2-5,4.7.2             -5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.7.2-5,4.80-7,4.80-7,4.80-7,4.80-7,4.86+dfsg-1,444-4,4:4.7.2-1,4             :4.7.2-1,4:4.7.2-1,5.0,5.0.1,5.0.23,5.1.118-1~dfsg-0.1,5.1.1alpha+20120614-2,5.1.1alpha+20120614-2,5.1.29-5,5.1.29-5             ,5.11-2,5.11-2,5.14.2-21,5.14.2-21,5.14.2-21,5.8.11-3,5.9-10,5.9-10,5.9-10,5.9-10,5.9-10,5.9-10,6.1,6.2+dfsg-0.1,6.2             +dfsg-0.1,6.4.4,6.4.4,7.1-1,7.1wheezy1,7.26.0-1+wheezy3,7.26.0-1+wheezy3,7.26.0-1+wheezy3,7.6.q-24,7.6.q-24,8.1.2-0.             20111106cvs-1,8.13-3.5,9-3,9-3,9-3,9.0.3\r\n}\r\n````\r\n\r\n````\r\n[INFO    ] Fetching file \'salt://users.sls\'\r\n[INFO    ] Fetching file \'salt://users/init.sls\'\r\n[INFO    ] Executing state pkg.installed for sudo\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/root\'\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/root\'\r\n[INFO    ] Executing command \'apt-get -q -y  -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef                install sudo\' in directory \'/root\'\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/root\'\r\n[ERROR   ] Installed Packages:\r\n changed from 0.0.7,0.02-5,0.04-2+b1,0.04-7+b1,0.06-7,0.08-2,0.1.10-1,0.1.21,0.1.4-2,0.109.1,0.12-3,0.12.0-4,0.13+nm     \r\n````\r\n\r\nTo be honest I have no idea what is wrong. \r\n\r\nOn ` 3.2.0-4-amd64 #1 SMP Debian 3.2.41-2+deb7u2 x86_64 GNU/Linux`, installed \r\nusing dev version of salt `wget -O - https://github.com/saltstack/salt-bootstrap/raw/develop/bootstrap-salt.sh | sh -s -- git develop`, it works fine on other machines, that had been instaled previously... '
6087,'terminalmage','groupadd fails when group is already present\nSeveral packages manage users/groups in order to run their daemons (for instance, mysql). However, this is not guaranteed and in the end it\'s up to the administrator to decide whether he would like to run the package with the default user/group that the package was bundled with.\r\n\r\nIn this spirit, I am guaranteeing that a given user/group is present on the machine to run that daemon. Nonetheless, if a package has already added that group, then `groupadd` fails with "group \'<group>\' already exists".\r\n\r\nThis produces an error when executing the state, which makes other dependent states fail too. My question is whether this behaviour should silently report as "no changes required" or not.\r\n\r\nSample output:\r\n\r\n```\r\n[salt.loaded.int.module.cmdmod][ERROR   ] Command \'groupadd mysql\' failed with return code:\r\n[salt.loaded.int.module.cmdmod][ERROR   ] stderr: groupadd: group \'mysql\' already exists\r\n```\r\n\r\nThanks for the clarifications.'
6086,'thatch45',"Conditional prereq\nWhen updating some log file paths, sometimes it is useful to add a `prereq` that gracefully shuts down the service before attempting to do so. I believe this was one of the original scopes for the `prereq` feature.\r\n\r\nThe `prereq` works correctly in these situations. However, let's say the old log file is managed by `file.absent`:\r\n\r\n```YAML\r\nservice-remove-old-log-file:\r\n  file:\r\n    - absent\r\n    - name: /var/log/service/old.log\r\n    - prereq:\r\n      - cmd: service-stop\r\n```\r\n\r\nOn repeated runs, the file is already absent (the state `service-remove-old-log-file` returns no changes), yet I observe that the `service-stop` command is still executed. While this is actually the intentional behaviour of `prereq`, wouldn't it be interesting to pass an option on whether the `prereq` should preemptively test if the `service-remove-old-log-file` state was going to require changes at all?"
6085,'terminalmage',"user.present: can't set empty password?\nPre-0.16, I used to be able to do:\r\n\r\n```\r\nuser-{{ user }}:\r\n  user.present:\r\n    - name: {{ user }}\r\n    - home: {{ args.get('homedir','/home/' + user) }}\r\n    - shell: {{ args.get('shell','/bin/bash') }}\r\n    - uid: {{ args['uid'] }}\r\n    - gid: {{ args['gid'] }}\r\n    - fullname: {{ args['fullname'] }}\r\n    - password: {{ args.get('password','!') }}\r\n```\r\n(note the '!' after password).\r\n\r\nHowever, this now failed with \r\n```\r\n----------\r\n        Comment:   These values could not be changed: {'passwd': '!'}\r\n```\r\n\r\nHow do I set/enforce an empty password?\r\n"
6082,'terminalmage','Module function augeas.setvalue is not available\nUsing salt 0.16.0, when running `state.highstate`, `python-augeas` is not picked up by salt when applying states.\r\n\r\n```YAML\r\npython-augeas:\r\n  pkg:\r\n    - installed\r\n\r\nlib-ini:\r\n  module:\r\n    - run\r\n    - name: augeas.setvalue\r\n    - args:\r\n      - /files/etc/lib/file.ini/some_setting\r\n      - "25"\r\n    - require:\r\n      - pkg: lib\r\n      - pkg: python-augeas\r\n ```\r\nThe output is always like this:\r\n\r\n```\r\n----------\r\n    State: - module\r\n    Name:      augeas.setvalue\r\n    Function:  run\r\n        Result:    False\r\n        Comment:   Module function augeas.setvalue is not available\r\n        Changes:\r\n```\r\n\r\nI have attempted to work around this problem by restarting the minion using the service state, but the issue remains present.\r\n\r\n```\r\nlib-salt-minion-restart:\r\n  service:\r\n    - running\r\n    - name: salt-minion\r\n    - watch:\r\n      - pkg: python-augeas\r\n```\r\n\r\nShouldn\'t salt be able to detect `python-augeas` without even requiring a restart?'
6079,'cachedout',"mount.mounted doesn't unmount + mount on change\nWhen you switch the device of a mount point the fstab gets updated, but the mount point it self still point to the old device. At least this is true for nfs mount points were only the hostname changes."
6078,'UtahDave','AttributeError when running pkg.latest state in Windows\nHi Guys,\r\n\r\nWindows 2008 R2 64-bit, salt 0.15.3\r\nmaster: RHEL 6.4 64bit, salt 0.15.3\r\n\r\nI\'m building my windows repository now and I\'m having problem using the pkg.latest state (pkg.installed is working)\r\n\r\n### 7zip/init.sls\r\n```yaml\r\n7zip:\r\n  9.20.00.0:\r\n    installer: salt://win/repo/7zip/7z920-x64.msi\r\n    full_name: \'7-Zip 9.20 (x64 edition)\'\r\n    reboot: False\r\n    install_flags: \' /q \'\r\n    msiexec: True\r\n    uninstaller: salt://win/repo/7zip/7z920-x64.msi\r\n    uninstall_flags: \' /qn\'\r\n```\r\n\r\n```\r\nsalt win pkg.refresh_db --out json\r\n{\r\n    "win": true\r\n}\r\nsalt win pkg.available_version 7zip --out json\r\n{\r\n    "win": {\r\n        "7-Zip 9.20 (x64 edition)": ""\r\n    }\r\n}\r\n```\r\n\r\nWhen I execute the pkg.latest state on the windows minion I\'m getting the following error\r\n### Error\r\n```\r\nsalt win state.single pkg.latest name=\'7zip\'\r\nwin:\r\n----------\r\n    State: - pkg\r\n    Name:      7zip\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1201, in call\r\n  File "salt/states/pkg.py", line 509, in latest\r\n  File "salt/modules/win_pkg.py", line 165, in version\r\nAttributeError: \'str\' object has no attribute \'iteritems\'\r\n```\r\n\r\nThanks,\r\nRan'
6077,'terminalmage','"augeas.ls" is not available.\naugeas module doesn\'t work\r\n   \r\n    salt \'a.test.com\' augeas.ls /files/etc/passwd                   \r\n       a.test.com:\r\n          "augeas.ls" is not available.\r\n\r\nother module works fine:\r\n    \r\n    salt \'\'a.test.com\' file.file_exists /etc/passwd                   \r\n     a.test.com:\r\n         True\r\n\r\nI copy part of  augeas_cfg.py,/usr/bin/python /var/tmp/aa.py ,it works:\r\n\r\n    from augeas import Augeas\r\n    def _lstrip_word(string, prefix):\r\n        \'\'\'\r\n        Return a copy of the string after the specified prefix was removed\r\n        from the beginning of the string\r\n        \'\'\'\r\n    \r\n        if string.startswith(prefix):\r\n            return string[len(prefix):]\r\n        return string\r\n    \r\n    def ls(path):  # pylint: disable=C0103\r\n        \'\'\'\r\n        List the direct children of a node\r\n    \r\n        CLI Example::\r\n    \r\n            salt \'*\' augeas.ls /files/etc/passwd\r\n        \'\'\'\r\n        def _match(path):\r\n            \'\'\' Internal match function \'\'\'\r\n            try:\r\n                matches = aug.match(path)\r\n            except RuntimeError:\r\n                return {}\r\n    \r\n            ret = {}\r\n            for _ma in matches:\r\n                ret[_ma] = aug.get(_ma)\r\n            return ret\r\n    \r\n        aug = Augeas()\r\n    \r\n        path = path.rstrip(\'/\') + \'/\'\r\n        match_path = path + \'*\'\r\n    \r\n        matches = _match(match_path)\r\n        ret = {}\r\n    \r\n        for key, value in matches.iteritems():\r\n            name = _lstrip_word(key, path)\r\n            if _match(key + \'/*\'):\r\n                ret[name + \'/\'] = value  # has sub nodes, e.g. directory\r\n            else:\r\n                ret[name] = value\r\n        return ret\r\n    print ls(\'/files/etc/passwd\')\r\n\r\n\r\n#/usr/bin/python /var/tmp/aa.py \r\n{\'adm/\': None, \'daemon/\': None, \'rpcuser/\': None, \'nobody/\': None, \'zwcwatch/\': None, \'operator/\': None, \'sshd/\': None, \'mfs/\': None, \'mysql/\': None, \'bin/\': None, \'mqadmin/\': None, \'halt/\': None, \'postfix/\': None, \'lp/\': None, \'apache/\': None, \'vcsa/\': None, \'games/\': None, \'uucp/\': None, \'haldaemon/\': None, \'root/\': None, \'saltsync/\': None, \'gopher/\': None, \'nfsnobody/\': None, \'ftp/\': None, \'tcpdump/\': None, \'rpc/\': None, \'mail/\': None, \'nscd/\': None, \'dbus/\': None, \'www/\': None, \'abrt/\': None, \'lighttpd/\': None, \'sync/\': None, \'ldap/\': None, \'zabbix/\': None, \'ntp/\': None, \'saslauth/\': None, \'nslcd/\': None, \'shutdown/\': None}'
6057,'thatch45','Unable to "watch" a state which is a "prereq"\nIf i have a state (file.managed below) which is a prereq_in it cannot be watched by another state. The below SLS file causes infinite recursion-- which doesn\'t make a ton of sense to me. If i am watching a state it shouldn\'t matter if that state has a prereq-- just if it changed or not, unless i\'m missing something.\r\n\r\n```\r\nremove_from_lb:\r\n  cmd.run:\r\n    - name: /home/thjackso/tmp/rotation/oor.sh\r\n\r\n# Command to add host to LB\r\nadd_to_lb:\r\n    cmd.wait:\r\n        - name: /home/thjackso/tmp/rotation/ir.sh\r\n\r\n/home/thjackso/workspace/salt/atsdeploy/test/haproxy.cfg:\r\n  file.managed:\r\n    - source: {{ salt[\'pillar.get\'](\'haproxy:source\', \'http://localhost/cfg/default\') }}\r\n    - user: thjackso\r\n    - group: eng\r\n    - mode: 644\r\n    - source_hash: {{ salt[\'pillar.get\'](\'haproxy:hash\', \'md5=7822bba98f950fc9e12e1211548fd730\') }}\r\n    - prereq_in:\r\n      - cmd: remove_from_lb\r\n\r\nhaproxy:\r\n  service:\r\n    - running\r\n    # remove from LB if you have a change\r\n    - prereq_in:\r\n      - cmd: remove_from_lb\r\n    # add it back when you are done\r\n    #- watch_in:\r\n    #  - cmd: add_to_lb\r\n    - watch:\r\n      - file: /home/thjackso/workspace/salt/atsdeploy/test/haproxy.cfg\r\n```'
6056,'thatch45',"MasterHighState __init__ warnings\n```\r\n************* Module salt.state\r\nW0233:2310,8:MasterHighState.__init__: __init__ method from a non direct base class 'BaseHighState' is called\r\nW0231:2301,4:MasterHighState.__init__: __init__ method from base class 'HighState' is not called\r\n```"
6053,'terminalmage','supervisord.running Returns Errors on test=True\n`supervisord.running` returns invalid response when running with `test=True`\r\n\r\nHere\'s an example response:\r\n```\r\n\r\n----------\r\n    State: - supervisord\r\n    Name:      celery\r\n    Function:  running\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1204, in call\r\n    self.verify_ret(ret)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 580, in verify_ret\r\n    \'Malformed state return, return must be a dict\'\r\nSaltException: Malformed state return, return must be a dict\r\n\r\n        Changes:   \r\n----------\r\n```\r\n\r\nLooks like the [return value is missing](https://github.com/saltstack/salt/blob/develop/salt/states/supervisord.py#L78)'
6052,'terminalmage',"Believes Git fileserver is enabled, when it isn't\nAfter upgrading to 0.16.0, I noticed my master/minion logs started showing this error:\r\n\r\n    2013-07-08 20:24:35,112 [salt.loaded.int.pillar.git_pillar][ERROR   ] Git fileserver backend is enabled in configuration but could not be loaded, is git-python installed?\r\n\r\nI do have my file server stored in git, but I am not using the git backend. My config files are pretty old (created somewhere in 10.2), but it doesn't state anything about which file server backend to use. \r\n\r\nI'm thinking salt is seeing that the file root is in a git repo, and is assuming that I want to use the git backend when in fact I do not."
6049,'UtahDave','Using the helper batch file to install (bootstrap64.bat) contains error "Setup script exited with error: command \'swig.exe\' failed: No such file or directory"\n1. Read the docs from URL "https://salt.readthedocs.org/en/v0.11.1/topics/installation/windows.html" and it makes reference to "Single command bootstrap script"\r\n2. So I downloaded file "bootstrap64.bat" from URL "http://csa-net.dk/salt/bootstrap64.bat"\r\n3. Executed on clean Windows 2008 R2 server\r\n4. During the install, the follow error during call "Executing: cmd.exe /c c:\\salt\\python27\\scripts\\easy_install m2crypto"\r\n\r\nerror: Setup script exited with error: command \'swig.exe\' failed: No such file or directory\r\n\r\n\r\nc:\\salt\\logs\\bootstrap.log: http://pastebin.com/52aLvp09\r\nc:\\salt\\logs\\python_install.log http://pastebin.com/xsKx2Ry3\r\n\r\n\r\n\r\n'
6048,'terminalmage',"cp.get_dir copies down more directories than specified.\nUsing cp.get_dir like this works:\r\n\r\n```\r\nsudo salt -G 'os:Ubuntu' cp.get_dir salt://wordpress/wordpress/ '/tmp/davetest'\r\n```\r\n\r\nBut if in the source directory there is a ```wordpress``` directory as well as a ```wordpress-backup``` directory, both directories will be copied down to the target directory.\r\n\r\nPossible regex not being anchored correctly? "
6042,'terminalmage','Amazon/RedHat pkg requirement fails when pkg already installed\nminion# salt-call state.sls geoip\r\n```Textile\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[WARNING ] The function \'grains()\' defined in \'/usr/lib/python2.6/site-packages/salt/loader.py\' is not yet using the new \'default_path\' argument to `salt.config.load_config()`. As such, the \'SALT_MINION_CONFIG\' environment variable will be ignored\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[WARNING ] The function \'grains()\' defined in \'/usr/lib/python2.6/site-packages/salt/loader.py\' is not yet using the new \'default_path\' argument to `salt.config.load_config()`. As such, the \'SALT_MINION_CONFIG\' environment variable will be ignored\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Fetching file \'salt://geoip.sls\'\r\n[INFO    ] Fetching file \'salt://geoip/init.sls\'\r\n[INFO    ] Executing state pkg.installed for geoip\r\nLoaded plugins: amazon-id, rhui-lb\r\nLoaded plugins: amazon-id, rhui-lb\r\nRepository epel-testing is listed more than once in the configuration\r\nRepository epel-testing-debuginfo is listed more than once in the configuration\r\nRepository epel-testing-source is listed more than once in the configuration\r\nRepository epel is listed more than once in the configuration\r\nRepository epel-debuginfo is listed more than once in the configuration\r\nRepository epel-source is listed more than once in the configuration\r\nRepository rhui-us-east-1-client-config-server-6 is listed more than once in the configuration\r\nRepository rhui-us-east-1-rhel-server-releases is listed more than once in the configuration\r\nRepository rhui-us-east-1-rhel-server-releases-optional is listed more than once in the configuration\r\nRepository rhui-us-east-1-rhel-server-releases-source is listed more than once in the configuration\r\nRepository rhui-us-east-1-rhel-server-releases-optional-source is listed more than once in the configuration\r\nRepository rhel-source is listed more than once in the configuration\r\nRepository rhel-source-beta is listed more than once in the configuration\r\nLoaded plugins: amazon-id, rhui-lb\r\n[INFO    ] Selecting "geoip" for installation\r\nPackage GeoIP-1.4.8-1.el6.x86_64 already installed and latest version\r\n[INFO    ] Upgrade failed, trying downgrade\r\nOnly Upgrade available on package: GeoIP-1.4.8-1.el6.x86_64\r\n[INFO    ] Resolving dependencies\r\n[INFO    ] Processing transaction\r\nRunning rpm_check_debug\r\nLoaded plugins: amazon-id, rhui-lb\r\n[ERROR   ] No changes made for geoip\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      geoip\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following packages failed to install/update: geoip.\r\n        Changes:   \r\n----------\r\n    State: - file\r\n    Name:      /etc/GeoIP.conf\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:   \r\n----------\r\n    State: - file\r\n    Name:      /usr/share/GeoIP/GeoIP.dat\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:   \r\n----------\r\n    State: - file\r\n    Name:      /usr/share/GeoIP/GeoIPRegion.dat\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:   \r\n```\r\n\r\nslave# salt --versions-report\r\n```Textile\r\n           Salt: 0.16.0\r\n         Python: 2.6.6 (r266:84292, Oct 12 2012, 14:23:48)\r\n         Jinja2: 2.7\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n```\r\n\r\nmaster# salt --versions-report\r\n```Textile\r\n           Salt: 0.16.0-585-g5fc836f\r\n         Python: 2.7.3 (default, Apr 10 2013, 06:20:15)\r\n         Jinja2: 2.7\r\n       M2Crypto: 0.21.1\r\n msgpack-python: Not Installed\r\n   msgpack-pure: 0.1.3\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.1.11\r\n            ZMQ: 2.1.11\r\n```\r\n\r\nmaster# cat geoip/init.sls \r\n```YAML\r\ngeoip:\r\n  pkg:\r\n    {% if grains[\'os\'] == \'Gentoo\' %}\r\n    - name: geoip\r\n    {% elif grains[\'os\'] == \'RedHat\' or grains[\'os\'] == \'Amazon\' %}\r\n    - name: geoip\r\n    {% elif grains[\'os\'] == \'Ubuntu\' %}\r\n    - name: geoip-bin\r\n    - require:\r\n      - pkg: geoip-dev\r\n    {% endif %}\r\n    - installed\r\n\r\n/etc/GeoIP.conf:\r\n  file.managed:\r\n    - source: salt://geoip/GeoIP.conf\r\n    - mode: 644\r\n    - user: root\r\n    - group: root\r\n    - require:\r\n      - pkg: geoip\r\n\r\n/usr/share/GeoIP/GeoIP.dat:\r\n  file.managed:\r\n    - source: salt://geoip/GeoIP.dat\r\n    - mode: 644\r\n    - user: root\r\n    - group: root\r\n    - makedirs: True\r\n    - replace: False\r\n    - require:\r\n      - pkg: geoip\r\n\r\n/usr/share/GeoIP/GeoIPRegion.dat:\r\n  file.managed:\r\n    - source: salt://geoip/GeoIPRegion.dat\r\n    - mode: 644\r\n    - user: root\r\n    - group: root\r\n    - makedirs: True\r\n    - replace: False\r\n    - require:\r\n      - pkg: geoip\r\n\r\n{% if grains[\'os\'] == \'Ubuntu\' %}\r\ngeoip-dev:\r\n  pkg:\r\n    - name: libgeoip-dev\r\n    - installed\r\n{% endif %}\r\n```'
6035,'cro','TypeError exception ignored when python-yaml is not installed\nArguably, this isn\'t a big deal, but better keep a reference around:\r\n\r\nOn a machine without python-yaml installed, running the salt-master (v0.16-699-g18e270d) produces the following exception. Notable is the last line. The\r\n\r\n    Traceback (most recent call last):\r\n      File "/home/madduck/code/salt/scripts/salt-master", line 6, in <module>\r\n        from salt.scripts import salt_master\r\n      File "/home/madduck/code/salt/salt/__init__.py", line 19, in <module>\r\n        from salt.utils import migrations\r\n      File "/home/madduck/code/salt/salt/utils/__init__.py", line 51, in <module>\r\n        import salt.minion\r\n      File "/home/madduck/code/salt/salt/minion.py", line 27, in <module>\r\n        import yaml\r\n    ImportError: No module named yaml\r\n    Exception TypeError: "\'NoneType\' object is not callable" in <function _removeHandlerRef at 0xcc99b0> ignored\r\n\r\nThe function `_removeHandlerRef` appears to be defined within the logging framework. I suspect that somewhere, Salt is passing unchecked data (`None` in this case) to the logging module.\r\n'
6033,'terminalmage'," file.mkdir applying incorrect permissions\n_Original thread: https://groups.google.com/forum/#!topic/salt-users/ozjebLCvv04_\r\n\r\nIf I run the following command on a system where none of the directories exist except the first directory (the number of subdirectories is irrelevant)... \r\n\r\n    salt somehost.somedomain file.mkdir /opt/myspecialapp/data/v9.0/data mode=700 \r\n\r\nI end up with each of the directories (except the first because it already exists) with 0457 permissions. This isn't right (obviously) and breaks things spectacularly as the owner of the directory is not able to traverse into it! \r\n\r\nHere's are some tests with different mode combinations -\r\n\r\n**GOOD**\r\n```sh\r\nlinux:~ # salt-call file.mkdir /opt/blah/blah1/blah2\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nlocal:\r\n    None\r\nlinux:~ # tree -p /opt/blah\r\n/opt/blah\r\n└── [drwxr-xr-x]  blah1\r\n    └── [drwxr-xr-x]  blah2\r\n```\r\n\r\n```sh\r\nlinux:~ # salt-call file.mkdir /opt/foo/blah1/blah2 mode=755\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nlocal:\r\n    None\r\nlinux:~ # tree -p /opt/foo\r\n/opt/foo\r\n└── [drwxr-xr-x]  blah1\r\n    └── [drwxr-xr-x]  blah2\r\n```\r\n\r\n**BAD**\r\n```sh\r\nlinux:~ # salt-call file.mkdir /opt/meh/blah1/blah2 mode=700\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nlocal:\r\n    None\r\nlinux:~ # tree -p /opt/meh\r\n/opt/meh\r\n└── [dr--r-xrwx]  blah1\r\n    └── [dr--r-xrwx]  blah2\r\n```\r\n\r\n```sh\r\nlinux:~ # salt-call file.mkdir /opt/bar/blah1/blah2 mode=750\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nlocal:\r\n    None\r\nlinux:~ # tree -p /opt/bar\r\n/opt/bar\r\n└── [drwxr-xr-x]  blah1\r\n    └── [drwxr-xr-x]  blah2\r\n```\r\n\r\nNotice how mode 755 (also the default mode) works fine, but mode 700 or 750 doesn't.\r\n\r\nThis problem can be reproduced on Salt 0.15.9 and Salt 0.16.0."
6031,'s0undt3ch','support for multiple extra-search-dir-s in virtualenv\nvirtualenv allows multiple extra search directories to be specified by including multiple `--extra-search-dir` arguments (e.g. `virtualenv --extra-search-dir /foo --extra-search-dir /bar`\r\n\r\n`modules.virtualenv.create()` only has a string parameter `extra_search_dir`, and thus does not support this virtualenv feature. It ought to.'
6030,'s0undt3ch',"virtualenv --never-download is deprecated\nSee https://github.com/pypa/virtualenv/blob/5016707a064745da3e4fdbd72edc534838862c96/virtualenv.py#L734\r\n\r\nThis affects `modules.virtualenv.create()`'s corresponding `never_download` parameter."
6029,'s0undt3ch',"virtualenv --distribute is deprecated\nSee https://github.com/pypa/virtualenv/blob/5016707a064745da3e4fdbd72edc534838862c96/virtualenv.py#L754\r\n\r\nThis affects `modules.virtualenv.create()`'s corresponding `distribute` parameter."
6027,'s0undt3ch','virtualenv.create: no_site_packages & system_site_packages both control the same setting\n`--no-site-packages`/`no_site_packages `\r\nand `--system-site-packages`/`system_site_packages`\r\n[both control the same boolean flag within virtualenv](https://github.com/pypa/virtualenv/blob/5016707a064745da3e4fdbd72edc534838862c96/virtualenv.py#L674) (they just twiddle it in opposite directions), so accepting both of them as separate parameters to `virtualenv.create` is kinda weird and allows the expression of the absurd combination `no_site_packages=True AND system_site_packages=True`. I suggest that there be only 1 boolean parameter to control the accessibility of system site packages.\r\n\r\nAlso, according to virtualenv itself:\r\n`The --no-site-packages flag is deprecated; it is now the default behavior.`'
6025,'terminalmage',"pip.installed doesn't allow empty (or omitted) name when using editable\nThe following should work to install your packages locally (ala `python setup.py develop` see [pip docs](http://www.pip-installer.org/en/latest/logic.html#editable-installs)):\r\n```\r\nlocal_package_install:\r\n    pip.installed:\r\n        - editable: /dir/to/package\r\n```\r\n\r\nwhich should call the corresponding `pip` command\r\n```\r\npip install --editable=/dir/to/package\r\n```\r\n\r\nBut `pip.installed` always expects a valid `name` parameter and only empties it if a requirements file is supplied ([here](https://github.com/saltstack/salt/blob/develop/salt/states/pip.py#L112))."
6021,'terminalmage',"supervisord.mod_watch not respecting update=True flag.\n`mod_watch` in [states/supervisord.py](https://github.com/saltstack/salt/blob/develop/salt/states/supervisord.py#L194) doesn't include the `update` argument, and the arguments are passed by position, causing incorrect values.\r\n\r\nIn my case, it's not passing update=True, so the supervisor daemon isn't reloading with updated config settings."
6020,'terminalmage','Inner loop variable is not being passed to jinja template\nI have a `users\\init.sls` state which essentially looks like this:\r\n\r\n    {% for user, args in pillar.get(\'users\', {}).iteritems() %}\r\n    {% for key in args[\'ssh_auth\'].get(\'private_keys\', []) %}\r\n    /home/{{ user }}/.ssh/{{ key }}:\r\n      file.managed:\r\n        - source: salt://users/private_key.jinja2\r\n        - template: jinja\r\n        - defaults:\r\n          user: {{ user }}\r\n          key: {{ key }}\r\n    {% endfor %}\r\n    {% endfor %}\r\n\r\nFile `users/private_key.jinja2` is one-liner:\r\n\r\n    {{ pillar[\'users\'][user][\'ssh_auth\'][\'private_keys\'][key] }}\r\n\r\nPillar `users\\init.sls` is like this:\r\n\r\n    users:\r\n      username:\r\n        ssh_auth:\r\n          private_keys:\r\n            id_web: |\r\n              -----BEGIN RSA PRIVATE KEY-----\r\n              ...\r\n              -----END RSA PRIVATE KEY-----\r\n\r\nWhen I remove `user` from template defaults, everything still works. But when I remove `key` (inner loop variable) from template defaults, Salt can\'t render the template:\r\n\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.7/dist-packages/salt/utils/templates.py", line 63, in render_tmpl\r\n        output = render_str(tmplstr, context, tmplpath)\r\n      File "/usr/lib/python2.7/dist-packages/salt/utils/templates.py", line 116, in render_jinja_tmpl\r\n        output = jinja_env.from_string(tmplstr).render(**context)\r\n      File "/usr/lib/python2.7/dist-packages/jinja2/environment.py", line 894, in render\r\n        return self.environment.handle_exception(exc_info, True)\r\n      File "<template>", line 1, in top-level template code\r\n    UndefinedError: dict object has no element Undefined\r\n'
6009,'cachedout',"smtp returner does not work\nor at least i can't figure out how.\r\n\r\nplease see discussion: https://groups.google.com/forum/#!topic/salt-users/W07kQY24Vts\r\n\r\nfor reference: #305 is still open.\r\n\r\n@techhat: perhaps this one is something for you."
5998,'cachedout',"state.sls should bail out, when desired SLS does not exist\ncurrently when i deploy a state manually and have a typo in state-name, this seems to be up-to-date. instead it should show an error, so i can notice my mistake.\r\n<pre>\r\npille@salt-master ~ % time sudo salt -v 'target*' state.sls test.tset-statte                                                                                                                                                                                \r\nExecuting job with jid 20130706130410285321\r\n-------------------------------------------\r\n\r\nExecution is still running on target\r\nExecution is still running on target\r\ntarget:\r\n\r\n0.21s user 0.08s system 1% cpu 22.174 total\r\n</pre>"
5994,'UtahDave','Windows minion locks up the computer \nI found a problem with the Windows minion. On one of my computers it hard locks the computer after some time. The computer is not reacting to any input/network ping any more after the incident. The num-lock key is not showing any reaction on the keyboards LED.\r\n\r\nWhile i can reproduce the problem only on one (relatively high-end) system, i think i had a similar problem on the other machines until i accepted the key on the server.\r\n\r\nTo me it sound not like a salt issue, because it would be very sad if a userspace  tool could crash windows like this.\r\n\r\n\r\nSystem details:\r\nDell Precision T5400 \r\nDual XEON with 16GB RAM\r\nWindows 7 64bit\r\nSalt 0.15.3 AMD64'
5989,'s0undt3ch','SaltLoggingClass uses __new__ but isn\'t new-style under Py2.6\nWhen running under Python 2.6, pylint reports:\r\n```\r\n************* Module salt.log\r\nE1002:142,4:SaltLoggingClass.__new__: Use of super on an old style class\r\n```\r\nThis is a problem because:\r\n```py\r\nPython 2.6.8 (unknown, Mar 11 2013, 19:23:16) \r\n>>> class Foo: # old-style class\r\n...     def __new__(*args, **kwargs):\r\n...         print "hello from new"\r\n...         print args, kwargs\r\n... \r\n>>> x = Foo()\r\n>>> # __new__() was not called.\r\n```'
5977,'cro','Documentation : Inconsistent "env" parameter in file and pkg state\nThe "env" parameter is not always use in a standard way from the documentation.\r\n\r\nhttp://docs.saltstack.com/ref/states/all/salt.states.file.html#module-salt.states.file\r\nhttp://docs.saltstack.com/ref/states/all/salt.states.pkg.html#module-salt.states.pkg\r\n\r\nsalt.states.file.append(... __env__=\'base\')\r\nsalt.states.file.managed(... env=None)\r\nsalt.states.file.patch(..env=\'base\')\r\n\r\nThe pkg state doesn\'t seem to support the "env" parameter but the file server can? Should I do a feature request for that?\r\n\r\nWorking (not documented):\r\n- jdk: salt://java/jdk-7u21-linux-x64.rpm?env=dev\r\n\r\nNot working? :\r\n- jdk: salt://java/jdk-7u21-linux-x64.rpm\r\n- env: dev'
5970,'s0undt3ch','salt-call does not parse /etc/salt/minion.d/* config files\nIt looks like this would need to be added to the [SaltCallOptionParser](https://github.com/saltstack/salt/blob/develop/salt/utils/parsers.py#L1366)'
5969,'s0undt3ch','Deployed latest salt-develop on Windows 2008 R1, upon starting there is an error message "ImportError: No module named fcntl".\nDeployed latest salt-develop on Windows 2008 R1, upon starting there is an error message "ImportError: No module named fcntl".\r\n\r\nI noticed that if I edit file: c:\\salt\\python27\\Lib\\site-packages\\salt\\ssh\\shell.py  @line12: and comment out\r\n\r\n#import salt.utils.nb_popen  it goes further. Thought I read somewhere that nb_open was not for Windows?\r\n\r\npython salt-minion -l debug -c c:\\salt\\conf\r\n\r\nTraceback (most recent call last):\r\n  File "salt-minion", line 6, in <module>\r\n    from salt.scripts import salt_minion\r\n  File "c:\\salt\\Python27\\lib\\site-packages\\salt\\scripts.py", line 11, in <module\r\n>\r\n    import salt.cli\r\n  File "c:\\salt\\Python27\\lib\\site-packages\\salt\\cli\\__init__.py", line 18, in <m\r\nodule>\r\n    import salt.ssh\r\n  File "c:\\salt\\Python27\\lib\\site-packages\\salt\\ssh\\__init__.py", line 11, in <m\r\nodule>\r\n    import salt.ssh.shell\r\n  File "c:\\salt\\Python27\\lib\\site-packages\\salt\\ssh\\shell.py", line 12, in <modu\r\nle>\r\n    import salt.utils.nb_popen\r\n  File "c:\\salt\\Python27\\lib\\site-packages\\salt\\utils\\nb_popen.py", line 16, in\r\n<module>\r\n    import fcntl\r\nImportError: No module named fcntl'
5965,'cachedout','new module in _modules needs restart?\nI\'m trying to work around my salt-call problems by writing a new module. So I put a short python file in _modules, ran `saltutil.sync_all` for a few minions and tried to run the module, but it returned\r\n```\r\nweb-2:\r\n    "mrten_svn.update" is not available.\r\n```\r\nAfter a `restart salt-minion` however, I got an empty reply and in the minions\' log was an error. After this, I fixed the error, ran `sync_all` again and got the same error! Only after restarting the minion again my change got picked up. \r\n\r\nSo, the question is: do I need to restart the minions after I update a module? Because that isn\'t really mentioned in the manual...'
5964,'terminalmage',"output for saltutil.sync_all weird\nI ran salt '*' saltutil.sync_all and got this:\r\n\r\n```\r\nrecursor-1:\r\n    |_\r\n      - modules.mrten_svn\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\ntahoe:\r\n    |_\r\n      - modules.mrten_svn\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\nmx-2:\r\n    |_\r\n      - modules.mrten_svn\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\n    |_\r\n```\r\nSeemed a bit weird... (or at least, missing information)."
5961,'thatch45',"0.15.1 => 0.15.3 Upgrade: Master Pub Key Not Auto-Accepted on Minions after Key-Regen\nAs requested I'm logging an issue I experienced upgrading from 0.15.1 => 0.15.3 via apt-get on Ubuntu 12.04 servers. After upgrading the salt-master and performing a `salt-run manage.key_regen`, all minions failed to cache/trust the masters new public key, meaning the master could no longer issue commands to minions (but minions could salt-call from master).\r\n\r\nThis was resolved manually by deleting the old pub-key on all minions manually and restarting salt-minion (a time-consuming process).\r\n\r\nI can't be 100% certain it wasn't a result of something specific I did when upgrading though ;-)\r\n\r\nSee discussion here:\r\nhttps://groups.google.com/forum/#!topic/salt-users/nY6rrjPr7kM\r\n\r\n\r\n"
5955,'terminalmage','pkgrepo.managed makes repos disabled, unless told not to.\nIf I add a repo which uses keyid and keyserver, all is fine and dandy:\r\n\r\njava-repo:\r\n  pkgrepo.managed:\r\n    - name: deb http://www.duinsoft.nl/pkg debs all\r\n    - file: /etc/apt/sources.list.d/duinsoft.list\r\n    - keyid: 5CB26B26\r\n    - keyserver: keys.gnupg.net\r\n\r\nBut if I add a repo, which does not need anything:\r\n\r\npartner-repo:\r\n  pkgrepo.managed:\r\n    - name: deb http://archive.canonical.com/ubuntu precise partner\r\n    - file: /etc/apt/sources.list.d/partner.list\r\n\r\nIt ends up disabled. More precisely the line in the file specified, has a # in front of it.\r\nIf I add disabled: False, it works correctly:\r\n\r\npartner-repo:\r\n  pkgrepo.managed:\r\n    - name: deb http://archive.canonical.com/ubuntu precise partner\r\n    - file: /etc/apt/sources.list.d/partner.list\r\n    - disabled: False\r\n\r\nThis is counter intuitive and should be the other way around.'
5940,'s0undt3ch',"Can't specify multiple mirrors to pip.installed\n`pip install` can take multiple mirrors, but there isn't a good way to tell salt to do this.  We've currently resorting to:\r\n\r\n    foo:\r\n      pip.installed:\r\n        ...\r\n        mirrors: http://... --mirror http://... --mirror http://...\r\n\r\nIt would be better if mirrors could be a list."
5928,'thatch45','return format of publish.publish changed\nWhen calling `sudo salt-call publish.publish api01 grains.item datacenter` I used to get\r\n\r\n```\r\napi01:\r\n    rackspace\r\n```\r\n\r\nFollowing 0.16.0, I get\r\n```\r\napi01:\r\n    ----------\r\n    out:\r\n        grains\r\n    ret:\r\n        ----------\r\n        datacenter:\r\n            rackspace\r\n```'
5927,'terminalmage',"Custom State Module Documentation change\nIt might be worthwhile to note on http://docs.saltstack.com/ref/states/writing.html that adding a  custom state module with the same name as a default module will override the default module.  My testing seems to indicate so (particularly i was testing an updated selinux.py state and didn't want to change packaged files)."
5921,'terminalmage',"pillar.libvirt.gen_hyper_keys(): Undefined variable 'minion_id'\n```\r\n************* Module salt.pillar.libvirt\r\nE0602: 68,36:gen_hyper_keys: Undefined variable 'minion_id'\r\n```"
5919,'cro',"Throw error if `require` has a function and module defined\nShould state requires have the function associated with them as well?\r\n\r\nFor example, the require passes in this example even though it's contradictory to what we want:\r\n\r\n```\r\nsmbfs:\r\n    pkg.removed\r\n\r\n/mnt/ostenso:\r\n    mount.mounted:\r\n        - device: //server/share\r\n        - fstype: smbfs\r\n        - mkmnt: True\r\n        - require:\r\n            - pkg.installed: smbfs\r\n```\r\n\r\nI don't think the require should allow mount to run in this case."
5918,'terminalmage',"Setting mysql root password the first time\nI'm having an issue setting the mysql root password the first time on a new masterless configuration.\r\n\r\nWhat I want to do is change it from the default to something else.  I think you have the option to set it when you install typically, but that step is skipped with the automated install.\r\n\r\nHere's what I think the issue is:\r\n\r\nI have to define the username and password to access the mysql database in the config file, but when configuring the root user in the salt state file, it trys to access the database using the password that has not yet been set.\r\n\r\nIs this a known issue?  I saw issue #3831 but `_connect()` still uses the root credentials in the config file (i.e. works for everyone but root)."
5907,'terminalmage',"Erratic behavior of service state due to stale services cache during highstate run\nThis bug took a lot of time to troubleshoot to the point that I could give a cohesive report.  It occurs with salt-0.15.3 and salt-0.16.0, at least.  The following SLS *may* reproduce the problem (on CentOS 6):\r\n\r\n```\r\nautofs:\r\n  pkg:\r\n    - latest\r\n  service.running:\r\n    - enable: True\r\n    - watch: \r\n      - pkg: autofs\r\n```\r\n\r\nWhat happens is that the autofs service is *started* but not *enabled* on the first `state.highstate` run.  Running a second `state.highstate` corrects the error by *enabling* the service.  A key detail is that the init script for the service is installed as part of the package.\r\n\r\nThe problem appears to be caused by the services cache in `__context__['service.all']` not being updated after the package installation.  As a consequence, `service.running()` erroneously assumes that the service is not available and returns an error that is not returned to the caller.  The error doesn't make it back to the caller because the result of `service.mod_watch()` (which *does* successfully restart the service) is returned instead.\r\n\r\nIf I disable the services cache then everything works correctly.  Doing a quick grep, it appears that the services cache is only actually cleared by functions in `state.service`.  This means that this bug likely appears in other circumstances too, like when installing an init script via the `file.managed` state.\r\n\r\nUnfortunately, I don't see a way to make the services cache work properly with states that change what services are available to the system.  Each state with such potential would need extra code to determine the cases in which the cache should be cleared.  For example, clearing the cache on every `file.managed()` would make the cache almost pointless, so `file.managed()` would need to check the target and clear the cache of it is an init script.  This seems unnecessarily ugly.\r\nA better solution may be to add `modules.rh_service.available()` with a dedicated code path that runs `chkconfig --list <service>` instead of calling `_services()`."
5894,'terminalmage','service.running: failure to start with enable: True reports no changes / no errors\nI believe there is a bug in [service.running](https://github.com/saltstack/salt/blob/develop/salt/states/service.py#L290-L299).\r\n\r\nIf a service fails to start, but enable: is provided, it returns only the status of service._enable and fails to return the failure to start.'
5880,'terminalmage',"npm.installed issue with working, up-to-date packages\nI'm having strange errors today with the following state:\r\n\r\n```\r\nnpm-packages:\r\n  npm.installed:\r\n    - names:\r\n      - bower\r\n      - grunt-cli\r\n```\r\n\r\n```\r\n----------\r\n    State: - npm\r\n    Name:      bower\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   Error installing 'bower': npm ERR! invalid: async@0.1.22 /usr/lib/node_modules/bower/node_modules/async\r\nnpm ERR! invalid: watch@0.5.1 /usr/lib/node_modules/forever/node_modules/watch\r\nnpm ERR! not ok code 0\r\n        Changes:   \r\n----------\r\n    State: - npm\r\n    Name:      grunt-cli\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   Error installing 'grunt-cli': npm ERR! invalid: async@0.1.22 /usr/lib/node_modules/bower/node_modules/async\r\nnpm ERR! invalid: watch@0.5.1 /usr/lib/node_modules/forever/node_modules/watch\r\nnpm ERR! not ok code 0\r\n        Changes:   \r\n```\r\n\r\n```\r\n$ npm --version\r\n1.3.1\r\n$ node --version\r\nv0.10.12\r\n$ salt-minion --version\r\nsalt-minion 0.15.3\r\n```\r\n\r\nAny idea? `sudo npm install bower grunt-cli -g` does work on the remote server.\r\n"
5848,'thatch45','Bug in batch system\nhttp://docs.saltstack.com/topics/targeting/batch.html\r\n\r\n```\r\nsalt -b 10 \\* cmd.run \'echo `hostname` | grep f >/dev/null 2>&1 && sleep 120; date\'\r\n```\r\n\r\nThe glob I used in my environment matched about 125 minions, with about 5 of them having an `f` in their hostname. The output shows that the whole group will wait for the "slow" minion, and then next group won\'t run until the whole group has returned.\r\n\r\nThe docs seem to say there\'s a queue that\'s pulled from as quickly as possible, but it seems that it\'s actually being batched (I guess appropriately, given the feature is called "batching") '
5840,'UtahDave',"can't use custom modules and states in Windows minion\nWindows 2008 R2 64 Bit, Salt-0.15.3\r\nwhen running ``` salt \\* saltutil.sync_modules``` on the master\r\nWindows minion doesn't get the custom modules/states from _modules / _states\r\n\r\nAfter running the minion in debug mode I've noticed these log entries:\r\n```\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\modules, it is not a directory\r\n...\r\n...\r\n...\r\n[INFO    ] Creating module dir 'c:\\salt\\var\\cache\\salt\\minion\\extmods\\modules'\r\n```\r\n\r\nbut the directory doesn't exists after ``` salt \\* saltutil.sync_modules```\r\nand eventually all my custom modules & states are not copied to the minion \r\n\r\n(mailing list link https://groups.google.com/forum/#!topic/salt-users/nMe6EUPvVDk)"
5839,'whiteinge','Dash docset feed returning 404\nhttp://media.readthedocs.org/dash/salt/latest/Salt.xml is returning `404`.'
5837,'UtahDave','file.managed woking on windows is error\nthe sls file is :\r\n========================================================\r\nc:\\test.txt:\r\n   file.managed:\r\n           - source: salt://test.conf\r\n           - template: jinja\r\n\r\n========================================================\r\nthe result like this :\r\n    State: - file\r\n    Name:      c:\\test.txt\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1201, in call\r\n  File "salt/states/file.py", line 827, in managed\r\n  File "salt/modules/file.py", line 1519, in manage_file\r\nNameError: global name \'contextlib\' is not defined\r\n\r\n===========================================================\r\nthe version of salt-master is salt-0.15.1\r\nthe version of windows minion is salt-0.15.3.win32'
5817,'cachedout',"mysql_grants must be in the same order returned by  the mysql SHOW GRANTS  query\nExample:\r\n\r\ngrant_example:\r\n  mysql_grants.present:\r\n    - grant: SELECT, ALTER, LOCK TABLES\r\n    - database: example_db.*\r\n    - user: example_user\r\n    - host: localhost  \r\n\r\nThe above works because SELECT, ALTER, LOCK TABLES are in the same order as SHOW GRANTS FOR USER 'example_user'@'localhost';  would return\r\n\r\ngrant_example:\r\n  mysql_grants.present:\r\n    - grant: ALTER, LOCK TABLES, SELECT\r\n    - database: example_db.*\r\n    - user: example_user\r\n    - host: localhost  \r\n\r\nThe above will cause the fallowing query to be executed \r\n\r\nGRANT ALTER, LOCK TABLES, SELECT to 'example_user'@'localhost'; \r\n\r\nHowever when the code checks the the return of the SHOW GRANTS FOR 'example_user'@'localhost' against the string passed to 'grant' in mysql_grants.present it reports a failure even though the correct permissions are in mysql."
5801,'basepi','module service not working on openSUSE 11.x\nHi,\r\nI have an issue on openSUSE releases 11.x, it seem\'s they where not able to use the service module,.\r\nService on openSUSE 11 are not using the systems as on 12.x but more the redhat services (/etc/init.d/, chkconfig, ..)\r\n\r\nI have made some changes in the rh_service.py module but it seem\'s Im\' not a great python dev.\r\nIf you can correct this it will save my life :)\r\n\r\nif you need more infos . let me know\r\nRegards\r\nClaude\r\n\r\n```\r\n# cat /etc/issue\r\nWelcome to openSUSE 11.3 "Teal" - Kernel \\r (\\l).\r\n#./salt-call service.get_all\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nFunction service.get_all is not available\r\n# find ../. -name service.py\r\n.././lib/python2.7/site-packages/salt/states/service.py\r\n.././lib/python2.7/site-packages/salt/modules/service.py\r\n# ./salt-call --versions-report\r\n           Salt: 0.15.90\r\n         Python: 2.7.5 (default, Jun 25 2013, 13:19:03)\r\n         Jinja2: 2.7\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.3\r\n# ./salt-call grains.item os os_family oscodename osfullname osrelease\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\nos:\r\n    openSUSE\r\nos_family:\r\n    Suse\r\noscodename:\r\n    x86_64\r\nosfullname:\r\n    openSUSE\r\nosrelease:\r\n    11.3\r\n```'
5799,'terminalmage','Grains with : in values can\'t be matched\nAs an example, say we have a grain called "example" with a value of "test::me", matching the grain is impossible, currently:\r\n\r\n  salt -G \'example:test::me\' test.ping\r\n\r\n^^ returns no results'
5793,'basepi',"file.managed with no source and replace=false does not set mode on creation\n```\r\nsalt-call state.single file.managed name=/tmp/test user=root group=root mode=440 replace=False\r\n```\r\n\r\n```\r\nsalt-call state.single file.managed name=/tmp/test user=root group=root mode=440 replace=False content='#for local content'\r\n```\r\n\r\nBoth of these will create the file on first pass, and then set the permissions on the second pass. I have not tested with a source specified."
5780,'terminalmage','NameError with pkg state (global name \'rtag\' is not defined)\nWith installing the latest git version of salt stack, I always get the following error when using the \'pkg\' state:\r\n\r\n        State: - pkg\r\n         Name:      nginx\r\n         Function:  installed\r\n             Result:    False\r\n             Comment:   An exception occurred in this state: Traceback (most recent call      last):\r\n       File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1238, in call\r\n         *cdata[\'args\'], **cdata[\'kwargs\'])\r\n        File "/usr/lib/python2.7/dist-packages/salt/states/pkg.py", line 376, in installed\r\n         if salt.utils.is_true(refresh) or os.path.isfile(rtag):\r\n     NameError: global name \'rtag\' is not defined\r\n\r\nIt doesn\'t matter what package, or how many... I always get this error.\r\nIf I install the package repository version, this error disappears.'
5779,'terminalmage','sys.doc with garbage parameters explodes with "TypeError: string indices must be integers, not str"\nVersion 0.15.3 master, on fedora 18\r\n\r\n```\r\n$ sudo salt \\* sys.doc test.ping  random_junk othercrap=morecrap\r\nBDB2053 Freeing read locks for locker 0x30a6: 28087/139717077964736\r\nBDB2053 Freeing read locks for locker 0x30a7: 28087/139717077964736\r\nBDB2053 Freeing read locks for locker 0x30a8: 28087/139717077964736\r\nBDB2053 Freeing read locks for locker 0x30a9: 28087/139717077964736\r\nTraceback (most recent call last):\r\n  File "/bin/salt", line 10, in <module>\r\n    salt_main()\r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 103, in salt_main\r\n    client.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 107, in run\r\n    self._output_ret(ret, out)\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 125, in _output_ret\r\n    self._print_docs(ret)\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 155, in _print_docs\r\n    if ret[host][fun]:\r\nTypeError: string indices must be integers, not str\r\n$\r\n```\r\n\r\nSame with ```sys.doc test.ping randomjunk```\r\n\r\nI know it\'s a garbage parameter, but it shouldn\'t just explode with a traceback.  (I would naively assume it should try and run sys.doc for all of them?)'
5776,'basepi','UnicodeDecodeError: \'utf8\' codec can\'t decode byte 0x86 in position 16: invalid start byte\n```\r\n/etc/powerdns/zones:\r\n  file.recurse:\r\n    - source: salt://machine/ns/master/zones\r\n    - template: mako\r\n    - exclude_pat: "*.svn*"\r\n    - clean: true\r\n```\r\n\r\n```\r\nns2:\r\n----------\r\n    State: - file\r\n    Name:      /etc/powerdns/zones\r\n    Function:  recurse\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1201, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/file.py", line 1317, in recurse\r\n    manage_file(dest, src)\r\n  File "/usr/lib/pymodules/python2.7/salt/states/file.py", line 1240, in manage_file\r\n    **pass_kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/states/file.py", line 787, in managed\r\n    **kwargs\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/file.py", line 1340, in check_managed\r\n    **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/file.py", line 1153, in get_managed\r\n    **kwargs\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/templates.py", line 53, in render_tmpl\r\n    tmplstr = tmplsrc.read()\r\n  File "/usr/lib/python2.7/codecs.py", line 671, in read\r\n    return self.reader.read(size)\r\n  File "/usr/lib/python2.7/codecs.py", line 477, in read\r\n    newchars, decodedbytes = self.decode(data, self.errors)\r\nUnicodeDecodeError: \'utf8\' codec can\'t decode byte 0x86 in position 16: invalid start byte\r\n```\r\nI love to believe this, but what file has the invalid byte? Could file.recurse log this?'
5772,'basepi',"[Docs] : the cmd.run env parameter's default is a tuple, should be a dict\nMaybe it's not only a doc issue.\r\nIn the docs \r\nhttp://docs.saltstack.com/ref/modules/all/salt.modules.cmdmod.html#salt.modules.cmdmod.run\r\n\r\nThe env var is documented as a tuple (its default value)\r\n```\r\nenv=()\r\n```\r\nit should be a dict\r\n```\r\nenv={}\r\n```\r\nsince you can set it through yaml or through \r\n```\r\nenv={'PATH':'/usr/local/bin'}\r\n```\r\n"
5768,'thatch45','Minion-master authentication fails if minions added concurrently\nOriginal thread - https://groups.google.com/forum/#!topic/salt-users/wU9Xz7QyhwQ\r\n\r\nI\'m working on a project that provides a light web interface on top of Salt (and other things). One of the things the web interface can do is add and configure a host for Salt control using SSH. \r\n\r\nWhen it adds a host for Salt control, the web interface will delete all existing keys on the salt master for that host (using salt-api), configure and start up the minion on the host (over SSH), wait for the key to appear before accepting it and pinging that host to confirm (all with salt-api). The web interface can do this on multiple hosts concurrently, however when I try that, nearly all the minions crash or become unresponsive. \r\n\r\nOn the unresponsive minions, I\'ll see the following exception repeated - \r\n\r\n```\r\n2013-06-26 07:44:04,429 [salt.minion][INFO] Authentication with master \r\nsuccessful! \r\n2013-06-26 07:44:04,429 [salt.minion][CRITICAL] An exception occurred \r\nwhile polling the minion \r\nTraceback (most recent call last): \r\n  File "/usr/lib/python2.7/site-packages/salt/minion.py", line 1015, in tune_in \r\n    self._handle_payload(payload) \r\n  File "/usr/lib/python2.7/site-packages/salt/minion.py", line 514, in \r\n_handle_payload \r\n    \'clear\': self._handle_clear}[payload[\'enc\']](payload[\'load\']) \r\n  File "/usr/lib/python2.7/site-packages/salt/minion.py", line 525, in \r\n_handle_aes \r\n    data = self.crypticle.loads(load) \r\n  File "/usr/lib/python2.7/site-packages/salt/crypt.py", line 413, in loads \r\n    data = self.decrypt(data) \r\n  File "/usr/lib/python2.7/site-packages/salt/crypt.py", line 396, in decrypt \r\n    raise AuthenticationError(\'message authentication failed\') \r\nAuthenticationError: message authentication failed \r\n```\r\n\r\nOn the hosts where the minions have died (but left the pidfile behind), if I try to start the minion manually on the console, I\'ll get the following before it dies - \r\n\r\n```\r\n<snip> \r\n[DEBUG] Loaded minion key: /etc/salt/pki/minion/minion.pem \r\n[DEBUG] Decrypting the current master AES key \r\n[DEBUG] Loaded minion key: /etc/salt/pki/minion/minion.pem \r\n[INFO] Authentication with master successful! \r\n[DEBUG] Loaded minion key: /etc/salt/pki/minion/minion.pem \r\n[DEBUG] Decrypting the current master AES key \r\n[DEBUG] Loaded minion key: /etc/salt/pki/minion/minion.pem \r\n[DEBUG] Loaded minion key: /etc/salt/pki/minion/minion.pem \r\nTraceback (most recent call last): \r\n  File "./salt-minion", line 11, in <module> \r\n    load_entry_point(\'salt==0.15.9\', \'console_scripts\', \'salt-minion\')() \r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 29, in \r\nsalt_minion \r\n    minion.start() \r\n  File "/usr/lib/python2.7/site-packages/salt/__init__.py", line 198, in start \r\n    self.prepare() \r\n  File "/usr/lib/python2.7/site-packages/salt/__init__.py", line 186, in prepare \r\n    self.minion = salt.minion.Minion(self.config) \r\n  File "/usr/lib/python2.7/site-packages/salt/minion.py", line 455, in __init__ \r\n    opts[\'environment\'], \r\n  File "/usr/lib/python2.7/site-packages/salt/pillar/__init__.py", \r\nline 59, in compile_pillar \r\n    aes = key.private_decrypt(ret[\'key\'], 4) \r\nTypeError: string indices must be integers, not str \r\n```\r\n\r\nWhen this happens, on the master, I\'ll start seeing the following line appear multiple times in the log - \r\n\r\n```[DEBUG] Failed to authenticate message ```\r\n\r\nRestarting the minions does not help, however, if I restart the master (after they\'ve all been added), then restart the minions, everything just works. \r\n\r\nThe \'add host\' process also works fine if I add each host serially (i.e. one at a time). If I add hosts serially, then add others in parallel, then those hosts I add in serial will become uncontactable as well with the same symptoms. Adding 3 or more hosts in parallel consistently produces this problem (it occasionally works for 2 hosts). \r\n\r\nI am therefore led to believe there is a threading issue somewhere inside Salt where the minion keys are stored, and if they\'re modified quickly within a very small timeframe with separate calls, corruption may occur that corrupts the entire in-memory store until the store is re-read when the master is restarted. \r\n\r\nThe version details are as follows (all hosts tested are identical) - \r\n\r\n```\r\nSalt: 0.15.90 \r\nPython: 2.7.3 (default, Mar 6 2013, 17:02:04) \r\nJinja2: 2.6 \r\nM2Crypto: 0.21.1 \r\nmsgpack-python: 0.3.0 \r\nmsgpack-pure: Not Installed \r\npycrypto: 2.6 \r\nPyYAML: 3.10 \r\nPyZMQ: 13.0.0 \r\nZMQ: 3.2.2 \r\n\r\n2.6.18-274.el5PAE #1 SMP Fri Jul 8 17:59:09 EDT 2011 i686 i686 i386 GNU/Linux \r\nRed Hat Enterprise Linux Server release 5.7 (Tikanga) \r\n```\r\n\r\nI\'m not familiar enough with the encryption innards of Salt to debug there, but let me know if there\'s anything else I can provide that may help. \r\n\r\nThis issue maybe related to #5599.'
5767,'cro','State compiler stack trace\nVersions:\r\nsalt-minion --versions-report\r\nSalt: 0.15.3\r\nPython: 2.7.3 (default, Apr 10 2013, 06:20:15)\r\nJinja2: 2.6\r\nM2Crypto: 0.21.1\r\nmsgpack-python: 0.1.10\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.4.1\r\nPyYAML: 3.10\r\nPyZMQ: 13.0.0\r\nZMQ: 3.2.2\r\n\r\nSalt-master is not returning the stacktrace from the minion, just the name.\r\n\r\nstacktrace from minion in debug mode: http://cl.ly/Pu5g\r\n\r\n```\r\n[DEBUG   ] "Missing args" caused by exc: \'NoneType\' object is not iterable\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 443, in _thread_return\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 201, in highstate\r\n    cache_name=kwargs.get(\'cache_name\', \'highstate\')\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 2087, in call_highstate\r\n    return self.state.call_high(high)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1415, in call_high\r\n    ret = self.call_chunks(chunks)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1233, in call_chunks\r\n    running = self.call_chunk(low, running, chunks)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1316, in call_chunk\r\n    status = self.check_requisite(low, running, chunks)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1270, in check_requisite\r\n    for req in low[r_state]:\r\nTypeError: \'NoneType\' object is not iterable\r\n```\r\n'
5763,'whiteinge','Add the ability to add minion keys to the key wheel module\nAdd a function to the key wheel module that will take a key for a minion and add it to the pki dir on the master.'
5748,'thatch45',"publish.publish doesn't return anything\nRunning latest develop. I have a minion, dufresne, set up as a peer with full power:\r\n```\r\npeer:\r\n    dufresne:\r\n        - .*\r\n```\r\nPinging that minion from the master works:\r\n```\r\n[root@saltmaster ~]# salt dufresne test.ping\r\ndufresne:\r\n    True\r\n```\r\nHaving that minion ping itself from the master returns nothing:\r\n```\r\n[root@dufresne ~]# salt-call publish.publish dufresne test.ping\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[INFO    ] Publishing 'test.ping' to tcp://<redacted>:4506\r\n```\r\nPerhaps I'm doing something wrong?"
5746,'UtahDave','pkg.available_version raises exception when package not found (windows)\nWhen i run\r\n\r\n    salt win2k8 pkg.available_version chrome\r\n\r\nI get the following response when the package i am looking for do not exists in the windows repo.\r\n\r\n    win2k8:\r\n        Traceback (most recent call last):\r\n          File "C:\\salt\\salt-amd64\\salt.egg\\salt\\minion.py", line 630, in _thread_return\r\n            ret[\'return\'] = func(*args, **kwargs)\r\n          File "C:\\salt\\salt-amd64\\salt.egg\\salt\\modules\\win_pkg.py", line 98, in     latest_version\r\n            return ret[names[0]]\r\n        KeyError: \'chrome\'\r\n\r\nIt should return error with following comment\r\n\r\n    Comment:   Package chrome not found in the repository.\r\n\r\nTested on develop branch and on windows 2k8 minion'
5742,'cro','file.directory \'Changes\' log incomplete / confusing\nI have two conflicting states:\r\n```\r\n/var/lib/subversion:\r\n  file.directory:\r\n    - group: svn\r\n    - dir_mode: 700\r\n    - file_mode: 640\r\n    - recurse:\r\n      - group\r\n    - require:\r\n      - user: user-svn\r\n    - exclude_pat: "*.ssh*"\r\n```\r\nand another that sets the group of the .ssh dir to something else.\r\n\r\nThis is the output:\r\n```\r\nsvn:\r\n----------\r\n    State: - file\r\n    Name:      /var/lib/subversion/.ssh\r\n    Function:  directory\r\n        Result:    True\r\n        Comment:   Directory /var/lib/subversion/.ssh updated\r\n        Changes:   mode: 700\r\n\r\n----------\r\n    State: - file\r\n    Name:      /var/lib/subversion\r\n    Function:  directory\r\n        Result:    True\r\n        Comment:   Directory /var/lib/subversion updated\r\n        Changes:   mode: 700\r\n```\r\nNote the two `mode: 700`s. I think there should be a group in there...\r\n\r\nConfused the hell out of me until I set the mode of the above state to 700, first I thought it was recursing the mode as well even though it is not set in recurse.'
5741,'basepi','file.directory traceback: \'bool\' is not iterable\n```\r\n# repository dir\r\n/var/lib/subversion:\r\n  file.directory:\r\n    - group: svn\r\n    - dir_mode: 750\r\n    - file_mode: 640\r\n    - recurse: true\r\n    - require:\r\n      - user: user-svn\r\n```\r\n\r\n```\r\n----------\r\n    State: - file\r\n    Name:      /var/lib/subversion\r\n    Function:  directory\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1201, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/file.py", line 956, in directory\r\n    if not set([\'user\', \'group\', \'mode\']) >= set(recurse):\r\nTypeError: \'bool\' object is not iterable\r\n\r\n        Changes:\r\n```\r\nProbably should test if `recurse` is a list...'
5737,'cachedout',"new grains not refreshed for matches\nWhen a new grain is distributed to the minions by using saltutil.sync_grains, it's data is listed when doing a grains.items but the data cannot be used while matching based on the grains using -G option.\r\n\r\nFor example, writing a grain like the below\r\n```\r\ndef something():\r\n     return {'spell':'alohomora'}\r\n```\r\n\r\nAnd running \r\n```salt '*' saltutil.sync_grains```\r\nwill distribute the grain.\r\n\r\nAnd it will even show in the output of \r\n```sudo salt '*' grains.items```\r\n\r\nBut matching minions, based on the newly written grain data does **not** work.\r\n```salt -G 'spell:alohomora' test.ping```\r\n\r\nUnless, the minion is restarted OR *saltutils.sync_modules* is run from the master, matches based on this new grain do not seem to work.\r\n\r\nShouldn't there be some command to get the minion to send/store the newly gained grain ? And shouldn't it be triggered automatically when we do *sync_grains* ?"
5733,'terminalmage','pkg.installed always run apt-get update when it is not necessary\nThis is my only state, I want to install pkg from a deb file, so there is no need to run update at all.\r\n\r\n```python\r\nnginx:\r\n  pkg:\r\n    - installed\r\n    - sources:\r\n      - nginx: http://nginx.org/packages/ubuntu/pool/nginx/n/nginx/nginx_1.4.1-1~precise_i386.deb\r\n```\r\n\r\nBut salt do update:\r\n```bash\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/home/hvn\'\r\nk[WARNING ] "name" parameter will be ignored in favor of "sources".\r\n```\r\n\r\nI\'m using `0.15.3`\r\n'
5729,'cro','150+ salt-minions running on 0.15.3\nMy salt-master runs in an HA cluster and one we were rebuilding the master over the weekend so the salt-master was down for over 48hrs, all of my minions have spawed around 100-150 processes all with separate PIDs but they are all forked from PID 1 as if they are coming from systemd.  I am not sure why I am seeing 100+ minions though, systemd manifest also looks pretty straight forward.  Curious if this is a bug I have hit I wouldn\'t imagine this to be the normal behavior.\r\n\r\nHas anyone seen anything like that with the latest salt?  Any strace shows them in epoll_wait(), resumes then an attempt to open a socket to the master and then fails. The cycle repeats as epoll should in an event loop. \r\n\r\n[pid 32378] <... epoll_wait resumed> {}, 256, 5005) = 0\r\n[pid 32378] socket(PF_INET, SOCK_STREAM|SOCK_CLOEXEC, IPPROTO_TCP) = 28\r\n[pid 32378] fcntl(28, F_GETFL)          = 0x2 (flags O_RDWR)\r\n[pid 32378] fcntl(28, F_SETFL, O_RDWR|O_NONBLOCK) = 0\r\n[pid 32378] connect(28, {sa_family=AF_INET, sin_port=htons(4506), sin_addr=inet_addr("10.8.4.97")}, 16) = -1 EINPROGRESS (Operation now in progress)\r\n[pid 32378] epoll_ctl(20, EPOLL_CTL_ADD, 28, {...}) = 0\r\n[pid 32378] epoll_ctl(20, EPOLL_CTL_MOD, 28, {...}) = 0\r\n[pid 32378] epoll_wait(20, {?} 0x7fe192ffb1d0, 256, -1) = 1\r\n[pid 32378] getsockopt(28, SOL_SOCKET, SO_ERROR, [111], [4]) = 0\r\n[pid 32378] epoll_ctl(20, EPOLL_CTL_DEL, 28, {...}) = 0\r\n[pid 32378] close(28)                   = 0\r\n[pid 32378] epoll_wait(20, \r\n'
5716,'terminalmage',"pillar.data and state.highstate do not always update the pillar on the minion\nI had a pillar that generated an error (bug #5608 test, it's called list-all-minions).\r\n\r\nhere I remove the reference to the pillar:\r\n```\r\nroot@salt-master:/home/salt/conf/machine/monitoring/munin-master# vi /home/salt/pillar/top.sls\r\n```\r\nAnd here I run highstate on that minion:\r\n```\r\nroot@salt-master:/home/salt/conf/machine/monitoring/munin-master# salt monitoring state.highstate test=true\r\nmonitoring:\r\n    Data failed to compile:\r\n----------\r\n    Pillar failed to render with the following messages:\r\n----------\r\n    Rendering SLS list-all-minions failed, render error:\r\n\r\n\r\n[snip same traceback as in #5608]\r\n```\r\npillar data works:\r\n```\r\nroot@salt-master:/home/salt/conf/machine/monitoring/munin-master# salt monitoring pillar.data\r\nmonitoring:\r\n    ----------\r\n    allhosts:\r\n        ----------\r\n        bulk:\r\n[lots of private pillar data]\r\n```\r\nBut state.highstate still does not work:\r\n```\r\nroot@salt-master:/home/salt/conf/machine/monitoring/munin-master# salt monitoring state.highstate test=true\r\nmonitoring:\r\n    Data failed to compile:\r\n----------\r\n    Pillar failed to render with the following messages:\r\n----------\r\n    Rendering SLS list-all-minions failed, render error:\r\n\r\n[same trackback as in bug #5608]\r\n```\r\n\r\nIs this related to test=true looking at another cache? No, without test=true I still get the error.\r\n\r\nI thought you had said pillar.data always updates the cache on the minion..."
5706,'s0undt3ch',"Runners not logging to file\nUsing salt 0.15.3 I created a runner but I am unable to get it to log to a file.  (None of the current runners are using logging)\r\n\r\nHere is my runner (testrun.py) I was using to test logging...\r\n```\r\nimport salt.output\r\nimport logging\r\n\r\nlog = logging.getLogger(__name__)\r\n\r\ndef test():\r\n    ret = 'Testing logging'\r\n    log.info('Test log message')\r\n    log.error('Test error log message')\r\n    salt.output.display_output(ret, '', __opts__)\r\n    return ret\r\n```"
5700,'thatch45','Input/output error when do not redirect output FD\'s\nI\'ve found a lot of open issues on github/saltstack related to this code:\r\n\r\n\\# A normal daemonization redirects the process output to /dev/null.\r\n\\# Unfortunately when a python multiprocess is called the output is\r\n\\# not cleanly redirected and the parent process dies when the\r\n\\# multiprocessing process attempts to access stdout or err.\r\n\\#dev_null = open(\'/dev/null\', \'rw\')\r\n\\#os.dup2(dev_null.fileno(), sys.stdin.fileno())\r\n\\#os.dup2(dev_null.fileno(), sys.stdout.fileno())\r\n\\#os.dup2(dev_null.fileno(), sys.stderr.fileno())\r\n\r\nbut have not found clear description of consequences. I already mentioned this issue in #5199. \r\n\r\nLet\'s say I logged to SSH and restarted some salt process (master, minion or api) and logged out. Processes have not changed its output FD\'s and since my pts doesn\'t exist anymore (deleted after SSH stopped) we got:\r\n\r\nsalt-master 32288 root    0u   CHR  136,2      0t0       4 /dev/pts/2 (deleted)\r\nsalt-master 32288 root    1u   CHR  136,2      0t0       4 /dev/pts/2 (deleted)\r\nsalt-master 32288 root    2u   CHR  136,2      0t0       4 /dev/pts/2 (deleted)\r\n\r\nThen when this process (master, minion or api) put output to stdout or err, we got \r\n\r\nsalt-api output:\r\nwrite(1, "127.0.0.1 - - [24/Jun/2013:12:41:43] \\"GET /jobs/20130624124143585565 HTTP/1.1\\" 500 57 \\"\\" \\"\\"\\n", 92) = -1 EIO (Input/output error) \r\n\r\nminion output:\r\n2013-05-22 15:31:33,433 [salt.minion ][ERROR ] Exception [Errno 5] Input/output error occurred in scheduled job\r\n\r\nand the same happens to master.\r\n\r\nThe way to workaround this - start/restart processes from server console.\r\n\r\nI\'d like @basepi and @whiteinge (as we told on IRC) to take a look at this. I realize that this cannot be fixed as 1+1, but should be aware of.'
5682,'terminalmage','int/float version numbers in winrepo cause tracebacks in pkg states\nyaml.safe_load is used to load the winrepo data, but if a version number is something like **3** or **2.0**, it gets loaded as an int or float rather than a string. This causes issues in package states.\r\n\r\nThe version numbers should be normalized to strings when the winrepo.genrepo runner generates winrepo.p.'
5677,'whiteinge',"minions hangs on highstate sent by the master if HOME environment variable is not set properly\nhttps://groups.google.com/forum/#!topic/salt-users/K1Qbz87_vHc\r\n\r\nOn CentOS 5/EL5, salt-minion will hangs when sent a state.highstate from a master if HOME is not set properly.\r\n\r\n**Doesn't work**\r\n> env -i /etc/init.d/salt-minion start\r\n\r\n**Works**\r\n> env -i HOME=/root /etc/init.d/salt-minion start\r\n\r\nTurns out the 'service' utility script uses env -i and sets LANG, PATH & TERM but does not set HOME which causes the issue.\r\n\r\nHere's the part in my minion log when the minion hangs although this doesn't tell much\r\n```\r\n2013-06-21 15:58:08,514 [salt.loader                                 ][DEBUG   ] loading states in ['/var/cache/salt/minion/extmods/states', '/usr/lib/python2.6/site-packages/salt/states']\r\n2013-06-21 15:58:08,514 [salt.loader                                 ][DEBUG   ] Skipping /var/cache/salt/minion/extmods/states, it is not a directory\r\n2013-06-21 15:58:08,541 [salt.loader                                 ][DEBUG   ] Loaded mdadm as virtual raid\r\n2013-06-21 15:58:08,548 [salt.loader                                 ][DEBUG   ] loading render in ['/var/cache/salt/minion/extmods/renderers', '/usr/lib/python2.6/site-packages/salt/renderers']\r\n2013-06-21 15:58:08,548 [salt.loader                                 ][DEBUG   ] Skipping /var/cache/salt/minion/extmods/renderers, it is not a directory\r\n2013-06-21 15:58:38,174 [salt.minion                                 ][INFO    ] User root Executing command saltutil.find_job with jid 20130621155838212110\r\n2013-06-21 15:58:38,175 [salt.minion                                 ][DEBUG   ] Command details {'tgt_type': 'glob', 'jid': '20130621155838212110', 'tgt': 's001*', 'ret': '', 'user': 'root', 'arg': ['20130621155807344086'], 'fun': 'saltutil.find_job'}\r\n2013-06-21 15:58:38,213 [salt.loaded.int.module.cmdmod               ][INFO    ] Executing command 'ps -efH' in directory '/root'\r\n2013-06-21 15:58:38,311 [salt.loaded.int.module.cmdmod               ][DEBUG   ] output: UID          PID    PPID  C STIME TTY          TIME CMD\r\n```"
5668,'basepi','Touch state module is limited to creating files as root\nSometimes I just want a file to exist, without needing anything to actually exist in it. The touch command is great for this.\r\n\r\nUnfortunately, the existing touch state module does not seem to allow for it to create files with any user other than root. It would be great if such an option could be implemented.\r\n\r\nAs an optional addition to this request, it might be nice if there was a flag to say "only if it doesn\'t already exist". Currently I use "- unless: test -f <filename>" for the functionality, but having it built into Salt might be nicer.\r\n\r\nCheers,\r\nAdam'
5637,'UtahDave','Minion not connecting to master after logging out from windows server\nFrom salt-users:\r\n\r\n> We have minion running on Windows Server 2008 & configured to run as service. Its connected with the Master on a Redhat, it works perfect when the user logged into the server is still connected. Once we logout the user, the connection between the minion & master is lost even though the minion is still running as service.\r\n\r\n>Also tried starting minion in debug mode but not able to figure out how to set the master configuration. As per the default installation configuration it works fine, if i start in debug mode with C:\\salt\\conf its not working. Still it takes the default "salt" as master.\r\n\r\n>Thanks & Regards,\r\nAnbu\r\n\r\nThis smells familiar.... I think the problem it is then time.sleep() raises an IOError when a user logs off (Interrupted System Call IIRC). It\'s pure evil! I presume it interrupts any system call - zmq socket calls etc...'
5613,'terminalmage','spurious file.directory error\nRelated to #5612, \r\n\r\nIf you say\r\n```\r\nssh-chroot:\r\n  file.directory:\r\n    - name: /var/lib/svn-chroot\r\n    - mode: "000"\r\n```\r\nyou get \r\n```\r\n----------\r\n    State: - file\r\n    Name:      /var/lib/svn-chroot\r\n    Function:  directory\r\n        Result:    False\r\n        Comment:   Failed to change mode to 0\r\n        Changes:\r\n```\r\n(but it works when you just say `- mode: 0`)'
5609,'s0undt3ch','Expose args for controlling log file and log level\nThis is a duplicate of https://github.com/saltstack/salt-cloud/issues/616 and since the fix is applied to salt I was asked to recreate the issue over here.'
5606,'terminalmage',"pkgutil module doesn't support multiple targets in latest_version\nShould be very easy to add."
5579,'UtahDave','Minions arbitrarily lose connections to Master\nMinions seem to randomly drop their connection to the master. My minions are Windows but other (admittedly anecdotal) sources also mention this with Linux minions. As one data point, here is a note from OpenCredo.com:\r\n\r\n*The only real problem I found was that connections between the salt-master and a minion would sometimes go AWOL. A read around the web told me that others have experienced it too in this particular version of Salt (0.10.5).*\r\nSource: http://www.opencredo.com/blog/a-dive-into-salt-stack\r\n\r\nNot sure were to start for this, but dropped minion connections is a daily thing for us.'
5574,'basepi','Improve docs of cmd.script state function\nHi,\r\n\r\nIt should be mentioned that cmd.script takes "args" as argument, beside name, source, etc\r\nWithout "args", there\'s no way one could pass arguments to the script as\r\n"name: script_name arg1" doesn\'t work.\r\n\r\nThis is needed because some people forget the fact that cmd.script is just a wrapper of cmdmod.script execution module that does take "args" as argument when specifying script args, or think that passing args to script is possible only when using the execution module.\r\n'
5573,'basepi',"salt-call state.highstate test=True not reporting changes\nOS - CentOS 6.4\r\nSalt - EPEL 6 - 0.15.3\r\n\r\nI have a situation where whenever I update httpd on server with a very custom app, I need to run salt-call state.highstate to restore some nonstandard permissions.  This is the YAML that keeps the permissions:\r\n\r\n    {% for path in '/var/www', '/var/www/cgi-bin' %}\r\n    {{ path }}:\r\n      file.directory:\r\n        - user: webuserguy\r\n        - group: webgroup\r\n        - mode: '2755'\r\n        - require:\r\n          - user: webuserguy\r\n          - pkg: httpd\r\n    {% endfor %}\r\n\r\nWhen I run salt-call state.highstate test=True, it doesn't report these changes will be made, but when I run without test=True, it makes the changes.\r\n\r\n----------\r\n    State: - file\r\n    Name:      /var/www/cgi-bin\r\n    Function:  directory\r\n        Result:    True\r\n        Comment:   Directory /var/www/cgi-bin updated\r\n        Changes:   group: webgroup\r\n                   mode: 2755\r\n                   user: webuserguy\r\n                   \r\n----------\r\n    State: - file\r\n    Name:      /var/www\r\n    Function:  directory\r\n        Result:    True\r\n        Comment:   Directory /var/www updated\r\n        Changes:   group: webgroup\r\n                   mode: 2755\r\n                   user: webuserguy\r\n\r\nMy first guess is the mode that is tripping it up.  We have to keep it 2755 so any new files created have the same ownership as the directory.\r\n\r\nThanks!"
5550,'terminalmage','`mysql_user.present` with `- password: null` Should Clear the Password for the User\nRight now, `mysql.user_exists` accepts password=None which skips checking the password when None, preventing the system from clearing passwords already set on a user.\r\n'
5540,'basepi','salt-run jobs.active throws NameError\n    # salt-run --version\r\n    salt-run 0.15.3\r\n    \r\n    # salt-run jobs.active\r\n    Traceback (most recent call last):\r\n      File "/usr/bin/salt-run", line 10, in <module>\r\n        salt_run()\r\n      File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 89, in salt_run\r\n        client.run()\r\n      File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 275, in run\r\n        runner.run()\r\n      File "/usr/lib/pymodules/python2.7/salt/runner.py", line 83, in run\r\n        self.opts[\'fun\'], self.opts[\'arg\'], self.opts)\r\n      File "/usr/lib/pymodules/python2.7/salt/runner.py", line 49, in cmd\r\n        return self.functions[fun](*args, **kwargs)\r\n      File "/usr/lib/pymodules/python2.7/salt/runners/jobs.py", line 35, in active\r\n        \'User\': load.get(\'user\', \'root\')}\r\n    NameError: global name \'load\' is not defined\r\n\r\n    # salt --versions-report\r\n               Salt: 0.15.3\r\n             Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.4.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 13.0.0\r\n                ZMQ: 3.2.2\r\n'
5534,'basepi','git.latest Fails if using new branch name as rev\nI just switched the rev on my git.latest state to a new branch. When running highstate, git.latest fails because it says it can\'t find the pathspec.\r\n\r\n      File "/usr/lib/pymodules/python2.7/salt/state.py", line 1210, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n    File "/var/cache/salt/minion/extmods/states/git.py", line 70, in latest\r\n    __salt__[\'git.checkout\'](target, rev, force=force)\r\n    File "/usr/lib/pymodules/python2.7/salt/modules/git.py", line 351, in checkout\r\n    return _git_run(cmd, cwd=cwd, runas=user)\r\n    File "/usr/lib/pymodules/python2.7/salt/modules/git.py", line 75, in _git_run\r\n    raise exceptions.CommandExecutionError(result[\'stderr\'])\r\n    CommandExecutionError: error: pathspec \'staging\' did not match any file(s) known to git.\r\n\r\nShouldn\'t it be fetching remote before checking if the pathspec exists?\r\n\r\nI\'ve tried adding the always_fetch option and it does not appear to change the behavior.  I am also using the --force-checkout option, fwiw.'
5532,'UtahDave',"Powershell as an alternate shell for cmd.script\nI want to be able to do this:\r\n\r\n    salt '*' cmd.script salt://win/repo/scripts/awesome_sauce.ps1 shell='powershell' args='-ingredients secret'\r\n\r\nCurrently (v 0.15.1), when I do this, the args are silently ignored. This is because Salt executes the Powershell script from within a cmd.exe environment, which causes the args to be ignored.  We should have a shell=powershell option so we can easily run Powershell scripts on Minions."
5509,'terminalmage','pkg.remove & pkg.purge on CentOS 6.3 failing with Salt 0.15.3\nI just recently started getting this error on my CentOS 6.3 test systems when using salt 0.15.3:\r\n\r\n<pre>\r\nTypeError: remove() got multiple values for keyword argument \'pkgs\'\r\n</pre>\r\n\r\nTo reproduce the error, I created the following two salt state files:\r\n\r\n* /srv/salt/ftop/init.sls\r\n<pre>\r\n    ftop:\r\n        pkg.installed\r\n</pre>\r\n\r\n* /srv/salt/ftop/absent.sls\r\n<pre>\r\n    ftop:\r\n        pkg.removed\r\n</pre>\r\n\r\nNow, simply run an install and then attempt an uninstall:\r\n\r\n<pre>\r\nroot@salt:~# salt \'slave01*\' state.sls ftop\r\nslave01.spop.sjc.shn:\r\n----------\r\n    State: - pkg\r\n    Name:      ftop\r\n    Function:  installed\r\n        Result:    True\r\n        Comment:   The following packages were installed/updated: ftop.\r\n        Changes:   ftop: { new : 1.0-3.el6\r\nold :\r\n}\r\n</pre>\r\n\r\n<pre>\r\nroot@salt:~# salt \'slave01*\' state.sls ftop.absent\r\nslave01.spop.sjc.shn:\r\n----------\r\n    State: - pkg\r\n    Name:      ftop\r\n    Function:  removed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/state.py", line 1201, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.6/site-packages/salt/states/pkg.py", line 720, in removed\r\n    return _uninstall(action=\'remove\', name=name, pkgs=pkgs, **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/states/pkg.py", line 678, in _uninstall\r\n    changes = __salt__[\'pkg.{0}\'.format(action)](name, pkgs=pkgs, **kwargs)\r\nTypeError: remove() got multiple values for keyword argument \'pkgs\'\r\n\r\n        Changes:\r\n</pre>'
5500,'terminalmage','Nodegroup matching bug: traceback when nodegroups are empty. \nIf the **nodegroups** line is uncommented and there are no defined nodegroups, what is supposed to happen is that the salt CLI should say something like "nodegroup foo is not available in /etc/salt/master". Instead, there is now a traceback:\r\n\r\n```\r\n(saltdev)erik@virtucentos:~/saltdev% egrep -A3 -B3 \'^nodegroups:\' etc/salt/master\r\n#   group1: \'L@foo.domain.com,bar.domain.com,baz.domain.com and bl*.domain.com\'\r\n#   group2: \'G@os:Debian and foo.domain.com\'\r\n\r\nnodegroups:\r\n\r\n#####     Range Cluster settings     #####\r\n##########################################\r\n```\r\n\r\nAnd the traceback is here:\r\n\r\n```\r\n(saltdev)[root@virtucentos saltdev]# salt -c etc/salt -N www test.ping\r\nTraceback (most recent call last):\r\n  File "/home/erik/saltdev/bin/salt", line 8, in <module>\r\n    load_entry_point(\'salt==0.15.0-1262-g16bcbee\', \'console_scripts\', \'salt\')()\r\n  File "/home/erik/salt/salt/scripts.py", line 103, in salt_main\r\n    client.run()\r\n  File "/home/erik/salt/salt/cli/__init__.py", line 117, in run\r\n    for full_ret in cmd_func(**kwargs):\r\n  File "/home/erik/salt/salt/client.py", line 357, in cmd_cli\r\n    **kwargs)\r\n  File "/home/erik/salt/salt/client.py", line 213, in run_job\r\n    **kwargs)\r\n  File "/home/erik/salt/salt/client.py", line 1005, in pub\r\n    if tgt not in self.opts[\'nodegroups\']:\r\nTypeError: argument of type \'NoneType\' is not iterable\r\n```\r\n\r\nShould be easy to fix once it is tracked down.'
5464,'terminalmage','Allow passive service disabled behavior when service doesn\'t exist.\nI have a number of service declarations in my states for newly built machines which establishes a baseline of running services. In some cases, these states declare "dead" and "disabled" on services which may not be installed on the host leading to some really nasty errors where there should, for all intents and purposes. be none.\r\n\r\nThe service state should probably check for the existance of the service before attempting to disable the service:\r\n\r\nState:\r\nxfs:\r\n  service:\r\n    - dead\r\n    - enable: False\r\n\r\nResult:\r\n----------\r\n    State: - service\r\n    Name:      xfs\r\n    Function:  dead\r\n        Result:    False\r\n        Comment:   Failed when setting service xfs to not start at boot, but the service was already running\r\n        Changes:\r\n\r\nLooks like salt is somewhat confused here too, since the error Comment message makes no sense (I\'ve checked, xfs isn\'t installed on the host nor is it running).\r\n\r\nThe logic here should probably go:\r\n* Does the service exist?\r\n  - Yes: \r\n      - Is the service running?\r\n         - Yes: Stop the service and Disable\r\n         - No: Disable the service\r\n  - No: There\'s nothing we can do, log this and return Result: True \r\n          (Same as if the service was already disabled)\r\n\r\nAlternatively, the current behavior would be fine if the Comment above made more sense and with the addition of an ignore-missing: argument which would cause the state to exit with a non-error result if the service doesn\'t exist. Something like this:\r\n\r\nState:\r\nxfs:\r\n  service:\r\n    - dead\r\n    - enable: False\r\n    - ignore-missing: True\r\n\r\n\r\n'
5463,'terminalmage',"Random Cron Timing Feature\nOccasionally, it's necessary to create a cronjob with a random minute or hour to stagger the execution of that cronjob across a range of time on several hosts to normalize load on a service. For example, we have a Spacewalk server and we don't run rhnsd, instead we schedule a cronjob on all hosts to check into the server periodcally. If all hosts were to check in at exactly the same time, it would probably overload the Spacewalk server (or at least slow down the check-ins substantially). To manage this random cronjob state, I'm using jinja to generate a random value for it like so:\r\n\r\n/usr/sbin/rhn_check > /dev/null 2>&1:\r\n  cron.present:\r\n    - user: root\r\n    - minute: {{ range(59)|random }}\r\n\r\nThe unfortunate side effect is that every time the state is run, a new value will be generated and the cronjob will be changed.\r\n\r\nWhat I propose is a keyword of RANDOM which will generate a random time when the initial cronjob state is run and then skip changing the time on future runs as long as the cronjob exists:\r\n\r\n/usr/sbin/rhn_check > /dev/null 2>&1:\r\n  cron.present:\r\n    - user: root\r\n    - minute: RANDOM\r\n\r\n"
5462,'UtahDave',"virtualenv doesn't work on windows\nThe virtualenv & pip modules assume binaries are in bin (they are in Scripts on windows) and they run '. bin/active' to activate the virtualenv which is sh/bash specific.\r\n\r\nIt's a simple fix. I'll submit a pull request soon when I have time to prepare it."
5460,'basepi',"State: file.managed: no replace & contents\nSalt can't create file with specific contents without replacement:\r\n```\r\n/tmp/file:\r\n  file.managed:\r\n    - replace: False\r\n    - contents: test\r\n```\r\n* Output: no actions\r\n* Result: file touched and empty\r\n\r\nThanks\r\n\r\n--\r\nSalt: 0.15.3\r\nPython: 2.7.3 (default, Sep 26 2012, 21:51:14)\r\nJinja2: 2.6\r\nM2Crypto: 0.21.1\r\nmsgpack-python: 0.1.10\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.6\r\nPyYAML: 3.10\r\nPyZMQ: 13.0.0\r\nZMQ: 3.2.2"
5457,'terminalmage',"don't output diff of binary files\nafter outputting the diff of a binary distributed using salt,\r\nthe console is messed up.\r\n\r\n<pre>\r\n    State: - file\r\n    Name:      /opt\r\n    Function:  recurse\r\n        Result:    None\r\n        Comment:   #### /opt/MegaRAID/MegaCli/MegaCli ####\r\nThe following values are set to be changed:\r\ndiff: --- \r\n+++ \r\n@@ -1,2967 +1,2922 @@\r\n-^?ELF^A^A^A^...\r\n</pre>\r\n\r\nIMHO salt should not diff binaries already, so this might be a bug in file.recurse.\r\n"
5455,'terminalmage','pkgs latest version test fails\nSystem have latest netcat installed, salt fails to test versions: \r\n\r\n```\r\ncore:\r\n  pkg.latest:\r\n    - pkgs:\r\n      - wget\r\n      - curl\r\n      - netcat\r\n```\r\n\r\nSee typo on compare-versions:\r\n```\r\n[INFO    ] Executing command \'apt-cache -q policy netcat | grep Candidate\' in directory \'/root\'\r\n[INFO    ] Executing command \'dpkg --compare-versions "1,1.10-40" lt "1.10-40"\' in directory \'/root\'\r\n[INFO    ] Executing command \'apt-get -q -y -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef   install netcat\' in directory \'/root\'\r\n...\r\n----------\r\n    State: - pkg\r\n    Name:      core\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   Package netcat failed to update. The following packages were already up-to-date: curl,  wget\r\n        Changes:   \r\n----------\r\n\r\n```\r\n\r\n```\r\nroot@ns4000892:~# dpkg --list | grep netcat\r\nii  netcat                           1.10-40                          all          TCP/IP swiss army knife -- transitional package\r\nii  netcat-openbsd                   1.105-7ubuntu1                   amd64        TCP/IP swiss army knife\r\nii  netcat-traditional               1.10-40                          amd64        TCP/IP swiss army knife\r\nii  netcat6                          1.0-8                            amd64        TCP/IP swiss army knife with IPv6 support\r\n\r\nminion# salt-minion --versions-report\r\n           Salt: 0.15.3\r\n         Python: 2.7.4 (default, Apr 19 2013, 18:28:01)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.2.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n\r\nRunning on: Ubuntu 13.04 raring\r\nLinux hostx 3.8.13-xxxx-grs-ipv6-64 #3 SMP Fri May 31 13:26:19 CEST 2013 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\nmaster# salt --versions-report\r\n           Salt: 0.15.1\r\n         Python: 2.6.6 (r266:84292, Feb 22 2013, 00:00:18)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.9.final\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n```'
5454,'whiteinge','config examples page format broken...\nhttp://docs.saltstack.com/ref/configuration/examples.html#configuration-examples-master\r\n\r\nlooks crazy... http://cl.ly/image/162m2t3e3D0N'
5450,'UtahDave','locale.getpreferredencoding causing traceback\nWhen running ```salt -G \'os:Windows\' pkg.list_pkgs``` I\'m getting the following stacktrace returned:\r\n\r\n```\r\nubuntu@ip-10-152-148-5:~$ sudo salt \\* pkg.list_pkgs\r\nlkjlkj:\r\n    Traceback (most recent call last):\r\n      File "c:\\salt\\python27\\lib\\site-packages\\salt\\minion.py", line 646, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "c:\\salt\\python27\\lib\\site-packages\\salt\\modules\\win_pkg.py", line 217, in list_pkgs\r\n        for key, val in _get_reg_software().iteritems():\r\n      File "c:\\salt\\python27\\lib\\site-packages\\salt\\modules\\win_pkg.py", line 297, in _get_reg_software\r\n        encoding = locale.getpreferredencoding()\r\n    AttributeError: \'module\' object has no attribute \'getpreferredencoding\'\r\n```\r\n\r\nOn that windows machine if I open a python interpreter and import locale   locale.getpreferredencoding()  works just fine. I\'m not sure if it has something to do with threading.\r\n\r\n@djs52, do you have any idea why this would be stacktracing?'
5449,'thatch45','file_roots gets assigned the content of pillar_roots\nVersion: 0.15.1 (seen on 0.15.3 too)\r\nOS: Fedora 18\r\n\r\nThe content of the file_roots master option is replaced by the content of pillar_roots, example:\r\n\r\n(master conf)\r\nfile_roots:\r\n  base:\r\n    - /srv/salt/states\r\n\r\npillar_roots:\r\n  base1:\r\n    - /srv/salt/pillar\r\n\r\n\r\nAnd when running "salt \'*\' pillar.data" I can see that file_roots contains:\r\nfile_roots:\r\n  base1:\r\n    - /srv/salt/pillar\r\n\r\nExpected:\r\nfile_roots:\r\n  base:\r\n    - /srv/salt/states\r\n\r\nThis is (for me at least) preventing the minions to fetch the .sls and other files from the states dir, because it tries to fetch them from the pillar dir instead.'
5440,'basepi',"minion uses wrong default environment when using git backend\nWhen using git as a backend and the environment is not explicitly specified, salt may select an environment/branch other than the base/master. I've specifically seen this with state.show_top and state.highstate.\r\n\r\nHere's the (truncated) top file for the main branch:\r\n```\r\nroot@SALT:/srv/salt# cat top.sls \r\nbase:\r\n  '*':\r\n    - salt\r\n    - users\r\n    - log\r\n    - ssh\r\n    - snmp\r\n```\r\n\r\nOne of the servers (it's affecting all servers)\r\n```\r\nroot@SERVER1:~# salt-call state.show_top\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\n[INFO    ] Fetching file 'salt://top.sls'\r\nbase:\r\n    ----------\r\n    *:\r\n        - salt\r\n        - users\r\n        - log\r\n\r\n[truncated]\r\n```\r\n\r\nIt's missing the last two states, ssh and snmp. It seems to be picking up the top.sls file from an env/branch other than the base/master. It matched two branches, so I removed both of them from git and tested it again. It picked a different branch this time, still not the master. It's not picking the newest branch or the earliest in the alphabet, I'm unsure how it decides what to use.\r\n\r\nHere are select parts of the salt master conf file:\r\n```\r\nfile_roots:\r\n  base:\r\n    - /srv/salt\r\nfileserver_backend:\r\n  #- roots\r\n  - git\r\ngitfs_remotes:\r\n  - git://[servername]/salt\r\npillar_roots:\r\n  base:\r\n    - /srv/pillar\r\n```\r\n\r\nRunning state.sls with env=branchname works just fine and uses the environment specified.\r\n\r\nI'm using Salt 0.15.1, but I'm reasonably sure I've seen similar behavior in earlier versions. The master is on Ubuntu 12.04 LTS with git 1.7.9.5 and python-git 0.1.6.\r\n\r\nOriginal mailing list post here\r\nhttps://groups.google.com/d/msg/salt-users/HMQ1sZxB2sY/Ylc6uFuhpGQJ"
5431,'terminalmage','Incredibly annoying "Unable to import "softwareproperties.ppa" warning\nThis warning is written into the logs or the cli whenever grains are accessed. Maybe it would be better to only show this warning when something requiring the library is actively used (like trying to add a ppa, or install a package from a ppa)?'
5429,'terminalmage','pkg.latest_version does not work for 32-bit pkgs on 64-bit RHEL/CentOS\nBecause the package names passed to this function are used in the command run to find the updated packages, using 32-bit package names like **openssl.i686** will return no results.'
5425,'terminalmage',"Highstate rapid fire: Executing command 'runlevel /run/utmp' in directory '/root'\nHowdy,\r\n\r\nWhen doing a highstate, Salt seems to go into a rapid fire loop of \r\n\r\n```\r\n2013-06-06 18:05:49,915 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,920 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n```\r\n\r\nover and over all with-in a very short time.  It is also doing this on a very simple Salt State.  I hope everything below helps.\r\n\r\n\r\n#### OS\r\n```\r\nUbuntu Server 13.04\r\n```\r\n\r\n\r\n#### Salt State\r\n```\r\niptables_load:\r\n  pkg.installed:\r\n    - name: iptables-persistent\r\n  service.running:\r\n    - name: iptables-persistent\r\n```\r\n\r\n#### Salt Minion Log\r\n```\r\n2013-06-06 18:05:49,878 [salt.state       ][INFO    ] Executing state service.running for iptables-persistent\r\n2013-06-06 18:05:49,879 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'service iptables-persistent status' in directory '/root'\r\n2013-06-06 18:05:49,889 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,899 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,900 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,907 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,908 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,913 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,915 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,920 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,922 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,929 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,930 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,937 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,938 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,944 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,952 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,959 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,960 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,967 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,968 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,973 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,975 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,981 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,982 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,989 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,991 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:49,996 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:49,998 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,004 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,006 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,012 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,013 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,022 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,023 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,030 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,031 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,038 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,040 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,046 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n2013-06-06 18:05:50,048 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'runlevel /run/utmp' in directory '/root'\r\n2013-06-06 18:05:50,055 [salt.loaded.int.module.cmdmod][DEBUG   ] output: N 2\r\n```\r\n\r\n#### Salt Version\r\n    salt --versions-report\r\n        Salt: 0.15.3\r\n        Python: 2.7.4 (default, Apr 19 2013, 18:28:01)\r\n        Jinja2: 2.6\r\n        M2Crypto: 0.21.1\r\n        msgpack-python: 0.2.0\r\n        msgpack-pure: Not Installed\r\n        pycrypto: 2.6\r\n        PyYAML: 3.10\r\n        PyZMQ: 13.0.0\r\n        ZMQ: 3.2.2\r\n\r\n"
5423,'basepi',"file.recurse\nStrange error uploading files using file.recurse.\r\nIf I understand correctly we are talking about a typo.\r\n\r\nImagine state definition:\r\n\r\n    /srv/moinmoin/data:\r\n      file:\r\n        - recurse\r\n        - source: salt://moinmoin/seed/data\r\n        - include_empty: True\r\n        - user: moinmoin\r\n        - group: moinmoin\r\n        - file_mode: 644\r\n        - dir_mode: 755\r\n        - require:\r\n          - user: moinmoin\r\n\r\n\r\nstete.highstate output:\r\n\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: ----------\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out:     State: - file\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out:     Name:      /srv/moinmoin/data\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out:     Function:  recurse\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out:         Result:    False\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out:         Comment:   #### /srv/moinmoin/data/pages/__init__.py ####\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: Source file salt://|moinmoin/seed/data/pages/__init__.py not found\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: \r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: #### /srv/moinmoin/data/plugin/__init__.py ####\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: Source file salt://|moinmoin/seed/data/plugin/__init__.py not found\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: \r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: #### /srv/moinmoin/data/testme ####\r\n    [ec2-x-y-z-a.eu-west-1.compute.amazonaws.com] out: Source file salt://|moinmoin/seed/data/testme not found\r\n\r\n\r\nFollowing seems to fix it:\r\n\r\n    /states/file.py:\r\n        def manage_file(path, source):\r\n    -        source = '{0}|{1}'.format(source[:7], source[7:])\r\n    +        source = '{0}{1}'.format(source[:7], source[7:])\r\n "
5422,'UtahDave',"win_pkg.install doesn't support pkgs\nThe Windows package installer blindly uses name to find the package rather than checking pkgs. "
5421,'terminalmage',"Grain matching issue\nI get a list back in the grain and its matching doesn't work as expected. Mentioned this on IRC, but though it got little attention, I believe this is a real issue.\r\n\r\nFirst globbing:\r\n\r\n    salt@MASTER:~$ salt -G 'ec2_tags:*XXXXX-production*' test.ping\r\n    No minions matched the target. No command was sent, no jid was assigned.\r\n\r\nThen regexp:\r\n\r\n    salt@MASTER:~$ salt --grain-pcre 'ec2_tags:.*XXXXX-production.*'  test.ping\r\n    ip-MINION-1.eu-west-1.compute.internal:\r\n        True\r\n    ip-MINION-2.eu-west-1.compute.internal:\r\n        True\r\n\r\nAre the grains made strings differently for different matching methods? Shouldn't they behave the same?\r\n\r\nVersions:\r\n\r\n               Salt: 0.15.2\r\n             Python: 2.7.3 (default, Jan  2 2013, 13:56:14)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 2.2.0\r\n                ZMQ: 2.2.0\r\n"
5407,'terminalmage','RHEL 6.3 : "service.reload" is not available\nI can\'t seem to reload a service (while "service squid reload" works)\r\n\r\n```bash\r\n# salt \'sproxy01a*\' service.reload squid\r\nsproxy01a.example.org:\r\n    "service.reload" is not available.\r\n```\r\n\r\n```bash\r\n# salt \'sproxy01a*\' grains.get os\r\nsproxy01a.example.org:\r\n    RedHat\r\n```\r\n\r\n```bash\r\n# salt \'sproxy01a*\' grains.get osrelease\r\nsproxy01a.example.org:\r\n    6.3\r\n```\r\n\r\nAlso I have the following state : \r\n\r\n```bash\r\nsquid:\r\n  pkg:\r\n    - installed\r\n    - name: {{ pillar[\'convention-os\'][\'pkg\'][\'squid\'] }}\r\n\r\n  service:\r\n    - running\r\n    - enable: True\r\n    - reload: True\r\n    - name: {{ pillar[\'convention-os\'][\'service\'][\'squid\'] }}\r\n    - watch:\r\n       - pkg: squid\r\n       - file: {{ pillar[\'convention-os\'][\'directory\'][\'squid\'] }}/squid.conf\r\n       - file: {{ pillar[\'convention-os\'][\'directory\'][\'squid\'] }}/ACL/not_cached.conf\r\n```\r\n\r\nAs a result, Squid restarts when I make a change in squid.conf, instead of reloading. The whole Squid cluster is down for about a minute.\r\n\r\nTo my knowledge, this affects RHEL 5.x and RHEL 6.x.\r\n\r\nAnother user faces the problem on SuSE : \r\nhttps://github.com/saltstack/salt/pull/4491#issuecomment-16699687\r\n\r\nThanks !'
5406,'UtahDave','file.managed still not working on Windows\n@UtahDave will hate me for this...\r\n\r\nUsing the same configuration, running the 0.15.3 package on the website, another new missing global.\r\n\r\nConfiguration:\r\n```\r\nnutconf:\r\n  file.managed:\r\n    - name: \'c:\\program files\\nut\\etc\\nut.conf\'\r\n    - source: salt://ups/nut-slave.conf\r\n```\r\n\r\nOutput:\r\n```\r\n    State: - file\r\n    Name:      c:\\program files\\nut\\etc\\nut.conf\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1201, in call\r\n  File "salt/states/file.py", line 827, in managed\r\n  File "salt/modules/file.py", line 1519, in manage_file\r\nNameError: global name \'contextlib\' is not defined\r\n```\r\n\r\nAdding\r\n```\r\n  import contextlib\r\n  import difflib\r\n```\r\nto the top of win_file.py deals with this, but leaves me with\r\n```\r\n    State: - file\r\n    Name:      c:\\program files\\nut\\etc\\nut.conf\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "c:\\python27\\lib\\site-packages\\salt\\state.py", line 1201, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "c:\\python27\\lib\\site-packages\\salt\\states\\file.py", line 792, in managed\r\n    source, source_hash = __salt__[\'file.source_list\'](KeyError: \'file.source_list\'\r\n```\r\nat which point I\'m stuck...'
5401,'cro',"Shortened style for includes still requires foldername in init.sls\nSo this is probably me doing something wrong, however in a folder 'foo', I have bar.sls and init.sls.\r\n\r\ninit.sls contains\r\n\r\n```\r\ninclude:\r\n  - .bar\r\n```\r\n\r\nI'd expect that to be /srv/salt/foo/bar.sls, however it seems to be looking at /srv/salt/bar.sls , therefore making me have to put - .foo.bar which in my mind is wrong.\r\n\r\nThoughts?"
5400,'thatch45','About file permission for  var/cache/salt/master/.dfn \n(salt)➜  .salt $ salt-key -D\r\nThe following keys are going to be deleted:\r\nRejected Keys:\r\nwinxp-1\r\nProceed? [N/y] y\r\nTraceback (most recent call last):\r\n  File "/home/shelwin/.virtualenvs/env/salt/bin/salt-key", line 8, in <module>\r\n    load_entry_point(\'salt==0.15.1\', \'console_scripts\', \'salt-key\')()\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/scripts.py", line 50, in salt_key\r\n    saltkey.run()\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/cli/__init__.py", line 209, in run\r\n    key.run()\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/key.py", line 206, in run\r\n    self.delete_all()\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/key.py", line 112, in delete_all\r\n    self.delete(\'*\')\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/key.py", line 106, in delete\r\n    self.key.delete_key(match)\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/key.py", line 401, in delete_key\r\n    salt.crypt.dropfile(self.opts[\'cachedir\'])\r\n  File "/home/shelwin/.virtualenvs/env/salt/lib/python2.7/site-packages/salt/crypt.py", line 37, in dropfile\r\nIOError: [Errno 13] Permission denied: \'/home/shelwin/.salt/var/cache/salt/master/.dfn\'\r\n\r\n(salt)➜  .salt $ ll /home/shelwin/.salt/var/cache/salt/master/.dfn\r\n-r-------- 1 shelwin shelwin 76 6月   5 17:14 /home/shelwin/.salt/var/cache/salt/master/.dfn'
5399,'UtahDave','Microsoft Visual Studio Express 2008 link is broken\nFollowing through the instructions at http://docs.saltstack.com/topics/installation/windows.html. It seems that VS Express 2008 is hard to get hold of these days. This stackexchange question has a couple of links which still seem to be live: http://stackoverflow.com/questions/15318560/visual-c-2008-express-download-link-dead'
5395,'UtahDave','Threaded (and multiprocessing on windows) minion block while running command\nhttps://github.com/saltstack/salt/blob/f92ceca7b8dc4585e8dbd3e1b93e02b33656c060/salt/minion.py#L586\r\n\r\nA process/thread is started and then immediately joined (i.e wait for it to end).\r\n\r\nOn *nix, with multiprocessing==True, the process is daemonized so join returns almost immediately.\r\n\r\nFor the threaded minion on *nix or both the multiprocessing and threaded minion on windows this blocks the minion until the job completes. saltutils.find_job request are queued and the job probably times out on the client side.\r\n'
5391,'UtahDave',"win_interface.interfaces returns an IPV4 broadcast / netmask for an IPV6 address\nitem['broadcast'] = iface.DefaultIPGateway[0] isn't right for the v6 address\r\n\r\n```\r\nsalt minion network.interfaces\r\nminion:\r\n    ----------\r\n    Red Hat VirtIO Ethernet Adapter:\r\n        ----------\r\n        hwaddr:\r\n            52:54:00:6D:D4:4E\r\n        inet:\r\n            ----------\r\n            - address:\r\n                169.254.192.30\r\n            - broadcast:\r\n\r\n            - label:\r\n                Red Hat VirtIO Ethernet Adapter\r\n            - netmask:\r\n                255.255.0.0\r\n            ----------\r\n            - address:\r\n                fe80::1dad:f9b9:b665:c01e\r\n            - broadcast:\r\n\r\n            - label:\r\n                Red Hat VirtIO Ethernet Adapter\r\n            - netmask:\r\n                255.255.0.0\r\n        up:\r\n            True\r\n    Red Hat VirtIO Ethernet Adapter #2:\r\n        ----------\r\n        hwaddr:\r\n            52:54:00:A5:A8:5E\r\n        inet:\r\n            ----------\r\n            - address:\r\n                192.168.253.101\r\n            - broadcast:\r\n                192.168.253.1\r\n            - label:\r\n                Red Hat VirtIO Ethernet Adapter #2\r\n            - netmask:\r\n                255.255.255.0\r\n            ----------\r\n            - address:\r\n                fe80::297c:8918:41a4:d461\r\n            - broadcast:\r\n                192.168.253.1\r\n            - label:\r\n                Red Hat VirtIO Ethernet Adapter #2\r\n            - netmask:\r\n                255.255.255.0\r\n        up:\r\n            True\r\n```"
5390,'UtahDave',"win_network.nslookup only returns the first part of an AAAA response.\nLine 139, someone forgot that AAAA records are colon seperated!\r\n\r\n\r\n```\r\nsalt minion network.nslookup google.co.uk\r\n\r\nminion:\r\n    ----------\r\n    - Server:\r\n        upstreamdns\r\n    ----------\r\n    - Address:\r\n        192.168.1.100\r\n    ----------\r\n    - Name:\r\n        google.co.uk\r\n    ----------\r\n    - Addresses:\r\n        2a00\r\n```\r\n\r\n```\r\nsalt minion cmd.run 'nslookup google.co.uk'\r\nminion:\r\n    Non-authoritative answer:\r\n    Server:  upstreamdns\r\n    Address:  192.168.1.100\r\n\r\n    Name:    google.co.uk\r\n    Addresses:  2a00:1450:4009:809::1018\r\n          173.194.41.183\r\n          173.194.41.191\r\n          173.194.41.184\r\n```"
5383,'terminalmage','ssh_auth.absent does not work\nthe following does not work on my servers (salt-0.15.1).\r\nthis is the substituted sls from debug log:\r\n<pre>\r\n# TODO: THIS SEEMS TO BE BUGGY\r\n# revoke permission for the following ex-employees:\r\n\r\nssh_auth_dev1:\r\n  ssh_auth:\r\n    - absent\r\n    - user: rails\r\n    - source: salt://users/files/authorized_keys.dev1\r\n\r\nssh_auth_dev2:\r\n  ssh_auth:\r\n    - absent\r\n    - user: rails\r\n    - source: salt://users/files/authorized_keys.dev2\r\n\r\n</pre>\r\n\r\n<pre>\r\n----------\r\n    State: - ssh_auth\r\n    Name:      ssh_auth_dev1\r\n    Function:  absent\r\n        Result:    True\r\n        Comment:   Key not present\r\n        Changes:   \r\n----------\r\n    State: - ssh_auth\r\n    Name:      ssh_auth_dev2\r\n    Function:  absent\r\n        Result:    True\r\n        Comment:   Key not present\r\n        Changes:   \r\n</pre>\r\n\r\nkeys are still present in ~rails/.ssh/authorized_keys.\r\neach salt://users/files/authorized_keys.dev* file contains two keys (full lines including comment)'
5377,'UtahDave','Windows grains.ipv4 shows ipv6 addresses too!\nSorry about this @UtahDave *hides*\r\n\r\n```\r\nsalt minion grains.item ipv4\r\nminion:\r\n  ipv4:\r\n      169.254.154.48\r\n      192.168.253.101\r\n      fe80::643b:36a:edba:5a5e\r\n      fe80::a057:2b8d:a8db:9a30\r\n```'
5374,'terminalmage',"ssh-auth becomes a chatty Cathy during test runs\n```\r\n[root@salt cquinn]# salt 'www*' state.highstate; salt 'www*' state.highstate test=True\r\nwww.epicquinn.net:\r\nwww.epicquinn.net:\r\n----------\r\n    State: - ssh_auth\r\n    Name:      AAAAB3NzaC1yc2EAAAABIwAAAQEA0Cgc+ktd2DILpbjlv0mhnApFz0p2oDw2K1v3JtGraXh0ja+c3n52DSYiHXVrdoFG58me9P094DlZiFutkt7MRfBiH059MAj4LvzcoxVmFYz78REOytKp5zXe3SpTVoNxRVIqFCmql4xlnBL+sxHbsbH1HLaO3e2kbF0GkgE2zuLWJ3vf1DA0SpqzCoX6HMgMSkMXMQAGNXKJuOo01vm5sZR4vMiCxVOqa/YX9q5WwlJmMNTzTZ/fXtNSMQR46EBlzXNCXRWLnMiAjj2onyFOasvtdHdKJGBU7D0+OooKIQ3x7iDnCVoxHQHDyOUImMqg0/5NAoNKo9k6iS7JQ53Aww==\r\n    Function:  present\r\n        Result:    None\r\n        Comment:   Key AAAAB3NzaC1yc2EAAAABIwAAAQEA0Cgc+ktd2DILpbjlv0mhnApFz0p2oDw2K1v3JtGraXh0ja+c3n52DSYiHXVrdoFG58me9P094DlZiFutkt7MRfBiH059MAj4LvzcoxVmFYz78REOytKp5zXe3SpTVoNxRVIqFCmql4xlnBL+sxHbsbH1HLaO3e2kbF0GkgE2zuLWJ3vf1DA0SpqzCoX6HMgMSkMXMQAGNXKJuOo01vm5sZR4vMiCxVOqa/YX9q5WwlJmMNTzTZ/fXtNSMQR46EBlzXNCXRWLnMiAjj2onyFOasvtdHdKJGBU7D0+OooKIQ3x7iDnCVoxHQHDyOUImMqg0/5NAoNKo9k6iS7JQ53Aww== for user cquinn is set to be updated\r\n        Changes:   \r\n----------\r\n    State: - ssh_auth\r\n    Name:      AAAAB3NzaC1yc2EAAAABIwAAAgEA6p1M3AocqeI5y7YlUXrOwv8U3CgkAp3MhC23+jsf0vakCBzLMpYBYUtH2JKRZS0JtyaGNX/+3kmFrP1zNEQgqT4zmkbren3tj7W7DGtzMnqb4XPFA/8aTbMS64c63CZQcoJcQcFLpg6mrpdCb7FG4gfqtM1SHsvqtw9pUuD/bV1BSFHBTpWlFVIeG5Wh92CajsZm2nIrR5Mn5jmGShFRz70aKS0UhVK4PjDbl+QntCKRbstAyII7ftYlmBo7GgXy/n5UmWejsOjJbEXLnSeXnMNLStZUDWtHwIS/IxYbMPOSklHzTEJiI3K6H5tkJrc5vlP3/QGBh36K16Hlc/2pvTPD9NIzU0kt1zI8G6WUo8Ug5L5AhyMoL5BpZMJX9wvDJKpYJkqY3vUlWS6XJiOOaTTgpoQ23sOdMQLZcdSPIZVo0uQxqP2mtP8c/eO5ra0yFMsbBfdAG4OH3k3ujcRS9xcpMZL4fNMWoj3qJHdalOsQazZvOYIkMOYTNShx1NV2CAk/beteiEFc99+EltRAUbA8gvLYyznINOaA0GcJjvz7XvKtEBp4UVGAhYqbOCwJectUroSSji/J3ybfFXsWndMGz1FaerLWihh9j6FhVJJvSAoDb5R34oisCBvrsl9XF1g2sjfjlDe16EW9+RHTMU8ktg7SqJWQFLwVO8RLksU=\r\n    Function:  present\r\n        Result:    None\r\n        Comment:   Key AAAAB3NzaC1yc2EAAAABIwAAAgEA6p1M3AocqeI5y7YlUXrOwv8U3CgkAp3MhC23+jsf0vakCBzLMpYBYUtH2JKRZS0JtyaGNX/+3kmFrP1zNEQgqT4zmkbren3tj7W7DGtzMnqb4XPFA/8aTbMS64c63CZQcoJcQcFLpg6mrpdCb7FG4gfqtM1SHsvqtw9pUuD/bV1BSFHBTpWlFVIeG5Wh92CajsZm2nIrR5Mn5jmGShFRz70aKS0UhVK4PjDbl+QntCKRbstAyII7ftYlmBo7GgXy/n5UmWejsOjJbEXLnSeXnMNLStZUDWtHwIS/IxYbMPOSklHzTEJiI3K6H5tkJrc5vlP3/QGBh36K16Hlc/2pvTPD9NIzU0kt1zI8G6WUo8Ug5L5AhyMoL5BpZMJX9wvDJKpYJkqY3vUlWS6XJiOOaTTgpoQ23sOdMQLZcdSPIZVo0uQxqP2mtP8c/eO5ra0yFMsbBfdAG4OH3k3ujcRS9xcpMZL4fNMWoj3qJHdalOsQazZvOYIkMOYTNShx1NV2CAk/beteiEFc99+EltRAUbA8gvLYyznINOaA0GcJjvz7XvKtEBp4UVGAhYqbOCwJectUroSSji/J3ybfFXsWndMGz1FaerLWihh9j6FhVJJvSAoDb5R34oisCBvrsl9XF1g2sjfjlDe16EW9+RHTMU8ktg7SqJWQFLwVO8RLksU= for user cquinn is set to be updated\r\n        Changes:  \r\n```"
5368,'thatch45','Authorization should be explicitly required for wheel functions\nI\'ve got salt-master set up with a couple of minions (all Salt 0.15.1), and an instance of salt-api (0.8.1) running on the same server as the master. In the master configuration file, external_auth is set up as follows -\r\n\r\n```yaml\r\nexternal_auth:\r\n    pam:\r\n        sam:\r\n            - test.ping\r\n```\r\n\r\nThrough curl to salt-api, logged in as the user \'sam\', I can call \'test.ping\' successfully, but not \'state.highstate\', which gives me a 401 Unauthorized message. This is all as expected.\r\n\r\nHowever, it seems that under the same conditions, I can call the wheel functions (e.g. key.list_all or key.accept) successfully.\r\n\r\n```sh\r\n[sam@sandboxbravo ~]$ curl -i http://localhost:8000 -H \'Accept:\r\napplication/json\' -d \'fun=key.accept\' -d \'match=localhost.localdomain\'\r\n-d \'client=wheel\' -H \'X-Auth-Token:\r\nfdc5428447f748acf87e7c17d3038e5ffa6e5d1d\'\r\nHTTP/1.1 200 OK\r\nContent-Length: 52\r\nVary: Accept-Encoding\r\nServer: CherryPy/3.2.4\r\nAllow: GET, HEAD, POST\r\nCache-Control: private\r\nDate: Fri, 31 May 2013 20:02:30 GMT\r\nContent-Type: application/json\r\nSet-Cookie: session_id=fdc5428447f748acf87e7c17d3038e5ffa6e5d1d; expires=Sat, 01\r\n Jun 2013 06:02:30 GMT; Path=/\r\n\r\n{"return": [{"minions": ["localhost.localdomain"]}]}\r\n```\r\n\r\nThis does not seem right; I should not be able to call any wheel functions unless explicitly permitted. This may also be possible through the Python client as well. It has been confirmed as an issue by Seth on the mailing list - https://groups.google.com/forum/#!topic/salt-users/4l213EIWeTM'
5364,'thatch45',"mount point can't be none\nI'm trying to add a swap partition using salt states, so I wrote this:\r\n```\r\nswap:\r\n  mount.mounted:\r\n    - name: none\r\n    - device: /mnt/swap\r\n    - fstype: swap\r\n    - opts:\r\n      - sw\r\n    - order: last\r\n```\r\n\r\nhowever, it causes this error:\r\n\r\n```\r\n    State: - mount\r\n    Name:      none\r\n    Function:  mounted\r\n        Result:    False\r\n        Comment:   mount: mount point none does not exist and updated the entry in the fstab\r\n        Changes:   persist: update\r\n```\r\n\r\nwhich makes me feel like this:\r\n```\r\n:(\r\n```"
5359,'basepi',"The next stage still executes even when the previous stage returns error state\nThe overstate.sls looks like the following:\r\n\r\n```\r\na_service:\r\n  match: '*'\r\n  sls:\r\n     - a_service\r\n\r\nb_service:\r\n  match: '*'\r\n  sls:\r\n    - b_service\r\n  require:\r\n    - a_service\r\n```\r\n\r\nThere are three nodes, A, B, C.\r\n\r\nAll will execute a_service in the first stage. If only B fails when executing a_service, the second stage (b_service) will still execute, i.e. A and C execute b_service.\r\n\r\nSalt version is 0.15.1.\r\n\r\n"
5352,'terminalmage',"states.cmd.run() ignores kwargs\n```\r\n************* Module salt.states.cmd\r\nW0613:357,0:run: Unused argument 'kwargs'\r\n```\r\n\r\nIt seems to be the only non-`wait*` function that does that."
5340,'UtahDave','MSI installation fails without reboot statement\nThis should default to False'
5336,'UtahDave','file.managed not working on Windows again\nNot quite #5091. Using the same configuration, running the 0.15.2 package on the website, new missing global.\r\n\r\nConfiguration:\r\n```\r\nnutconf:\r\n  file.managed:\r\n    - name: \'c:\\program files\\nut\\etc\\nut.conf\'\r\n    - source: salt://ups/nut-slave.conf\r\n```\r\nOutput:\r\n```\r\n    State: - file\r\n    Name:      c:\\program files\\nut\\etc\\nut.conf\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1201, in call\r\n  File "salt/states/file.py", line 821, in managed\r\n  File "salt/modules/file.py", line 1516, in manage_file\r\nNameError: global name \'_is_bin\' is not defined\r\n```\r\n'
5334,'UtahDave','pkg.list_pkgs on Windows minion returns com_error\nPossible duplicate of #4522, but using the latest Windows 32 bit installer from the website (marked 0.15.2), I still see\r\n\r\n```\r\ndjs-admin@master:~$ sudo salt \'windows.*\' pkg.list_pkgs\r\nwindows:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 443, in _thread_return\r\n      File "salt/modules/win_pkg.py", line 214, in list_pkgs\r\n      File "salt/modules/win_pkg.py", line 251, in _get_msi_software\r\n      File "win32com/client/util.py", line 84, in next\r\n    com_error: (-2147217392, \'OLE error 0x80041010\', None, None)\r\n```\r\n\r\nThis is on Server 2003.'
5270,'terminalmage',"pip.installed ignores version when upgrade is false; inconsistent with pip.install\nThis state declaration:\r\n\r\n```YAML\r\nuwsgi-production:\r\n  pip.installed:\r\n    - name: uWSGI==1.9.11\r\n```\r\n\r\ndoes not respect the package version requirement when `upgrade` is false and the present version (from `pip freeze`) is `uWSGI==1.9.10`.\r\n\r\nThe corresponding `pip.install` module (like the `pip` command itself) does respect the version requirement despite `upgrade` being false:\r\n\r\n`salt-call pip.install uWSGI==1.9.11`\r\n\r\nThe `pip.installed` state should treat version requirements the same, i.e. should respect the version requirement despite `upgrade` being false. (I believe pip's `--upgrade` switch does *not* determine whether to upgrade at all; rather, it determines whether to upgrade to the *latest* version satisfying the requirements, when they are already satisfied.)"
5254,'thatch45','Custom grains should be able to override existing items in the grains dict\nCurrently if a custom grain returns a dict with a key that conflicts with a key already in the grains dict, the new value just gets dropped. Only new grains keys and values show up in the grains dict.'
5246,'UtahDave','System module unavailable on Windows\nI need to be able to shutdown a Windows VM, would be useful if I could issue that via salt rather than using cmd.run!'
5209,'terminalmage','File server does not respect environment\nSalt version: 0.14.1\r\nOS: CentOS release 6.3\r\n\r\nThe file server config:\r\n\r\n```yaml\r\nfileserver_backend:\r\n  - git\r\n  - roots\r\n\r\ngitfs_remotes:\r\n  - file:///usr/local/git-repos/salt-fileserver.git\r\n```\r\n\r\nAccording to the comment section in the default master config:\r\n    \r\n```yaml\r\n# When using the git backend branches and tags are translated into salt\r\n# environments.\r\n```\r\n\r\nMy `pillar` file:\r\n\r\n```yaml\r\nrepoforge_src: \'salt://rpms/centos5/rpmforge-release-0.5.3-1.el5.rf.x86_64.rpm\'\r\n```\r\n\r\nMy `rpmforge.sls` file:\r\n\r\n```yaml\r\nrepoforge:\r\n pkg:\r\n    - installed\r\n    - sources:\r\n      - rpmforge-release: {{ pillar[\'repoforge_src\'] }}\r\n```\r\n\r\nWhen I run: `sudo salt \'dev_machine\' state.sls repoforge env=dev`, the result is:\r\n```\r\n----------\r\n    State: - pkg\r\n    Name:      repoforge\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following package(s) failed to install/update: rpmforge-release.\r\n        Changes:   \r\n```\r\n\r\nIn the minion log:\r\n\r\n```\r\n2013-05-23 12:04:39,540 [salt.loaded.int.module.cp                   ][ERROR   ] Unable to cache file "salt://rpms/centos6/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm" from env "base".\r\n```\r\n\r\nSalt minion tries to get the file from `base` environment ( which I think is the branch master of file server) instead of the `dev` environment.\r\n\r\nThe following command works fine: \r\n\r\n```sh\r\n sudo salt \'dev_machine\' cp.get_file salt://rpms/centos6/rpmforge-release-0.5.3-1.el6.rf.x86_64.rpm /tmp/aa env=dev\r\n```\r\n\r\nSo is this a bug or I misunderstand the document?'
5195,'UtahDave','LDAP not available on Windows\nWould be totally sweet if I could query AD via LDAP (or better, AD directly)\r\n\r\nAny chance of adding that?'
5190,'basepi',"The state git.latest triggers watch in other states even if nothing changed\nA state which watches git.latest gets triggered every time even if nothing changed.\r\n\r\nIt's possible it only happens when force is set to True"
5166,'terminalmage','pkg.latest fails to install\nReading the `salt.states.pkg.latest` doc string:\r\n```\r\n Verify that the named package is installed and the latest available\r\npackage. If the package can be updated this state function will update\r\nthe package. Generally it is better for the\r\n:mod:`installed <salt.states.pkg.installed>` function to be\r\nused, as :mod:`latest <salt.states.pkg.latest>` will update the package\r\nwhenever a new package is available.\r\n```\r\n\r\nIf I understand correctly, if the package is not installed, it will be installed. If its installed and an upgrade is available, upgrade it.\r\n\r\n```shell\r\n# salt-call state.single pkg.latest name=gcc\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state pkg.latest for gcc\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Executing command \'zypper refresh\' in directory \'/root\'\r\n[INFO    ] Executing command \'zypper list-updates\' in directory \'/root\'\r\n[ERROR   ] No information found for "gcc".\r\n[ERROR   ] No changes made for gcc\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      gcc\r\n    Function:  latest\r\n        Result:    False\r\n        Comment:   No information found for "gcc".\r\n        Changes: \r\n```\r\nPackage is not installed and it fails to install.\r\nLet\'s install it:\r\n```shell\r\n# salt-call state.single pkg.installed name=gcc\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state pkg.installed for gcc\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Executing command \'zypper -n install -l "gcc"\' in directory \'/root\'\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Installed Packages:\r\nglibc-devel changed from absent to 2.11.1-0.38.1\r\ngcc changed from absent to 4.3-62.198\r\ngcc43 changed from absent to 4.3.4_20091019-0.7.35\r\nlinux-kernel-headers changed from absent to 2.6.32-1.4.13\r\n\r\n[INFO    ] Loading fresh modules for state activity\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      gcc\r\n    Function:  installed\r\n        Result:    True\r\n        Comment:   The following package(s) were installed/updated: gcc.\r\n        Changes:   glibc-devel: { new : 2.11.1-0.38.1\r\nold : \r\n}\r\n                   gcc: { new : 4.3-62.198\r\nold : \r\n}\r\n                   gcc43: { new : 4.3.4_20091019-0.7.35\r\nold : \r\n}\r\n                   linux-kernel-headers: { new : 2.6.32-1.4.13\r\nold : \r\n}\r\n```\r\nNice!\r\n\r\nNow, let\'s run latest again:\r\n```\r\n# salt-call state.single pkg.latest name=gcc\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n[WARNING ] Although \'dmidecode\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[WARNING ] Although \'lspci\' was found in path, the current user cannot execute it. Grains output might not be accurate.\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state pkg.latest for gcc\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[INFO    ] Executing command \'zypper refresh\' in directory \'/root\'\r\n[INFO    ] Executing command \'zypper list-updates\' in directory \'/root\'\r\n[INFO    ] No changes made for gcc\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      gcc\r\n    Function:  latest\r\n        Result:    True\r\n        Comment:   Package gcc is already up-to-date.\r\n        Changes:\r\n```\r\n\r\nNow it worked.\r\n\r\nI have detected this in rpm based distributions though I think that\'s not important here, and so far I haven\'t tested in, for example, deb based distributions.'
5154,'basepi'," incorrectly-reporting-new-user bug\non `salt 0.15.1`\r\n\r\nstate:\r\n\r\ndovecot-agent:\r\nuser:\r\n- present\r\n- uid: 4000\r\n- groups:\r\n- mail\r\n\r\noutput:\r\n\r\n    State: - user\r\n    Name:      dovecot-agent\r\n    Function:  present\r\n    Result:    True\r\n    Comment:   New user dovecot-agent created\r\n    Changes:   shell: /bin/sh\r\n               workphone: \r\n               uid: 4000\r\n               passwd: x\r\n               roomnumber: \r\n               groups: ['dovecot-agent']\r\n               home: /home/dovecot-agent\r\n               name: dovecot-agent\r\n               gid: 4000\r\n               fullname: \r\n               homephone: \r\n\r\noutput lack of group `mail` when user `dovecot-agent` added to `mail` group:\r\n\r\nroot@cloud:/srv/salt# groups dovecot-agent\r\ndovecot-agent : dovecot-agent mail\r\n"
5136,'UtahDave',"win_file mod: get_user() defined twice\n```\r\n************* Module salt.modules.win_file\r\nW0601: 48,12:__virtual__: Global variable 'get_user' undefined at the module level\r\nE0102:191,0:get_user: function already defined line 57\r\n```"
5131,'terminalmage',"ignored arguments in user modules\n```\r\n************* Module salt.modules.pw_user\r\nW0613: 62,8:add: Unused argument 'unique'\r\nW0613:103,31:delete: Unused argument 'force'\r\n************* Module salt.modules.solaris_user\r\nW0613:121,31:delete: Unused argument 'force'\r\n```\r\n\r\nPresumably, either it should be documented that these options are ignored/not-applicable (& only included for interface compatibility), or fallbacks should be implemented, or attempting to request the unimplemented behavior should return an error."
5129,'terminalmage',"modules.rh_ip.up/down: `opts` arguments are ignored\n```\r\n************* Module salt.modules.rh_ip\r\nW0613:845,28:down: Unused argument 'opts'\r\nW0613:883,26:up: Unused argument 'opts'\r\n```"
5091,'UtahDave','file.managed not working on Windows\nRunning 0.15.1 on Windows, with this state:\r\n```\r\nnutconf:\r\n  file.managed:\r\n    - name: \'c:\\program files\\nut\\etc\\nut.conf\'\r\n    - source: salt://ups/nut-slave.conf\r\n```\r\nI get:\r\n```\r\n    State: - file\r\n    Name:      c:\\program files\\nut\\etc\\nut.conf\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1208, in call\r\n  File "salt/states/file.py", line 769, in managed\r\n  File "salt/modules/file.py", line 1340, in check_managed\r\n  File "salt/modules/file.py", line 1171, in get_managed\r\nNameError: global name \'__salt__\' is not defined\r\n```\r\n'
5086,'UtahDave','Salt-call on Windows says "KeyError: \'nested\'"\nOn my Windows minion, with 0.15.1, I can\'t use salt-call at all:\r\n\r\n```\r\nC:\\salt>salt-call -c c:\\salt\\conf test.ping\r\n[INFO    ] Configuration file path: c:\\salt\\conf\\minion\r\nTraceback (most recent call last):\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chainload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-call__.py", line 14, in <module>\r\n  File "salt/scripts.py", line 76, in salt_call\r\n  File "salt/cli/__init__.py", line 257, in run\r\n  File "salt/cli/caller.py", line 173, in run\r\n  File "salt/output/__init__.py", line 29, in display_output\r\n  File "salt/output/__init__.py", line 68, in get_printout\r\nKeyError: \'nested\'\r\n```\r\n\r\nThe minion runs fine, and I can execute command remotely, just not locally.'
5084,'techhat',"modules.bluez.un/pair are broken\nThey both take `address` as a parameter, yet also attempt to invoke the `address()` module-level function.\r\nIf you try to use the function-call operator on a string, you're gonna have a bad time."
5061,'terminalmage',"Parity between network.ip_addrs and network.hwaddr\nShould `network.hwaddr` have an underscore, i.e. `network.hw_addr`, just like `network.ip_addrs` so that there is parity between them?  I intuitively guessed the latter and when it didn't work ended up digging through the docs to find the underscore was incorrect.\r\n\r\nThanks!"
5059,'terminalmage',"ipcidr matching in top file broken\nI'm trying to target minions in my pillar top.sls file by IP Cidr, and it seems to not be working.  From the command-line I can successfully use something like:\r\nsalt -S '172.16.0.0/21' test.ping\r\n\r\n# salt-call test.version\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\nlocal:\r\n    0.14.0\r\n\r\n\r\nI've tried a number of methods and the only ones I can get to work is a grain_pcre match and a compound match, once again using grain_pcre syntax.\r\n\r\nWhat didn't work:\r\n\r\n    'S@172.16.0.0/21':\r\n      - match: compound\r\n      - ...\r\n\r\n    '172.16.0.0/21':\r\n      - match: ipcidr\r\n      - ...\r\n\r\nWhat did work, but isn't really what you'd want:\r\n\r\n    'P@ipv4:(172.16)':\r\n      - match: compound\r\n      - ...\r\n\r\n    'ipv4:(172.16)':\r\n      - match: grain_pcre\r\n      - ...\r\n\r\n\r\nComment by Thomas Hatch in mailing list\r\n''''\r\nI am wondering if the top file is not binding to the matcher correctly here, can you open an issue and we will look into it?\r\n''''"
5055,'s0undt3ch','minion fails to return (can\'t serialize)\nWhile running https://github.com/SmartReceipt/salt_hosts\r\nI get `Minion did not return`. Checking `/var/log/salt/minion` on the minion reveals the following.\r\n\r\n```\r\n2013-05-16 17:56:11,935 [salt.state       ][INFO    ] No changes made for zookeeper03\r\n2013-05-16 17:56:11,936 [salt.state       ][INFO    ] Executing state stateconf.set for _slsmod_hosts\r\n2013-05-16 17:56:11,936 [salt.state       ][INFO    ] No changes made for _slsmod_hosts\r\n2013-05-16 17:56:11,937 [salt.minion      ][WARNING ] Missing arguments executing "state.sls": ArgSpec(args=[\'mods\', \'env\', \'test\', \'exclude\'], varargs=None, keywords=\'kwargs\', defaults=(\'base\', None, None))\r\n2013-05-16 17:56:11,937 [salt.minion      ][DEBUG   ] "Missing args" caused by exc: can\'t serialize <module \'hosts\' from \'/var/cache/salt/minion/files/base/hosts/init.sls\'>\r\nTraceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 428, in _thread_return\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 323, in sls\r\n    serial.dump(high, fp_)\r\n  File "/usr/lib/pymodules/python2.7/salt/payload.py", line 115, in dump\r\n    fn_.write(self.dumps(msg))\r\n  File "/usr/lib/pymodules/python2.7/salt/payload.py", line 109, in dumps\r\n    return msgpack.dumps(msg)\r\n  File "_msgpack.pyx", line 169, in msgpack._msgpack.packb (msgpack/_msgpack.c:2384)\r\n  File "_msgpack.pyx", line 153, in msgpack._msgpack.Packer.pack (msgpack/_msgpack.c:2020)\r\n  File "_msgpack.pyx", line 136, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1761)\r\n  File "_msgpack.pyx", line 136, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1761)\r\n  File "_msgpack.pyx", line 142, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1862)\r\n  File "_msgpack.pyx", line 136, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1761)\r\n  File "_msgpack.pyx", line 148, in msgpack._msgpack.Packer._pack (msgpack/_msgpack.c:1956)\r\nTypeError: can\'t serialize <module \'hosts\' from \'/var/cache/salt/minion/files/base/hosts/init.sls\'>\r\n2013-05-16 17:56:11,940 [salt.minion      ][INFO    ] Returning information for job: 20130516175605043663\r\n```'
5045,'UtahDave',"redundant Windows check in modules.win_disk ?\n`usage()` checks that the OS is Windows. Doesn't the check in `__virtual__()` already take care of that?"
5035,'thatch45',"Including from sls files is not relative to file, is relative to global directory\nSee example below.  sls files appear to include everything from the root directory, which means files in embedded folder structures need to be aware of the global structure.\r\n\r\n```\r\ntop.sls:\r\nbase:\r\n  '*':\r\n    - sls.tester\r\n \r\nsls/tester.sls:\r\ninclude:\r\n  - python\r\n \r\ntest:\r\n  file:\r\n    - directory\r\n    - name: /tmp/test\r\n    - require:\r\n      - pkg: python\r\n \r\nsls/python.sls:\r\npython:\r\n  pkg:\r\n    - installed\r\n \r\n \r\n$salt um1 state.highstate\r\num1\r\n----------\r\n    State: - file\r\n    Name:      /tmp/test\r\n    Function:  directory\r\n        Result:    False\r\n        Comment:   The following requisites were not found:\r\n                   require: {'pkg': 'python'}\r\n```"
5031,'basepi','[0.15.1] salt-call fails with list as input argument\nHi,\r\nFor example:\r\n```\r\n[user@salt-minion ~]# sudo salt-call state.single user.present qwertyuser home="/home/qwertyuser" shell="/bin/bash" groups="[foo,bar]"\r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/home/user\'\r\n[INFO    ] Loading fresh modules for state activity\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.6/site-packages/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/__init__.py", line 257, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/caller.py", line 165, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/caller.py", line 78, in call\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/state.py", line 510, in single\r\n    kwargs[key] = parse_kwval(value)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/state.py", line 497, in parse_kwval\r\n    return _yaml_load(value, _YamlCustomLoader)\r\n  File "/usr/lib/python2.6/site-packages/yaml/__init__.py", line 69, in load\r\n    loader = Loader(stream)\r\n  File "/usr/lib/python2.6/site-packages/salt/utils/yamlloader.py", line 38, in __init__\r\n    yaml.SafeLoader.__init__(self, stream)\r\n  File "/usr/lib/python2.6/site-packages/yaml/loader.py", line 24, in __init__\r\n    Reader.__init__(self, stream)\r\n  File "/usr/lib/python2.6/site-packages/yaml/reader.py", line 85, in __init__\r\n    self.determine_encoding()\r\n  File "/usr/lib/python2.6/site-packages/yaml/reader.py", line 124, in determine_encoding\r\n    self.update_raw()\r\n  File "/usr/lib/python2.6/site-packages/yaml/reader.py", line 178, in update_raw\r\n    data = self.stream.read(size)\r\nAttributeError: \'list\' object has no attribute \'read\'\r\n```\r\n\r\nLooks like issue in the Caller.parse_args method - it transforms string \'[foo,bar]\' into list [\'foo\', \'bar\'] which cannot be processed in yaml/reader.py\r\n\r\n```\r\nsalt --versions-report\r\nSalt: 0.15.1\r\nPython: 2.6.6 (r266:84292, Feb 21 2013, 23:54:59)\r\nJinja2: 2.2.1\r\nM2Crypto: 0.20.2\r\nmsgpack-python: 0.1.9.final\r\nmsgpack-pure: Not Installed\r\npycrypto: 2.0.1\r\nPyYAML: 3.10\r\nPyZMQ: 2.2.0.1\r\nZMQ: 3.2.3\r\n```'
5029,'basepi',"on failure salt.states.user.present do not return exact reason of failure\nSalt 0.15.1 on Ubuntu Precise:\r\n\r\nstate:\r\n\r\n```\r\njoe:\r\n  user:\r\n    - present\r\n    - home: /tmp/inexistant/joe\r\n```\r\n\r\nfail because useradd can't create ``/tmp/inexistant/joe`` as ``/tmp/inexistant`` do not exists.\r\n\r\nbut, the only thing that show in ``['comment']`` is ``Failed to create new user joe``, which isn't helpful.\r\n\r\nto troubleshoot this, I had to hack the code to retreive the output of ``useradd`` command. for some reason, it didn't appears in the debug log... because I don't see anything that set quiet=True."
5027,'terminalmage',"salt.modules.upstart.status don't work anymore on non-upstart services\nTested on Ubuntu 12.04 with Salt 0.15.1:\r\n\r\nprepare the test:\r\n\r\n```\r\nroot@integration-2:~# apt-get install postgresql\r\n[snip]\r\nroot@integration-2:~# service postgresql status\r\nRunning clusters: 9.1/main\r\nroot@integration-2:~# salt-call service.status postgresql\r\nlocal:\r\n    True\r\n```\r\n\r\ngood, what we expect.\r\n\r\n```\r\nroot@integration-2:~# service postgresql stop\r\n * Stopping PostgreSQL 9.1 database server                                                                                                                                               [ OK ]\r\nroot@integration-2:~# service postgresql status\r\nRunning clusters:\r\nroot@integration-2:~# salt-call service.status postgresql\r\n[snip]\r\nlocal:\r\n    True\r\n```\r\n\r\nfail!"
5019,'basepi','Regression: state "user.present" fails if the user already exists with the specified groups.\nThis is a regression.\r\nThis bug doesn\'t appear to exist in 0.13.1. It does appear to exist in 0.15.1\r\n\r\nThis is on Ubuntu if that helps.\r\n\r\nIf I have a state like:\r\n\r\n    obama:\r\n      user.present:\r\n        - groups:\r\n          - presidents\r\n\r\nRunning it the first time gives me this:\r\n\r\n    State: - user\r\n    Name:      obama\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   New user obama created\r\n        Changes:   shell: /bin/sh\r\n                   workphone: \r\n                   uid: 1001\r\n                   passwd: x\r\n                   roomnumber: \r\n                   groups: [\'obama\']\r\n                   home: /home/obama\r\n                   name: obama\r\n                   gid: 1003\r\n                   fullname: \r\n                   homephone: \r\n                   \r\nRunning a subsequent times gives:\r\n\r\n    State: - user\r\n    Name:      obama\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   These values could not be changed: {\'groups\': [\'presidents\']}\r\n        Changes:   \r\n\r\n\r\nRunning "salt-call user.list_groups obama" gives:\r\n\r\n    local:\r\n        - admin\r\n        - obama\r\n'
5018,'basepi','Non-Zero return code on successful highstate.\nUploading the salt config files with fabric to a masterless minion and running `salt-call --local state.highstate` can result in a non-zero return code. \r\n\r\n    Fatal error: sudo() received nonzero return code 2 while executing!\r\n\r\nThe salt states all execute with only `INFO` or `WARNING` messages, with most warnings being: \r\n\r\n    [WARNING ] Unable to import "softwareproperties.ppa": No module named pycurl\r\n'
5013,'basepi','The file.recurse method inserts an extraneous pipe character into the source, causing Salt to be unable to locate and transfer files\nA state file that uses file.recurse that was working in 0.14 now outputs several error messages similar to this one in 0.15.1:\r\n\r\n    #### /var/www/test/current/.git/objects/53/06d04c4baeb1ae1439b1a10de76297136ad4f7 ####\r\n    Source file salt://|application/test/codebase/.git/objects/53/06d04c4baeb1ae1439b1a10de76297136ad4f7 not found\r\n\r\nHowever, the file in question does exist in /srv/salt/application/test/codebase/.git/objects/53/06d04c4baeb1ae1439b1a10de76297136ad4f7'
5006,'basepi','groups and optional_groups\nhttps://gist.github.com/Katafalkas/6c350262cf0dcd793008\r\n\r\nThis is the code example.\r\n\r\nWhen none of the users are created yet - it all seem to work fine. But when I repeat the same state - I expect it to say that none was updated and output come out in green.\r\n\r\nBut what it actually does is: comes out red and gives errors about :\r\n\r\n----------\r\n    State: - user\r\n    Name:      jack\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   These values could not be changed: {\'groups\': [\'www-data\']}\r\n        Changes:   \r\n----------\r\n    State: - ssh_auth\r\n    Name:      ssh-rsa AAAAB3NzaC1yc2EAAAABJQAAAQEAuWg\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   One or more requisite failed\r\n        Changes:   \r\n\r\nIf I comment out the " - groups: [www-data, ] " it works fine.\r\nIf I change " - groups: [www-data, ] " to " - optional_groups: [www-data, ] " it gives EXACTLY the same error.\r\n\r\nI am not sure if this is a bug or I am doing something wrong, But after conversation on irc, UtahDave suggested to post it here.\r\n\r\n\r\n# salt --version\r\nsalt 0.15.1\r\n'
5003,'terminalmage','pkg.installed fails sometimes on already installed package\nsince i\'ve upgraded to 0.15.1. my simple state fails:\r\n<pre>\r\nnfs-client:                                                                                                                                                                                                                                                                            \r\n  pkg:                                                                                                                                                                                                                                                                                 \r\n    - installed                                                                                                                                                                                                                                                                        \r\n </pre>\r\n\r\n<pre>pille@appstack2 current % sudo salt-call state.sls NFS.client           \r\n[INFO    ] Configuration file path: /etc/salt/minion\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/home/pille\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Fetching file \'salt://NFS/client.sls\'\r\n[INFO    ] Executing state pkg.installed for nfs-client\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/home/pille\'\r\n[INFO    ] Executing command \'apt-get -q update\' in directory \'/home/pille\'\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/home/pille\'\r\n[INFO    ] Executing command \'apt-get -q -y -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef   install nfs-client\' in directory \'/home/pille\'\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/home/pille\'\r\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/home/pille\'\r\n[ERROR   ] No changes made for nfs-client\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      nfs-client\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following package(s) failed to install/update: nfs-client.\r\n        Changes:   \r\n</pre>\r\n\r\nit\'s completely fine that no changes were made, because it\'s up-to-date, but this should not result in an error.\r\n\r\nsince it\'s a virtual package and this does not happen to all my package states, it may be connected to #2922.'
5001,'terminalmage',"group.absent fail with group that isn't present\n0.15.1:\r\n\r\n```\r\n2013-05-13 05:22:57,042 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'userdel graphite' in directory '/root'\r\n2013-05-13 05:22:57,068 [salt.state       ][INFO    ] {'graphite': 'removed'}\r\n2013-05-13 05:22:57,072 [salt.state       ][INFO    ] Executing state group.absent for graphite\r\n2013-05-13 05:22:57,072 [salt.loaded.int.module.cmdmod][INFO    ] Executing command 'groupdel graphite' in directory '/root'\r\n2013-05-13 05:22:57,088 [salt.loaded.int.module.cmdmod][ERROR   ] Command 'groupdel graphite' failed with return code: 6\r\n2013-05-13 05:22:57,089 [salt.loaded.int.module.cmdmod][ERROR   ] stderr: groupdel: group 'graphite' does not exist\r\n```"
4998,'terminalmage','yumpkg5 output parsing breaks on long rpm names\nWhen Yum detects that it is running with a pipe, it limits its output to 80 col\r\n\r\n    $ yum list available hadoop-0.20-mapreduce-jobtracker hue-plugins \r\n    Loaded plugins: fastestmirror\r\n    Available Packages\r\n    hadoop-0.20-mapreduce-jobtracker.x86_64       0.20.2+1359-1.cdh4.2.1.p0.9.el5   cloudera-cdh4\r\n    hue-plugins.x86_64                            2.2.0+194-1.cdh4.2.1.p0.8.el5     cloudera-cdh4\r\n\r\n    $ yum list available hadoop-0.20-mapreduce-jobtracker hue-plugins | cat\r\n    Loaded plugins: fastestmirror\r\n    Available Packages\r\n    hadoop-0.20-mapreduce-jobtracker.x86_64\r\n                               0.20.2+1359-1.cdh4.2.1.p0.9.el5         cloudera-cdh4\r\n    hue-plugins.x86_64         2.2.0+194-1.cdh4.2.1.p0.8.el5           cloudera-cdh4\r\n\r\nThis causes the [_parse_yum](https://github.com/saltstack/salt/blob/ef90d58ef13c71a335f0a6bf851883bb0546b342/salt/modules/yumpkg5.py#L45) function to skip the hadoop package since the first line has only one section and the next line has two sections\r\n\r\n    line: Available Packages\r\n    split: [\'Available\', \'Packages\']\r\n    line: hadoop-0.20-mapreduce-jobtracker.x86_64\r\n    split: [\'hadoop-0.20-mapreduce-jobtracker.x86_64\']\r\n    line:                                0.20.2+1359-1.cdh4.2.1.p0.9.el5     cloudera-cdh4\r\n    split: [\'0.20.2+1359-1.cdh4.2.1.p0.9.el5\', \'cloudera-cdh4\']\r\n    line: hue-plugins.x86_64             2.2.0+194-1.cdh4.2.1.p0.8.el5       cloudera-cdh4\r\n    split: [\'hue-plugins.x86_64\', \'2.2.0+194-1.cdh4.2.1.p0.8.el5\', \'cloudera-cdh4\']\r\n\r\nOne idea for a fix (though it feels like a very ugly fix) is to look for extra whitespace like "\\n<space>*" and fix that up, and then do the line.split()\r\n\r\nSee also: http://lists.baseurl.org/pipermail/yum/2009-February/022279.html'
4975,'thatch45','Minion never returns from state and/or master never sees state completion\nTrying to install CouchDB on a minion when salt-minion was started from its init script fails. When started from the command line it works.\r\n\r\nInstalling other packages work. Only CouchDB was found so far not to work. \r\n\r\nThe command:\r\n\r\n    salt "ip-10-196-38-159" state.sls couchdb dev -v\r\n\r\nThe state file:\r\n\r\n    couchdb:\r\n      pkg:\r\n        - installed\r\n\r\nNote the following command does not work either (the logs below are from running this command):\r\n\r\n    salt "ip-10-196-38-159" pkg.install couchdb\r\n\r\nVersions:\r\n\r\nsalt-minion --version: 0.15.1\r\nsalt-master --version: 0.15.1\r\n\r\nBoth running on Ubuntu 12.10 under AWS EC2.\r\n\r\nThe output from `pkg.install couchdb`:\r\n\r\n    Executing job with jid 20130510152803303471\r\n    -------------------------------------------\r\n\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    Execution is still running on ip-10-196-38-159\r\n    ^CExiting on Ctrl-C\r\n    This job\'s jid is:\r\n    20130510152803303471\r\n    The minions may not have all finished running and any remaining minions will return upon completion. To look up the return data for this job later run:\r\n    salt-run jobs.lookup_jid 20130510152803303471\r\n\r\nAt this point, CouchDB is installed on the minion. I can query it: couchdb -V returns *couchdb - Apache CouchDB 1.2.0*. No matter how long I wait, it keeps going.\r\n\r\nRunning the minion in debug mode shows that the following command is executed repeatedly:\r\n\r\n    saltutil.find_job\r\n\r\nAn excerpt from the minion debug log:\r\n\r\n    2013-05-10 19:30:30,156 [salt.minion      ][INFO    ] User hdemers Executing command saltutil.find_job with jid 20130510153030089224\r\n    2013-05-10 19:30:30,161 [salt.minion      ][DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20130510153030089224\', \'tgt\': \'ip-10-196-38-159\', \'ret\': \'\', \'user\': \'hdemers\', \'arg\': [\'20130510152803303471\'], \'fun\': \'saltutil.find_job\'}\r\n    2013-05-10 19:30:30,192 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'ps -efH\' in directory \'/root\'\r\n    2013-05-10 19:30:30,214 [salt.loaded.int.module.cmdmod][DEBUG   ] output: UID        PID  PPID  C STIME TTY          TIME CMD\r\n    root         2     0  0 19:26 ?        00:00:00 [kthreadd]\r\n    root         3     2  0 19:26 ?        00:00:00   [ksoftirqd/0]\r\n    root         4     2  0 19:26 ?        00:00:00   [kworker/0:0]\r\n    root         5     2  0 19:26 ?        00:00:00   [kworker/u:0]\r\n    root         6     2  0 19:26 ?        00:00:00   [migration/0]\r\n    root         7     2  0 19:26 ?        00:00:00   [watchdog/0]\r\n    root         8     2  0 19:26 ?        00:00:00   [cpuset]\r\n    root         9     2  0 19:26 ?        00:00:00   [khelper]\r\n    root        10     2  0 19:26 ?        00:00:00   [kdevtmpfs]\r\n    root        11     2  0 19:26 ?        00:00:00   [netns]\r\n    root        12     2  0 19:26 ?        00:00:00   [xenwatch]\r\n    root        13     2  0 19:26 ?        00:00:00   [xenbus]\r\n    root        14     2  0 19:26 ?        00:00:00   [sync_supers]\r\n    root        15     2  0 19:26 ?        00:00:00   [bdi-default]\r\n    root        16     2  0 19:26 ?        00:00:00   [kintegrityd]\r\n    root        17     2  0 19:26 ?        00:00:00   [kblockd]\r\n    root        18     2  0 19:26 ?        00:00:00   [ata_sff]\r\n    root        19     2  0 19:26 ?        00:00:00   [khubd]\r\n    root        20     2  0 19:26 ?        00:00:00   [md]\r\n    root        21     2  0 19:26 ?        00:00:00   [kworker/0:1]\r\n    root        22     2  0 19:26 ?        00:00:00   [kworker/u:1]\r\n    root        23     2  0 19:26 ?        00:00:00   [khungtaskd]\r\n    root        24     2  0 19:26 ?        00:00:00   [kswapd0]\r\n    root        25     2  0 19:26 ?        00:00:00   [ksmd]\r\n    root        26     2  0 19:26 ?        00:00:00   [fsnotify_mark]\r\n    root        27     2  0 19:26 ?        00:00:00   [ecryptfs-kthrea]\r\n    root        28     2  0 19:26 ?        00:00:00   [crypto]\r\n    root        37     2  0 19:26 ?        00:00:00   [kthrotld]\r\n    root        38     2  0 19:26 ?        00:00:00   [khvcd]\r\n    root        39     2  0 19:26 ?        00:00:00   [binder]\r\n    root        58     2  0 19:26 ?        00:00:00   [deferwq]\r\n    root        59     2  0 19:26 ?        00:00:00   [charger_manager]\r\n    root        60     2  0 19:26 ?        00:00:00   [devfreq_wq]\r\n    root       186     2  0 19:26 ?        00:00:00   [jbd2/xvda1-8]\r\n    root       187     2  0 19:26 ?        00:00:00   [ext4-dio-unwrit]\r\n    root       312     2  0 19:26 ?        00:00:00   [kjournald]\r\n    root       853     2  0 19:26 ?        00:00:00   [flush-202:1]\r\n    root       854     2  0 19:26 ?        00:00:00   [flush-202:16]\r\n    root         1     0  0 19:26 ?        00:00:00 /sbin/init\r\n    root       300     1  0 19:26 ?        00:00:00   upstart-udev-bridge --daemon\r\n    root       306     1  0 19:26 ?        00:00:00   /sbin/udevd --daemon\r\n    root       356   306  0 19:26 ?        00:00:00     /sbin/udevd --daemon\r\n    root       357   306  0 19:26 ?        00:00:00     /sbin/udevd --daemon\r\n    root       446     1  0 19:26 ?        00:00:00   upstart-socket-bridge --daemon\r\n    root       499     1  0 19:26 ?        00:00:00   dhclient -1 -v -pf /run/dhclient.eth0.pid -lf /var/lib/dhcp/dhclient.eth0.leases eth0\r\n    root       678     1  0 19:26 ?        00:00:00   /usr/sbin/sshd -D\r\n    root       895   678  0 19:26 ?        00:00:00     sshd: ubuntu [priv] \r\n    ubuntu    1040   895  0 19:26 ?        00:00:00       sshd: ubuntu@pts/0  \r\n    ubuntu    1041  1040  0 19:26 pts/0    00:00:00         -bash\r\n    ubuntu    1198  1041  0 19:27 pts/0    00:00:00           tail -f /var/log/salt/minion\r\n    102        689     1  0 19:26 ?        00:00:00   dbus-daemon --system --fork\r\n    syslog     704     1  0 19:26 ?        00:00:00   rsyslogd -c5\r\n    root       762     1  0 19:26 tty4     00:00:00   /sbin/getty -8 38400 tty4\r\n    root       769     1  0 19:26 tty5     00:00:00   /sbin/getty -8 38400 tty5\r\n    root       780     1  0 19:26 tty2     00:00:00   /sbin/getty -8 38400 tty2\r\n    root       781     1  0 19:26 tty3     00:00:00   /sbin/getty -8 38400 tty3\r\n    root       786     1  0 19:26 tty6     00:00:00   /sbin/getty -8 38400 tty6\r\n    root       796     1  0 19:26 ?        00:00:00   cron\r\n    daemon     797     1  0 19:26 ?        00:00:00   atd\r\n    root       808     1  0 19:26 ?        00:00:00   acpid -c /etc/acpi/events -s /var/run/acpid.socket\r\n    whoopsie   819     1  0 19:26 ?        00:00:00   whoopsie\r\n    root       827     1  0 19:26 tty1     00:00:00   /sbin/getty -8 38400 tty1\r\n    root       898     1  0 19:26 ?        00:00:00   /usr/sbin/console-kit-daemon --no-daemon\r\n    root       965     1  0 19:26 ?        00:00:00   /usr/lib/policykit-1/polkitd --no-debug\r\n    root      1148     1  0 19:27 ?        00:00:01   python /usr/bin/salt-minion\r\n    root      1201     1  0 19:28 ?        00:00:00   python /usr/bin/salt-minion\r\n    root      1204  1201  0 19:28 ?        00:00:00     [sh] <defunct>\r\n    couchdb   1860     1  0 19:28 ?        00:00:00   /bin/sh -e /usr/bin/couchdb -a /etc/couchdb/default.ini -a /etc/couchdb/local.ini -b -r 5 -p /var/run/couchdb/couchdb.pid -o /dev/null -e /dev/null -R\r\n    couchdb   1873  1860  0 19:28 ?        00:00:00     /bin/sh -e /usr/bin/couchdb -a /etc/couchdb/default.ini -a /etc/couchdb/local.ini -b -r 5 -p /var/run/couchdb/couchdb.pid -o /dev/null -e /dev/null -R\r\n    couchdb   1874  1873  0 19:28 ?        00:00:00       /usr/lib/erlang/erts-5.9.1/bin/beam -Bd -K true -A 4 -- -root /usr/lib/erlang -progname erl -- -home /var/lib/couchdb -- -noshell -noinput -os_mon start_memsup false start_cpu_sup false disk_space_check_interval 1 disk_almost_full_threshold 1 -sasl errlog_type error -couch_ini /etc/couchdb/default.ini /etc/couchdb/local.ini /etc/couchdb/default.ini /etc/couchdb/local.ini -s couch -pidfile /var/run/couchdb/couchdb.pid -heart\r\n    couchdb   1882  1874  0 19:28 ?        00:00:00         heart -pid 1874 -ht 11\r\n    couchdb   1905  1874  0 19:28 ?        00:00:00         sh -s disksup\r\n    root      1953     1  0 19:30 ?        00:00:00   python /usr/bin/salt-minion\r\n    root      1954  1953  0 19:30 ?        00:00:00     /bin/sh -c ps -efH\r\n    root      1955  1954  0 19:30 ?        00:00:00       ps -efH\r\n    2013-05-10 19:30:30,283 [salt.minion      ][INFO    ] Returning information for job: 20130510153030089224\r\n\r\n\r\nThe corresponding master log:\r\n\r\n    [INFO    ] Clear payload received with command publish\r\n    [INFO    ] User hdemers Published command saltutil.find_job with jid 20130510153030089224\r\n    [DEBUG   ] Published command details {\'tgt_type\': \'glob\', \'jid\': \'20130510153030089224\', \'tgt\': \'ip-10-196-38-159\', \'ret\': \'\', \'user\': \'hdemers\', \'arg\': [\'20130510152803303471\'], \'fun\': \'saltutil.find_job\'}\r\n    [INFO    ] AES payload received with command _return\r\n    [INFO    ] Got return from ip-10-196-38-159 for job 20130510153030089224\r\n\r\nThis:\r\n\r\n    salt-run jobs.lookup_jid 20130510152803303471\r\n\r\nreturns nothing. And this:\r\n\r\n    salt-run jobs.active\r\n\r\nreturns:\r\n\r\n    \'20130510152803303471\':\r\n      Arguments:\r\n      - couchdb\r\n      Function: pkg.install\r\n      Returned:\r\n      - jid\r\n      Running: []\r\n      Target: ip-10-196-38-159\r\n      Target-type: glob\r\n\r\n\r\nRunning the minion from the command line works, no problem at all. Only when started from its init script does things break.\r\n'
4971,'basepi','salt.states.module.wait of salt.modules.pip.install fail (exit code 1) but result=True\nwith 0.15.1.\r\n\r\nit\'s caused by a missing requirement, but still, pip fail, it ``sys.exit(1)`` but module still return result==True\r\n\r\n```\r\n----------\r\n    State: - module\r\n    Name:      pip.install\r\n    Function:  wait\r\n        Result:    True\r\n        Comment:   Module function pip.install executed\r\n        Changes:   ret: {\'pid\': 17390,\r\n \'retcode\': 1,\r\n \'stderr\': \'\',\r\n \'stdout\': "Downloading/unpacking http://cairographics.org/releases/py2cairo-1.8.10.tar.gz (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 10))\\n  Storing download in cache at /var/cache/pip/http%3A%2F%2Fcairographics.org%2Freleases%2Fpy2cairo-1.8.10.tar.gz\\n  Running setup.py egg_info for package from http://cairographics.org/releases/py2cairo-1.8.10.tar.gz\\n    cairo >= 1.8.10 detected\\n    \\nRequirement already up-to-date: pip>=1.3.1 in /usr/local/graphite/lib/python2.7/site-packages/pip-1.3.1-py2.7.egg (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 2))\\nDownloading/unpacking django==1.3.7 (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 3))\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2FD%2FDjango%2FDjango-1.3.7.tar.gz\\n  Running setup.py egg_info for package django\\n    \\nRequirement already up-to-date: txamqp in /usr/local/graphite/lib/python2.7/site-packages (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 4))\\nDownloading/unpacking python-memcached (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 5))\\n  Downloading python-memcached-1.51.tar.gz\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fp%2Fpython-memcached%2Fpython-memcached-1.51.tar.gz\\n  Running setup.py egg_info for package python-memcached\\n    \\n    warning: no files found matching \'*.rst\'\\n    warning: no files found matching \'*.txt\'\\n    warning: no files found matching \'MakeFile\'\\n    warning: no previously-included files matching \'*.pyc\' found anywhere in distribution\\n    warning: no previously-included files matching \'.gitignore\' found anywhere in distribution\\n    warning: no previously-included files matching \'.DS_Store\' found anywhere in distribution\\nDownloading/unpacking pytz (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 6))\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fp%2Fpytz%2Fpytz-2013b.zip\\n  Running setup.py egg_info for package pytz\\n    \\n    warning: no files found matching \'*.pot\' under directory \'pytz\'\\n    warning: no previously-included files found matching \'test_zdump.py\'\\nDownloading/unpacking simplejson (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 7))\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fs%2Fsimplejson%2Fsimplejson-3.3.0.tar.gz\\n  Running setup.py egg_info for package simplejson\\n    \\nDownloading/unpacking django-tagging (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 8))\\n  Downloading django-tagging-0.3.1.tar.gz\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fd%2Fdjango-tagging%2Fdjango-tagging-0.3.1.tar.gz\\n  Running setup.py egg_info for package django-tagging\\n    \\nDownloading/unpacking psycopg2 (from -r /usr/local/graphite/salt-graphite-web-requirements.txt (line 9))\\n  Storing download in cache at /var/cache/pip/https%3A%2F%2Fpypi.python.org%2Fpackages%2Fsource%2Fp%2Fpsycopg2%2Fpsycopg2-2.5.tar.gz\\n  Running setup.py egg_info for package psycopg2\\n    \\n    Error: pg_config executable not found.\\n    \\n    Please add the directory containing pg_config to the PATH\\n    or specify the full executable path with the option:\\n    \\n        python setup.py build_ext --pg-config /path/to/pg_config build ...\\n    \\n    or with the pg_config option in \'setup.cfg\'.\\n    Complete output from command python setup.py egg_info:\\n    running egg_info\\n\\ncreating pip-egg-info/psycopg2.egg-info\\n\\nwriting pip-egg-info/psycopg2.egg-info/PKG-INFO\\n\\nwriting top-level names to pip-egg-info/psycopg2.egg-info/top_level.txt\\n\\nwriting dependency_links to pip-egg-info/psycopg2.egg-info/dependency_links.txt\\n\\nwriting manifest file \'pip-egg-info/psycopg2.egg-info/SOURCES.txt\'\\n\\nwarning: manifest_maker: standard file \'-c\' not found\\n\\n\\n\\nError: pg_config executable not found.\\n\\n\\n\\nPlease add the directory containing pg_config to the PATH\\n\\nor specify the full executable path with the option:\\n\\n\\n\\n    python setup.py build_ext --pg-config /path/to/pg_config build ...\\n\\n\\n\\nor with the pg_config option in \'setup.cfg\'.\\n\\n----------------------------------------\\nCommand python setup.py egg_info failed with error code 1 in /usr/local/graphite/build/psycopg2\\nStoring complete log in /root/.pip/pip.log"}\r\n```\r\n\r\nI use module.wait because of #2757'
4969,'UtahDave',"ipv4 grain on Windows is blank\nsalt 'minion' grains.item ipv4\r\n\r\n```\r\n  ipv4:\r\n```"
4968,'UtahDave','win_network.interfaces needs fixing\n```\r\n    Missing arguments executing "network.interfaces": ArgSpec(args=[], varargs=None, keywords=None, defaults=None)\r\n```\r\n\r\nIs all I get, granted the documentation doesn\'t specify anything about what goes in or what comes out either.'
4939,'UtahDave',"inconsistent checks for Windows in __virtual__()\nThe `shadow` module uses ` salt.utils.is_windows()`; other modules seem to check `__grains__['os']`."
4927,'cro','network.traceroute fails with "list index out of range" traceback on OSX (Darwin)\nTrying to call network.traceroute on an OSX (Darwin) minion fails with a Traceback:\r\n#########\r\n$ salt \'*\' network.traceroute cuda-fs1\r\nTraceback (most recent call last):\r\n      File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/salt/minion.py", line 428, in _thread_return\r\n        ret[\'return\'] = func(*args, **kwargs)\r\n      File "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/salt/modules/network.py", line 291, in traceroute\r\n        traceroute_version = re.findall(\'version (\\d+)\\.(\\d+)\\.(\\d+)\', out2)[0]\r\n    IndexError: list index out of range\r\n#########\r\n\r\nSame exact thing works for Windows & Linux minions.  Master is running 0.14.1.'
4906,'cachedout',"Huge load when restarting salt-master 0.15.0 on Ubuntu 12.04 amd64\nOn our Ubuntu 12.04 amd64 master we see huge system loads when restarting salt-master. The watchdog kernel threads go nuts and loads shoots up to 60. It takes about a minute for things to settle down eventually...\r\n\r\nThis is a server with a 6-core Opteron processor and 16GB of memory that typically has a load under 1 and a third of the memory available for buffers and cache. Although load shoots up fairly high during the salt-master restart, I/O usage doesn't seem to be an issue, so I think the storage is irrelevant. Also, memory usage looks normal during the restart of the salt-master service. \r\n\r\nWith top I see CPU time consumed mostly by _us_ and _sy_ processes, with the latter being predominant. They are mostly watchdog and kworker threads. On the user side, I see concurrent _lspci_ processes unexpectedly topping the load chart. Maybe it's worth noting that a single lspci takes quite some time to run on this system:\r\n\r\nsalt@boiu:~$ time lspci &>/dev/null\r\n\r\nreal\t0m3.394s\r\nuser\t0m0.072s\r\nsys\t0m0.216s\r\n\r\nDebug log collected during a problematic restart: http://pastebin.com/D07uxqbe . Any tips for further investigation are welcome...\r\n\r\nThanks!"
4902,'UtahDave','File state broken on Windows\n```\r\n        State: - file\r\n        Name:      c:\\test\r\n        Function:  recurse\r\n            Result:    False\r\n            Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n      File "salt/state.py", line 1215, in call\r\n      File "salt/states/file.py", line 1295, in recurse\r\n      File "salt/states/file.py", line 1249, in manage_directory\r\n      File "salt/states/file.py", line 935, in directory\r\n      File "salt/modules/file.py", line 1244, in check_perms\r\n    NameError: global name \'__salt__\' is not defined\r\n```\r\n\r\n'
4880,'s0undt3ch','deal w/ shadowing of built-ins; fix all pylint W0622 problems\n* add pragma comments where shadowing unavoidable\r\n* apply variable renaming refactoring elsewhere'
4831,'basepi',"minions down report via salt -v  is not consistent\nsalt version: 0.14.1\r\nsalt -v doesn't report downed minion consistently.\r\n\r\nsalt -v '*' test.ping  : reports downed minion with 'Minion did not return' message correctly.\r\n\r\nWhile invocations like below won't report downed minion at all.\r\nsalt -I 'roles:Myrole' test.ping\r\nsalt -G 'os_family=RedHat' test.ping\r\nand other variations like there.\r\n\r\nIt would be a nice feature to report downed minion consistently with salt -v option (or may be an additional command line option salt --down-report ) this would be very helpful when pushing states to large number of minions and knowing if any minion is missed in that cycle.\r\n\r\n\r\n\r\n"
4829,'thatch45',"Salt Mine returns what appears to be messagepack encoded data.\nWhen querying the Salt Mine the data that gets returned is still encoded in what I'm guessing is messagepack.\r\n\r\nRunning Ubuntu 12.04 and the latest Salt from git. Slightly newer than 0.15. (4 May 2013)\r\n\r\nHere are the commands I ran to reproduce this issue.\r\n\r\n```bash\r\n[boucha@dasalt salt (develop u=)]$ sudo salt \\* mine.get '*' network.interfaces\r\ndave.saltstack.com:\r\n    i�'��\x03�ؿнw��T42\r\n>�\x13���CG�K         a�_�~�R� R�M�>M��S�˺d��I\x17�61#}\r\n```"
4798,'terminalmage','downgrading a package\nwhen i introduced a version in a pkg-state for a package that already was installed, it failed downgrading it, afterwards:\r\n<pre>\r\n2013-05-03 09:18:20,568 [salt.state       ][INFO    ] Executing state pkg.installed for mongodb-10gen                                                                                                                                                                                  \r\n2013-05-03 09:18:20,568 [salt.loaded.int.module.cmdmod][INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/root\'                                                                                                              \r\n2013-05-03 09:18:20,586 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'aptitude search "?name(^.+$) ?virtual ?reverse-provides(?installed)"\' in directory \'/root\'                                                                                                        \r\n2013-05-03 09:18:21,463 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" lt "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:21,468 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" eq "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:21,471 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" gt "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:21,475 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'apt-get -q update\' in directory \'/root\'                                                                                                                                                           \r\n2013-05-03 09:18:27,670 [salt.loaded.int.module.cmdmod][INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/root\'                                                                                                              \r\n2013-05-03 09:18:27,693 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'aptitude search "?name(^.+$) ?virtual ?reverse-provides(?installed)"\' in directory \'/root\'                                                                                                        \r\n2013-05-03 09:18:28,572 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'apt-get -q -y -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef   install "mongodb-10gen=2.0.6"\' in directory \'/root\'                                                         \r\n2013-05-03 09:18:29,005 [salt.loaded.int.module.cmdmod][ERROR   ] Command \'apt-get -q -y -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef   install "mongodb-10gen=2.0.6"\' failed with return code: 100                                                           \r\n2013-05-03 09:18:29,006 [salt.loaded.int.module.cmdmod][ERROR   ] stdout: Reading package lists...                                                                                                                                                                                     \r\nBuilding dependency tree...                                                                                                                                                                                                                                                            \r\nReading state information...                                                                                                                                                                                                                                                           \r\nThe following packages will be DOWNGRADED:                                                                                                                                                                                                                                             \r\n  mongodb-10gen                                                                                                                                                                                                                                                                        \r\n0 upgraded, 0 newly installed, 1 downgraded, 0 to remove and 0 not upgraded.                                                                                                                                                                                                           \r\nNeed to get 35.6 MB of archives.                                                                                                                                                                                                                                                       \r\nAfter this operation, 137 MB disk space will be freed.                                                                                                                                                                                                                                 \r\n2013-05-03 09:18:29,006 [salt.loaded.int.module.cmdmod][ERROR   ] stderr: E: There are problems and -y was used without --force-yes                                                                                                                                                    \r\n2013-05-03 09:18:29,006 [salt.loaded.int.module.apt][ERROR   ] Error:  E: There are problems and -y was used without --force-yes                                                                                                                                                       \r\n2013-05-03 09:18:29,006 [salt.loaded.int.module.cmdmod][INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W" in directory \'/root\'                                                                                                              \r\n2013-05-03 09:18:29,029 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'aptitude search "?name(^.+$) ?virtual ?reverse-provides(?installed)"\' in directory \'/root\'                                                                                                        \r\n2013-05-03 09:18:29,901 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" lt "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:29,906 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" eq "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:29,909 [salt.loaded.int.module.cmdmod][INFO    ] Executing command \'dpkg --compare-versions "2.4.3" gt "2.0.6"\' in directory \'/root\'                                                                                                                                  \r\n2013-05-03 09:18:29,913 [salt.state       ][ERROR   ] Error:  E: There are problems and -y was used without --force-yes                                                              \r\n</pre>\r\n\r\ndid not much debugging on this, but a local \'</code>aptitude install mongodb-10gen=2.0.6</code>\' worked.'
4797,'terminalmage',"Add `git ls-remote` to git module\nI want to be able to clone arbitrary git identifiers (e.g. master) to canonical directories (i.e. their hash). At the moment, I can't really do this, as I can only operate on local repos.\r\n\r\nAdding `git ls-remote` support would allow me to do this."
4792,'terminalmage',"unused arguments in file module\npylint:\r\n```\r\nW0613:1307,8:check_managed: Unused argument 'makedirs'\r\nW0613:1362,8:check_file_meta: Unused argument 'template'\r\nW0613:1469,16:manage_file: Unused argument 'template'\r\n```"
4765,'terminalmage',"modules.cron.rm_job() doesn't check that temporal fields match\npylint:\r\n```\r\n************* Module salt.modules.cron\r\nW0613:229,25:rm_job: Unused argument 'hour'\r\nW0613:229,31:rm_job: Unused argument 'dom'\r\nW0613:229,36:rm_job: Unused argument 'month'\r\nW0613:229,43:rm_job: Unused argument 'dow'\r\nW0613:229,17:rm_job: Unused argument 'minute'\r\n```"
4704,'whiteinge','Remove sed dependency from file.comment and file.uncomment\nPer a conversation with Seth, remove the need for sed for the file.comment and file.uncomment functions.'
4663,'UtahDave',"undefined var `flags` referenced in salt.modules.win_file's contains() function\nhttps://github.com/saltstack/salt/blob/4e90a149d6d2b82e61183c2bf699a90724e7f11f/salt/modules/win_file.py#L540"
4630,'UtahDave','pkg.install on Windows fails, pkg.available_version always blank\nHi,\r\n\r\nI\'m running Salt 0.14.1 on master/minion.\r\n\r\nI am following the guide at http://docs.saltstack.com/ref/windows-package-manager.html\r\n\r\n1. My master config has the Windows repo at /srv/salt/windows/repo (instead of /srv/salt/win/repo) - not sure if this is relevant\r\n\r\n1. I have created /srv/salt/windows/repo/octopus/init.sls:\r\n```\r\noctopus:\r\n  1.5.1652:\r\n    installer: salt://windows/repo/octopus/Octopus.Tentacle.1.5.1.1652.msi\r\n    full_name: Octopus Tentacle\r\n    reboot: False\r\n    msiexec: True\r\n    locale: en_US\r\n```\r\n\r\nIncluded in this directory is the MSI as well.\r\n\r\n1. I follow the directions to refresh the package manager database:\r\n\r\n```\r\nuser@host:/srv/salt/windows/repo/octopus# salt-run winrepo.genrepo\r\n{\'octopus\': {\'1.5.1652\': {\'full_name\': \'Octopus Tentacle\',\r\n                          \'installer\': \'salt://windows/repo/octopus/Octopus.Tentacle.1.5.1.1652.msi\',\r\n                          \'locale\': \'en_US\',\r\n                          \'msiexec\': True,\r\n                          \'reboot\': False}}}\r\n```\r\n\r\nTell minion to now refresh:\r\n```\r\nuser@host:/srv/salt/windows/repo/octopus# salt myhost pkg.refresh_db -v\r\nExecuting job with jid 20130428120611705949\r\n-------------------------------------------\r\n\r\nmyhost:\r\n    True\r\n```\r\n\r\nAttempt to list available packages (first sign of trouble, no results):\r\n```\r\nuser@host:/srv/salt/windows/repo/octopus# salt myhost pkg.available_version octopus -v\r\nExecuting job with jid 20130428120706925663\r\n-------------------------------------------\r\n\r\nmyhost:\r\n    ----------\r\nuser@host:/srv/salt/windows/repo/octopus#\r\n```\r\n\r\nAttempt to install anyway:\r\n```\r\nuser@host:/srv/salt/windows/repo/octopus# salt myhost pkg.install octopus -v\r\nExecuting job with jid 20130428115823613331\r\n-------------------------------------------\r\n\r\nmyhost:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 423, in _thread_return\r\n      File "salt/modules/win_pkg.py", line 367, in install\r\n    AttributeError: \'str\' object has no attribute \'keys\'\r\n```\r\n\r\nThis makes me unable to install the required packages on the hosts.'
4627,'terminalmage',"salt.states.mount opts not all being deployed to fstab\nFor some reason, not all opts aren't deployed on certain nfs mounts. There doesn't seem to be any pattern so I'm guessing it's a bug.\r\n\r\n\r\nHere's my .sls file\r\n```\r\n/var/www/wp-content/blogs.dir:\r\n  mount.mounted:\r\n    - device: nfs:/mnt/nfs/blogs.dir\r\n    - fstype: nfs\r\n    - mkmnt: True\r\n    - opts: _netdev,auto,soft,retrans=10,nfsvers=3\r\n    \r\n    \r\n/var/www/nfs-mountpoint:\r\n  mount.mounted:\r\n    - device: nfs:/mnt/nfs/files\r\n    - fstype: nfs\r\n    - mkmnt: True\r\n    - opts: _netdev,auto,soft,retrans=10,nfsvers=3\r\n    \r\n    \r\n/var/www/vault:\r\n  mount.mounted:\r\n    - device: nfs:/mnt/nfs/vault\r\n    - fstype: nfs\r\n    - mkmnt: True\r\n    - opts: _netdev,auto,soft,retrans=10,nfsvers=3\r\n    \r\n    \r\n/media/mkt-vault:\r\n  mount.mounted:\r\n    - device: nfs:/mnt/nfs/mkt_vault\r\n    - fstype: nfs\r\n    - mkmnt: True\r\n    - opts: _netdev,auto,soft,retrans=10,nfsvers=3\r\n\r\n```\r\n\r\n\r\n```\r\n> cat /etc/fstab\r\n\r\nnfs:/mnt/nfs/mkt_vault   /media/mkt-vault        nfs     _netdev,auto    0 0\r\nnfs:/mnt/nfs/vault       /var/www/vault    nfs     _netdev,auto,soft,retrans=10,nfsvers=3  0 0\r\nnfs:/mnt/nfs/blogs.dir   /var/www/wp-content/blogs.dir     nfs     _netdev,auto,soft,retrans=10,nfsvers=3  0 0\r\nnfs:/mnt/nfs/files       /var/www/nfs-mountpoint    nfs     _netdev,auto    0 0\r\n```"
4621,'terminalmage',"test=True is ignored on pkg.upgrade\nA quick check of my minions show that the package was indeed installed.  And the output is colorized as if test=True were not present.\r\n\r\ncquinn@salt ~ % sudo salt '*' pkg.upgrade test=True\r\nvirt1.epicquinn.net:\r\n    ----------\r\n    curl:\r\n        ----------\r\n        new:\r\n            7.19.7-36.el6_4\r\n        old:\r\n            7.19.7-35.el6\r\n    libblkid:\r\n        ----------\r\n        new:\r\n            2.17.2-12.9.el6_4.2\r\n        old:\r\n            2.17.2-12.9.el6\r\n    libcurl:\r\n        ----------\r\n        new:\r\n            7.19.7-36.el6_4\r\n        old:\r\n            7.19.7-35.el6\r\n    libuuid:\r\n        ----------\r\n        new:\r\n            2.17.2-12.9.el6_4.2\r\n        old:\r\n            2.17.2-12.9.el6\r\n    util-linux-ng:\r\n        ----------\r\n        new:\r\n            2.17.2-12.9.el6_4.2\r\n        old:\r\n            2.17.2-12.9.el6"
4617,'thatch45',"Salt event sender hangs on minion under certain circumstances\nRunning: `__salt__['event.fire_master']({...}, 'newpackage')` on one of my minions hangs indefinitely, and the only way to recover is to:\r\n\r\n- kill -9 process sending event\r\n- stop minion and kill all stray salt-minion processes\r\n- restart master\r\n- start minion\r\n\r\nThis only happens when one of the machines listed in the salt master's key listing has gone away (ec2 instance died).  Strangely, after doing the procedure above, events work again, regardless of whether the key for the gone minion is deleted or not.\r\n\r\nWhat this means in practice for me is that, every time EC2 flushes one of my machines down the drain, I can reliably predict that the build that sends the event will become wedged forever, until it fails because I kill the event sender -9.\r\n\r\nI am using \r\n\r\nii  libzmq3                 3.2.2+dfsg-1precise     lightweight messaging kernel (shared library)\r\nii  salt-minion             0.13.1-3precise         This package represents the client package for salt\r\n\r\nThanks in advance."
4600,'thatch45',"Matching on nodegroups fails\nIn pillars/top.sls, if I have a nodegroup targetted:\r\n\r\n    'layer1':\r\n      - match: nodegroup\r\n      - secrets\r\n\r\nIt seems to be ignored.  I've tested other variations which are also ignored:\r\n\r\n    'N@layer1':\r\n      - match: compound\r\n      - secrets\r\n\r\nThe nodegroup itself is well defined, as it can be used in matching in the CLI successfully:\r\n\r\n    # salt -N 'layer1' test.ping\r\n    theserver:\r\n        True\r\n\r\n"
4589,'UtahDave','user.absent stacktraces on Windows.\nuser.absent stacktraces like this:\r\n\r\n```\r\n----------\r\n    State: - user\r\n    Name:      testuser\r\n    Function:  absent\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "C:\\salt\\python27\\lib\\site-packages\\salt\\state.py", line 1217, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "C:\\salt\\python27\\lib\\site-packages\\salt\\states\\user.py", line 386, in absent\r\n    ret[\'result\'] = __salt__[\'user.delete\'](name, purge, force)\r\nTypeError: delete() takes exactly 1 argument (3 given)\r\n\r\n        Changes:   \r\n```\r\nObserved on Windows 2008 server\r\nLatest Salt from git.'
4577,'UtahDave',"init.sls not picked up by default for Windows minion\nWe use salt 0.14 with a `base` directory containing multiple `services/name` subdirectories. And then, to include an sls per-machine, we add something like this in our `top.sls`:\r\n\r\n    'linuxhost':\r\n      - services.name\r\n\r\nwhere the state is actually described in an `init.sls` file in `base/services/name/`.\r\n\r\nHowever for a Windows minion with 0.14.1 we have to add the `.init` suffix to have it work in our setup:\r\n\r\n    'windowshost':\r\n      - services.name.init\r\n\r\nWithout the `.init` suffix we get *No states found for this minion*  when applying the highstate state for the Windows minion, which was rather confusing at first..."
4564,'UtahDave','Fix Windows user add.\nWhile fixing bug #4484 I came across this error.\r\nOnce #4484 was fixed, you can now create the user. But now the second time you run the state you get this error:\r\n\r\n```\r\nubuntu@ip-10-195-227-27:/srv/salt$ sudo salt \'lk*\' state.sls testuser\r\nlkjlkj:\r\n----------\r\n    State: - user\r\n    Name:      testuser\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   New user testuser created\r\n        Changes:   comment: \r\n                   profile: \r\n                   name: testuser\r\n                   groups: [\'Users\']\r\n                   active: Yes\r\n                   home: \r\n                   fullname: \r\n                   logonscript: \r\n                   \r\nubuntu@ip-10-195-227-27:/srv/salt$ sudo salt \'lk*\' state.sls testuser\r\nlkjlkj:\r\n----------\r\n    State: - user\r\n    Name:      testuser\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "C:\\salt\\python27\\lib\\site-packages\\salt\\state.py", line 1217, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "C:\\salt\\python27\\lib\\site-packages\\salt\\states\\user.py", line 255, in present\r\n    homephone)\r\n  File "C:\\salt\\python27\\lib\\site-packages\\salt\\states\\user.py", line 69, in _changes\r\n    if __salt__[\'file.gid_to_group\'](gid or lusr[\'gid\']) in \\\r\nKeyError: \'gid\'\r\n\r\n        Changes:   \r\nubuntu@ip-10-195-227-27:/srv/salt$ \r\n\r\n```'
4562,'thatch45',"salt-key -d  causes all minions to not respond\nThis is running the latest salt from git on Ubuntu 12.04\r\nIf you run ```salt-key -d 'minionname'``` no minions will return until you restart the salt-master.\r\n\r\n```bash\r\nubuntu@ip-10-195-227-27:~$ sudo salt \\* test.version\r\nms-1:\r\n    0.14.0-582-g6f17a5d\r\nms-3:\r\n    0.14.0-582-g6f17a5d\r\nms-0:\r\n    0.14.0-582-g6f17a5d\r\nec2master:\r\n    0.14.0-570-gd497151\r\nms-2:\r\n    0.14.0-582-g6f17a5d\r\nubuntu@ip-10-195-227-27:~$ sudo salt-key -d ms-1\r\nThe following keys are going to be deleted:\r\nAccepted Keys:\r\nms-1\r\nProceed? [N/y] y\r\nubuntu@ip-10-195-227-27:~$ sudo salt \\* test.version\r\nubuntu@ip-10-195-227-27:~$ \r\nubuntu@ip-10-195-227-27:~$ sudo service salt-master restart\r\nsalt-master stop/waiting\r\nsalt-master start/running, process 14163\r\nubuntu@ip-10-195-227-27:~$ sudo salt \\* test.version\r\nms-0:\r\n    0.14.0-582-g6f17a5d\r\nms-2:\r\n    0.14.0-582-g6f17a5d\r\nms-3:\r\n    0.14.0-582-g6f17a5d\r\nec2master:\r\n    0.14.0-570-gd497151\r\n\r\n```"
4544,'cachedout','ext_pillar: hiera not working without a key\nThe current implementation for `ext_pillar hiera` uses the hiera command line utility:\r\n\r\nhttps://github.com/saltstack/salt/blob/develop/salt/pillar/hiera.py#L32-37\r\n\r\nThis implementation never passes a key for the item to look up and thus fails\r\n\r\nHere is the help of the hiera command:\r\n```\r\n[root@arch-dev-vbox ~]# hiera --help\r\nUsage: hiera [options] key [default value] [variable=\'text\'...]\r\n\r\nThe default value will be used if no value is found for the key. Scope variables\r\nwill be interpolated into %{variable} placeholders in the hierarchy and in\r\nreturned values.\r\n\r\n    -V, --version                    Version information\r\n    -d, --debug                      Show debugging information\r\n    -a, --array                      Return all values as an array\r\n    -h, --hash                       Return all values as a hash\r\n    -c, --config CONFIG              Configuration file\r\n    -j, --json SCOPE                 JSON format file to load scope from\r\n    -y, --yaml SCOPE                 YAML format file to load scope from\r\n    -m, --mcollective IDENTITY       Use facts from a node (via mcollective) as scope\r\n    -i, --inventory_service IDENTITY Use facts from a node (via Puppet\'s inventory service) as scope\r\n\r\n```\r\n\r\nIf no key is supplied:\r\n```\r\n[root@arch-dev-vbox ~]# hiera\r\nPlease supply a data item to look up\r\n```\r\nIn my settings (the default hiera one) I have a file `/var/lib/hiera/global.yaml`  which has the following (testing) content:\r\n```\r\n[root@arch-dev-vbox ~]# cat /var/lib/hiera/global.yaml \r\nminions:\r\n  host1:\r\n    - group: server\r\n  host2:\r\n    - group: web-server%{environment}\r\n```\r\nIf I want to get this info into pillar, I need to use the key "minions":\r\n```\r\n[root@arch-dev-vbox ~]# hiera -c /etc/hiera.yaml minions environment=\'Linux\'\r\n{"host1"=>[{"group"=>"server"}], "host2"=>[{"group"=>"web-serverLinux"}]}\r\n```'
4522,'UtahDave','windows2003 minion always return error with the module pkg.install\nHi, All\r\n\r\nI want to use windows 2003 client , so I download it from here http://docs.saltstack.com/topics/installation/windows.html!\r\n\r\nFirst, I install Salt-Minion-0.14.1-win32-Setup.exe on my windows 2003 (i386) which is a virtual machine(vmare), config it, \r\nand then, by this doc http://docs.saltstack.com/ref/windows-package-manager.html?highlight=windows, create my own software repository which has firefox and httpd.\r\n\r\nbelow, is my command output: \r\n```\r\n[root@c6x64 httpd]# salt \'win2003_3\' test.ping\r\nwin2003_3:\r\n    True\r\n\r\n[root@c6x64 httpd]# salt-run winrepo.genrepo\r\n{\'firefox\': {\'15.0.1\': {\'full_name\': \'Mozilla Firefox 15.0.1 (x86 en-US)\',\r\n                        \'install_flags\': \' -ms\',\r\n                        \'installer\': \'salt://win/repo/firefox/English/Firefox Setup 15.0.1.exe\',\r\n                        \'locale\': \'en_US\',\r\n                        \'reboot\': False,\r\n                        \'uninstall_flags\': \' /S\',\r\n                        \'uninstaller\': \'%ProgramFiles(x86)%/Mozilla Firefox/uninstall/helper.exe\'},\r\n             \'16.0.2\': {\'full_name\': \'Mozilla Firefox 16.0.2 (x86 en-US)\',\r\n                        \'install_flags\': \' -ms\',\r\n                        \'installer\': \'salt://win/repo/firefox/English/Firefox Setup 16.0.2.exe\',\r\n                        \'locale\': \'en_US\',\r\n                        \'reboot\': False,\r\n                        \'uninstall_flags\': \' /S\',\r\n                        \'uninstaller\': \'%ProgramFiles(x86)%/Mozilla Firefox/uninstall/helper.exe\'},\r\n             \'17.0.1\': {\'full_name\': \'Mozilla Firefox 17.0.1 (x86 en-US)\',\r\n                        \'install_flags\': \' -ms\',\r\n                        \'installer\': \'salt://win/repo/firefox/English/Firefox Setup 17.0.1.exe\',\r\n                        \'locale\': \'en_US\',\r\n                        \'reboot\': False,\r\n                        \'uninstall_flags\': \' /S\',\r\n                        \'uninstaller\': \'%ProgramFiles(x86)%/Mozilla Firefox/uninstall/helper.exe\'}},\r\n \'httpd\': {\'2.0.64\': {\'full_name\': \'Httpd 2.0.64 (x86)\',\r\n                      \'install_flags\': \' /quiet\',\r\n                      \'installer\': \'salt://pkg/windows/xp/i386/httpd-2.0.64-win32-x86-no_ssl.msi\',\r\n                      \'reboot\': False},\r\n           \'2.2.21\': {\'full_name\': \'Httpd 2.2.21 (x86)\',\r\n                      \'install_flags\': \' /quiet\',\r\n                      \'installer\': \'salt://pkg/windows/xp/i386/httpd-2.2.21-win32-x86-no_ssl.msi\',\r\n                      \'reboot\': False},\r\n           \'2.2.22\': {\'full_name\': \'Httpd 2.2.22 (x86)\',\r\n                      \'install_flags\': \' /quiet\',\r\n                      \'installer\': \'salt://pkg/windows/xp/i386/httpd-2.2.22-win32-x86-no_ssl.msi\',\r\n                      \'reboot\': False}}}\r\n```\r\n```\r\n[root@c6x64 httpd]# salt \'win2003_3\' pkg.refresh_db\r\nwin2003_3:\r\n    True\r\n\r\n[root@c6x64 httpd]# salt \'win2003_3\' pkg.available_version firefox\r\nwin2003_3:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 423, in _thread_return\r\n      File "salt/modules/win_pkg.py", line 68, in latest_version\r\n      File "salt/modules/win_pkg.py", line 159, in list_pkgs\r\n      File "salt/modules/win_pkg.py", line 200, in _get_msi_software\r\n      File "win32com/client/util.py", line 84, in next\r\n    com_error: (-2147217392, \'OLE error 0x80041010\', None, None)\r\n\r\n[root@c6x64 httpd]# salt \'win2003_3\' pkg.install firefox\r\nwin2003_3:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 423, in _thread_return\r\n      File "salt/modules/win_pkg.py", line 365, in install\r\n      File "salt/modules/win_pkg.py", line 159, in list_pkgs\r\n      File "salt/modules/win_pkg.py", line 200, in _get_msi_software\r\n      File "win32com/client/util.py", line 84, in next\r\n    com_error: (-2147217392, \'OLE error 0x80041010\', None, None)\r\n```\r\nyou see, when I run pkg.*,  the minion always return this error: com_error: (-2147217392, \'OLE error 0x80041010\', None, None).\r\n'
4507,'UtahDave','Windows highstate not working at all in 0.14.1\n    [DEBUG   ] Failed to import output nested, this is most likely NOT a problem:\r\n    Traceback (most recent call last):\r\n      File "salt/loader.py", line 552, in gen_functions\r\n      File "salt/output/nested.py", line 5, in <module>\r\n    ImportError: No module named numbers\r\n\r\n        "local": {\r\n            "no_|-states_|-states_|-None": {\r\n                "comment": "No states found for this minion",\r\n                "__run_num__": 0,\r\n                "changes": {},\r\n                "result": false,\r\n                "name": "No States"\r\n'
4503,'basepi','file:// protocol and master git branch issues\nbase does not seem to attach to the master branch in gitfs when running from a file:// local git repo reference'
4492,'UtahDave','state pkg.removed do not work on windows minion\nI am messing around with a windows repo and so far i got pkg.installed to work proper and applications install and i can remove them via the pkg.remove module but i am not able to use the pkg.removed state on a windows minion target. It just outputs as follow\r\n\r\n    ----------\r\n    State: - pkg\r\n    Name:      chrome\r\n    Function:  removed\r\n        Result:    True\r\n        Comment:   Package chrome is not installed\r\n        Changes:\r\n\r\nBut when i try to uninstall it via "salt win2k8 pkg.remove chrome" it runs the uninstaller and the application is removed as expected.\r\n\r\nAfter some digging in the code the problem comes from\r\nhttps://github.com/saltstack/salt/blob/develop/salt/states/pkg.py#L626\r\nand i think the problem is that the module call done at https://github.com/saltstack/salt/blob/develop/salt/modules/win_pkg.py#L133 do not return the proper data because i can see all my packages when i run the module pkg.list_pkgs'
4485,'UtahDave','Unable to use log.debug() in a module on a windows minion\nI have noticed that logging is not working inside modules on a windows-minion.\r\n\r\nI have tried to add a simple log.debug("foobar") in \r\n\r\n     - win_pkg.py\r\n    def install(name=None, refresh=False, **kwargs):\r\n        log.debug("foobar")\r\n\r\nto have it print out some random debugging info. but when i run the function from my master nothing is printed on the windows minion.\r\n\r\nI have tried to do the same thing but in another module on a ubuntu minion and that works perfect with the same code but in the test.ping module for example.\r\n\r\nWhen i start my windows-minion i set logging level to all and i can still see output like\r\n\r\n    [TRACE   ] Added data.load to module\r\n    [DEBUG   ] Loaded win_service as virtual service\r\n    [INFO    ] User root Executing command pkg.install with jid\r\n    \r\nbut no output from modules is shown.\r\n\r\nI use the latest development on both master and minion and running on win8 and ubuntu 12.04 LTS'
4484,'UtahDave','user.present dosen\'t work on windows\nThe state user.present dosen\'t work under windows.\r\n\r\nThe state that causes the problem is:\r\n\r\n```Administrator:\r\n  user.present:\r\n    - name: Administrator\r\n    - password: examplepassword123\r\n```\r\n\r\nAnd the error:\r\n```windowshost:\r\n----------\r\n    State: - user\r\n    Name:      Administrator\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1210, in call\r\n  File "salt/states/user.py", line 242, in present\r\n  File "salt/states/user.py", line 54, in _changes\r\n  File "salt/modules/win_useradd.py", line 251, in getent\r\nTypeError: \'bool\' object has no attribute \'__getitem__\'\r\n\r\n        Changes:\r\n```\r\n\r\nSeems to be related to user.info returning a bool for a user that doesn\'t exist at line 178 of win_useradd.py in contrast to unix version returning an empty dictionary.\r\n\r\n\r\n\r\n\r\nReported by AndrewAu via IRC.\r\n\r\n\r\n'
4481,'basepi','file.sed exit code error\nHi all,\r\n\r\nI have a problem, I hope to get a solution to.\r\nprior to upgrading to version 14, this worked without a flaw..\r\n\r\nmaybe it\'s not the right way of doing this, if so, please let me know.\r\n\r\nfirst off, I am trying ot edit a configuration file for a particular package.\r\nI see online that alot of people simply host the config file and then ensure that it is copied to the minion as is.\r\nI don\'t like this idea since the configuration file may change with updates.\r\n\r\nSo I decided to edit the values inside the file with sed.\r\n\r\nmy state file has these lines:\r\n\r\n/etc/apt-cacher-ng/acng.conf:\r\n  file.sed:\r\n    - before: \'.*\'\r\n    - after: "CacheDir: /srv/aptcacher-ng"\r\n    - limit: \'^CacheDir:\'\r\n    - options: \'-r\'\r\n    - flags: \'\'\r\n    - require:\r\n      - pkg: apt-cacher-ng\r\n\r\nThis works. (on the first pass)\r\nThen on the second pass, I get the following error:\r\n\r\n[INFO    ] Executing command "sed -n -r -e \'/^CacheDir:/ s/.*/$/p\' /etc/apt-cacher-ng/acng.conf" in directory \'/root\'\r\n[INFO    ] Executing command "sed -i.bak -r \'/^CacheDir:/ s/.*/CacheDir: \\\\/srv\\\\/aptcacher-ng/\' /etc/apt-cacher-ng/acng.conf" in directory \'/root\'\r\n[ERROR   ] No changes made for /etc/apt-cacher-ng/acng.conf\r\n\r\n----------\r\n    State: - file\r\n    Name:      /etc/apt-cacher-ng/acng.conf\r\n    Function:  sed\r\n        Result:    False\r\n        Comment:   sed ran without error, but no changes were made\r\n        Changes:\r\n\r\ni\'m not sure how that can be an error since the end result is exactly how i want it.\r\nIs there a better way of doing this? or is this a bug?\r\n\r\nlooking forward to any solutions anyone can provide.\r\n\r\nmy salt version is 0.14.0-369-g71951e5\r\n\r\nthanks,\r\nMarc'
4452,'UtahDave','file.recurse locking up windows minion\nUsing a file.recurse state sometimes locks up the minion.\r\n\r\nusing clean: True seems to make it happen every time\r\nwithout clean: True, it seems to work a small number of times, then the minion stops responding.\r\n\r\nLooking at the debug logs, it seems to stop before it gets to the grains.\r\n![stuck1](https://f.cloud.github.com/assets/2563935/360764/a059f2e4-a1a8-11e2-87f4-4b35c6bd6fe2.png)\r\n![stuck2](https://f.cloud.github.com/assets/2563935/360766/a5437f00-a1a8-11e2-9a31-a9ce3ee10e41.png)\r\n\r\nInterestingly, it prints out the name of a file being managed in the same sls file (mercurial.ini).\r\n\r\nJoe\r\n'
4410,'cro',"service.start salt-minion hangs from console on CentOS\nPer salt/utils/__init__.py:\r\n\r\n```\r\n175     # A normal daemonization redirects the process output to /dev/null. \r\n176     # Unfortunately when a python multiprocess is called the output is \r\n177     # not cleanly redirected and the parent process dies when the \r\n178     # multiprocessing process attempts to access stdout or err. \r\n179     #dev_null = open('/dev/null', 'rw') \r\n180     #os.dup2(dev_null.fileno(), sys.stdin.fileno()) \r\n181     #os.dup2(dev_null.fileno(), sys.stdout.fileno()) \r\n182     #os.dup2(dev_null.fileno(), sys.stderr.fileno()) \r\n```\r\n\r\nThis causes service.start to hang if executed from the console:\r\n\r\n```\r\n# cat /etc/issue\r\nCentOS release 6.3 (Final)\r\nKernel \\r on an \\m\r\n\r\n# salt-call service.start salt-minion\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Executing command '/sbin/service salt-minion start' in directory '/root'\r\n(hangs here)\r\n```\r\n\r\nI see two ways to potential fix this:\r\n1.  Close the inherited file handles in the python code\r\n2.  Change the init scripts, redirecting the start of the daemon to /dev/null.  I've verified that this is a workaround."
4409,'terminalmage',"Top.sls files in an environment directory combine with the main from ./base/\nProblem:   A rogue cp accidentally put a top.sls file in an environment directory unbeknownst to me.  When running highstate on an unrelated env, a state that had been removed from ./base/top.sls was still being invoked.  This gave the appearance of a caching issue.\r\n\r\nExample env structure that yields two top.sls files being sourced.  In this case, running highstate on staging, sources the topfile from prod.\r\n\r\n./base/top.sls\r\n./staging/\r\n./prod/top.sls\r\n\r\nWhen running salt <hostname> state.highstate for a host in the staging env, salt-master iterates through every defined environment and attempts to stat ./$env/top.sls, and if found, sources the discovered top.sls regardless of env.\r\n\r\nhttps://gist.github.com/cmeisinger/ca9f2e0fe0d028035e03\r\n\r\nExpected Result:   I see no mention in the docs regarding multiple top.sls files.   With that in mind, I would expect ./base/top.sls to be the only one sourced for states.  I'm unsure of why salt-master recursively scans each env.\r\n\r\nI'm also unsure if this is intended and just not documented, or a bug, so treat it as you will. :)"
4402,'thatch45','publish.publish should return same info as running on salt-master\nWhen I run a command directly via salt on the salt-master, I get responses from all my minions. When I run the same command on a minion via salt-call publish.publish, I get responses from only about 3/4 of my minions.\r\n\r\nI updated salt and 0mq to lastest stable:\r\n\r\n```\r\nroot@salt:~# salt \\*  test.version --out=yaml | sort \r\napi01: 0.14.0\r\napp04: 0.14.0\r\napp05: 0.14.0\r\napp06: 0.14.0\r\ndata01: 0.14.0\r\ndata02: 0.14.0\r\ndb-admin03: 0.14.0\r\ndb-clone: 0.14.0\r\ndb-command02: 0.14.0\r\ndb-staging01: 0.14.0\r\nflint-mq01: 0.14.0\r\nflint-mux01: 0.14.0\r\nfront02: 0.14.0\r\nfront03: 0.14.0\r\nfront05: 0.14.0\r\nfront06: 0.14.0\r\ngraphite01: 0.14.0\r\nheartbeat01: 0.14.0\r\njenkins01: 0.14.0\r\nls-broker01: 0.14.0\r\nls-elasticsearch01: 0.14.0\r\nls-indexer01: 0.14.0\r\nls-shipper01: 0.14.0\r\nls-web01: 0.14.0\r\nmail02: 0.14.0\r\nmq01: 0.14.0\r\nproc01: 0.14.0\r\nproc02: 0.14.0\r\nrepo01: 0.14.0\r\nrsdev01: 0.14.0\r\nrsdev02: 0.14.0\r\nsalt: 0.14.0\r\nspf01: 0.14.0\r\nstaging01: 0.14.0\r\nstaging02: 0.14.0\r\nstaging03: 0.14.0\r\nstaging04: 0.14.0\r\nstaging05: 0.14.0\r\nstaging-heartbeat01: 0.14.0\r\nws-trans02: 0.14.0\r\nws-trans03: 0.14.0\r\nws-trans04: 0.14.0\r\nws-trans05: 0.14.0\r\nws-trans06: 0.14.0\r\n```\r\n\r\nAnd also the master:\r\n```\r\nroot@salt:~# salt --versions-report\r\n           Salt: 0.14.0\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: not installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n```\r\n\r\nI\'m using the following command on the master for testing:\r\n```\r\nroot@salt:~# salt \'*\' network.ip_addrs --out=yaml | grep \':$\'  | sort\r\napi01:\r\napp04:\r\napp05:\r\napp06:\r\ndata01:\r\ndata02:\r\ndb-admin03:\r\ndb-clone:\r\ndb-command02:\r\ndb-staging01:\r\nflint-mq01:\r\nflint-mux01:\r\nfront02:\r\nfront03:\r\nfront05:\r\nfront06:\r\ngraphite01:\r\nheartbeat01:\r\njenkins01:\r\nls-broker01:\r\nls-elasticsearch01:\r\nls-indexer01:\r\nls-shipper01:\r\nls-web01:\r\nmail02:\r\nmq01:\r\nproc01:\r\nproc02:\r\nrepo01:\r\nrsdev01:\r\nrsdev02:\r\nsalt:\r\nspf01:\r\nstaging01:\r\nstaging02:\r\nstaging03:\r\nstaging04:\r\nstaging05:\r\nstaging-heartbeat01:\r\nws-trans02:\r\nws-trans03:\r\nws-trans04:\r\nws-trans05:\r\nws-trans06:\r\n```\r\n\r\nWhich looks like it\'s getting results from all minions. However\r\n\r\n```\r\nahammond@staging05:~$ sudo salt-call publish.publish \'*\' network.ip_addrs --out=yaml | grep \':$\' | sort\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Package debconf-utils is not installed.\r\napi01:\r\napp04:\r\napp05:\r\ndata01:\r\ndata02:\r\ndb-command02:\r\ndb-staging01:\r\nflint-mq01:\r\nfront03:\r\nfront05:\r\nfront06:\r\ngraphite01:\r\nheartbeat01:\r\njenkins01:\r\nls-broker01:\r\nls-elasticsearch01:\r\nls-indexer01:\r\nls-shipper01:\r\nls-web01:\r\nmail02:\r\nmq01:\r\nproc01:\r\nproc02:\r\nrepo01:\r\nsalt:\r\nstaging01:\r\nstaging02:\r\nstaging03:\r\nstaging04:\r\nstaging05:\r\nstaging-heartbeat01:\r\nws-trans02:\r\nws-trans03:\r\nws-trans04:\r\nws-trans05:\r\nws-trans06:\r\n```\r\n\r\nEven when I increase the timeout for publish.publish, I still am not getting responses from all systems.\r\n\r\n```\r\nahammond@staging05:~$ sudo salt-call publish.publish \'*\' network.ip_addrs \'\' \'glob\' 30 --out=yaml | grep \':$\' | sort\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Package debconf-utils is not installed.\r\napi01:\r\napp04:\r\napp05:\r\napp06:\r\ndata01:\r\ndata02:\r\ndb-admin03:\r\ndb-clone:\r\ndb-staging01:\r\nflint-mq01:\r\nflint-mux01:\r\nfront02:\r\nfront05:\r\nfront06:\r\nheartbeat01:\r\nls-broker01:\r\nls-indexer01:\r\nls-shipper01:\r\nls-web01:\r\nmail02:\r\nmq01:\r\nproc01:\r\nproc02:\r\nrepo01:\r\nrsdev01:\r\nrsdev02:\r\nsalt:\r\nspf01:\r\nstaging01:\r\nstaging02:\r\nstaging03:\r\nstaging04:\r\nstaging-heartbeat01:\r\nws-trans02:\r\nws-trans05:\r\nws-trans06:\r\n```\r\n\r\nI ran this command a number of times and saved the output, looking for commonalities in which minions are not responding, but all of the minions responded at least once. As near as I can tell, which minions are included in the output is random, but it always hovers around 36. Changing the command to test.ping has no effect on the number of responding minions, nor which minions respond.\r\n\r\nI checked logfiles for both minion and master. The only error I see is "The value for the \'max_open_files\' setting, 100000, is higher than what the user running salt is allowed to raise to, 4096. Defaulting to 4096."'
4383,'terminalmage',"Rework the user.info functions\nFrom a conversation with @basepi in #salt-devel\r\n\r\nThe ``info()`` functions in useradd.py, pw_user.py, solaris_user.py, and win_useradd.py should be updated so that, if the user does not exist, an empty dict is returned. This will make it fall in line with other salt module execution functions, and return a value for which ``bool(value) == False`` when the item being queried (in this case a user) does not exist. For instance, ``pkg.version`` returns an empty string when the package is not installed.\r\n\r\nThis is a delicate change, as all refs will also need to be updated. They currently just expect the keys to exist, and should be using dict.get anyway.\r\n\r\nI'll look into this tonight."
4377,'UtahDave','Bug? Something wrong with the module reg\nI tried to use the module reg to manage windows registry. But maybe there are something wrong with this module, here is an example:\r\nWhen I trying to create a key in the path **"HKEY_LOCAL_MACHINE\\SOFTWARE\\salt"** , I\'d like to use reg module like this:\r\n```\r\nsalt \'*\' reg.create_key \'HKEY_LOCAL_MACHINE\' \'SOFTWARE\\salt\' \'test_key\' \'test_value\'\r\n```\r\nIf you tried this command, you will see that such command will create a new path **"HKEY_LOCAL_MACHINE\\SOFTWARE\\salt\\test_key"**, and the key you want to create will be stored in this path, why? I do not want to create a new path...\r\n\r\nMy salt master version is 0.13.3 and the minion version is 0.14.0.'
4355,'UtahDave','Managing file with template: jinja replaces Windows style new lines for Unix ones.\nAs in this case I am attempting to manage a bat file, the new lines are quite important.\r\n\r\nTemplate:\r\n```\r\n@echo off\r\nLine2\r\nLine3\r\n{{installRoot}}\r\n{{studyName}}\r\nLine6\r\n```\r\n\r\nSLS:\r\n```\r\ntest-fake.bat:\r\n  file:\r\n    - managed\r\n    - name: C:\\software\\current\\test-fake.bat\r\n    - template: jinja\r\n    - source: salt://templates/fake.bat.tpl\r\n    - context:\r\n      studyName: testName\r\n      installRoot: testRoot\r\n```\r\n\r\nResult with no template: jinja line:\r\n![NoTemplate_WithCR](https://f.cloud.github.com/assets/3900979/331525/407b77de-9bf0-11e2-8260-30b35ed1cd4a.png)\r\n\r\n\r\nResult with template: jinja line:\r\n![Template_WithoutCR](https://f.cloud.github.com/assets/3900979/331527/47a574c4-9bf0-11e2-987a-846e68b0dbcc.png)\r\n\r\nHow can we keep the windows new lines when running the template through jinja?'
4354,'basepi',"Jinja error in pillar init.sls is silent\nNo logs on master or anywhere in between caught a bad jinja tag in an init.sls yaml file.  The pillar data on the minion got wiped out with no trace.  The error was only present a later jinja error where the non-existent pillar data was referenced.  Let me know if this is new and hard to reproduce and I'll get my data over.\r\n\r\nin an init.sls\r\n```\r\nsomeval:\r\n    - {{ nonexistent }}\r\n```\r\n\r\nshould be all that's required to wipe minion pillar data on pillar_update"
4351,'terminalmage',"Group membership cached and not updated.\nIn salt/modules/useradd.py around line 436 we have '''def list_groups(name):'''\r\n\r\nFunny story...\r\n\r\nA state can cause group memberships to change, then this cache becomes invalid but is not being invalidated. This is causing some instances where the state is showing that it failed but everything happened as it was supposed to.\r\n\r\nSolution 1: Invalidate the cache every time any operation happens that changes that information.\r\n\r\n    __context__['useradd_getgrall'] = None\r\n\r\nSolution 2: Remove the use of the cache.\r\n\r\nBoth are incredibly easy solutions. The first comes with the the issue that some other modules may alter the groups during execution and not be aware that cache needs to be invalidated. The second comes with recreating the issue that this existed for in the first place (getting group information can take a long time).\r\n\r\nI personally like the first best, but that needs to come with the disclaimer that I'm apparently to blame for this."
4336,'terminalmage','user state with groups always fails on initial run\nIf you had the following state:\r\n\r\n```\r\nmaspwr:\r\n  user.present:\r\n    - groups:\r\n      - sudo\r\n```\r\n\r\nand maspwr already existed with the following groups: `maspwr, foobar`.  This state will fail on its first run.\r\n\r\nThe reason is that `user.getent` (which is called from the user state, https://github.com/saltstack/salt/blob/develop/salt/states/user.py#L54) seems to return a cached version, https://github.com/saltstack/salt/blob/develop/salt/modules/useradd.py#L174.  If you rerun the state the changes to the groups (which were applied successfully) will be picked up and everything passes.\r\n\r\nI believe the fix will be to remove the cached `user.getent` either conditionally or outright.  Let me know if you need more info.'
4321,'terminalmage','osrelease grain does not get created on Solaris systems\nFrom a discussion in IRC. Details to follow.'
4315,'terminalmage','rh_service throwing IndexError during highstate\nJust built from latest git head (0.14.0-227-g6845bc5), and I\'m now seeing an error for every service.running declaration.  This is on CentOS 6.3, using python 2.7.3.\r\n\r\nFor every service I have salt managing, I get output like this when I apply a highstate:\r\n\r\n    ----------\r\n        State: - service\r\n        Name:      httpd\r\n        Function:  running\r\n            Result:    False\r\n            Comment:   An exception occured in this state: Traceback (most recent call last):\r\n      File "/usr/local/lib/python2.7/site-packages/salt/state.py", line 1214, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n      File "/usr/local/lib/python2.7/site-packages/salt/states/service.py", line 248, in running\r\n    if __salt__[\'service.status\'](name, sig):\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/rh_service.py", line 252, in status\r\n        if _service_is_upstart(name):\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/rh_service.py", line 108, in _service_is_upstart\r\n        return name in get_all(limit=\'upstart\')\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/rh_service.py", line 189, in get_all\r\n        return sorted([x for x, y in _services().iteritems()\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/rh_service.py", line 121, in _services\r\n        name = cols[0]\r\n    IndexError: list index out of range\r\n\r\nThe same thing happens when running a highstate via salt-call, or via state.single, too.\r\n\r\nI\'m happy to provide further details if needed.'
4295,'terminalmage',"salt re-executes chkconfig --list and runlevel over and over and over\n```bash\r\n# salt-call --versions-report\r\n           Salt: 0.14.0\r\n         Python: 2.6.6 (r266:84292, Dec  7 2011, 20:48:22)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.9.final\r\n   msgpack-pure: not installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.1.9\r\n```\r\n\r\nThe only interesting thing about the state is that there are 3-4 different files that extend the httpd state file and add watches for specific virtual host config files they add. Standard CentOS test box.\r\n\r\n```bash\r\n2013-03-27 08:47:46,521 [salt.state       ][INFO    ] Executing state service.running for httpd\r\n2013-03-27 08:47:46,521 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,525 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,544 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,549 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,567 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/service httpd status' in directory '/root'\r\n2013-03-27 08:47:46,583 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,587 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,605 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,610 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,629 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,634 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,652 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:46,657 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:46,676 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/service httpd start' in directory '/root'\r\n2013-03-27 08:47:47,294 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:47,300 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:47,317 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:47,323 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:47,340 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:47,346 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:47,363 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:47,369 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:47,389 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/runlevel' in directory '/root'\r\n2013-03-27 08:47:47,394 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig --list' in directory '/root'\r\n2013-03-27 08:47:47,413 [salt.loaded.int.module.cmdmod][INFO    ] Executing command '/sbin/chkconfig httpd on' in directory '/root'\r\n2013-03-27 08:47:47,471 [salt.state       ][INFO    ] {'httpd': True}\r\n```"
4283,'UtahDave','Windows has no grains.virtual\nAny chance of fixing this!'
4266,'UtahDave','salutil.sync_modules not working on windows\nsaltutil.sync_{modules, states, all} not working on windows.\r\n\r\nGuessing it might be a path separator issue.\r\n\r\nUsed to work a little while ago (0.12 definitely, 0.13 not so sure)'
4255,'UtahDave',"MSIExec doesn't default to False\nwin_pkg falls over if it's not declared, it'd be nice to have it default ."
4192,'UtahDave',"State file.recurse on windows produces empty directory w/ success message\nThis is different than #3669, it looks like it has been rebroken since then.\r\n\r\nWhen I try to distribute a directory from a linux master to windows minion (both 0.13.3) I get an empty directory on windows rather than the contents I was expecting.\r\n\r\nI believe the issue was introduced by https://github.com/saltstack/salt/commit/2dac284c667e29cd90dd7c186a8b68f7bf98d355; the addition of `os.path.sep` causes a mismatch such that we skip every file (via the continue on line 1130 of that patch)\r\n\r\nI reach that conclusion because when I run, on the windows box, `salt-call -c c:\\salt\\conf cp.list_master` it lists all the files with forward slashes. This code (state file.recurse), I believe, is running on the windows side where `os.path.sep` will be a backslash.\r\n\r\nNote it also lists them all as relative paths which is contrary to the comment a couple lines down of `# fn_ here is the absolute source path of the file to copy from; `\r\n\r\nBecause it skips every file as being 'irrelevant' it returns success.\r\n\r\ne.g.\r\n```yaml\r\nd:\\foo:\r\n  file.recurse:\r\n    - source: salt://foomodule/foosource\r\n```\r\n\r\n`cp.list_master` will return:\r\n```\r\nfoomodule/foosource/file1\r\nfoomodule/foosource/file2\r\n```\r\nand each will be tested (and fail) against `foomodule/foosource\\`"
4181,'thatch45','Eauth RunnerClient needs refinement\n'
4180,'thatch45',"Wheel client doesn't raise traceback on auth error\n```python\r\n\r\n>>> import salt.wheel\r\n>>> WC = salt.wheel.Wheel(SLC.opts)\r\n>>> WC.master_call(fun='key.list_all', eauth='pam', username='saltdev', password='WRONG')\r\n''\r\n\r\n```\r\n\r\nWe need to raise ``salt.exceptions.EauthAuthenticationError`` here."
4179,'thatch45',"Wheel client doesn't support token auth\n```python\r\n\r\n>>> import salt.wheel\r\n>>> WC = salt.wheel.Wheel(SLC.opts)\r\n>>> WC.master_call(fun='key.list_all', eauth='pam', username='saltdev', password='saltdev')\r\n{'local': ['master.pem', 'master.pub'],\r\n 'minions': ['ms-1', 'ms-2', 'ms-3', 'ms-4'],\r\n 'minions_pre': [],\r\n 'minions_rejected': []}\r\n\r\n>>> WC.master_call(fun='key.list_all', eauth='pam', token='69f5647b52b654c97ade8d3ff88374fdeca66f28')\r\n''\r\n\r\n```"
4174,'terminalmage','OSError being thrown from file.managed\nThe only thing I changed in the output below was my specific file\'s name to "myfile".\r\nI have a stipulation where the bug comes from. I had a long running salt-minion and at one point it was using a state that had "myfile" as a file, but I changed the state and now "myfile" is a folder- when it tries to create the folder to clone the contents from the salt master it fails.\r\n\r\nOn the minion I did: rm -rf /var/cache/salt/* && service salt-minion restart\r\nThen tried to invoke that state from the master it worked.\r\n\r\nIt\'s not a crippling but but kind of a major pain in the ass, if you\'re rapidly developing a distributed service and are using salt to push the changes.\r\n\r\nFunction:  managed\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/state.py", line 1197, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.6/site-packages/salt/states/file.py", line 718, in managed\r\n    **kwargs\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/file.py", line 974, in get_managed\r\n    sfn = __salt__[\'cp.cache_file\'](source, env)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/cp.py", line 218, in cache_file\r\n    result = __context__[\'cp.fileclient\'].cache_file(path, env)\r\n  File "/usr/lib/python2.6/site-packages/salt/fileclient.py", line 114, in cache_file\r\n    return self.get_url(path, \'\', True, env)\r\n  File "/usr/lib/python2.6/site-packages/salt/fileclient.py", line 307, in get_url\r\n    return self.get_file(url, dest, makedirs, env)\r\n  File "/usr/lib/python2.6/site-packages/salt/fileclient.py", line 625, in get_file\r\n    with self._cache_loc(data[\'dest\'], env) as cache_dest:\r\n  File "/usr/lib64/python2.6/contextlib.py", line 16, in __enter__\r\n    return self.gen.next()\r\n  File "/usr/lib/python2.6/site-packages/salt/fileclient.py", line 92, in _cache_loc\r\n    os.makedirs(destdir)\r\n  File "/usr/lib64/python2.6/os.py", line 157, in makedirs\r\n    mkdir(name, mode)\r\nOSError: [Errno 17] File exists: \'/var/cache/salt/minion/files/master/myfile\''
4171,'seanchannel','virtualenv.managed do not work when runas www-data\nInteresting enough continuing to fix the collateral damages cause by the upgrade to 0.13.3-1 I found out that virtualenv  state is also broken if used in conjunction with `runas`: \r\n\r\n```\r\n$ sudo -E salt-call state.single virtualenv.managed /srv/virtualenvs/foo runas=www-data\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/home/ubuntu\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state virtualenv.managed for /srv/virtualenvs/foo\r\n[ERROR   ] No changes made for /srv/virtualenvs/foo\r\nlocal:\r\n----------\r\n    State: - virtualenv\r\n    Name:      /srv/virtualenvs/foo\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1205, in call\r\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\r\n  File "/usr/lib/python2.7/dist-packages/salt/states/virtualenv.py", line 111, in managed\r\n    runas=runas)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/virtualenv.py", line 77, in create\r\n    return __salt__[\'cmd.run_all\'](cmd, runas=runas)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/cmdmod.py", line 389, in run_all\r\n    template=template, rstrip=rstrip, umask=umask)\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/cmdmod.py", line 214, in _run\r\n    raise CommandExecutionError(msg)Interesting enough continuing to fix the collateral damages cause by the upgrade to 0.13.3-1 I found out that virtualenv  state is also broken if used in conjunction with `runas`: \r\n\r\n```\r\n\r\nA work around is to run this code with `group: www-data` instead of `runas: www-data`\r\n\r\nThis issue is very similar to this #4168\r\n'
4168,'seanchannel','cmd.run do not work if user is www-data\n```\r\n$  getent passwd www-data\r\nwww-data:x:33:33:www-data:/var/www:/bin/sh\r\n~\r\n$  sudo -E salt-call state.single cmd.run ls user=www-data\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/home/yml\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state cmd.run for ls\r\n[ERROR   ] No changes made for ls\r\nlocal:\r\n----------\r\n    State: - cmd\r\n    Name:      ls\r\n    Function:  run\r\n        Result:    False\r\n        Comment:   Environment could not be retrieved for User \'www-data\'\r\n        Changes:   \r\n~\r\n$  sudo -E salt-call state.single cmd.run ls group=www-data\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[INFO    ] Executing command \'ps -efH\' in directory \'/home/yml\'\r\n[INFO    ] Loading fresh modules for state activity\r\n[INFO    ] Executing state cmd.run for ls\r\n[INFO    ] Executing command \'ls\' in directory \'/root\'\r\n[INFO    ] {\'pid\': 1550, \'retcode\': 0, \'stderr\': \'\', \'stdout\': \'\'}\r\nlocal:\r\n----------\r\n    State: - cmd\r\n    Name:      ls\r\n    Function:  run\r\n        Result:    True\r\n        Comment:   Command "ls" run\r\n        Changes:   pid: 1550\r\n                   retcode: 0\r\n                   stderr: \r\n                   stdout:\r\n\r\n```\r\n\r\nThe workaround is to replace the `user: www-data` by `group: www-data`'
4163,'UtahDave',"pkg.available_version in win_pkg only returns one version.\n```yaml\r\nfirefox:\r\n  17.0.3:\r\n    installer: 'salt://win/repo/firefox/Firefox Setup 17.0.3esr.exe'\r\n    full_name: Mozilla Firefox 17.0.3 (x86 en-GB)\r\n    locale: en_GB\r\n    install_flags: ' /INI=c:\\firefoxinstaller.ini'\r\n    uninstaller: '%ProgramFiles%/Mozilla Firefox/uninstall/helper.exe'\r\n    uninstall_flags: ' /S'\r\n\r\n  10.0.12:\r\n    installer: 'salt://win/repo/firefox/Firefox Setup 10.0.12esr.exe'\r\n    full_name: Mozilla Firefox 10.0.12 (x86 en-GB)\r\n    locale: en_GB\r\n    install_flags: ' /INI=c:\\firefoxinstaller.ini'\r\n    uninstaller: '%ProgramFiles%/Mozilla Firefox/uninstall/helper.exe'\r\n    uninstall_flags: ' /S'\r\n```\r\n\r\nGenrepo shows both are there, refresh_db succeeds however only Mozilla Firefox 17.0.3 (x86 en-GB) shows up."
4153,'terminalmage',"[REQ] Ensure that ssh_known_hosts doesn't require ~/.ssh/known_hosts to be pre-existing\nSalt is often used to clone repositories with git. This often requires an SSH known_hosts file to be pre-existing. Using ssh_known_hosts on an untouched host results in error due to a missing known_hosts file. This feature request is for salt to automatically create that file.\r\n\r\nAdditionally, and this should be given its own feature request, the git salt state should ensure that ssh_known_hosts can be bypassed."
4140,'UtahDave',"Windows Repo isn't using fullname\nTrying to deploy Firefox 17.0.3 ESR with\r\n\r\nfirefox:\r\n  17.0.3:\r\n    installer: 'salt://win/repo/firefox/Firefox Setup 17.0.3esr.exe'\r\n    full_name: Mozilla Firefox 17.0.3 (x86 en-GB)\r\n    locale: en_GB\r\n    install_flags: ' /INI=c:\\firefoxinstaller.ini'\r\n    uninstaller: '%ProgramFiles%/Mozilla Firefox/uninstall/helper.exe'\r\n    uninstall_flags: ' /S'\r\n\r\nfirefox:\r\n  pkg:\r\n    - installed \r\n\r\nReturns\r\n\r\n    State: - pkg\r\n    Name:      firefox\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following package(s) failed to install/update: firefox.\r\n        Changes:   Mozilla Firefox 17.0.3 (x86 en-GB): {'new': '17.0.3', 'old': ''}\r\n\r\nSurely it should match with the fullname?"
4135,'thatch45',"Error in merging responses from Syndic Master's minion and minions controlled by Syndic Master\nHi,\r\n\r\nI've been working on a Master - Syndic Master - Minion setup and require the Syndic Master to also run a Minion daemon (e.g., to copy files from the Master Master).\r\n\r\nHowever, while setting up a Syndic Master that also runs a Minion daemon, I noticed a potential bug. When issuing a salt '*' cmd.run 'uname -a' call on the Master Master, only the response from the Syndic Master's Minion is shown but not the responses from the Minions controlled by the Syndic Master -- but the command is actually executed on all of the Minions controlled by the Syndic Master (and their replies are received by the Master Master) according to the debug logs. If I target only the Syndic Master's Minion or only the Minions controlled by the Syndic Master, the output is correct. There seems to be a bug when merging both responses in case of a salt '*' call ...  It's a minor issue but a likely source for confusion ...\r\n\r\nSome more details:\r\n\r\nSalt version: 0.13.2 (installed via bootstrap-salt.sh, version 1.5.1)\r\n\r\nSetup:\r\n    Node 01 (node01.internal): Salt Master Master;\r\n    Node 02 (node02.internal): Salt Master, Salt Syndic, Salt Minion\r\n    Nodes 10-15 (node10|11|...|15.internal): Salt Minions\r\n\r\nExample:\r\n[node01]$ sudo salt 'node02*' cmd.run 'uname -n'\r\nnode02.internal:\r\n    node02\r\n\r\n[node01]$ sudo salt 'node1*' cmd.run 'uname -n'\r\nnode10.internal:\r\n    node10\r\nnode11.internal:\r\n    node11\r\nnode12.internal:\r\n    node12\r\n...\r\n\r\n[node01]$ sudo salt '*' cmd.run 'uname -n'\r\nnode02.internal:\r\n    node02\r\n\r\nNo responses from nodes 10 to note 15 displayed but they appear in the debug log:\r\n\r\n[INFO    ] AES payload received with command _return\r\n[INFO    ] Got return from node02.internal for job 20130318053806399843\r\n[INFO    ] AES payload received with command _syndic_return\r\n[INFO    ] Got return from node10.internal for job 20130318053806399843\r\n...\r\n"
4122,'UtahDave',"Default options on windows minion not set proper (salt - 0.13.2)\nWhen installing the minion on a windows machine (from the windows binaries) most of the default paths set in config.py - DEFAULT_MINION_OPTS variable points to unix paths. \r\n\r\nExample:\r\n\r\n    'pki_dir': '/etc/salt/pki/minion'\r\n    'cachedir': '/var/cache/salt/minion',\r\n    'conf_file': '/etc/salt/minion',\r\n\r\nWhen running salt-minion from a cmd window it first complains that it cannot find any configuration files because it tries to look for them in c:\\etc\\salt\\minion\\ but the conf file is acctually located in c:\\salt\\conf\\\r\n\r\nTo manually fix the conf file issue for example i need to input the config file as an input argument\r\n\r\n    salt-minion.exe --config-dir c:\\salt\\conf\r\n\r\nTo fix all other paths they have to be manually set in the minion config file.\r\n\r\n\r\nSecond issue is that in the minion config file all paths should be rendered to windows paths when the minion is installed on windows via the windows binaries.\r\n\r\nCurrently i get by default\r\n    \r\n    'conf_file': '/etc/salt/minion',\r\n\r\n"
4109,'terminalmage',"grains.items vs. pillar.data\nI'm still a n00b, mind you, but I always have to try `pillar.items` or `grains.data` before correctly choosing `pillar.data` and `grains.items`.\r\n\r\nAny thoughts on normalizing the namespace so that there is parity between these and a user can make an intuitive leap between the two?  I would vote for `pillar.items` and `grains.items` since the term `items` feels friendlier."
4107,'terminalmage',"Regression in pkg.list_upgrades\nHi there, \r\nThere appears to have been a regression in the pkg.list_upgrades function between version 0.13.1 and 0.13.2 on Ubuntu 10.04LTS. \r\n\r\nThe 0.13.2 version does not display any output:\r\n\r\n```bash\r\n[root@web1.e.ord ~]# salt-call -l debug apt.list_upgrades refresh=False\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[DEBUG   ] loading grain in ['/var/cache/salt/minion/extmods/grains', '/usr/lib/pymodules/python2.6/salt/grains']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/grains, it is not a directory\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] loading module in ['/var/cache/salt/minion/extmods/modules', '/usr/lib/pymodules/python2.6/salt/modules']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/modules, it is not a directory\r\n[DEBUG   ] Loaded groupadd as virtual group\r\n[DEBUG   ] Loaded linux_sysctl as virtual sysctl\r\n[DEBUG   ] Loaded parted as virtual partition\r\n[DEBUG   ] Loaded apt as virtual pkg\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded upstart as virtual service\r\n[DEBUG   ] Loaded useradd as virtual user\r\n[DEBUG   ] Loaded dpkg as virtual lowpkg\r\n[DEBUG   ] Loaded grub_legacy as virtual grub\r\n[DEBUG   ] Loaded debconfmod as virtual debconf\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] loading returner in ['/var/cache/salt/minion/extmods/returners', '/usr/lib/pymodules/python2.6/salt/returners']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/returners, it is not a directory\r\n[DEBUG   ] Loaded syslog_return as virtual syslog\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] loading states in ['/var/cache/salt/minion/extmods/states', '/usr/lib/pymodules/python2.6/salt/states']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/states, it is not a directory\r\n[DEBUG   ] Loaded debconfmod as virtual debconf\r\n[DEBUG   ] loading render in ['/var/cache/salt/minion/extmods/renderers', '/usr/lib/pymodules/python2.6/salt/renderers']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/renderers, it is not a directory\r\nFunction apt.list_upgrades is not available\r\n[root@web1.e.ord ~]# salt-call -l debug pkg.list_upgrades\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[DEBUG   ] loading grain in ['/var/cache/salt/minion/extmods/grains', '/usr/lib/pymodules/python2.6/salt/grains']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/grains, it is not a directory\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[DEBUG   ] loading module in ['/var/cache/salt/minion/extmods/modules', '/usr/lib/pymodules/python2.6/salt/modules']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/modules, it is not a directory\r\n[DEBUG   ] Loaded groupadd as virtual group\r\n[DEBUG   ] Loaded linux_sysctl as virtual sysctl\r\n[DEBUG   ] Loaded parted as virtual partition\r\n[DEBUG   ] Loaded apt as virtual pkg\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded upstart as virtual service\r\n[DEBUG   ] Loaded useradd as virtual user\r\n[DEBUG   ] Loaded dpkg as virtual lowpkg\r\n[DEBUG   ] Loaded grub_legacy as virtual grub\r\n[DEBUG   ] Loaded debconfmod as virtual debconf\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] loading returner in ['/var/cache/salt/minion/extmods/returners', '/usr/lib/pymodules/python2.6/salt/returners']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/returners, it is not a directory\r\n[DEBUG   ] Loaded syslog_return as virtual syslog\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] loading states in ['/var/cache/salt/minion/extmods/states', '/usr/lib/pymodules/python2.6/salt/states']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/states, it is not a directory\r\n[DEBUG   ] Loaded debconfmod as virtual debconf\r\n[DEBUG   ] loading render in ['/var/cache/salt/minion/extmods/renderers', '/usr/lib/pymodules/python2.6/salt/renderers']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/renderers, it is not a directory\r\n[INFO    ] Executing command 'apt-get -q update' in directory '/root'\r\n[DEBUG   ] stdout: Hit http://198.101.148.142 lucid Release.gpg\r\nHit http://198.101.148.142 lucid Release\r\nHit http://apt.newrelic.com newrelic Release.gpg\r\nHit http://apt.newrelic.com newrelic Release\r\nIgn http://198.101.148.142 lucid/main Packages\r\nIgn http://198.101.148.142 lucid/main Packages\r\nHit http://198.101.148.142 lucid/main Packages\r\nHit http://apt.newrelic.com newrelic/non-free Packages\r\nHit http://www.serverdensity.com all Release.gpg\r\nHit http://security.ubuntu.com lucid-security Release.gpg\r\nHit http://security.ubuntu.com lucid-security Release\r\nGet:1 http://www.serverdensity.com all Release [1637B]\r\nHit http://security.ubuntu.com lucid-security/main Packages\r\nHit http://security.ubuntu.com lucid-security/restricted Packages\r\nHit http://security.ubuntu.com lucid-security/universe Packages\r\nHit http://security.ubuntu.com lucid-security/main Sources\r\nHit http://security.ubuntu.com lucid-security/restricted Sources\r\nHit http://security.ubuntu.com lucid-security/universe Sources\r\nIgn http://www.serverdensity.com all/main Packages\r\nHit http://ppa.launchpad.net lucid Release.gpg\r\nHit http://archive.ubuntu.com lucid Release.gpg\r\nHit http://archive.ubuntu.com lucid-updates Release.gpg\r\nIgn http://www.serverdensity.com all/main Packages\r\nHit http://www.serverdensity.com all/main Packages\r\nHit http://ppa.launchpad.net lucid Release\r\nHit http://archive.ubuntu.com lucid Release\r\nHit http://archive.ubuntu.com lucid-updates Release\r\nHit http://ppa.launchpad.net lucid/main Packages\r\nHit http://archive.ubuntu.com lucid/main Packages\r\nHit http://archive.ubuntu.com lucid/restricted Packages\r\nHit http://archive.ubuntu.com lucid/universe Packages\r\nHit http://archive.ubuntu.com lucid/main Sources\r\nHit http://archive.ubuntu.com lucid/restricted Sources\r\nHit http://archive.ubuntu.com lucid/universe Sources\r\nHit http://archive.ubuntu.com lucid-updates/main Packages\r\nHit http://archive.ubuntu.com lucid-updates/restricted Packages\r\nHit http://archive.ubuntu.com lucid-updates/universe Packages\r\nHit http://archive.ubuntu.com lucid-updates/main Sources\r\nHit http://archive.ubuntu.com lucid-updates/restricted Sources\r\nHit http://archive.ubuntu.com lucid-updates/universe Sources\r\nFetched 1637B in 0s (2890B/s)\r\nReading package lists...\r\n[INFO    ] Executing command 'apt-get --just-print dist-upgrade' in directory '/root'\r\n[DEBUG   ] stdout: Reading package lists...\r\nBuilding dependency tree...\r\nReading state information...\r\nThe following NEW packages will be installed:\r\n  hiera libjson-ruby libjson-ruby1.8 linux-image-2.6.32-45-virtual ruby\r\nThe following packages will be upgraded:\r\n  facter gnupg gnupg-curl gpgv libc-bin libc-dev-bin libc6 libc6-dev libcap2\r\n  libfreetype6 libfreetype6-dev libglib2.0-0 libgnutls26 libperl5.10 libpq-dev\r\n  libpq5 libssl-dev libssl0.9.8 libxml2 linux-image-virtual linux-libc-dev\r\n  linux-virtual lsb-base lsb-release nginx-common nginx-extras nscd openssl\r\n  perl perl-base perl-modules puppet puppet-common sudo unattended-upgrades\r\n  vim vim-common vim-nox vim-runtime vim-tiny\r\n40 upgraded, 5 newly installed, 0 to remove and 0 not upgraded.\r\nInst perl-modules [5.10.1-8ubuntu2.1] (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nInst libc-dev-bin [2.11.1-0ubuntu7.11] (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates) [libc6-dev ]\r\nInst libc6-dev [2.11.1-0ubuntu7.11] (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates) []\r\nInst libc-bin [2.11.1-0ubuntu7.11] (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates) [libc6 ]\r\nConf libc-bin (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates) [libc6 ]\r\nInst libc6 [2.11.1-0ubuntu7.11] (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nConf libc6 (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nInst linux-libc-dev [2.6.32-44.98] (2.6.32-45.104 Ubuntu:10.04/lucid-updates)\r\nInst perl [5.10.1-8ubuntu2.1] (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates) []\r\nInst libperl5.10 [5.10.1-8ubuntu2.1] (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates) []\r\nInst perl-base [5.10.1-8ubuntu2.1] (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nConf perl-base (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nInst linux-image-2.6.32-45-virtual (2.6.32-45.104 Ubuntu:10.04/lucid-updates)\r\nInst libssl-dev [0.9.8k-7ubuntu8.13] (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates) []\r\nInst libssl0.9.8 [0.9.8k-7ubuntu8.13] (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates)\r\nConf libssl0.9.8 (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates)\r\nInst lsb-base [4.0-0ubuntu8] (4.0-0ubuntu8.1 Ubuntu:10.04/lucid-updates)\r\nConf lsb-base (4.0-0ubuntu8.1 Ubuntu:10.04/lucid-updates)\r\nInst gpgv [1.4.10-2ubuntu1.1] (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nInst gnupg [1.4.10-2ubuntu1.1] (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nInst gnupg-curl [1.4.10-2ubuntu1.1] (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nInst libcap2 [1:2.17-2ubuntu1] (1:2.17-2ubuntu1.1 Ubuntu:10.04/lucid-updates)\r\nInst libgnutls26 [2.8.5-2ubuntu0.2] (2.8.5-2ubuntu0.3 Ubuntu:10.04/lucid-updates)\r\nInst lsb-release [4.0-0ubuntu8] (4.0-0ubuntu8.1 Ubuntu:10.04/lucid-updates)\r\nInst sudo [1.7.2p1-1ubuntu5.4] (1.7.2p1-1ubuntu5.6 Ubuntu:10.04/lucid-updates)\r\nInst vim [2:7.2.330-1ubuntu3] (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates) []\r\nInst vim-nox [2:7.2.330-1ubuntu3] (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates) []\r\nInst vim-runtime [2:7.2.330-1ubuntu3] (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates) []\r\nInst vim-tiny [2:7.2.330-1ubuntu3] (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates) []\r\nInst vim-common [2:7.2.330-1ubuntu3] (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nInst libglib2.0-0 [2.24.1-0ubuntu1] (2.24.1-0ubuntu2 Ubuntu:10.04/lucid-updates)\r\nInst libxml2 [2.7.6.dfsg-1ubuntu1.6] (2.7.6.dfsg-1ubuntu1.7 Ubuntu:10.04/lucid-updates)\r\nInst ruby (4.2 Ubuntu:10.04/lucid)\r\nInst facter [1.6.10-1puppetlabs1] (1.6.17-1puppetlabs1 Cashstar:198.101.148.142)\r\nInst libfreetype6-dev [2.3.11-1ubuntu2.6] (2.3.11-1ubuntu2.7 Ubuntu:10.04/lucid-updates) []\r\nInst libfreetype6 [2.3.11-1ubuntu2.6] (2.3.11-1ubuntu2.7 Ubuntu:10.04/lucid-updates)\r\nInst libjson-ruby1.8 (1.1.9-1 Ubuntu:10.04/lucid)\r\nInst libjson-ruby (1.1.9-1 Ubuntu:10.04/lucid)\r\nInst libpq-dev [8.4.14-0ubuntu10.04] (8.4.16-0ubuntu10.04 Ubuntu:10.04/lucid-updates) []\r\nInst libpq5 [8.4.14-0ubuntu10.04] (8.4.16-0ubuntu10.04 Ubuntu:10.04/lucid-updates)\r\nInst linux-virtual [2.6.32.31.37] (2.6.32.45.52 Ubuntu:10.04/lucid-updates) []\r\nInst linux-image-virtual [2.6.32.31.37] (2.6.32.45.52 Ubuntu:10.04/lucid-updates)\r\nInst nginx-common [1.2.3-0ubuntu0ppa3~lucid] (1.2.4-2ubuntu0ppa4~lucid Cashstar:198.101.148.142) [nginx-extras ]\r\nInst nginx-extras [1.2.3-0ubuntu0ppa3~lucid] (1.2.4-2ubuntu0ppa4~lucid Cashstar:198.101.148.142)\r\nInst nscd [2.11.1-0ubuntu7.11] (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nInst openssl [0.9.8k-7ubuntu8.13] (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates)\r\nInst hiera (1.1.2-1puppetlabs1 Cashstar:198.101.148.142)\r\nInst puppet [2.7.19-1puppetlabs2] (3.0.2-1puppetlabs1 Cashstar:198.101.148.142) []\r\nInst puppet-common [2.7.19-1puppetlabs2] (3.0.2-1puppetlabs1 Cashstar:198.101.148.142)\r\nInst unattended-upgrades [0.55ubuntu7] (0.55ubuntu8 Ubuntu:10.04/lucid-updates)\r\nConf perl (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nConf perl-modules (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nConf libc-dev-bin (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nConf linux-libc-dev (2.6.32-45.104 Ubuntu:10.04/lucid-updates)\r\nConf libc6-dev (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nConf libperl5.10 (5.10.1-8ubuntu2.2 Ubuntu:10.04/lucid-updates)\r\nConf linux-image-2.6.32-45-virtual (2.6.32-45.104 Ubuntu:10.04/lucid-updates)\r\nConf libssl-dev (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates)\r\nConf gpgv (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nConf gnupg (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nConf gnupg-curl (1.4.10-2ubuntu1.2 Ubuntu:10.04/lucid-updates)\r\nConf libcap2 (1:2.17-2ubuntu1.1 Ubuntu:10.04/lucid-updates)\r\nConf libgnutls26 (2.8.5-2ubuntu0.3 Ubuntu:10.04/lucid-updates)\r\nConf lsb-release (4.0-0ubuntu8.1 Ubuntu:10.04/lucid-updates)\r\nConf sudo (1.7.2p1-1ubuntu5.6 Ubuntu:10.04/lucid-updates)\r\nConf vim-common (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nConf vim-runtime (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nConf vim (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nConf vim-nox (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nConf vim-tiny (2:7.2.330-1ubuntu3.1 Ubuntu:10.04/lucid-updates)\r\nConf libglib2.0-0 (2.24.1-0ubuntu2 Ubuntu:10.04/lucid-updates)\r\nConf libxml2 (2.7.6.dfsg-1ubuntu1.7 Ubuntu:10.04/lucid-updates)\r\nConf ruby (4.2 Ubuntu:10.04/lucid)\r\nConf facter (1.6.17-1puppetlabs1 Cashstar:198.101.148.142)\r\nConf libfreetype6 (2.3.11-1ubuntu2.7 Ubuntu:10.04/lucid-updates)\r\nConf libfreetype6-dev (2.3.11-1ubuntu2.7 Ubuntu:10.04/lucid-updates)\r\nConf libjson-ruby1.8 (1.1.9-1 Ubuntu:10.04/lucid)\r\nConf libjson-ruby (1.1.9-1 Ubuntu:10.04/lucid)\r\nConf libpq5 (8.4.16-0ubuntu10.04 Ubuntu:10.04/lucid-updates)\r\nConf libpq-dev (8.4.16-0ubuntu10.04 Ubuntu:10.04/lucid-updates)\r\nConf linux-image-virtual (2.6.32.45.52 Ubuntu:10.04/lucid-updates)\r\nConf linux-virtual (2.6.32.45.52 Ubuntu:10.04/lucid-updates)\r\nConf nginx-common (1.2.4-2ubuntu0ppa4~lucid Cashstar:198.101.148.142)\r\nConf nginx-extras (1.2.4-2ubuntu0ppa4~lucid Cashstar:198.101.148.142)\r\nConf nscd (2.11.1-0ubuntu7.12 Ubuntu:10.04/lucid-updates)\r\nConf openssl (0.9.8k-7ubuntu8.14 Ubuntu:10.04/lucid-updates)\r\nConf hiera (1.1.2-1puppetlabs1 Cashstar:198.101.148.142)\r\nConf puppet-common (3.0.2-1puppetlabs1 Cashstar:198.101.148.142)\r\nConf puppet (3.0.2-1puppetlabs1 Cashstar:198.101.148.142)\r\nConf unattended-upgrades (0.55ubuntu8 Ubuntu:10.04/lucid-updates)\r\n[DEBUG   ] loading output in ['/var/cache/salt/minion/extmods/output', '/usr/lib/pymodules/python2.6/salt/output']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/output, it is not a directory\r\n[DEBUG   ] Loaded json_out as virtual json\r\n[DEBUG   ] Loaded yaml_out as virtual yaml\r\n[DEBUG   ] Loaded pprint_out as virtual pprint\r\n\r\n[root@web1.e.ord ~]#\r\n```"
4094,'terminalmage',"Update salt-key man page (for -l | --list argument)\nsalt-key 0.13.1 man page states:\r\n\r\n       -l, --list\r\n              List the unaccepted minion public keys.\r\n\r\nThis is out-dated. -l, --list now supports 'accepted', 'unaccepted', and 'rejected' as values. E.g., salt-key --list=accepted or salt-key -l accepted. The current wording is somewhat misleading.\r\n\r\nCheers,\r\n\r\nMarkus"
4085,'s0undt3ch',"v0.13.2 self identifies as 0.13.1 when installing\nSetup.py is identifying the 0.13 branch as 0.13.1. I've checked out the v0.13.2 tag.\r\nI ran ```git fetch --tags upstream```\r\n\r\nI first saw this when building the Salt Windows installers, but I got the same issue on Ubuntu 12.04. Here's the output from Ubuntu 12.04:\r\n\r\n```\r\n[boucha@dasalt salt ((v0.13.2))]$ git describe\r\nv0.13.2\r\n```\r\n\r\nThen when I install salt with setup.py I get the following:\r\n\r\n```\r\n[boucha@dasalt salt ((v0.13.2))]$ sudo python setup.py install --force\r\nCannot load esky build target\r\nPlease install the 'esky' and the 'bbfreeze' modules to enable this functionality\r\n/home/boucha/salt/salt/version.py:99: UserWarning: In order to get the proper salt version with the git hash you need to update salt's local git tags. Something like: 'git fetch --tags' or 'git fetch --tags upstream' if you followed salt's contribute documentation. The version string WILL NOT include the git hash.\r\n  __version__, __version_info__ = __get_version(__version__, __version_info__)\r\nsetup.py:99: UserWarning: In order to get the proper salt version with the git hash you need to update salt's local git tags. Something like: 'git fetch --tags' or 'git fetch --tags upstream' if you followed salt's contribute documentation. The version string WILL NOT include the git hash.\r\n  \r\n0.13.1\r\nrunning install\r\nrunning build\r\nrunning build_py\r\nrunning build_scripts\r\nrunning install_lib\r\ncopying build/lib.linux-x86_64-2.7/salt/auth/__init__.py -> /usr/local/lib/python2.7/dist-packages/salt/auth\r\ncopying build/lib.linux-x86_64-2.7/salt/auth/pam.py -> /usr/local/lib/python2.7/dist-packages/salt/auth\r\n<-- snip -->\r\ncopying doc/man/salt-run.1 -> /usr/local/share/man/man1\r\ncopying doc/man/salt-minion.1 -> /usr/local/share/man/man1\r\ncopying doc/man/salt.7 -> /usr/local/share/man/man7\r\nrunning install_egg_info\r\nWriting /usr/local/lib/python2.7/dist-packages/salt-0.13.1.egg-info\r\n```\r\n\r\n"
4063,'basepi',"file.sed's way of checking for success needs rethinking\nAs shown by issue #3342, the current way ```file.sed``` checks whether the operation was successful leads to problems. It checks whether it can find the string/regex specified by ```after``` in the line operated on. However, if this regex contains back-references, this will fail, because back-references only work in substitutions, not in matches.\r\n\r\nIt is probably better to return true if the substitution didn't return an error. As the operations need to be idempotent, it is probably not unusual that ```file.sed``` doesn't change anything on repeated invocation, so I guess this shouldn't be an error."
4062,'terminalmage',"ArchLinux - keyboard.set_sys doesn't work anymore\nkeyboard.set_sys doesn't work anymore on ArchLinux. This is because it tries to set the keyboard-layout in rc.conf which isn't present.\r\n\r\nSolution:\r\nlocalectl set-keymap [keymap]\r\n"
4059,'terminalmage',"Changing a user's shell errors out even though the change was successful.\nWhen attempting to change the shell with `user.present`, the shell successfully changes, but salt thinks it wasn't successful.  I tried running, the command manually and ensured the exit status is 0.  Without knowing the code too well, I have a hunch `user._changes()` doesn't reflect the change was made on the filesystem.\r\n\r\nMy state is pretty simple and initially the shell was `/bin/sh`:\r\n\r\n```\r\napp:\r\n  user.present:\r\n    - home: /app\r\n    - shell: /bin/bash\r\n```\r\n\r\nThe output is below:\r\n\r\n```\r\nubuntu@django-someapp-1234:~$ salt --versions-report\r\n           Salt: 0.13.0-672-gaeff594\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: not installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\nubuntu@django-someapp-1234:~$ sudo usermod -s /bin/sh app\r\nubuntu@django-someapp-1234:~$ sudo salt-call state.highstate -l debug\r\n[...]\r\n[INFO    ] Executing state user.present for app\r\n[INFO    ] Executing command 'usermod -s /bin/bash app' in directory '/home/ubuntu'\r\n[DEBUG   ] output:\r\n[ERROR   ] {'shell': '/bin/bash'}\r\n[...]\r\nlocal:\r\n----------\r\n    State: - user\r\n    Name:      app\r\n    Function:  present\r\n        Result:    False\r\n        Comment:   These values could not be changed: {'shell': '/bin/bash'}\r\n        Changes:   shell: /bin/bash\r\n[...]\r\nubuntu@django-someapp-1234:~$ grep app /etc/passwd\r\napp:x:1001:1001::/app:/bin/bash\r\nubuntu@django-someapp-1234:~$ sudo salt-call state.highstate -l debug\r\n[...]\r\n[INFO    ] Executing state user.present for app\r\n[INFO    ] No changes made for app\r\n[...]\r\nlocal:\r\n----------\r\n    State: - user\r\n    Name:      app\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   User app is present and up to date\r\n        Changes:\r\n```"
4058,'basepi',"File.append state module caching does not properly append data when looping\nDue to the nature of file.append, salt will only append the last iteration of a loop due to the cache mechanism with the salt 'cp' fileserver. Below is a sample from one of my SLS files:\r\n\r\n{% for arg, data in pillar['credentials'][repo].iteritems() %}\r\n/var/www/{{ repo }}/htdocs/file:\r\n  file.append:\r\n    - text: {{ arg }}='{{ data }}'\r\n{% endfor}"
4029,'seanchannel',"pkgrepo: ppa 'dist' name superfluous\nthis one is for @ahammond \r\nit looks like the 'dist' option may be unnecessary since it is part of the 'name' anyway.  I put something different for 'dist' and it was just ignored. \r\n\r\nfor anyone else... well it was already half-merged by *somebody* and it does work. :)"
4013,'thatch45','Module sync requires a "fake" call before the code change takes effect.\nWhen calling `salt \'*\' saltutil.sync_modules` to update module code, the new code will take effect two salt calls later.  For example, take a module that defines `hello`:\r\n\r\n```\r\ndef hello(name):\r\n    return name\r\n```\r\n\r\nand it\'s changed to:\r\n\r\n```\r\ndef hello(name):\r\n    return \'hello, {}!\'.format(name)\r\n```\r\n\r\nNow, running the function twice shows the issue:\r\n\r\n```\r\nlocal@bm-allinone:~/salt-bootstrap$ sudo salt \'*\' saltutil.sync_modules; sudo salt \'*\' baremetal.hello \'Utah\'; sudo salt \'*\' baremetal.hello \'Utah\'\r\nbm-django:\r\n    - modules.baremetal\r\nbm-django:\r\n    Utah\r\nbm-django:\r\n    hello, Utah!\r\nlocal@bm-allinone:~/salt-bootstrap$\r\n```\r\n\r\nI also tried to `sleep 10` between the sync and calling the function without success.  Calling a function, and then the desired function second works as expected.\r\n\r\n**Note:** the first salt call does not have to be function, it can be `test.ping` so long as the minion does something.'
3990,'seanchannel','state cmd.script does not honor \'cwd\', only uses /tmp\nI swear we\'ve been though this, but cmd.script seems to only use /tmp, even though the docs say it uses /root by default, and even when I specify a cwd. In this case, /tmp is mounted with \'noexec\', hence the permission denied error:\r\n\r\n  Sleeper1:\r\n    cmd.script:\r\n      - source: salt://sleeper1.sh\r\n      - cwd: /root\r\n\r\noutput:\r\n\r\n  minion:\r\n  ----------\r\n      State: - cmd\r\n      Name:      Sleeper1\r\n      Function:  script\r\n          Result:    False\r\n          Comment:   Command "Sleeper1" run\r\n          Changes:   pid: 21840\r\n                     retcode: 126\r\n                     stderr: /bin/sh: 1: /tmp/tmp2hKn69: Permission denied\r\n                     stdout: \r\n           '
3985,'thatch45','Cobbler API 405 error when calling `state.highstate`\nOn the develop branch as of ~12 hours ago, I am getting the error below when calling state.highstate.  Per conversation with @UtahDave on IRC, this is on a default bootstrap install; I have not attempted to configure cobbler.  Additionally, this is only happening when calling `state.highstate`; it does not happen when calling methods like `test.ping` or `cmd.run \'echo hi\'`.\r\n\r\nThanks!\r\n\r\n```\r\nlocal@bm-allinone:~$ sudo salt \'bm-console\' state.highstate\r\n2013-03-07 19:49:49,613 [salt.loaded.int.module.debconfmod][WARNING ] Package debconf-utils is not installed.\r\n2013-03-07 19:49:49,741 [salt.loaded.int.top.cobbler][ERROR   ] Could not connect to cobbler.\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/tops/cobbler.py", line 49, in top\r\n    data = server.get_blended_data(None, minion_id)\r\n  File "/usr/lib/python2.7/xmlrpclib.py", line 1224, in __call__\r\n    return self.__send(self.__name, args)\r\n  File "/usr/lib/python2.7/xmlrpclib.py", line 1578, in __request\r\n    verbose=self.__verbose\r\n  File "/usr/lib/python2.7/xmlrpclib.py", line 1264, in request\r\n    return self.single_request(host, handler, request_body, verbose)\r\n  File "/usr/lib/python2.7/xmlrpclib.py", line 1312, in single_request\r\n    response.msg,\r\nProtocolError: <ProtocolError for localhost/cobbler_api: 405 Not Allowed>\r\n```\r\n'
3984,'thatch45',"Minions fail to purge custom modules\nsalt-0.13.1\r\n\r\nI've got a salt-master and a salt-minion up and running (two separate boxes so that I can see the\r\nseparation between the two).\r\n\r\nOn the master I created a foo.py module per http://docs.saltstack.com/ref/modules/index.html\r\n\r\nmaster:/srv/salt/_modules/foo.py:\r\n\r\n```python\r\ndef foo():\r\n\treturn True\r\n```\r\nI then sync it and run it:\r\n\r\n```bash\r\n$ salt minion saltutil.sync_modules\r\nminion:\r\n   - modules.foo\r\n\r\n$ salt minion foo.foo\r\nminion:\r\n   True\r\n```\r\n\r\nThen I remove it from the master and resync:\r\n\r\n```bash\r\n$ rm -f /srv/salt/_modules/foo.py\r\n$ salt minion saltutil.sync_modules\r\n```\r\n\r\nThe module still runs (expected behavior is that it would be gone):\r\n```bash\r\n$ salt minion foo.foo\r\nminion:\r\n   True\r\n```\r\n\r\nand I see that the module is still on the minion:\r\n\r\n```bash\r\n$ cat /var/cache/salt/minion/files/base/_modules/foo.py\r\ndef foo():\r\n\treturn True\r\n```"
3953,'basepi','salt-call state.sls nonexistant.state returns no errors\nBut it should say that not state with said name exists and throw a friendly error message.'
3949,'thatch45','Add directory for master-side modules & files\nAdd a directory for master-only modules such as runners, wheel modules, netapi modules, and salt-ui static files.'
3947,'UtahDave','Salt-Minion on Windows 2003 Server continually crashes and restarts\nI am trying to run the minion service (13.1) on a Windows Server 2003 32-Bit machine, installed using the Windows installer.  When I try to start the service, it continually crashes and restarts.  Debugging it, this is what I see:\r\n\r\n\r\n[DEBUG   ] Loaded win_service as virtual service\r\n[DEBUG   ] Loaded win_useradd as virtual user\r\n[DEBUG   ] loading returner in [\'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\extmods\\\\returners\', \'c:\\\\salt\\salt-0.13.1-1-g6cbe0dc.win32\\\\salt-0.13.1_1_g6cbe0dc-py2.7.egg\\\\salt\\\\returners\']\r\n[DEBUG   ] Skipping c:\\salt\\var\\cache\\salt\\minion\\extmods\\returners, it is not a directory\r\n[DEBUG   ] Failed to import returner syslog_return, this is most likely NOT a problem: No module named syslog\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] Created pidfile: c:\\salt\\var\\run\\salt-minion.pid\r\nTraceback (most recent call last);\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chaniload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-minion__.py", line 17, in <module>\r\n  File "salt/scripts.py", line 29, in salt_minion\r\n  File "salt/__init__.py", line 189, in start\r\n  File "salt/minion.py", line 650, in tune_in\r\n  File "getpass.py", line 157, in getuser\r\nImportError: No module named pwd\r\n'
3944,'thatch45',"Make file creation in file.managed state optional.\nWhen using file.managed to tune permissions (no source option supplied), it will automatically create a file if one doesn't exist already using a touch. This isn't always necessary or desirable and it'd be useful to be able to skip this behavior somehow. \r\n\r\nUse Case: Security audit baselining. - You want to set up a nominal state (owner/permissions) for a number of files on all minions and control the behavior of file.managed in cases where the file doesn't exist. Specifically, you want to comply with section 2.2.3.4 of this: http://www.nsa.gov/ia/_files/os/redhat/rhel5-guide-i731.pdf \r\nA stock RHEL/CentOS install may or may not have all the software listed listed in the table on pages 32/33 installed. Using file.managed in it's current state to manage these files would result in many empty files suddenly being created in your system bin/sbin directories which isn't desirable in most cases.\r\n\r\nRather than setting this per-file on a per-minion basis, you'd want to set all the distro-provided binaries with base permissions globally whether they exist or not. In the case of non-existing files, the option to warn/create/skip would be useful here.\r\n"
3900,'seanchannel',"stop redirecting output to /dev/null in upstart\nPresently the upstart scripts redirect minion & master console output to /dev/null.  Removing this output redirection allows the output to be absorbed by rsyslog and appear in system logs without needing the user to configure anything (or just on the console, depending on how you have your system configured). \r\n\r\n*Your thoughts on this subject may vary*, but I like the way it works without redirection. If there is agreement here I might ask @dlindquist to make that change just so we don't stomp right over his other fresh commit. :)"
3877,'UtahDave',"Windows xp host (minion) , ubuntu vm (master) \nI am trying to setup the salt-master on ubuntu vm, running the the minion on a xp machine i am running version 0.13.1 on both systems. \r\n\r\nI can run salt-key -L\r\nI can see my minion key in the unaccepted list \r\n \r\nrunning salt-key -A\r\nkey moves the the accepted keys list \r\n\r\nif i try salt '*' test.ping \r\n\r\nI get nothing back after ~30 seconds \r\n\r\nCan anyone help?\r\n"
3873,'thatch45',"test state.highstate with an exit code for changes needed\nI'd like to verify that a highstate is already achieved on a minion.  At the meetup, @thatch45 mentioned it would be a 20 line change to check that state.highstate test=True would return all green.  How can it actually be accomplished?"
3869,'seanchannel',"static doesn't take arguments.\n"
3868,'thatch45','salt-call "lying" about waiting 10 seconds before attempting to re-authenticate\nWhen running ```salt-call``` on a new minion that has never talked to the master before, the new minion key is sent to the master for authorization. When the master has not approved the new key yet, the minion used to exit with exit code 42. This seems to have changed during the one of the past couple of releases.\r\n\r\nIt used to hit the first of the two cases in ```crypt.sign_in()```:\r\n\r\n```python\r\n        if not payload[\'load\'][\'ret\']:\r\n            log.critical(\r\n                \'The Salt Master has rejected this minion\\\'s public \'\r\n                \'key!\\nTo repair this issue, delete the public key \'\r\n                \'for this minion on the Salt Master and restart this \'\r\n                \'minion.\\nOr restart the Salt Master in open mode to \'\r\n                \'clean out the keys. The Salt Minion will now exit.\'\r\n            )\r\n            sys.exit(42)\r\n        else:\r\n            log.error(\r\n                \'The Salt Master has cached the public key for this \'\r\n                \'node, this salt minion will wait for {0} seconds \'\r\n                \'before attempting to re-authenticate\'.format(\r\n                    self.opts[\'acceptance_wait_time\']\r\n                )\r\n            )\r\n            return \'retry\'\r\n```\r\n\r\nNow, it seems to hit the second case, where it does not exit, but simply returns "retry" to the caller.\r\n\r\nWhen ```sign_in()``` is called from ```minion.py``` (line ~600), it then waits for the configured time before trying again in this infinite loop:\r\n\r\n```python\r\n        while True:\r\n            creds = auth.sign_in()\r\n            if creds != \'retry\':\r\n                log.info(\'Authentication with master successful!\')\r\n                break\r\n            log.info(\'Waiting for minion key to be accepted by the master.\')\r\n            time.sleep(self.opts[\'acceptance_wait_time\'])\r\n```\r\n\r\nHowever, when using ```salt-call```, the ```sign_in()``` method is called through the ```__authenticate()``` method in the ```SAuth``` class, where the infinite loop simply keeps calling ```sign_in()```:\r\n\r\n```python\r\n        while True:\r\n            creds = self.sign_in()\r\n            if creds == \'retry\':\r\n                continue\r\n            break\r\n```\r\n\r\nNo sleep here, even though the log output (which floods across the screen) says it would wait X seconds:\r\n\r\n```\r\n~ $ salt-call -l debug cmd.run "uptime"\r\n\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[DEBUG   ] loading grain in [\'/var/cache/salt/minion/extmods/grains\', \'/usr/lib/pymodules/python2.7/salt/grains\']\r\n[DEBUG   ] Skipping /var/cache/salt/minion/extmods/grains, it is not a directory\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion/minion.pem\r\n[...]\r\n```\r\n\r\nSo my questions are:\r\n1. Did something change in the generation of ```payload[\'load\'][\'ret\']``` that makes execution jump into the \'retry\' branch instead of the exit(42) branch of the if clause?\r\n2. Is it on purpose that the infinite loop in ```__authenticate()``` does not have the sleep statement in place?\r\n\r\nPersonally, I would prefer the ```salt-call``` would exit with an error code if the master has not approved the minion key yet, like it used to, but if this change was made on purpose, I would rather not have the log output flood the the screen with the "retrying in X seconds" message until someone on the master approves the new minion\'s key. Worse than the flood of error messages is the flood of network packets, hammering the master with authentication requests.\r\n\r\nThoughts?'
3859,'techhat','pkgrepo, gpgcheck: 0 doesnt written in the .repo file\n```\r\n10gen:\r\n  pkgrepo.managed:\r\n     - gpgcheck: 0\r\n```\r\n\r\nshould write gpgcheck=0 in .repo in /yum.repos.d/10gen.repo\r\nso the default value in /etc/yum.conf is bypassed'
3846,'thatch45','salt-syndic cant connect to local master-syndic when \'interface\' set in master config \nHi,\r\nwe are running with the following setup:\r\n\r\nServer01: \r\nsalt-master, interface 10.0.0.1\r\n\r\nServer02: \r\nvirtual-machine1: running master-syndic and syndic, (desired) interface 10.0.0.10\r\nvirtual-machine2: running minion, 10.0.0.11\r\n\r\nIf i publish a command on the master-master, i can see in the syndic-debug-output that it is received and that the syndic tries to publish it using the local master-syndic:\r\n###\r\nDEBUG   ] User root Executing syndic command test.ping with jid 20130220115454085120\r\n[DEBUG   ] Command details: {\'tgt_type\': \'glob\', \'jid\': \'20130220115454085120\', \'tgt\': \'server*\', \'ret\': \'\', \'to\': 4, \'user\': \'root\', \'arg\': [], \'fun\': \'test.ping\'}\r\n[DEBUG   ] User root Executing syndic command saltutil.find_job with jid 20130220115504105203\r\n[DEBUG   ] Command details: {\'tgt_type\': \'glob\', \'jid\': \'20130220115504105203\', \'tgt\': \'server*\', \'ret\': \'\', \'to\': 1, \'user\': \'root\', \'arg\': [\'20130220115454085120\'], \'fun\': \'saltutil.find_job\'}\r\n###\r\n\r\nBut after 60 seconds i get a timeout in the syndic-debug-output:\r\n###\r\nProcess Process-3:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.6/multiprocessing/process.py", line 232, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.6/multiprocessing/process.py", line 88, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/pymodules/python2.6/salt/minion.py", line 891, in syndic_cmd\r\n    data[\'to\']\r\n  File "/usr/lib/pymodules/python2.6/salt/client.py", line 1002, in pub\r\n    payload = sreq.send(\'clear\', payload_kwargs)\r\n  File "/usr/lib/pymodules/python2.6/salt/payload.py", line 160, in send\r\n    timeout * tried\r\nSaltReqTimeoutError: Waited 60 seconds\r\n\r\nProcess Process-4:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.6/multiprocessing/process.py", line 232, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.6/multiprocessing/process.py", line 88, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File "/usr/lib/pymodules/python2.6/salt/minion.py", line 891, in syndic_cmd\r\n    data[\'to\']\r\n  File "/usr/lib/pymodules/python2.6/salt/client.py", line 1002, in pub\r\n    payload = sreq.send(\'clear\', payload_kwargs)\r\n  File "/usr/lib/pymodules/python2.6/salt/payload.py", line 160, in send\r\n    timeout * tried\r\nSaltReqTimeoutError: Waited 60 seconds\r\n###\r\n\r\nAfter ruling out several thinks like firewalling, having accepted all keys, salt-versions are all matching, order_masters=True on master-master, syndic_master=<CORREPT_IP> in master-syndic, etc. i narrowed it down to the master-syndics \'interface\'-setting.\r\n\r\nIf i let the master-syndic bind to all interfaces, the syndic works as expected and proxys all requests and results. \r\n\r\nIf i specify a single IP to bind to in the master-syndics config, restart the master-syndic and the syndic, i get the above call-traces.\r\n\r\nI can reproduce the behaviour by switching that setting back and forth. My guess is, that the syndic does not honor the interface-setting from the masters config and tries to connect to a different interface than the master-syndic listens on.\r\n\r\nConsidering that i have several IP-addresses configured on the master-syndic from which a few are publically accessable, i would very much prefer to only bind to the internal 10.0.0.x IP and not have 450[5-6] accessable from the outside on all interfaces. \r\n\r\nSome more info regarding the above setup:\r\n\r\n - debian squeeze with all updates and backports enabled.\r\n - salt built as deb-package from current salt-git (3-4 days old)\r\n \r\n####\r\n$ salt --versions-report\r\nSalt: 0.13.0-257-g8307d38\r\nPython: 2.6.6 (r266:84292, Dec 26 2010, 22:31:48)\r\nJinja2: 2.5.5\r\nM2Crypto: 0.20.1\r\nmsgpack-python: 0.1.10\r\nmsgpack-pure: not installed\r\npycrypto: 2.1.0\r\nPyYAML: 3.09\r\nPyZMQ: 2.2.0\r\n####\r\n\r\n'
3834,'seanchannel','Add debconf-utils dependency to Ubuntu package.\nOn Ubuntu 12.04, I installed salt-master and salt-minion from ppa:saltstack/salt, and debconf-utils was not installed. After running `salt-call states.sls mystate`, I got "[WARNING ] Package debconf-utils is not installed."'
3825,'thatch45','custom modules without a "virtual" function do not import properly\nYou can confirm this by creating a file foo.py which contains a single function "bar", which returns True. When placed in salt://_modules, the module is successfully synced to /var/cache/salt/minion/extmods/modules, but does not show up in sys.list_modules and is not available for Salt CLI commands.\r\n\r\nAdding a `__virtual__` function fixes this, but if I understand correctly this is only supposed to be required when you want to do make a given module import with a different name, (i.e. make apt.py import as pkg on ubuntu).\r\n\r\nThis also happens with custom states.'
3823,'whiteinge','readthedocs is not updated with latest version info\nhttp://docs.saltstack.org/en/latest/contents.html\r\n\r\nCurrently says version 0.12.1 although the change log on rtd is 0.13.0 but 0.13.1 has been released.\r\n'
3803,'thatch45',"'salt-call state.running' doesn't show locally running states\nExecuting 'salt-call state.running' locally doesn't return equivalent information as 'salt \\* state.running' on the master. The result is always 'local:' regardless of whether there are states running.\r\n\r\nI'm trying to get modifications to the grains file to trigger salt-call state.highstate via inotify/incron, but instead of checking state.running to prevent concurrent runs, I currently need to check the process table."
3758,'techhat','Archlinux 2013.02.01 keyboard.get_sys  doesn\'t work anymore\n$ sudo salt-call --local keyboard.get_sys\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\n[INFO    ] Executing command \'grep KEYMAP /etc/rc.conf | grep -vE "^#"\' in directory \'/root\'\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 251, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 128, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 70, in call\r\n    ret[\'return\'] = self.minion.functions[fun](*args, **kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/keyboard.py", line 42, in get_sys\r\n    ret = out[1].replace(\'"\', \'\')\r\nIndexError: list index out of range\r\n\r\nThis is because it tries to look in rc.conf which isn\'t present.\r\ncmd = \'grep KEYMAP /etc/rc.conf | grep -vE "^#"\'\r\n\r\nSolution use localectl\r\n$ localectl\r\n   System Locale: LANG=nl_NL.UTF-8\r\n       VC Keymap: us\r\n      X11 Layout: n/a\r\n\r\nmore information:\r\nhttp://www.freedesktop.org/software/systemd/man/localectl.html\r\n\r\n'
3757,'techhat','Archlinux 2013.02.01 timezone.get_zone doesn\'t work anymore\n$ sudo salt-call --local timezone.get_zone\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\n[INFO    ] Executing command \'grep TIMEZONE /etc/rc.conf | grep -vE "^#"\' in directory \'/root\'\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 76, in salt_call\r\n    client.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 251, in run\r\n    caller.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 128, in run\r\n    ret = self.call()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/caller.py", line 70, in call\r\n    ret[\'return\'] = self.minion.functions[fun](*args, **kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/timezone.py", line 48, in get_zone\r\n    ret = out[1].replace(\'"\', \'\')\r\nIndexError: list index out of range\r\n\r\non archlinux is does : grep TIMEZONE /etc/rc.conf | grep -vE "^#\r\n\r\nThis isn\'t available anymore, rc.conf isn\'t on the system\r\n\r\nSolution: systemd timedatectl:\r\n$ timedatectl \r\n      Local time: zo 2013-02-17 13:34:34 CET\r\n  Universal time: zo 2013-02-17 12:34:34 UTC\r\n        RTC time: zo 2013-02-17 13:34:34\r\n        Timezone: Europe/Amsterdam (CET, +0100)\r\n     NTP enabled: yes\r\nNTP synchronized: yes\r\n RTC in local TZ: no\r\n      DST active: no\r\n Last DST change: DST ended at\r\n                  zo 2012-10-28 02:59:59 CEST\r\n                  zo 2012-10-28 02:00:00 CET\r\n Next DST change: DST begins (the clock jumps one hour forward) at\r\n                  zo 2013-03-31 01:59:59 CET\r\n                  zo 2013-03-31 03:00:00 CEST\r\n\r\nmore information:\r\nhttp://www.freedesktop.org/software/systemd/man/timedatectl.html\r\n'
3756,'techhat','Archlinux 2013.02.01 locale.get_locale doesn\'t work anymore\nOn archlinux  2013.02.01 using salt 0.13.1 locale.get_locale doesn\'t work:\r\n$ sudo salt-call --local grains.items\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\nlibvir: XML-RPC error : Failed to connect socket to \'/var/run/libvirt/libvirt-sock\': No such file or directory\r\n[INFO    ] Executing command \'grep "^LOCALE" /etc/rc.conf | grep -vE "^#"\' in directory \'/root\'\r\nlocal:\r\n\r\nThis is true because archlinux 2013.02.01 doesn\'t have /etc/rc.conf anymore.\r\nit uses: systemd localectl\r\n\r\n$ whereis localectl:\r\nlocalectl: /usr/bin/localectl\r\n\r\n$ localectl \r\nSystem Locale: LANG=nl_NL.UTF-8\r\n       VC Keymap: us\r\n      X11 Layout: n/a\r\n\r\nmore info:\r\nhttp://www.freedesktop.org/software/systemd/man/localectl.html\r\n\r\n'
3754,'s0undt3ch','Cli `--module-dirs` overrides instead of extends as suggested\nBy reading the help text for `--module-dirs`, *"specify an additional directories to pull modules from, multiple directories can be delimited by commas"*.\r\n\r\nThe [actual code](https://github.com/saltstack/salt/blob/develop/salt/utils/parsers.py#L1101) does not extend, instead, it overrides.'
3749,'s0undt3ch','While building RPMs in mock, and having python-mock installed causes windows tests to run\n -------------------------------------------------------------------------------\r\n --------  Failed Tests  -------------------------------------------------------\r\n   -> unit.modules.win_status_test.TestProcsComInitialization.test_initialize_and_unintialize_called  \r\n       Traceback (most recent call last):\r\n         File "/builddir/build/BUILD/salt-0.13.1/tests/unit/modules/win_status_test.py", line 187, in test_initialize_and_unintialize_called\r\n           pythoncom.CoInitialize.assert_has_calls(self.expected_calls)\r\n         File "/usr/lib/python2.6/site-packages/mock.py", line 891, in assert_has_calls\r\n           \'Actual: %r\' % (calls, self.mock_calls)\r\n       AssertionError: Calls not found.\r\n       Expected: [call(), call(), call(), call(), call()]\r\n       Actual: []\r\n   .............................................................................\r\n -------------------------------------------------------------------------------\r\n********************************************************************************\r\n============================  Overall Tests Report  ============================\r\nrunning test'
3745,'basepi','Expand gitfs for pillar\nI really like gitfs, and would like to see it expanded.  For starters, it should be documented; but more importantly I would also like to be able to back pillar data with gitfs.  I see this restructuring the config format.  Maybe something like:\r\n\r\n    fileserver_backend:\r\n      - git\r\n    pillar_backend:\r\n      - git\r\n\r\n    gitfs:\r\n      file:\r\n        remotes:\r\n          - git@server:salt-state.git\r\n      pillar:\r\n        remotes:\r\n          - git@server:salt-pillar.git\r\n\r\nThoughts?'
3743,'UtahDave',"rvm module hardcodes global rvm\nIn my environment (and I doubt I'm alone) I want rvm to install into ~/.rvm/ instead of /usr/local/rvm.  runas should support this, but does not.  I'm tinkering with a fix over at https://github.com/kb1jwq/salt/tree/rvm-nonroot but not getting very far due to my lack of Python foo.  Any help / suggestions would be appreciated."
3738,'basepi','Homebrew module attempts to run as root and fails\nbrew "cowardly refuses to run as root," and the test (who owns /usr/local) will return as root.  Modifying test now to equate "who owns the brew binary".'
3737,'basepi','Better messaging when no minions match target.\nWould be very useful to have better messaging when the targeting expression doesn\'t  match any minions. I am running salt 0.13.0-83-g0274d09.\r\n\r\n    ## If salt master is down\r\n    bash-3.2$ salt --out json kim.ho.minion test.ping\r\n    {}\r\n    bash-3.2$ $?\r\n    bash: 0: command not found\r\n     \r\n    ## If salt master is up and minion exists\r\n    bash-3.2$ salt --out json kim.ho.minion test.ping\r\n    {\r\n        "kim.ho.minion": true\r\n    }\r\n    bash-3.2$ $?\r\n    bash: 0: command not found\r\n     \r\n    ## If salt master is up and no minion exists\r\n    bash-3.2$ salt --out json nonexistent test.ping\r\n    bash-3.2$ $?\r\n    bash: 0: command not found\r\n\r\nThis used to exist in 0.10.3 https://github.com/saltstack/salt/issues/1681'
3733,'terminalmage',"pkg.list_upgrades didn't list all available upgrades, specifically salt-* packages to be upgraded.\npkg.list_upgrades didn't list all packages that were upgraded by a subsequent pkg.upgrade. This after manually running a pkg.refresh_db.\r\n\r\nThis is on a Ubuntu 12.04 system.\r\n\r\n    $ time sudo salt '*' pkg.refresh_db                                                                                                                                      \r\n    server01:                                                                   \r\n        ----------                                                              \r\n        :                                                                       \r\n            False                                                               \r\n    server02:                                                                   \r\n        ----------                                                              \r\n        :                                                                       \r\n            False                                                               \r\n                                                                                \r\n    $ time sudo salt '*' pkg.list_upgrades                                      \r\n    server01:                                                                   \r\n        ----------                                                              \r\n        language-selector-common:                                               \r\n            0.79.1                                                              \r\n        libpciaccess0:                                                          \r\n            0.12.902-1ubuntu0.1                                                 \r\n    server02:                                                                   \r\n        ----------                                                              \r\n        language-selector-common:                                               \r\n            0.79.1                                                              \r\n        libpciaccess0:                                                          \r\n            0.12.902-1ubuntu0.1                                                 \r\n                                                                                \r\n    $ time sudo salt '*' pkg.upgrade                                            \r\n    server01:                                                                   \r\n        ----------                                                              \r\n        language-selector-common:                                               \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.79.1                                                          \r\n            old:                                                                \r\n                0.79                                                            \r\n        libpciaccess0:                                                          \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.12.902-1ubuntu0.1                                             \r\n            old:                                                                \r\n                0.12.902-1                                                      \r\n        salt-common:                                                            \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.13.0-1precise                                                 \r\n            old:                                                                \r\n                0.12.1-precise                                                  \r\n        salt-minion:                                                            \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.13.0-1precise                                                 \r\n            old:                                                                \r\n                0.12.1-precise                                                  \r\n    server02:                                                                   \r\n        ----------                                                              \r\n        language-selector-common:                                               \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.79.1                                                          \r\n            old:                                                                \r\n                0.79                                                            \r\n        libpciaccess0:                                                          \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.12.902-1ubuntu0.1                                             \r\n            old:                                                                \r\n                0.12.902-1                                                      \r\n        salt-common:                                                            \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.13.0-1precise                                                 \r\n            old:                                                                \r\n                0.12.1-precise                                                  \r\n        salt-minion:                                                            \r\n            ----------                                                          \r\n            new:                                                                \r\n                0.13.0-1precise                                                 \r\n            old:                                                                \r\n                0.12.1-precise   "
3732,'basepi',"file.contains_regex is not going to work with third parameter\nI've filed this seperately from #2875 because the bug there is probably not really in file.uncomment but in file.contains_regex.\r\n\r\nfile.contains_regex (in salt/modules/file.py) is, as I read the code, never going to work correctly if you provide the third parameter. It reads chunks of undefined size and runs lstrip on those chunks. That would work if every chunk were one line, but my testing shows chunks are longer than that.\r\n\r\nfile.uncomment does not work because of this, because it uses the third parameter.\r\n```\r\nroot@database-2:/usr/share/pyshared/salt/modules# salt-minion --versions-report\r\n           Salt: 0.13.0\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n```\r\n\r\n"
3731,'terminalmage','pkg.installed sources list needs env option for salt source\nThe basic problem is that with states like this:\r\n```yaml\r\nmypkgs:\r\n  pkg.installed:\r\n    - sources:\r\n      - foo: salt://rpms/foo.rpm\r\n      - bar: http://somesite.org/bar.rpm\r\n      - baz: ftp://someothersite.org/baz.rpm\r\n      - qux: /minion/path/to/qux.rpm\r\n```\r\nFor the sources that use salt, there is no way to currently define the salt env from which that file should be retrieved. In my testing, it seems to assume "base" in all cases, regardless of the env that this salt state lives within.\r\n\r\nAt a minimum, it should at least assume the env of the containing state, not "base". A more advance option will probably require changes to the sources data structure.\r\n\r\nHere is an actual example I tested today on salt 0.13.0:\r\n\r\nThis state is in an sls file in an env named "dev".\r\n```yaml\r\njava-rpm-install:\r\n  pkg.installed:\r\n    - sources:\r\n      - jre: salt://media/java_jre/{{ pillar[\'jre_pkg\'] }}\r\n```\r\n```bash\r\n2013-02-14 15:14:25,110 [salt.loaded.int.module.cp                   ][ERROR   ] Unable to cache file "salt://media/java_jre/jre-7u10-linux-i586.rpm" from env "base".\r\n2013-02-14 15:14:25,135 [salt.loaded.int.module.pkg_resource         ][ERROR   ] Failed to cache salt://media/java_jre/jre-7u10-linux-i586.rpm. Are you sure this path is correct?\r\n2013-02-14 15:14:25,136 [salt.state                                  ][ERROR   ] No changes made for java-rpm-install\r\n```\r\n\r\nHere is a previous discussion on the mailing list:\r\nhttps://groups.google.com/forum/#!searchin/salt-users/pkg.installed$20env/salt-users/GXfmmWM99eY/OrmG5UnOAngJ\r\n\r\nI didn\'t see a previous issue documenting this problem, so I\'m putting it here to keep it in your minds as you are doing bug fixing for 13.1.\r\n\r\nNick'
3721,'thatch45','Make the bones to a private cloud controller\nSalt was originally created to be the communication layer for a cloud controller, just a lightweight cloud controller, this issue is to get the cloud controller functions build into a runner so Salt can deploy, configure and manage a simple cloud.'
3710,'seanchannel','Ubuntu fails to start on [fresh] install\n*CORRECTION:  affects all ubuntu/debian, afaik*\r\n\r\nOnly on Ubuntu 10.04, Lucid Lynx: minion terminates immediately during first install. This seems to be due to that OS version\'s apt trying to start the daemon before all the other deps are finished installing.  After the install procedure one can immediately start the minion successfully.\r\n\r\nWork-arounds are in place in salt-bootstrap and salt-cloud, but a general fix might be to special-case the daemon launch in the upstart job.\r\n\r\nI\'ll assign myself for now, but feel free...\r\n\r\n    Linux ip-10-170-78-67 2.6.32-347-ec2 #53-Ubuntu SMP Tue Aug 21 17:58:47 UTC 2012 x86_64 GNU/Linux\r\n    Ubuntu 10.04.4 LTS\r\n    \r\n    Welcome to Ubuntu!\r\n     * Documentation:  https://help.ubuntu.com/\r\n    \r\n      System information as of Wed Feb 13 18:15:19 UTC 2013\r\n    \r\n      System load:  0.0              Processes:           59\r\n      Usage of /:   9.2% of 7.87GB   Users logged in:     0\r\n      Memory usage: 5%               IP address for eth0: 10.170.78.67\r\n      Swap usage:   0%\r\n    \r\n      Graph this data and manage this system at https://landscape.canonical.com/\r\n    ---------------------------------------------------------------------\r\n    At the moment, only the core of the system is installed. To tune the \r\n    system to your needs, you can choose to install one or more          \r\n    predefined collections of software by running the following          \r\n    command:                                                             \r\n                                                                         \r\n       sudo tasksel --section server                                     \r\n    ---------------------------------------------------------------------\r\n    \r\n    0 packages can be updated.\r\n    0 updates are security updates.\r\n    \r\n    New release \'precise\' available.\r\n    Run \'do-release-upgrade\' to upgrade to it.\r\n    \r\n    A newer build of the Ubuntu lucid server image is available.\r\n    It is named \'release\' and has build serial \'20130124\'.\r\n    Get cloud support with Ubuntu Advantage Cloud Guest\r\n      http://www.ubuntu.com/business/services/cloud\r\n    \r\n    The programs included with the Ubuntu system are free software;\r\n    the exact distribution terms for each program are described in the\r\n    individual files in /usr/share/doc/*/copyright.\r\n    \r\n    Ubuntu comes with ABSOLUTELY NO WARRANTY, to the extent permitted by\r\n    applicable law.\r\n    \r\n    To run a command as administrator (user "root"), use "sudo <command>".\r\n    See "man sudo_root" for details.\r\n    \r\n    ubuntu@ip-10-170-78-67:~$ echo deb http://ppa.launchpad.net/saltstack/salt/ubuntu `lsb_release -sc` main | sudo tee /etc/apt/sources.list.d/saltstack.list\r\n    deb http://ppa.launchpad.net/saltstack/salt/ubuntu lucid main\r\n    ubuntu@ip-10-170-78-67:~$ wget -q -O- "http://keyserver.ubuntu.com:11371/pks/lookup?op=get&search=0x4759FA960E27C0A6" | sudo apt-key add -\r\n    OK\r\n    ubuntu@ip-10-170-78-67:~$ sudo apt-get update\r\n    Get:1 http://us-west-1.ec2.archive.ubuntu.com lucid Release.gpg [189B]\r\n    Ign http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid/main Translation-en_US                     \r\n    Ign http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid/universe Translation-en_US     \r\n    Get:2 http://us-west-1.ec2.archive.ubuntu.com lucid-updates Release.gpg [198B]           \r\n    Ign http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid-updates/main Translation-en_US             \r\n    Ign http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid-updates/universe Translation-en_US\r\n    Get:3 http://us-west-1.ec2.archive.ubuntu.com lucid Release [57.2kB]                                 \r\n    Get:4 http://us-west-1.ec2.archive.ubuntu.com lucid-updates Release [58.3kB]                         \r\n    Get:5 http://us-west-1.ec2.archive.ubuntu.com lucid/main Packages [1,383kB]                          \r\n    Get:6 http://security.ubuntu.com lucid-security Release.gpg [198B]                              \r\n    Get:7 http://ppa.launchpad.net lucid Release.gpg [316B]                       \r\n    Ign http://security.ubuntu.com/ubuntu/ lucid-security/main Translation-en_US  \r\n    Get:8 http://us-west-1.ec2.archive.ubuntu.com lucid/universe Packages [5,430kB]            \r\n    Ign http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main Translation-en_US                     \r\n    Ign http://security.ubuntu.com/ubuntu/ lucid-security/universe Translation-en_US                     \r\n    Get:9 http://security.ubuntu.com lucid-security Release [57.3kB]                                     \r\n    Get:10 http://ppa.launchpad.net lucid Release [13.9kB]                                               \r\n    Get:11 http://us-west-1.ec2.archive.ubuntu.com lucid/main Sources [659kB]                            \r\n    Get:12 http://ppa.launchpad.net lucid/main Packages [5,462B]                                         \r\n    Get:13 http://security.ubuntu.com lucid-security/main Packages [475kB]                               \r\n    Get:14 http://us-west-1.ec2.archive.ubuntu.com lucid/universe Sources [3,165kB]   \r\n    Get:15 http://security.ubuntu.com lucid-security/universe Packages [168kB]                \r\n    Get:16 http://security.ubuntu.com lucid-security/main Sources [135kB]                       \r\n    Get:17 http://us-west-1.ec2.archive.ubuntu.com lucid-updates/main Packages [665kB]\r\n    Get:18 http://us-west-1.ec2.archive.ubuntu.com lucid-updates/universe Packages [318kB]  \r\n    Get:19 http://us-west-1.ec2.archive.ubuntu.com lucid-updates/main Sources [234kB]       \r\n    Get:20 http://us-west-1.ec2.archive.ubuntu.com lucid-updates/universe Sources [107kB]\r\n    Get:21 http://security.ubuntu.com lucid-security/universe Sources [43.8kB]\r\n    Fetched 13.0MB in 5s (2,197kB/s)               \r\n    Reading package lists... Done\r\n    ubuntu@ip-10-170-78-67:~$ sudo apt-get install salt-minion\r\n    Reading package lists... Done\r\n    Building dependency tree       \r\n    Reading state information... Done\r\n    The following extra packages will be installed:\r\n      libpgm-5.1-0 libzmq3 msgpack-python python-crypto python-jinja2 python-zmq salt-common\r\n    Suggested packages:\r\n      python-crypto-dbg python-jinja2-doc\r\n    The following NEW packages will be installed:\r\n      libpgm-5.1-0 libzmq3 msgpack-python python-crypto python-jinja2 python-zmq salt-common salt-minion\r\n    0 upgraded, 8 newly installed, 0 to remove and 51 not upgraded.\r\n    Need to get 1,934kB of archives.\r\n    After this operation, 7,406kB of additional disk space will be used.\r\n    Do you want to continue [Y/n]? y\r\n    Get:1 http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid-updates/main python-crypto 2.0.1+dfsg1-4ubuntu2.2 [191kB]\r\n    Get:2 http://us-west-1.ec2.archive.ubuntu.com/ubuntu/ lucid/main python-jinja2 2.3.1-1 [160kB]\r\n    Get:3 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main libpgm-5.1-0 5.1.116~dfsg-2lucid1 [180kB]\r\n    Get:4 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main libzmq3 3.2.2+dfsg-1lucid [361kB]\r\n    Get:5 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main msgpack-python 0.1.9-2 [28.3kB]\r\n    Get:6 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main python-zmq 2.2.0.1-1lucid3 [248kB]\r\n    Get:7 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main salt-common 0.13.0-1lucid [752kB]\r\n    Get:8 http://ppa.launchpad.net/saltstack/salt/ubuntu/ lucid/main salt-minion 0.13.0-1lucid [15.5kB]\r\n    Fetched 1,934kB in 6s (320kB/s)                                                                      \r\n    Selecting previously deselected package libpgm-5.1-0.\r\n    (Reading database ... 24501 files and directories currently installed.)\r\n    Unpacking libpgm-5.1-0 (from .../libpgm-5.1-0_5.1.116~dfsg-2lucid1_amd64.deb) ...\r\n    Selecting previously deselected package libzmq3.\r\n    Unpacking libzmq3 (from .../libzmq3_3.2.2+dfsg-1lucid_amd64.deb) ...\r\n    Selecting previously deselected package msgpack-python.\r\n    Unpacking msgpack-python (from .../msgpack-python_0.1.9-2_amd64.deb) ...\r\n    Selecting previously deselected package python-crypto.\r\n    Unpacking python-crypto (from .../python-crypto_2.0.1+dfsg1-4ubuntu2.2_amd64.deb) ...\r\n    Selecting previously deselected package python-jinja2.\r\n    Unpacking python-jinja2 (from .../python-jinja2_2.3.1-1_amd64.deb) ...\r\n    Selecting previously deselected package python-zmq.\r\n    Unpacking python-zmq (from .../python-zmq_2.2.0.1-1lucid3_amd64.deb) ...\r\n    Selecting previously deselected package salt-common.\r\n    Unpacking salt-common (from .../salt-common_0.13.0-1lucid_all.deb) ...\r\n    Selecting previously deselected package salt-minion.\r\n    Unpacking salt-minion (from .../salt-minion_0.13.0-1lucid_all.deb) ...\r\n    Processing triggers for ufw ...\r\n    Processing triggers for man-db ...\r\n    Processing triggers for ureadahead ...\r\n    Setting up libpgm-5.1-0 (5.1.116~dfsg-2lucid1) ...\r\n    \r\n    Setting up libzmq3 (3.2.2+dfsg-1lucid) ...\r\n    \r\n    Setting up msgpack-python (0.1.9-2) ...\r\n    \r\n    Setting up python-crypto (2.0.1+dfsg1-4ubuntu2.2) ...\r\n    \r\n    Setting up python-jinja2 (2.3.1-1) ...\r\n    \r\n    Setting up python-zmq (2.2.0.1-1lucid3) ...\r\n    \r\n    Processing triggers for python-central ...\r\n    Setting up salt-common (0.13.0-1lucid) ...\r\n    \r\n    Setting up salt-minion (0.13.0-1lucid) ...\r\n    salt-minion start/running, process 1135\r\n    \r\n    Processing triggers for libc-bin ...\r\n    ldconfig deferred processing now taking place\r\n    Processing triggers for python-support ...\r\n    ubuntu@ip-10-170-78-67:~$ pgrep -fl salt\r\n    ubuntu@ip-10-170-78-67:~$ pgrep -fl salt\r\n    ubuntu@ip-10-170-78-67:~$ '
3708,'basepi','Convert gitfs to use libgit2 instead of gitpython\nlibgit2 seems to be where the traction is, need to convert the gitfs to use libgit2 instead of gitpython.'
3705,'UtahDave','Ensure pkg state works with Windows pkg manager.\nNeed to make sure that the Windows pkg manager plays well with the pkg states.'
3696,'UtahDave','cmd.run state in windows\nthe cmd.run state does not work in windows.\r\n\r\nThe initial error is:\r\n\r\nState cmd.run found in sls xyz is unavailable\r\n\r\nThis is caused by a failed import of the cmd state due to the import grp. grp is unix only.\r\nA warning about this is given in the debug output of the minion.\r\n\r\nIf the import grp line of salt/states/cmd.py is commented out, I get the following exception:\r\n\r\nminionname:\r\n----------\r\n    State: - cmd\r\n    Name:      ipconfig\r\n    Function:  run\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "salt/state.py", line 1197, in call\r\n  File "C:\\salt\\salt-0.12.0.win-amd64\\salt-0.12.0-py2.7.egg\\salt\\states\\cmd.py", line 410, in run\r\n    pgid = os.getegid()\r\nAttributeError: \'module\' object has no attribute \'getegid\'\r\n\r\n        Changes:\r\n~\r\n\r\n'
3693,'terminalmage','pkg.installed gives an error.\nView gist here: https://gist.github.com/jesusaurus/4775786\r\n\r\nThis is the error.\r\n\r\n```\r\n----------\r\n    State: - pkg\r\n    Name:      elasticsearch\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1197, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 303, in installed\r\n    modified = [x for x in changes.keys() if x in targets]\r\nUnboundLocalError: local variable \'changes\' referenced before assignment\r\n```'
3687,'terminalmage','pkg.latest with fromrepo does not work with apt \nFor now, pkg.latest uses pkg.available_version to determine is there is an updates version to install, but this function do only display the "Candidate:" field from apt-cache policy which does not honor the "fromrepo" argument of the pkg.latest state.\r\n\r\nNot sure if this is a bug of the pkg.available_version module or a bug of the pkg.latest state...\r\n\r\nMy point for now is that pkg.available_version  should be pkg.available_versions, and that pkg.latest  should check all available versions according the given "fromrepo"...\r\n\r\nDavid\r\n'
3675,'UtahDave',"missing output using state.highstate on windows\nIn continue of this [discussion](https://groups.google.com/forum/#!topic/salt-users/YE8g0OsQPvw) .\r\n\r\nWhen using state.highstate on a windows 2008 std R2 64bit minion the state is being enforced but no output is used by the command unlike the Linux minion.\r\ncmd.run, grains etc shows output, only state.highstate doesn't.\r\n\r\nsalt-minion version tried 0.12.1 and 0.13.0-testing\r\n\r\nThanks,\r\nRan"
3669,'UtahDave','file.recurse not working on Windows platform\n@joehealy reported the following:\r\n\r\n```\r\nfile.recurse does not work with errors like:\r\n\r\nc:/path1\\abc\\idef\\interfaces\r\n\r\nSource file salt://path1/abc\\idef\\interfaces not found\r\n```'
3667,'UtahDave','Windows Package manager allow for msiexec\nThe following ended up working:\r\n\r\n    uninstaller: msiexec\r\n    uninstall_flags: \' /qn /x\r\n"C:\\salt\\var\\cache\\salt\\minion\\files\\base\\win\\repo\\7zip\\7z922-x64.msi"\r\n\'\r\n\r\nAdd msiexec option and a better way of making this work in the yaml.  \r\n\r\nmaybe  ```uninstall_msiexec: True```'
3665,'thatch45','gitfs_remote cached repository persists after config change\nTested on 0bcc2a491b51869bf93c5522168790dd9d66c738. When repository defined in `gitfs_remotes` is changed, the checked out copy persists under `/var/cache/salt/master/gitfs` and is availadle for all salt commands. To actually apply gitfs changes one should clear mentioned directory by hand and restart master.'
3648,'thatch45',"pip install JUST requirements.txt\n```\r\nnodename:\r\n   pip.installed:\r\n      - requirements: /path/to/requirements.txt\r\n```\r\n\r\nthis, besides installing packages from requirements.txt would also install nodename. A work around that works is:\r\n```\r\nnodename:\r\n   pip.installed:\r\n      - name: ''\r\n      - requirements: /path/to/requirements.txt\r\n```\r\n\r\nbut would be nice to not install nodename if requirements is present.\r\n"
3638,'terminalmage','CentOS 6.3 pkg.installed of 32bit pkgs on 64bit system fails\nI posted this on the google group to get input and verify the problem. At least one other user had the same problem, so I\'m now posting it as a bug.\r\n\r\nWhen I tried to install 32bit package (i686) on a 64bit (x86_64) CentOS 6.3, the package gets installed, but the salt state fails. If I run the salt state when the pkg is already installed, the salt state still fails. I\'m running 0.12.1 on the master. I tried this with both 0.11.1 and the latest git code on the minion.\r\n\r\nHere is the install state result where the package is not already installed:\r\n\r\n$ sudo salt-call -l debug state.single pkg.installed name=glibc.i686\r\n\r\n<snip other debug info>\r\n\r\n[INFO    ] Executing state pkg.installed for glibc.i686\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[DEBUG   ] output: redhat-logos_|-60.0.14_|-12.el6.centos\r\n\r\n<snip long list of pkgs>\r\n\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[DEBUG   ] output: redhat-logos_|-60.0.14_|-12.el6.centos\r\n\r\n<snip long list of pkgs>\r\n\r\nLoaded plugins: fastestmirror\r\n[INFO    ] Selecting "glibc.i686" for installation\r\nLoading mirror speeds from cached hostfile\r\n * base: mirrors.versaweb.com\r\n * epel: mirrors.kernel.org\r\n * extras: mirror.stanford.edu\r\n * updates: centos.mirror.ndchost.com\r\n[DEBUG   ] Added 1 transactions\r\n[INFO    ] Resolving dependencies\r\n[INFO    ] Processing transaction\r\nRunning rpm_check_debug\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n[DEBUG   ] output: redhat-logos_|-60.0.14_|-12.el6.centos\r\n\r\n<snip long list of pkgs>\r\n\r\n[ERROR   ] No changes made for glibc.i686\r\n\r\n<snip other debug info>\r\n\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      glibc.i686\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   Package glibc.i686 failed to install\r\n        Changes:   \r\n\r\n$ rpm -qa | grep glibc\r\nglibc-common-2.12-1.80.el6_3.7.x86_64\r\nglibc-headers-2.12-1.80.el6_3.7.x86_64\r\nglibc-2.12-1.80.el6_3.7.i686\r\nglibc-2.12-1.80.el6_3.7.x86_64\r\nglibc-devel-2.12-1.80.el6_3.7.x86_64\r\n\r\nAs you can see the pkg gets installed as \'glibc-2.12-1.80.el6_3.7.i686\'\r\n\r\n\r\nHere is the install state result when the pkg is already installed:\r\n\r\n$ sudo salt-call -l debug state.single pkg.installed name=glibc.i686\r\n\r\n<snip lots of debug info>\r\n\r\n[INFO    ] Executing state pkg.installed for glibc.i686\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n\r\n<snip long list of pkgs>\r\n\r\n[INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/root\'\r\n\r\n<snip long list of pkgs>\r\n\r\nLoaded plugins: fastestmirror\r\n[INFO    ] Selecting "glibc.i686" for installation\r\nLoading mirror speeds from cached hostfile\r\n * base: mirrors.versaweb.com\r\n * epel: mirrors.kernel.org\r\n * extras: mirror.stanford.edu\r\n * updates: linux.mirrors.es.net\r\nPackage glibc-2.12-1.80.el6_3.7.i686 already installed and latest version\r\n[DEBUG   ] Added 0 transactions\r\n[INFO    ] Upgrade failed, trying downgrade\r\n[INFO    ] Resolving dependencies\r\n[INFO    ] Processing transaction\r\nRunning rpm_check_debug\r\n[ERROR   ] No changes made for glibc.i686\r\n\r\n<snip debug info>\r\n\r\nlocal:\r\n----------\r\n    State: - pkg\r\n    Name:      glibc.i686\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   An exception occured in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/state.py", line 1206, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.6/site-packages/salt/states/pkg.py", line 185, in installed\r\n    **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/modules/yumpkg.py", line 350, in install\r\n    yb.processTransaction(rpmDisplay=yumlogger)\r\n  File "/usr/lib/python2.6/site-packages/yum/__init__.py", line 4928, in processTransaction\r\n    self._doTestTransaction(callback,display=rpmTestDisplay)\r\n  File "/usr/lib/python2.6/site-packages/yum/__init__.py", line 5002, in _doTestTransaction\r\n    raise Errors.YumRPMCheckError,retmsgs\r\nYumRPMCheckError: [u\'ERROR with rpm_check_debug vs depsolve:\', \'glibc-common = 2.12-1.80.el6_3.6 is needed by glibc-2.12-1.80.el6_3.6.i686\', u\'Please report this error at http://bugs.centos.org/set_project.php?project_id=16&ref=http://bugs.centos.org/bug_report_page.php?category=yum\']\r\n\r\n        Changes: \r\n\r\n\r\n$ salt-minion --versions\r\n           Salt: 0.11.1\r\n         Python: 2.6.6 (r266:84292, Sep 11 2012, 08:34:23)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.9.final\r\n   msgpack-pure: not installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n\r\n\r\nNick'
3633,'UtahDave','state.svn does not honor test=true\nThe state.svn module does not check whether test=true, so checks out or updates a subversion repository when testing. Checked in latest git.'
3628,'basepi','Unable to use UTF-8 string in pillar\nWhen using UTF-8 string in pillar, I get this error when using it in a jinja renderer:\r\n\r\n```python\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/templates.py", line 55, in render_tmpl\r\n    output = render_str(tmplstr, context, tmplpath)\r\n  File "/usr/lib/pymodules/python2.7/salt/utils/templates.py", line 98, in render_jinja_tmpl\r\n    output = jinja_env.from_string(tmplstr).render(**context)\r\n  File "/usr/lib/python2.7/dist-packages/jinja2/environment.py", line 894, in render\r\n    return self.environment.handle_exception(exc_info, True)\r\n  File "<template>", line 2, in top-level template code\r\nUnicodeDecodeError: \'ascii\' codec can\'t decode byte 0xc3 in position 64: ordinal not in range(128)\r\n```'
3599,'UtahDave','strange behaviour with sync_modules and os_family grain.\nI\'ve run on a strange behavior of sync_modules:\r\n\r\n```\r\nsome_user@ubuntu_1004:~$ sudo salt windows_box saltutil.sync_modules\r\nProcess MWorker-6:\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.6/multiprocessing/process.py", line 232, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 557, in run\r\n    self.__bind()\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 494, in __bind\r\n    ret = self.serial.dumps(self._handle_payload(payload))\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 518, in _handle_payload\r\n    \'clear\': self._handle_clear}[key](load)\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 545, in _handle_aes\r\n    return self.aes_funcs.run_func(data[\'cmd\'], data)\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 1080, in run_func\r\n    ret = getattr(self, func)(load)\r\n  File "/usr/lib/pymodules/python2.6/salt/master.py", line 706, in _pillar\r\n    load[\'env\'])\r\n  File "/usr/lib/pymodules/python2.6/salt/pillar/__init__.py", line 76, in __init__\r\n    self.functions = salt.loader.minion_mods(self.opts)\r\n  File "/usr/lib/pymodules/python2.6/salt/loader.py", line 76, in minion_mods\r\n    whitelist=whitelist\r\n  File "/usr/lib/pymodules/python2.6/salt/loader.py", line 566, in gen_functions\r\n    mod.__init__(self.opts)\r\n  File "/usr/lib/pymodules/python2.6/salt/modules/apt.py", line 29, in __init__\r\n    if __virtual__():\r\n  File "/usr/lib/pymodules/python2.6/salt/modules/apt.py", line 20, in __virtual__\r\n    return \'pkg\' if __grains__[\'os_family\'] == \'Debian\' else False\r\nKeyError: \'os_family\'\r\nsome_user@ubuntu_1004:~$ sudo salt windows_box grains.item os_family\r\nwindows_box:\r\n    Windows\r\n```\r\n\r\nThe thing I can\'t get around is that when i specify the os_family grain manually in minion.conf - the error on salt-master disappear, the modules get synced, even though there are some errors in minions - similiar to those in Issue #2986. \r\n\r\nI tried 12.0, 12.1, develop on salt-minion, the salt master is from ubuntu repo - 0.12.1. I guess I\'m not acquainted with salt well enough to troubleshoot this on my own - can you show where to look at the code for creating/accessing __grains__?\r\n'
3596,'thatch45','gitfs: not all tags are being fetched\nThe gitfs fileserver backend is not checking out all of the tags in the repo with which I am testing.\r\n\r\nIn the clone on my workstation:\r\n```\r\n% git tag -l | fgrep 3.5.45\r\n3.5.45\r\n% git tag -l | wc -l\r\n966\r\n```\r\n\r\nOn my dev VM, within *cachedir*/gitfs/0:\r\n```\r\n% git tag -l | fgrep 3.5.45\r\n% git tag -l | wc -l\r\n944\r\n```'
3564,'UtahDave','salt-minion 0.12.1 windows installer does not work\nRepro:\r\n\r\n 1. Download and run http://saltstack.org/static/downloads/Salt-Minion-0.12.1-Setup-win32.exe:\r\n 2. Start salt-minion:\r\n\r\n```\r\nC:\\salt>salt-minion.exe\r\nTraceback (most recent call last):\r\n  File "<string>", line 6, in <module>\r\n  File "__main__.py", line 726, in <module>\r\n  File "__main__.py", line 332, in bootstrap\r\n  File "__main__.py", line 359, in chainload\r\n  File "__main__.py", line 715, in _chainload\r\n  File "__main__.py", line 128, in <module>\r\n  File "__main__salt-minion__.py", line 9, in <module>\r\n  File "salt/__init__.py", line 12, in <module>\r\n  File "salt/version.py", line 70, in <module>\r\n  File "salt/version.py", line 34, in __get_version_info_from_git\r\n  File "subprocess.py", line 637, in __init__\r\nValueError: close_fds is not supported on Windows platforms if you redirect stdin/stdout/stderr\r\n```\r\n\r\nThe only way I found  to get a running salt-minon is to checkout tag v0.12.0 and setup.py install (provided all other dependencies are already installed) - which is ok for local development but not so great for deployment on multiple servers. It would also be great if there were some docs on creating the nsis installer (running makensis works only half way - I got stuck on figuring out which files should go to pkg/windows/buildenv).'
3550,'terminalmage',"Default outputter doesn't display unicode\nThis problem manifested itself in salt-cloud, which now uses salt's default outputter. Consider the following pprint output:\r\n\r\n 'docean': {u'bamboo-server': {u'backups_active': None,\r\n                               u'id': 1138,\r\n                               u'image_id': 2676,\r\n                               u'ip_address': u'X.X.X.X',\r\n                               u'name': u'bamboo-server',\r\n                               u'region_id': 1,\r\n                               u'size_id': 62,\r\n                               u'status': u'active'}},\r\n\r\nWith the default outputter it looks like:\r\n\r\ndocean:\r\n    ----------\r\n    bamboo-server:\r\n        ----------\r\n        backups_active:\r\n            None\r\n        id:\r\n            1138\r\n        image_id:\r\n            2676\r\n        ip_address:\r\n        name:\r\n        region_id:\r\n            1\r\n        size_id:\r\n            62\r\n        status:\r\n"
3549,'thatch45',"cp.list_minion giving double results\nroot@ragnarok # salt-call cp.list_minion \r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\nlibvir: XML-RPC error : Failed to connect socket to '/var/run/libvirt/libvirt-sock': No such file or directory\r\nlocal:\r\n    - /var/cache/salt/minion/files/base/_modules/a.py\r\n    - /var/cache/salt/minion/files/base/_modules/a.py\r\n    - /var/cache/salt/minion/files/base/_modules/spam.py\r\n    - /var/cache/salt/minion/files/base/_modules/spam.py\r\n    - /var/cache/salt/minion/files/base/acc.sls\r\n    - /var/cache/salt/minion/files/base/acc.sls\r\n    - /var/cache/salt/minion/files/base/alias.sls\r\n    - /var/cache/salt/minion/files/base/alias.sls\r\n    - /var/cache/salt/minion/files/base/cron.sls\r\n    - /var/cache/salt/minion/files/base/cron.sls\r\n    - /var/cache/salt/minion/files/base/crons.sls\r\n    - /var/cache/salt/minion/files/base/crons.sls\r\n    - /var/cache/salt/minion/files/base/http.sls\r\n    - /var/cache/salt/minion/files/base/http.sls\r\n    - /var/cache/salt/minion/files/base/overstate.sls\r\n    - /var/cache/salt/minion/files/base/overstate.sls\r\n    - /var/cache/salt/minion/files/base/ssh.sls\r\n    - /var/cache/salt/minion/files/base/ssh.sls\r\n    - /var/cache/salt/minion/files/base/sshd_config\r\n    - /var/cache/salt/minion/files/base/sshd_config\r\n    - /var/cache/salt/minion/files/base/test.sls\r\n    - /var/cache/salt/minion/files/base/test.sls\r\n    - /var/cache/salt/minion/files/base/testfile\r\n    - /var/cache/salt/minion/files/base/testfile\r\n    - /var/cache/salt/minion/files/base/tools.sls\r\n    - /var/cache/salt/minion/files/base/tools.sls\r\n    - /var/cache/salt/minion/files/base/top.sls\r\n    - /var/cache/salt/minion/files/base/top.sls\r\n    - /var/cache/salt/minion/proc/20130201131949832215\r\n"
3539,'thatch45',"Inconsistent puppet.facts responses\nI have one or more minions that are responding to a puppet.facts in the API but not the CLI. In the CLI, this returns true:\r\n\r\nsalt cbase01.eu1.foo.com puppet.facts --out text\r\n\r\nHowever, the following API code doesn't see it:\r\n\r\n```python\r\nimport salt.client\r\nclient = salt.client.LocalClient()\r\nret = client.cmd_iter('*', 'puppet.facts', [], timeout=60)\r\n    for h in ret:\r\n        for j in h:\r\n            data = h[j]['ret']\r\n            print data['fqdn']\r\n```\r\nI also tried with cmd_cli and got the same result. This host is behind a remote syndic. It's worth noting that the latency to this remote syndic seems high. The timeout on the client might be 60s, but how long does the master master wait on the response from the remote syndic? Some (but but all) of the minions behind this syndic are also doing the same thing. This is reproducable.\r\n\r\nVersions... all masters, syndics and minions are from development branch, git commit  3deb4ba1fa76adf5525798c8b58afde1e42158cf.\r\n\r\n\r\n\r\n"
3537,'thatch45',"When going through the API, selected_target_option is always 'None'.\nThat breaks grains-based minion-discovery for the batch run when running through the API. The expr_form-variable is always set, so we can default to that safely instead of glob.\r\n\r\nTested with own client (derived from LocalClient()) and salt binary using batch mode and grains-based minion-discovery."
3532,'UtahDave','network.interfaces in win_network\nWith 12.1 I get the following exception when trying to run network.interfaces on windows:\r\n\r\nroot@mattmaster:/srv/salt# salt min-win-new-01 network.interfaces\r\nmin-win-new-01:\r\n    Traceback (most recent call last):\r\n      File "salt/minion.py", line 359, in _thread_return\r\n      File "salt/modules/win_network.py", line 212, in interfaces\r\n      File "salt/modules/win_network.py", line 178, in _interfaces_ipconfig\r\n    TypeError: search() takes at least 2 arguments (1 given)\r\n\r\nI think it is just an argument issue with re.search, but there also some funny logic around as if I correct the search problem, then we are looking for an interface name in a dictionary that hasn\'t been initialised with that key yet.\r\n\r\n'
3529,'thatch45','Add nodegroup targeting to publish module\nPlease add nodegroup targeting to publish module.\r\nThe following command fails now:\r\n\r\n$ sudo salt-call publish.publish qa test.ping \'\' nodegroup\r\n\r\nReturns the following stacktrace:\r\n\r\n```\r\n2013-01-30 13:27:10,054 [salt.minion                                 ][CRITICAL] Traceback (most recent call last):\r\n  File "/usr/local/lib/python2.7/site-packages/salt/minion.py", line 714, in tune_in\r\n    self._handle_payload(payload)\r\n  File "/usr/local/lib/python2.7/site-packages/salt/minion.py", line 241, in _handle_payload\r\n    \'clear\': self._handle_clear}[payload[\'enc\']](payload[\'load\'])\r\n  File "/usr/local/lib/python2.7/site-packages/salt/minion.py", line 260, in _handle_aes\r\n    \'{0}_match\'.format(data[\'tgt_type\']))(data[\'tgt\']):\r\nTypeError: nodegroup_match() takes exactly 3 arguments (2 given)\r\n```\r\n\r\nThis is per an email from Daniel B from the mailing list.'
3523,'UtahDave','Salt on Windows: alert about "which"\nSalt commands run on Windows (using the published installer, 0.12.1, x86), complain about not being able to find "which". For example,\r\n\r\n```\r\nC:\\salt>salt-call -c c:\\salt\\conf test.ping\r\n\'which\' is not recognized as an internal or external command,\r\noperable program or batch file.\r\n[INFO    ] Loaded configuration file: c:\\salt\\conf\\minion\r\n←[0;36mlocal←[0m:\r\n←[1;33m    True←[0m\r\n```'
3516,'basepi',"Specify static file contents for file.managed \n`file.managed` is missing a way by which I can give it a static string to be\r\nwritten into the file, e.g.:\r\n\r\n    /etc/papersize:\r\n      file.managed:\r\n        - user: root\r\n        - […]\r\n        - contents: a4\r\n\r\nSure, this can be done by a combination of `file.touch` and `file.append`, but\r\nthat's a hack.\r\n"
3515,'thatch45',"Option to pass **kwargs to state.single function\nDon't have time to do a pull request for this, and it's kinda overkill too. If\r\nI want to call __salt__['state.single'](…) from code, I don't want to have to\r\npre-process `**kwargs`, but just pass them in (cf. #3513):\r\n\r\n    --- a/salt/modules/state.py\r\n    +++ b/salt/modules/state.py\r\n    @@ -311,6 +311,8 @@ def single(fun, name, test=None, kwval_as='yaml', **kwargs):\r\n        elif kwval_as == 'json':\r\n            def parse_kwval(value):\r\n                return json.loads(value)\r\n    +    elif kwval_as == 'verbatim':\r\n    +        parse_kwval = lambda value: value\r\n        else:\r\n            return 'Unknown format({0}) for state keyword arguments!'.format(\r\n                    kwval_as)\r\n"
3500,'cachedout','Improve documentation of file.accumulated\nThe file.accumulated state seems to be really useful, but the current documentation does not provide enough information to understand how to use it. Reproducing the example in #2521 would help a lot.'
3492,'terminalmage','Fix refs to pkg.compare\nI made some changes over the weekend to pkg.compare to implement #3278 but did not fix all refs to the compare function. Thought I had them all but missed at least one. Will fix this tomorrow.\r\n\r\nNote that this will likely have broken pkg.latest in git. Will be a simple fix, it is just too late tonight to get to it.'
3490,'terminalmage',"Document the difference between salt and salt-call\nWhile I am slowly starting to understand, a major source of confusion in my process of learning Salt has been the different paths of execution of publishing e.g. `test.ping` using `salt` vs. using `salt-call` to invoke that function.\r\n\r\nInitially, I was convinced that `salt-call` would do whatever necessary to cause exactly the same thing to happen as when `salt` published a command to that host. After finding out that this wasn't the case, I filed #2375.\r\n\r\nWhat is still unclear to me is what exactly happens after `salt` publishes a command. Which steps take place on the master? What is sent to the minion? What exactly does the minion do? What data are being sent back?\r\n\r\nAnd how does this path of execution compare to calling `salt-master` directly. which also causes data to flow between master and minion.\r\n\r\nI think that Salt's documentation could benefit from an explanation along with a walkthrough example. Hence this issue…"
3464,'UtahDave','state.highstate throws a stacktrace on 0.12.1\nWhen executing `salt * state.highstate` minion returns immediately and this stacktrace is found in minion\'s log:\r\n\r\n```\r\n2013-01-26 14:42:00,901 [salt.minion      ][WARNING ] The minion function caused an exception: Traceback (most recent call last):\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 359, in _thread_return\r\n    ret[\'return\'] = func(*args, **kwargs)\r\n  File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 136, in highstate\r\n    ret = st_.call_highstate()\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1998, in call_highstate\r\n    matches = self.top_matches(top)\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 1719, in top_matches\r\n    self.opts[\'nodegroups\']\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 872, in confirm_top\r\n    return getattr(self, funcname)(match)\r\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 1064, in compound_match\r\n    return eval(\' \'.join(results))\r\n  File "<string>", line 1, in <module>\r\nTypeError: \'bool\' object is not callable\r\n```\r\n\r\nOS: Ubuntu 12.04.1\r\nSalt: 0.12.1 (from ppa)\r\nPython: 2.7.3'
3434,'thatch45',"Syndic Minions not visible after upgrade to 0.12.\nWith salt version 0.12, comit id g671374a.\r\n\r\nWe are running in 3 Amazon EC2 locations, us-west-1, ap-northeast-1 and eu-west-1. The us-west-1 location is the master master. In each of ap-northeast-1 and eu-west-1, there is another master running, with a syndic, and minions behind that. The us-west-1 master has order_masters: True, and the other syndic masters have syndic_master <ip> where ip is the elastic IP of the master master in us-west-1.\r\n\r\nRunning a salt '*' test.ping on the us-west-1 master master only returns minions within us-west-1 that talk directly to the master master. No data is returned for the minions behind the syndic masters in ap-northeast-1 and eu-west-1. Running  salt '*' test.ping on the syndic masters correctly returns local minions.\r\n\r\nThis all worked before upgrading to 0.12, comit id g671374a, with version 10.5. I ran a tcpdump on the syndic master at eu-west-1, back to the elastic IP of the master master in us-west-1 and started the syndic master on eu-west-1, expecting to see some data flow. None. Data did flow at some arbitrary point later. Don't know what I did to initiate this however.\r\n\r\n\r\n\r\n\r\n\r\n"
3418,'UtahDave','states.hg does not work on windows\nThe hg state is unavailable due to the check of "hg" being on the path in the __virtual__ function.\r\n\r\nA search for hg.exe being on the path may permit it to work.\r\n\r\nI\'ll be checking to see if this is all that is needed tomorrow morning.\r\n'
3410,'terminalmage','Extend Pillar matcher to match nested pillars\nI use pillar extensively to organize data into dicts and sub-dicts. However, the pillar matcher only matches top-level keys. What I would like is to be able to do something like:\r\n\r\n```yaml\r\n\'base\':\r\n  \'foo->bar:baz\':\r\n    - match: pillar\r\n    - someslsfile\r\n```\r\n\r\nand match a pillar set up like so:\r\n\r\n```yaml\r\nfoo:\r\n  bar: baz\r\n```\r\n\r\nThe issue is selecting the best delimiter to use in order to indicate the fact that we\'re traversing down into a sub-dict. I chose "->", but I\'m open to other suggestions. @thatch45, what do you think?'
3402,'thatch45',"default dns_retry leads to a hanging boot if 'master' is not set in minion configuration\nAn addon introduced in #2507 could have rather be an optional thing. Currently if e.g. 'master' setting in the minion is missing or mangled, it basically enters into an endless cycle in rhel initscripts.\r\n\r\nA more reasonable behaviour would be to have a default retry set to 3 and max_retry: 2.  \r\n\r\nHappens on the 0.11 branch.\r\n\r\nPS. This might also be the cause here: https://github.com/saltstack/salty-vagrant/issues/54"
3390,'terminalmage','Support pkgs for pkg.purged/.removed\npkg.installed has pkgs, which has the advantage over names that it processes all packages with as few steps as possible, whereas names does the whole shebang for every package individually.\r\n\r\nIt would be nice if the same was possible for pkg.purged and pkg.removed.'
3384,'seanchannel','doc build error on auth import\njust a small annoyance to fix.\r\n\r\n    [...]\r\n    reading sources... [100%] ref/wheel/all/salt.wheel.auth\r\n    Traceback (most recent call last):\r\n      File "/usr/lib/python2.7/dist-packages/sphinx/ext/autodoc.py", line 321, in import_object\r\n        __import__(self.modname)\r\n    ImportError: No module named auth\r\n    \r\n    /home/sean/salt-build/salt-0.12.1/doc/ref/wheel/all/salt.wheel.auth.rst:8: WARNING: autodoc can\'t import/find module \'salt.wheel.auth\', it reported error: "No module named auth", please check your spelling and sys.path\r\n    [...]'
3382,'seanchannel','update debian install topic\nOn 01/21/2013 03:16 PM, martin f krafft sprach:\r\n\r\n> could the stuff on Debian squeeze at\r\n> http://docs.saltstack.org/en/latest/topics/installation/debian.html\r\n> please be updated according to\r\n> https://groups.google.com/d/msg/salt-users/Q7Q1_Cg4mg4/rYp3Nn0MRB4J'
3363,'terminalmage','Make functions that have a "test" mode case-insensitive\nIn our deployment of Salt, we use test=True heavily to sanity-check our SLS before running a highstate or calling state.sls. However, given that changes will be made if "Test" is used in place of "test", a simple mistake can result in premature execution of SLS that one did not intend to execute.\r\n\r\nMy proposed solution is to add a function to the **config** module, to which keyword args can be passed. if any kwarg matching "test" in any way (i.e. "TEST", "Test", "TeSt", etc) resolves to True, then the function returns True, otherwise it returns False. Then this common function can be used in state.highstate, state.sls, and any other state or module function which wishes to implement a test mode, to determine whether or not "test mode" is desired.\r\n\r\n@thatch45, any thoughts on this?'
3349,'UtahDave','win_file lacking functions\nThe module win_file.py lacks functions required for file.managed and other file/directory states.\r\n\r\n'
3347,'seanchannel','Basic apt-repo management functionality\nAdded in the core functionality needed to manage apt-based repos.\r\nThe only downside is that due to the way apt works, most of the\r\ninterfacing requires utilizing the formatted repo_str.  This is\r\nbecause the formats can vary some depending on specifics.\r\n\r\nSupport for the format "ppa:<project>/repo" has also been added\r\nprovided that the python libraries are installed.\r\n\r\nThis is all done via the python-bindings for apt, rather than calling\r\nout to the "aptitude" or "apt-get" binaries (since the bindings have\r\nsome validation in them).\r\n\r\nMinor modification of pkrepo state to detect when repos are already\r\nconfigured (instead of configuring them every time).  Also improved\r\n"changes" output for modified repos'
3342,'whiteinge','file.sed\'s \'before\' pattern needs an "ignore case" option\nhttps://groups.google.com/forum/#!topic/salt-users/Xl8_mZZhE98 has more details but basically it boils down to this:\r\n\r\n  * many config files have case insensitive keywords.\r\n  * in order to reliably detect (and therefore change) some setting, we cannot assume what case the keyword is laid out in.\r\n\r\nsshd_config for instance allows AllowUsers, allowusers, and while they may be the most common, someone may, pathologically, set it to \'aLLOWuSERS\'.  Unless one writes the \'before\' pattern laboriously out as \'[aA][lL][lL][oO]...\', this won\'t always work and may end up not making the change required.\r\n'
3321,'terminalmage',"FreeBSD-specific shadow module missing, breaking user state functionality\nIf a user state is set to enforce password setting, and the user already exists with a different password, when that state is run, the user's password will not be changed.\r\n\r\nThe reason for this is can be found on lines [55-56](https://github.com/saltstack/salt/blob/9abb56dc8f0d4a7535f4909b342b145ab618e3e9/salt/states/user.py#L55), [85-89](https://github.com/saltstack/salt/blob/9abb56dc8f0d4a7535f4909b342b145ab618e3e9/salt/states/user.py#L85), and [263-267](https://github.com/saltstack/salt/blob/9abb56dc8f0d4a7535f4909b342b145ab618e3e9/salt/states/user.py#L263) of `salt/states/user.py`, as well as some other minor lines. As the shadow module does not exist for FreeBSD, the user state is unable to detect whether the password is correct, and is also unable to set the password.\r\n\r\nThe Python `spwd` module should exist under FreeBSD, although commands to modify the password and other properties will involve the `pw usermod` command, along with the `-H` parameter and a dedicated file descriptor for hashed password input. This is because FreeBSD does not solely use the `/etc/shadow` and `/etc/passwd` files for user data - instead, this data is held in `/etc/master.passwd`, and several binary database files, as well as the legacy passwd and shadow files, are generated from this file."
3301,'UtahDave','postgres returner documentation inconsistency\nThe commands to run to install the postgresql returner database (on http://docs.saltstack.org/en/latest/ref/returners/all/salt.returners.postgres.html) appear to be incorrect.\r\n\r\nThe create table command for the table highstates is commented out, but an index is still created for it.\r\n\r\nLooking at the source code, it (the source code) seems to refer to a table salt_returns rather than the table returns (referred to in the docs).'
3297,'UtahDave','file.managed troubles on windows (0.11 and 0.12)\nI\'ve been having trouble with state.highstate on windows.\r\n\r\nThis has been happening with masters and minions on 0.11.1 and 0.12.\r\n\r\nThe windows minions have been installed via the various installers.\r\n\r\nIn each case, test.ping and cmd.run work fine.\r\n\r\nTo see what is happening, I run the minion via the cmd line with -l\r\ndebug (with multiprocessing: True in the minion file).\r\n\r\nWhen I try to run state.highstate with the following sls file:\r\n\r\nc:/Salt-Minion-0.11.1-Setup.exe\r\n  file.managed:\r\n    - source: salt://Salt-Minion-0.11.1-Setup.exe\r\n\r\nI get lots of errors like (in the minion):\r\n\r\nWin32 exception occurred releasing IUnknown at 0x04392ad0\r\n\r\nOn the master, the salt command returns a short while later with no response.\r\n\r\nLater cmd.run commands give exceptions relating from socket.pyx line\r\n175 (copy and paste is a little tricky due to current environment).\r\n\r\nIf I run with multiprocessing: False, I get the following exception:\r\n\r\n[CRITICAL] Failed to load grains defined in grain file core.os_data in function\r\n<function os_data at 0x0000000004855EB8>, error:\r\nTraceback (most recent call last):\r\n  File "salt/loader.py", line 715, in gen_grains\r\n  File "salt/grains/core.py", line 561, in os_data\r\n  File "salt/grains/core.py", line 272, in _memdata\r\n  File "wmi.py", line 1293, in connect\r\nx_wmi_uninitialised_thread: <x_wmi: WMI returned a syntax error: you\'re probably\r\n running inside a thread without first calling pythoncom.CoInitialize[Ex] (no un\r\nderlying exception)>\r\n\r\nWhat is the best way for me to proceed with troubleshooting this?\r\nShould I install from source or is it likely that I am missing a\r\ndependency?'
3278,'terminalmage','specify versions of lots of packages\nIt\'d be really cool if the "pkgs" feature of pkg.install would let you specify particular versions of each package (or some of them).  Something like:\r\n\r\n    mypkgs:\r\n      pkg.installed:\r\n        - pkgs:\r\n          - foo: 1.2.3\r\n          - bar\r\n          - baz: 4.5-6\r\n\r\nErik says, "to support this, specifying a version needs to be audited across package providers to ensure it works (and properly handle it when it is not supported)."'
3275,'terminalmage','SLES salt 0.11.1 no zypper pkgutil available due to grains osfamily\nI found an issue with Suse linux enterprise 11 SP2 and salt.\r\n\r\nFirst run:  salt \'*\' pkg.version salt-minion\r\n{\'##########.lan\': \'"pkg.version" is not available.\'}\r\n{\'##########.lan\': \'"pkg.version" is not available.\'}\r\n\r\nIf I look at the output of salt \'*\' grains.items, the osfamily grain is different:\r\n\r\n os: SUSE  Enterprise Server\r\n  os_family: SUSE  Enterprise Server\r\n  oscodename: x86_64\r\n  osfullname: SUSE Linux Enterprise Server\r\n  osrelease: 11\r\n\r\nSo I think the match in pkgutil.py is wrong for Suse linux enterprise 11 sp2\r\n\r\nFull grains.items:\r\n\r\n(had to remove some information privacy issues)\r\n#########.lan:\r\n  biosreleasedate: 06/22/2012\r\n  biosversion: 6.00\r\n  cpu_flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc up arch_perfmon pebs bts nopl xtopology tsc_reliable nonstop_tsc aperfmperf pni ssse3 cx16 sse4_1 sse4_2 x2apic popcnt hypervisor lahf_lm ida dts\r\n  cpu_model: Intel(R) Xeon(R) CPU           E5520  @ 2.27GHz\r\n  cpuarch: x86_64\r\n  defaultencoding: None\r\n  defaultlanguage: None\r\n  domain: infra.lan\r\n  fqdn: #########.lan\r\n  host: #########\r\n  id: #########.lan\r\n  kernel: Linux\r\n  kernelrelease: 3.0.13-0.27-default\r\n  localhost: #########\r\n  manufacturer: VMware, Inc.\r\n  mem_total: 743\r\n  nodename: #########\r\n  num_cpus: 1\r\n  os: SUSE  Enterprise Server\r\n  os_family: SUSE  Enterprise Server\r\n  oscodename: x86_64\r\n  osfullname: SUSE Linux Enterprise Server\r\n  osrelease: 11\r\n  path: /sbin:/usr/sbin:/usr/local/sbin:/usr/local/bin:/bin:/usr/bin:/usr/X11R6/bin\r\n  productname: VMware Virtual Platform\r\n  ps: ps -efH\r\n  pythonpath:\r\n      /usr/bin\r\n      /usr/lib/python26.zip\r\n      /usr/lib64/python2.6\r\n      /usr/lib64/python2.6/plat-linux2\r\n      /usr/lib64/python2.6/lib-tk\r\n      /usr/lib64/python2.6/lib-old\r\n      /usr/lib64/python2.6/lib-dynload\r\n      /usr/lib64/python2.6/site-packages\r\n      /usr/local/lib64/python2.6/site-packages\r\n  pythonversion: 2.6.0.final.0\r\n  saltpath: /usr/lib64/python2.6/site-packages/salt\r\n  saltversion: 0.11.1\r\n  serialnumber: VMware-#########\r\n  server_id: #########\r\n  shell: /bin/sh\r\n  virtual: VMware'
3272,'UtahDave','Crash on salt master / minion when pycassa is installed and Cassandra is not\nWhen running salt-minion -l debug:\r\n...\r\n [salt.loader      ][ERROR   ] Failed to read the virtual function for module: cassandra\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/dist-packages/salt/loader.py", line 584, in gen_functions\r\n    virtual = mod.__virtual__()\r\n  File "/usr/lib/python2.7/dist-packages/salt/modules/cassandra.py", line 45, in __virtual__\r\n    nt = __salt__[\'config.option\'](\'cassandra.nodetool\')\r\n...\r\n\r\nsalt runs this code because pycassa is installed\r\nsalt fails because nodetool, which is part of Cassandra, does not exist'
3266,'UtahDave','Windows multiprocessing fixes\nResolve any remaining issues with multiprocessing on windows'
3264,'UtahDave',"Windows 32 bit service\nHi, \r\n\r\nI've got following problem. I installed everything by the book, used bbfreeze and esky to freeze and have it running perfectly. \r\n\r\nBut, as soon as I install the salt-minion as a Server with NSSM, it exits constantly with status 1.\r\n\r\nSomebody else is having similar issues? Any ideas?\r\n\r\nThanks."
3217,'thatch45','Config option for default outputter\nWould be good to allow users to set a value in the configuration file(s) that allows a user to have a specific default for output.'
3214,'s0undt3ch',"Fix Travis-CI builds\nSee what's the complaint, where is it coming from and fix it."
3210,'terminalmage','Move common logic for pkg.version and pkg.available_version to pkg_resource module\nBoth of these functions do identical things regardless of the provider:\r\n\r\n**latest_version**\r\n1. Find available updates\r\n2. Do some logic to see if any of the passed packages are among the available updates, and create a string/dict to reflect this\r\n3. Return the result of the logic in step 2.\r\n\r\n**version**\r\n1. Get a list of installed packages.\r\n2. Do some logic to see if any of the passed packages are among the ones installed, and create a string/dict to reflect this.\r\n3. Return the result of the logic in step 2.\r\n\r\nThis common logic should be moved into the **pkg_resource** module to reduce code duplication. To that aim, a new function should be added to each pkg provider called **list_updates** which will do the provider-specific work of determining which updates are available. This function would also be useful to be run from CLI to determine which packages are available for upgrade.'
3202,'UtahDave',"file recurse does not exclude '..' directory \nI've created state with a wrong dir_mode  of 644 for file.recursive /etc/nginx dir. Salt-minion changed /etc directory's mode too.\r\n```\r\n/etc/nginx:\r\n   file.recurse:\r\n     - dir_mode: 644\r\n     - source: salt://nginx/nginx\r\n```\r\n\r\nrelated debug output: \r\n```\r\n   2013-01-09 16:06:54,540 [salt.state       ][INFO    ] {'/etc/nginx/..': {'mode': '644'}}\r\n```\r\n\r\nWhat should happen: file recurse should exclude '..' directory."
3194,'thatch45',"Salt client intermittently wedging after completing job\nWe are seeing the following behaviour intermittently. It occurs with a variety of commands, including `state.highstate` and some explicit `state.sls` calls.\r\n\r\nThe behavior is this:\r\n\r\n1. Call `salt <pattern> state.highstate`.\r\n2. The master executes the command on the minion successfully.\r\n3. Once the job completes the master logs the following: `[INFO    ] Got return from frontend.vm for job 20130108214051415088`\r\n4. At this point CPU on the salt-master pegs to 100%, all used by the `salt` command, and never terminates.\r\n\r\nI'm not sure what other information I can provide that will help. I'll try and reproduce under pdb, but its intermittent nature makes it difficult to debug.\r\n\r\n"
3189,'UtahDave',"CPU usage on windows XP\nHi,\r\n\r\nI've noticed excessive CPU usage by salt-minion on XP using the install @ http://saltstack.org/static/downloads/Salt-Minion-0.11.1-Setup-amd64.exe.\r\n\r\nThe usage spikes from none to 100% constantly with no jobs running\r\n![windows_cpu_usage](https://f.cloud.github.com/assets/142120/50392/892b2fae-59a3-11e2-90e9-604d277f5cd8.png)\r\n\r\nNothing else is running on the pc and task manager shows the usage is from the salt-minion process.\r\n\r\nUsing the same install on a windows 7 machine doesn't cause any cpu usage when the minion is idle.\r\n\r\nNot 100% sure how to debug this further, or if it's salt or this build of salt's issue.\r\n"
3183,'terminalmage',"Upstart not supported on CentOS 6\nhttp://projects.puppetlabs.com/issues/11989\r\n\r\nSalt has the same problem, except it doesn't allow for Puppet's workaround, where one can specify the service command manually."
3155,'s0undt3ch','salt.client should not override existing logging level\n```python\r\n>>> import logging\r\n>>> logging.basicConfig(level=logging.WARNING)\r\n>>> logging.root.level\r\n30\r\n>>> import salt.client\r\n>>> logging.root.level\r\n1\r\n>>>\r\n\r\n```\r\n\r\nIt\'s rather jarring to have already set my logging level and then have Salt override that.\r\n\r\nhttps://github.com/saltstack/salt/blob/aa6057060401b68344cc6b1047d3899320d2bf1f/salt/log.py#L151\r\n```python\r\nif logging.getLoggerClass() is not Logging:\r\n    \'\'\'\r\n    Replace the default system logger with a version that includes trace()\r\n    and garbage() methods.\r\n    \'\'\'\r\n    logging.setLoggerClass(Logging)\r\n    logging.addLevelName(TRACE, \'TRACE\')\r\n    logging.addLevelName(GARBAGE, \'GARBAGE\')\r\n    # Set the root logger at the lowest level possible\r\n    rootLogger = logging.getLogger()\r\n    # Add a Null logging handler until logging is configured(will be removed at\r\n    # a later stage) so we stop getting:\r\n    #   No handlers could be found for logger "foo"\r\n    rootLogger.addHandler(LoggingNullHandler)\r\n    rootLogger.setLevel(GARBAGE)\r\n```\r\n\r\nShould probably detect if logging has already been initialized and if so, don\'t reset the log level\r\n'
3150,'thatch45','Salt state signalising success while in fact failing\nI wrote a salt state to set up a VPN. The important snippet is:\r\n```\r\n service:\r\n    - running\r\n    - enable: True\r\n    - require:\r\n      - pkg: openvpn\r\n```\r\nNow I executed highstate at one point and got a message saying\r\n```\r\n----------\r\n    State: - service\r\n    Name:      openvpn\r\n    Function:  running\r\n        Result:    False\r\n        Comment:   Failed to restart the service\r\n        Changes:   openvpn: False\r\n```\r\nwhich is fine, but I also got\r\n```\r\n----------\r\n    State: - service\r\n    Name:      openvpn\r\n    Function:  running\r\n        Result:    True\r\n        Comment:   Service openvpn is already enabled, and is dead\r\n        Changes: \r\n```\r\non the next run. This is strange, because it is signalising success while the service is in fact enabled but not running. That is a bit strange I think. It should fail unless the service is running...'
3121,'techhat','Detection of Amazon Linux is faulty\nsalt does not work on Amazon linux clients by default.  This prevents services from being controlled and packages from being installed.\r\n\r\n\r\n/etc/system-release contains "Amazon Linux AMI release 2012.09" by default\r\n\r\n[root@salt etc]# salt \'*\' grains.item os\r\nsalt-minion-1: Amazon  AMI\r\n\r\n\r\nChanging /etc/system-release to  "Amazon Linux release 2012.09" temporarily fixes the issue:\r\n\r\n  os: Amazon\r\n  os_family: RedHat\r\n  oscodename: \r\n  osfullname: Amazon Linux\r\n  osrelease: 2012.09'
3116,'seanchannel','More dmesg errors from salt-minion\ninit: salt-minion main process (28580) terminated with status 2'
3110,'UtahDave',"file.recurse state doesn't fail if source directory doesn't exist\n.. instead it just creates an empty directory."
3105,'UtahDave','WMI calls fail if CoIntialize not called for each thread, breaks state.* calls\nCom library is not initialized in`grains/core.py:{272,458}`, therefor the following error is raised:\r\n```\r\nTraceback (most recent call last):\r\n  File "C:\\Python27\\lib\\site-packages\\salt\\loader.py", line 754, in gen_grains\r\n    ret = fun()\r\n  File "C:\\Python27\\lib\\site-packages\\salt\\grains\\core.py", line 559, in os_data\r\n    grains.update(_memdata(grains))\r\n  File "C:\\Python27\\lib\\site-packages\\salt\\grains\\core.py", line 272, in _memdata\r\n    wmi_c = wmi.WMI()\r\n  File "C:\\Python27\\lib\\site-packages\\wmi-1.4.9-py2.7.egg\\wmi.py", line 1293, in connect\r\n    raise x_wmi_uninitialised_thread ("WMI returned a syntax error: you\'re probably running inside a thread without first calling pythoncom.CoInitialize[Ex]")\r\nx_wmi_uninitialised_thread: <x_wmi: WMI returned a syntax error: you\'re probably running inside a thread without first calling pythoncom.CoInitialize[Ex] (no underlying exception)>\r\n```\r\nIn the Python WMI cookbook a try-finally pattern is advised for wmi calls (http://timgolden.me.uk/python/wmi/cookbook.html#use-wmi-in-a-thread). However I don\'t know the overhead of this loading-unloading sequence, maybe they could be done at thread level instead of per call (if threads are reused of course).\r\nIf we initialize com at per call bases, maybe a context manager, or even a function decorator implementation would be nice.'
3068,'thatch45','Git File Server - Undefined variables\nThere are at least 2 undefined variables, [here](https://github.com/saltstack/salt/blob/develop/salt/fileserver/gitfs.py#L131) and [here](https://github.com/saltstack/salt/blob/develop/salt/fileserver/gitfs.py#L107).\r\n\r\nThat last one is easy to fix, but you coded it @thatch45, so, you know it better.'
3043,'UtahDave',"win_network.py is missing functions. _ifconfig\nThere was some refactoring done a while back that wasn't tested enough. The function _ifconfig is missing and I believe several functions that had been refactored into salt/utils/interfaces.py, which was later removed, are missing.\r\n\r\nFor reference: https://github.com/saltstack/salt/blob/38bbd5fc20da9d504706d742cb462bc7e81eecc3/salt/utils/interfaces.py\r\n\r\nBasically, I need to audit this file and make sure it has all the standard network.py functions working."
3032,'s0undt3ch','Graceful CTRL-c of the master is broken with latest develop\ncc: @s0undt3ch , your change is what introduced this regression.\r\n\r\n\r\n```python\r\n[INFO    ] Got return from omniscience for job 20121226233641595295\r\n^CProcess Publisher-2:\r\nTraceback (most recent call last):\r\n  File "/usr/lib64/python2.7/multiprocessing/process.py", line 258, in _bootstrap\r\n[WARNING ] Stopping the Salt Master\r\n    self.run()\r\n  File "/home/jeff/git/salt/salt/master.py", line 333, in run\r\n\r\nExiting on Ctrl-c\r\n    if self.context.closed is False:\r\nAttributeError: \'Publisher\' object has no attribute \'context\'\r\nException AttributeError: "\'list\' object has no attribute \'closed\'" in <bound method ReqServer.__del__ of <salt.master.ReqServer object at 0x1a28950>> ignored\r\n```\r\n\r\nI would dig into it more, but am heading to bed now. Wanted to report the bug before forgetting it.'
3021,'terminalmage','Add support for portage binhosts\nBigger software packages can take quite a long time to install in Gentoo, as ebuild.py runs emerge in the default way, which causes every package to be compiled, every time. A Gentoo shop will likely have a binhost set up to alleviate the need for each host to compile each package. Salt should support binhosts in ebuild.py.\r\n\r\nRelated: [Portage binhosts](http://en.gentoo-wiki.com/wiki/Using_Portage_BINHOST)'
3008,'terminalmage',"pkg state doesn't work properly in gentoo-based distros\nIn gentoo-based distros, pkg.list_pkgs returns the full name of the package (with category name). So for instance, salt is app-admin/salt, and python is dev-lang/python. The pkg state (and, if necessary, ebuild.py) should be updated to allow the use of the package name only, provided that there is not more than one target that matches the desired package. If the full name is not used for a package which is ambiguous, the state should fail with a meaningful error message.\r\n\r\nI've got a Sabayon VM, so I'll tackle this.\r\n"
2986,'UtahDave','Windows freeze\nHi, I succesfully installed salt on Windows (2008 Server R2). Now, I wanted to freeze it so we may deploy on the rest of our servers. With about 100 systems you don\'t really want to download and install Visual Studio on each system. Even managed to succesfully freeze it like mentioned above (had to add Visual Studio to the env path - it could not find msvcp90.dll). But, as soon as I start the frozen app it relly freezes at "...[DEBUG] Loaded minion key: c:\\etc\\salt\\pki\\minion.pem ....". Do you have any idea? Started it with "-l all" but that\'s all the info I get. Worked like a charm on Linux (what else) but Win really gives me the headaches.'
2972,'UtahDave','cmd.script doesn\'t work on Windows\nI tried to use cmd.script to re-master some of my Windows minions, but it doesn\'t work.\r\n\r\n```\r\n[root@MASTER ~]# salt -C \'MINION-* and G@kernel:Windows\' cmd.script salt://salt/re_master.bat \r\n{\'MINION-WEB02\': \'Traceback (most recent call last):\\n  File "salt/minion.py", line 345, in _thread_return\\n  File "salt/modules/cmdmod.py", line 320, in script\\nAttributeError: \\\'module\\\' object has no attribute \\\'chown\\\'\\n\'}\r\n```\r\n\r\n`os.chown()` doesn\'t work on Windows http://docs.python.org/2/library/os.html#os.chown'
2967,'UtahDave','Fix typos and grammar. Some clarifications\n'
2959,'techhat',"SELinux module requires too many commands to work\nCurrently SELinux management module disables itself if it can't find ANY of the following commands:\r\n\r\nsemanage, seinfo, setenforce, setsebool\r\n\r\nNot all of these commands are required for every operation. Furthermore, seinfo is not even used in 0.10.5. These commands are spread out to three packages in RHEL6:\r\n\r\n      - policycoreutils\r\n      - policycoreutils-python\r\n      - setools-console\r\n\r\nIf all I want to do is to disable SELinux, I still need to install all of these packages to make the module happy.\r\n\r\nIn addition, when the module is disabled, selinux state gives a very unhelpful error:\r\n\r\nbools = __salt__['selinux.list_sebool']()\r\nKeyError: 'selinux.list_sebool'\r\n\r\nIt would be a lot better to check for a required utility in runtime and fail with a helpful error instead of disabling the entire module.\r\n"
2954,'whiteinge','Need sys.doc runner\nThe API should be able to fetch a single, aggregate copy of available functions and docs.'
2922,'terminalmage','virtual packages cannot be installed\nafter upgrading from salt-0.10.4 to salt-0.11.0, my states that install **virtual** packages (deb on ubuntu precise) fail without any reason.\r\n\r\nexample: nfs-client is virtual for nfs-common'
2920,'thatch45','Add docs for new Reactor system\n'
2914,'UtahDave',"file.recurse: include_pat only matches file's basename\n    apache/:\r\n      file.recurse:\r\n        - source: salt://tree/webserver/apache/\r\n        - include_pat: 'E@conf.d\\/ssl-site.conf'\r\n\r\nThe above state will not update any file, because it matches the regex against only the basename of the file.\r\n\r\nI can't think of any reason why the match should be restricted to the basename for a regex; match the whole path please?"
2908,'terminalmage','Custom init scripts on RH should be enabled by service.running\nCurrently, when pushing a custom init script, you also have to run chkconfig --add <foo> manually. It would be nice if service.running did this for you if the init script exists.\r\n\r\nThis is the current workaround:\r\n\r\n```yaml\r\n/etc/init.d/celeryd:\r\n  file.managed:\r\n    - source: salt://celery/celeryd.init\r\n    - mode: 755\r\n    - require:\r\n      - pkg: celery package\r\n  cmd.run:\r\n    - name: chkconfig --add celeryd\r\n    - unless: chkconfig --list celeryd\r\n```\r\n\r\n@archtaku'
2906,'terminalmage','pkg.latest is not as "pkgs" aware as pkg.installed\nSay we have a state like this on a CentOS system\r\n\r\n    foo:\r\n      pkg.latest:\r\n        - pkgs:\r\n          - bar\r\n          - baz\r\n\r\nIn the logs we will see output like this\r\n\r\n    [INFO    ] Executing state pkg.latest for foo\r\n    [INFO    ] Executing command \'yum -q list installed foo\' in directory \'/home/user\'\r\n    [INFO    ] Executing command \'yum -q list update foo\' in directory \'/home/user\'\r\n    [WARNING ] "name" parameter will be ignored in favor of "pkgs"\r\n    [INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in   directory \'/home/user\'\r\n    [INFO    ] Executing command \'yum -y install bar baz\' in directory \'/home/user\'\r\n    [INFO    ] Executing command \'rpm -qa --queryformat "%{NAME}_|-%{VERSION}_|-%{RELEASE}\\n"\' in directory \'/home/user\'\r\n    [ERROR   ] No changes made for foo\r\n\r\n\r\nhttps://github.com/saltstack/salt/blob/v0.11.0/salt/states/pkg.py#L216\r\n\r\n    version = __salt__[\'pkg.version\'](name)\r\n    avail = __salt__[\'pkg.available_version\'](name)\r\n\r\n    if not version:\r\n        # Net yet installed\r\n        has_newer = True\r\n\r\nSince this package does not exist, version will be \'\' so has_newer will be set to True\r\n\r\nand we pass the call along to pkg.install\r\n\r\nIf "bar" and "baz" are already updated, then pkg.install will return "no changes" so we\'ll ultimately hit\r\nret[\'comment\'] = \'Package {0} failed to install\'.format(name)\r\n\r\n'
2884,'techhat','ValueError in yumpkg\nI built from git head yesterday (12/11/12), and now I see this error from yumpkg:\r\n\r\n    ----------\r\n        State: - pkg\r\n        Name:      python27-distribute\r\n        Function:  installed\r\n            Result:    False\r\n            Comment:   An exception occured in this state: Traceback (most recent call last):\r\n      File "/usr/local/lib/python2.7/site-packages/salt/state.py", line 1206, in call\r\n        *cdata[\'args\'], **cdata[\'kwargs\'])\r\n      File "/usr/local/lib/python2.7/site-packages/salt/states/pkg.py", line 64, in installed\r\n        cver = __salt__[\'pkg.version\'](name)\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/yumpkg.py", line 187, in version\r\n        pkgs = list_pkgs(name)\r\n      File "/usr/local/lib/python2.7/site-packages/salt/modules/yumpkg.py", line 217, in list_pkgs\r\n        name, version, rel = line.split(\'_|-\')\r\n    ValueError: need more than 1 value to unpack\r\n\r\nThis is on CentOS 6.3.  Let me know if any further info is needed to help with tracking this down.  Thanks!'
2874,'thatch45','Restarts of salt minion or master display in dmesg on ubuntu\n[272609.320912] init: salt-minion main process (20816) killed by TERM signal\r\n\r\n[571049.363301] init: salt-master main process (32382) killed by KILL signal'
2873,'thatch45','Boolean values in pillars are buggy in certain situations\nI have the following pillar:\r\n\r\n           \'repo_checkout_submodules\': {\'common\': False,\r\n                                        \'parsoid/Parsoid\': False,\r\n                                        \'parsoid/config\': False,\r\n                                        \'slot0\': True,\r\n                                        \'slot1\': True},\r\n\r\nIn a module, I\'m getting the pillar:\r\n\r\n    checkout_submodules = __pillar__.get(\'repo_checkout_submodules\')\r\n    checkout_submodules = checkout_submodules[\'parsoid/Parsoid\']\r\n\r\nI\'m getting an error, when calling this from the master:\r\n\r\n\'lardner.pmtpa.wmnet\': {\'ret\': \'Traceback (most recent call last):\\n  File "/usr/lib/pymodules/python2.7/salt/minion.py", line 330, in _thread_return\\n    ret[\\\'return\\\'] = func(*args, **kw)\\n  File "/var/cache/salt/extmods/modules/deploy.py", line 79, in checkout\\n    checkout_submodules = checkout_submodules[repo]\\nTypeError: \\\'NoneType\\\' object has no attribute \\\'__getitem__\\\'\\n\'}\r\n\r\nI don\'t get this error when I run this via salt-call. I also don\'t get this error when I use a string, rather than a boolean.'
2852,'thatch45','Depending on how the service state is called, watch requisite will not always restart on file.managed modification\nIt may be best to first read the gist here first: https://gist.github.com/4251989\n\nEssentially depending on how I use the service state, the watch requisite is not restarting my daemon on modified files, but will always restart when placing the file fresh on a minion.\n\nMoving the state to its own ID Declaration, seems to clear up this inconsistent behavior\n'
2847,'techhat',"Either I'm crazy, or the yumpkg module looks broken\nNOTE: It is likely a bit of both!\n\n```bash\njeff@desktopmonster:~/src/git/salt/salt/modules (develop)$ git grep _compare_versions\nbrew.py:from salt.modules.yumpkg import _compare_versions\nbrew.py:    return _compare_versions(old, new)\npkgutil.py:def _compare_versions(old, new):\npkgutil.py:    return _compare_versions(old, new)\npkgutil.py:    return _compare_versions(old, new)\nyumpkg.py:    return _compare_versions(old, new)\n```\n\nThe ``yumpkg`` module doesn't have a callable named ``_compare_versions()`` from what I can visually see. Also, the ``brew`` is failing to import because it is attempting to import salt missing callable from ``yumpkg``. I'm filing this bug assuming it likely should be there, but not sure on the best course of action."
2838,'techhat',"pkg.latest not catching available updates (FreeBSD)\nI have a `pkg.latest` state to watch for updates on some core packages. I've discovered that it's not catching available updates on my FreeBSD machine.\n\n    [root@io ~]# pkg upgrade\n    Updating repository catalogue\n    repo.txz 100%   57KB  57.3KB/s  57.3KB/s   00:00\n    New version of pkg detected; it needs to be installed first.\n    After this upgrade it is recommended that you do a full upgrade using: 'pkg upgrade'\n    \n    The following packages will be upgraded:\n    \n            Upgrading pkg: 1.0.3 -> 1.0.3_1\n\n    The installation will require 2 B more space\n    \n    1 MB to be downloaded\n    \n    Proceed with upgrading packages [y/N]: N\n\nBut when I run my pkg state, I see this returned:\n\n    ----------\n    State: - pkg\n    Name:      pkg\n    Function:  latest\n        Result:    True\n        Comment:   Package pkg already at latest\n        Changes:\n\nMy state looks like:\n\n    base_packages:\n      pkg.latest:\n        - names:\n        - bash\n        - pkg\n        - ..."
2835,'UtahDave','0.10.5 minion exits immediately when master has not accepted key\n/etc/salt/minion:\n\n```\nmaster: salt\nsub_timeout: 60\nacceptance_wait_time: 10\n```\n\noutput:\n\n```\n# time /usr/bin/salt-minion; echo $?\n[WARNING ] Setting up the Salt Minion "[redacted]"\n[ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\n[ERROR   ] Failed to authenticate with the master, verify this minion\'s public key has been accepted on the salt master\n\nreal\t0m0.345s\nuser\t0m0.184s\nsys\t0m0.132s\n2\n```'
2828,'terminalmage','salt.modules.file.find issue with mtime\n0.10.5 on Ubuntu precise with 0.10.5-0precise packages\n\nI tried this line:\n\n```\nsalt \'*\' file.find /var/log name=\\*.[0-9] mtime=+30d size=+10m delete\n```\n\nfound in http://docs.saltstack.org/en/latest/ref/modules/all/salt.modules.file.html#salt.modules.file.find\n\n```\n$ salt \'*\' file.find /var/log name=\\*.[0-9] mtime=+30d size=+10m \n{\'minion\': \'error: invalid time interval: "+30d"\'}\n```'
2815,'thatch45',"Highstate run with test=True shows cron entries set to be updated which don't need updating\nI've got three cron entries for a specific host spread across different sls files. When running a highstate with test=True (as I always do before applying changes), two of the three cron entries show up as needing to be updated, like so: http://pastebin.com/167VRx63\n\nHowever, when running the highstate, no changes are actually applied to the crontab. There shouldn't be, these cron jobs are already present. So it looks like they are being misidentified when run with test=True."
2811,'UtahDave',"virtualenv.managed test=true disagrees with test=false\nRunning salt 0.10.5 on Ubuntu. I have\n```\n/opt/rhodecode-venv:\n  virtualenv.manage:\n    - no_site_packages: False # so Trac is available\n    - require:\n      - pkg: python-virtualenv\n      - pkg: python-dev\n```\nin my state file. When I run `sudo salt 'sole.*' state.highstate` I get\n\n```\n    State: - virtualenv\n    Name:      /opt/rhodecode-venv\n    Function:  manage\n        Result:    True\n        Comment:   virtualenv exists\n        Changes:   \n```\nbut when I run with test=true, I get\n```\n    State: - virtualenv\n    Name:      /opt/rhodecode-venv\n    Function:  manage\n        Result:    None\n        Comment:   Virtualenv {0} is set to be created or cleared\n        Changes:  \n```\nhighlighted in yellow. I would expect these to agree."
2807,'s0undt3ch','--json-out became less useful in 0.10.5\nOn salt 0.10.4, `--json-out` would return the ouput for each minion on a single line. I just updated to 0.10.5 and realized the output is now:\n\n<pre>\n$ sudo salt "*" network.ip_addrs --json-out\n{\n    ""host1": [\n        "10.1.2.206"\n    ]\n}\n{\n    "host2": [\n        "10.1.2.64"\n    ]\n}\n</pre>\n\nPreviously when scripting the ouput I could `json.loads()` each line separately but now it gets hard to properly separate each minion\'s ouput in a reliable way.\n\nThere is still `--raw-out` that spits a single line per minion but I really don\'t feel safe parsing data with `eval()`.\n\nWould it be possible to add a flag to output compact JSON to get the original behaviour back?'
2803,'techhat','manage.down not working in 0.10.5\n~# salt-run manage.up | wc\n    519     519   10237\n\n~# salt-key -L | wc\n    604     607   11703\n\n~# salt-run manage.down\n^^ nothing\n\nI do, however, get output from manage.down for hosts which have unaccepted keys.'
2795,'UtahDave',"Salt call hangs when calling a non-existent runner from a peer\nRather than informing the user that a runner doesn't exist, salt will instead hang forever. This also occurs if the runner has any syntax or runtime errors."
2786,'terminalmage','Add --disablerepo support to yumpkg{,5}.py\nCurrently there is an argument called "repo" for both of yumpkg{,5}.py, but no option to disable a repo. This was brought up on the mailing list today.\n\nI propose that we change the "repo" argument to "enablerepo", leaving support for the "repo" kwarg to be used for a few releases but ultimately deprecating it. Meanwhile we can include a warning-level log entry if it is used. Additionally, a "disablerepo" argument can be added to provide the requested functionality. \n\n\nThoughts?'
2783,'terminalmage','list_pkgs() output does not properly reflect the pkg database for providers which allow multiple versions of the same pkg\nI encountered this last week while converting the deployment of some of our internally-developed RPMs to use Salt. If the package manager supports having multiple versions of a package installed, then the data received from a call to pkg.list_pkgs (or pkg.version pkgname) will be unreliable. The reason for this is because a command is typically run to list the installed packages, and then each line of the output from that CLI command is parsed, and something like the following is done:\n\n    pkgs[name] = version\n\n\nHowever, the order of the packages returned from whatever CLI command was issued ("rpm -qa", etc.) is not guaranteed to be sorted in any way, so what this means is that as multiple versions of the same package are processed, the value corresponding to that dictionary key is overwritten, and the last version processed for the package name becomes what is returned, irrespective of whether or not that is actually the most recent version. And additionally, the presence of the other versions of that package is ignored.\n\nWhat I am proposing is that, if multiple versions of a package are installed, the looping logic in list_pkgs() will produce a sorted list of version numbers for that package. An example can be found here: http://pastebin.com/cQ9szXQj\n\nThis would require changes to pkg_resource.compare_versions() as well.\n\nAs this is an edge case, it doesn\'t seem like something that should hold up 0.10.6, but we should probably shoot for a resolution on this for 0.10.7.\n\n@thatch45 I\'m pretty familiar with the pkg providers from the work I\'ve been doing recently, so I can do the work for this. Just wanted to get your take on the proposed changes to the various list_pkgs() functions that I posted in the pastebin above.'
2775,'UtahDave','pkg.installed fails on some declarations due to list out of range.\nSome examples below. I include my declaration(s) and the error. Only some pkg.installed declarations are effected. Very weird.\n\nTried two different layouts here:\n<pre><code>\nsnmp-pkg:\n  pkg:\n    - installed\n    - name: snmpd\n</code></pre>\n\n<pre><code>\nsnmpd:\n  file.managed:\n    - source: salt://soe/etc/snmp/snmpd.conf\n    - name: /etc/snmp/snmpd.conf\n    - mode: 640\n    - require:\n      - pkg: snmpd\n  service:\n    - running\n    - require:\n      - pkg: snmpd\n    - watch:\n      - file: /etc/snmp/snmpd.conf\n  pkg:\n    - installed\n</code></pre>\n\n\n<pre><code>\n----------\n    State: - pkg\n    Name:      snmpd\n    Function:  installed\n        Result:    False\n        Comment:   An exception occured in this state: Traceback (most recent call last):\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 884, in call\n    *cdata[\'args\'], **cdata[\'kwargs\'])\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 60, in installed\n    cver = __salt__[\'pkg.version\'](name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 73, in version\n    pkgs = list_pkgs(name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 302, in list_pkgs\n    ret[cols[3]] = cols[4]\nIndexError: list index out of range\n\n        Changes:   \n----------\n</code></pre>\n\n<pre><code>\nmail:\n  pkg.installed:\n    - names:\n      - mailutils\n      - ssmtp\n</code></pre>\n\n<pre><code>\n----------\n    State: - pkg\n    Name:      ssmtp\n    Function:  installed\n        Result:    False\n        Comment:   An exception occured in this state: Traceback (most recent call last):\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 884, in call\n    *cdata[\'args\'], **cdata[\'kwargs\'])\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 60, in installed\n    cver = __salt__[\'pkg.version\'](name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 73, in version\n    pkgs = list_pkgs(name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 302, in list_pkgs\n    ret[cols[3]] = cols[4]\nIndexError: list index out of range\n\n        Changes:   \n----------\n\n----------\n    State: - pkg\n    Name:      mailutils\n    Function:  installed\n        Result:    False\n        Comment:   An exception occured in this state: Traceback (most recent call last):\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 884, in call\n    *cdata[\'args\'], **cdata[\'kwargs\'])\n  File "/usr/lib/pymodules/python2.7/salt/states/pkg.py", line 60, in installed\n    cver = __salt__[\'pkg.version\'](name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 73, in version\n    pkgs = list_pkgs(name)\n  File "/usr/lib/pymodules/python2.7/salt/modules/apt.py", line 302, in list_pkgs\n    ret[cols[3]] = cols[4]\nIndexError: list index out of range\n\n        Changes:   \n----------\n</code></pre>\n\n\nRunning debug mode for salt-minion on the client I am getting nothing out of the ordinary besides an ERROR line that reads ok.\n\n<pre><code>\n[INFO    ] Loading fresh modules for state activity\n[INFO    ] Executing state pkg.installed for ssmtp\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W ssmtp" in directory \'/root\'\n[DEBUG   ] stdout: install ok not-installed ssmtp\n[ERROR   ] No changes made for ssmtp\n\n\n[INFO    ] Loading fresh modules for state activity\n[INFO    ] Executing state pkg.installed for mailutils\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W mailutils" in directory \'/root\'\n[DEBUG   ] stdout: install ok not-installed mailutils\n[ERROR   ] No changes made for mailutils\n\n[INFO    ] Executing state pkg.installed for snmpd\n[INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version}\\n\' -W snmpd" in directory \'/root\'\n[DEBUG   ] stdout: install ok not-installed snmpd\n[ERROR   ] No changes made for snmpd\n</code></pre>\n'
2763,'s0undt3ch',"support remote log files\nIt would be convenient to gather minions' logs on a central host using remote logging for salt daemons instead of local files by specifying an rsyslog-like value for log_file in the config e.g.:\n\n  log_file: @@loghost:10514\n\nit's supported the python logging handlers, so this would be about parsing log_file before setting up the handler accordingly.\n\nI'll assign myself unless someone has this at their fingertips already (?)"
2761,'thatch45','Verify pillar match works with nested data\nMake sure that pillar can match list components like grains can'
2760,'thatch45',"API does not detect minions behind syndic\nSome minions are behind a syndic agent. When running a test.ping on the master master, results for all minions are returned, both local, and those behind the syndic agent. \n\nHowever, when the same thing is run from the Python API, only results for local (not behind syndic) minions are returned. Code used was:\n\nimport salt.client\nclient = salt.client.LocalClient()\nret = client.cmd('*', 'test.ping', arg=[], timeout=60)\nfor j in ret:\n    print j\n\nI also tried passing a host spec, specifically one of the hosts behind syndic. Still no data. Also tried cmd_iter. No data, immediately\n\nMaster, minion, syndic versions are 0.10.5.\n"
2748,'thatch45',"Hosts behind syndic report data as dict\nWhen running test.ping through syndic, minions behind syndic report data as a dictionary.\n\n[us1:i-157ddd4c] root@ppm01:~# salt '*' test.ping | grep web01\nweb01.us1.foo.com: True\n{'web01.ap1.foo.com': True}\n\nThe first host is local, the second host is on the other side of a syndic daemon.\n\nMaster version: 0.10.5-0precise\nMinion version: 0.10.5-0precise\nSyndic version: 0.10.5-0precise\n\n"
2684,'terminalmage','file.append should also create a file, if not present\n'
2664,'UtahDave','Add MySQL returner\n'
2629,'thatch45','salt-0.10.5.tar.gz contains a pyc file\nThe provided download file currently contains a pyc file (salt-0.10.5/doc/_ext/saltdocs.pyc) which can create problems with packaging.\n\nis it possible to remove pyc files before shipping?'
2594,'UtahDave','virtualenv.managed throws error "virtualenv exists"\nI have the following config:\n\n```\n/home/deploy/venv:\n  virtualenv.managed:\n    - no_site_packages: True\n    - runas: deploy\n    - clear: false\n    - requirements: salt://application/requirements.txt\n```\n\nWhen I run `salt \'*\' state.highstate` my minions return:\n\n```\n[saltmaster] out:     State: - virtualenv\n[saltmaster] out:     Name:      /home/deploy/venv\n[saltmaster] out:     Function:  managed\n[saltmaster] out:         Result:    False\n[saltmaster] out:         Comment:   virtualenv exists\n[saltmaster] out:         Changes:   \n```\n\nI can provide more information if needed.'
2571,'herlo','EPEL rpm spec file might need "pciutils" package\nOn a super bare bones CentOS 6.3 server, I get this:\n\n [root@server ~]# salt-call state.highstate test=True\n[INFO    ] Loaded configuration file: /etc/salt/minion\n[WARNING ] Both \'dmidecode\' and \'lspci\' failed to execute, either because they do not exist on the system of the user running this instance does not have the necessary permissions to execute them. Grains output might not be accurate.\n\nherlo has pointed out to me that dmidecode is already in the spec file.  When I install the "pciutils" package which has "lspci" the warning goes away.'
2566,'s0undt3ch',"Add `--out` to parsers and deprecate the other output options.\nThis issue is based on the conversation taken on issue #2563, namely:\r\n\r\n@thatch45:\r\n*Speaking of the parser system, we need to change how the outputter options work*\r\n\r\n@s0undt3ch:\r\n*@thatch45 what needs to change?*\r\n\r\n@thatch45:\r\n*so with the outputter system now modularized we need to change the ouputter options to be just one option, a --out, -o that takes an argument of an outputter. So it would look like this:\r\nsalt --out json * network.interfaces*\r\n\r\n*This makes it easy for someone to specify any available outputter and makes it easier for us to add them in.*\r\n\r\n*This may take more work to add in, but so far I don't think it would be too hard, and we may want to keep the old way for a few releases before tearing it out, in case people need to update scripts to the new way*\r\n\r\n@s0undt3ch:\r\n*Ok. So, we need to add --out and deprecate the other output options. Seems doable.*\r\n\r\n@thatch45:\r\n*I was planning on taking care of it, but if you want to you can go for it of course! I just wanted to put it on your radar since it is part of your domain*\r\n\r\nIt's on my radar now :)"
2563,'s0undt3ch',"Extend salt parsers code abstraction\nNow that [salt-cloud](https://github.com/saltstack/salt-cloud) is being switched to use salt's parsers code, there are still some parts which needs some extended abstraction.\r\n\r\nI'll add here what needs to be done as I find it."
2540,'UtahDave','Nodegroup matching broken in 0.10.4\nI get the below traceback when trying to match a nodegroup via CLI (haven\'t tested in a top.sls, yet): \r\n\r\nCLI example: salt -N nodegroup_name test.ping\r\n\r\nI also confirmed that this is still broken in git.\r\n\r\n\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt", line 10, in <module>\r\n    salt_main()\r\n  File "/usr/lib/python2.6/site-packages/salt/scripts.py", line 102, in\r\nsalt_main\r\n    client.run()\r\n  File "/usr/lib/python2.6/site-packages/salt/cli/__init__.py", line\r\n106, in run\r\n    for full_ret in local.cmd_cli(**kwargs):\r\n  File "/usr/lib/python2.6/site-packages/salt/client.py", line 240, in\r\ncmd_cli\r\n    **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/client.py", line 184, in\r\nrun_job\r\n    **kwargs)\r\n  File "/usr/lib/python2.6/site-packages/salt/client.py", line 863, in pub\r\n    tgt = salt.utils.minions.nodegroup_comp(\r\nAttributeError: \'module\' object has no attribute \'minions\''
2533,'terminalmage','Add watch support for pkg\nI have this huge list of packages that come with default installation that I want to get rid of. This is a one shot deal and this step take a LONG time.\r\n\r\nI wanted to have pkg.purged to be executed only if my /etc/apt/sources.list is changed, so my state look like:\r\n\r\n```\r\napt_sources:\r\n  file:\r\n    - managed\r\n    - name: /etc/apt/sources.list\r\n    - template: jinja\r\n    - user: root\r\n    - group: root\r\n    - mode: 644\r\n    - source: salt://apt/sources.jinja2\r\n    - context:\r\n      all_suites: main restricted universe multiverse\r\n\r\ncleanup:\r\n  pkg:\r\n    - purged\r\n    - watch:\r\n      - file: apt_sources\r\n    - names:\r\n      - pkg-1\r\n      - pkg-2\r\n```\r\n\r\nbut watch is currently not supported by pkg.'
2518,'UtahDave','file.managed state doesn\'t report when it can\'t update a file\nGiven a state like:\r\n```yaml\r\n/tmp/managed.txt:\r\n  file:\r\n    - managed\r\n    - mode: 644\r\n    - source: salt://managed\r\n```\r\n\r\nI made sure that the file wasn\'t what salt thinks it is supposed to be and impossible to write to as root:\r\n\r\n```bash\r\nroot@desktopmonster:~# cat /srv/salt/managed \r\nfoobar\r\nroot@desktopmonster:~# cat /tmp/managed.txt \r\nSat Nov 10 23:31:31 PST 2012\r\nroot@desktopmonster:~# chattr +i /tmp/managed.txt\r\nroot@desktopmonster:~# echo "" > /tmp/managed.txt \r\n-bash: /tmp/managed.txt: Permission denied\r\nroot@desktopmonster:~# salt \'*\' state.highstate\r\nlocalhost6.localdomain6:\r\n----------\r\n    State: - file\r\n    Name:      /tmp/managed.txt\r\n    Function:  managed\r\n        Result:    True\r\n        Comment:   File /tmp/managed.txt updated\r\n        Changes:   diff: ---  \r\n+++  \r\n@@ -1,1 +1,1 @@\r\n-Sat Nov 10 23:31:31 PST 2012\r\n+foobar\r\n```\r\n\r\nSo in theory now, salt had 0 problems updating the file. In reality, that didn\'t happen:\r\n\r\n```bash\r\nroot@desktopmonster:~# cat /tmp/managed.txt \r\nSat Nov 10 23:31:31 PST 2012\r\n```\r\n\r\nRaising an error to the user that the file write failed would be the expected behavior. This seems critical enough that I set the milestone to the next release. Please tell me no if that is wrong.\r\n\r\n'
2464,'UtahDave',"Cannot detect installed i386 pkg on amd64 Ubuntu system\nI'm running 0.10.4.\r\n\r\nThe pkg state does not properly detect when a i386 package is installed on an Ubuntu amd64 system.\r\n\r\nUbuntu now uses packagename:i386 to refer to the 32bit package that is installed on amd64. Here is an example:\r\n\r\n$ dpkg -l kes4lwks\r\nDesired=Unknown/Install/Remove/Purge/Hold\r\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\r\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\r\n||/ Name                  Version               Description\r\n+++-=====================-=====================-==========================================================\r\nun  kes4lwks              <none>                (no description available)\r\n\r\n$ dpkg -l kes4lwks:i386\r\nDesired=Unknown/Install/Remove/Purge/Hold\r\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\r\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\r\n||/ Name                  Version               Description\r\n+++-=====================-=====================-==========================================================\r\nii  kes4lwks:i386         8.0.0-35              Kaspersky Endpoint Security 8 for Linux\r\n\r\n\r\nI cannot get a salt state to detect that this package is installed.\r\n\r\nOn line 60 of the salt/states/pkg.py inside the installed() function is this:\r\n    cver = __salt__['pkg.version'](name)\r\n\r\nSo, the installed function is using the pkg.version function to check for the currently installed version of the package. \r\nHowever, as demonstrated here, it is not correctly detecting that the i386 package is installed:\r\n\r\n$ sudo salt 'myhost' pkg.list_pkgs kes4lwks\r\n{'myhost': {}}\r\n$ sudo salt 'myhost' pkg.list_pkgs kes4lwks:i386\r\n{'myhost: {'kes4lwks': '8.0.0-35'}}\r\n\r\n$ sudo salt 'myhost' pkg.version kes4lwks\r\n{'myhost': ''}\r\n$ sudo salt 'myhost' pkg.version kes4lwks:i386\r\n{'myhost': ''}\r\n\r\n\r\nThanks,\r\nNick"
2462,'s0undt3ch',"Don't hardcode unix paths in tests, windows needs testing too\nAfter @SEJeff's [comment](https://github.com/saltstack/salt/commit/33fa855bc41b72acb69358236c1c3508fc7923f3#commitcomment-2102492) I got to think about this issue.\r\n\r\nIn fact, there are lot's of tests which hardcode a path to `/tmp`, this won't work on windows.\r\n\r\nSo this ticket is to properly handle paths in tests, including states, so that they're not dependant on the system running tests, unless the system running is a major player on the test itself."
2406,'seanchannel',"Issue with the ppa's salt 0.10.4 packages and salt on Ubuntu 10.04\nYeah don't ask... It is broken. I ran the awesome command:\r\n``apt-get install --reinstall salt-master`` to test that the files I put in a repo are working properly.\r\n\r\n<pre>\r\nPreparing to replace salt-master 0.10.4-1lucid1 (using .../salt-master_0.10.4-1lucid1_all.deb) ...\r\nUnpacking replacement salt-master ...\r\ndpkg (subprocess): unable to execute old post-removal script: Exec format error\r\ndpkg: warning: old post-removal script returned error exit status 2\r\ndpkg - trying script from the new package instead ...\r\ndpkg (subprocess): unable to execute new post-removal script: Exec format error\r\ndpkg: error processing /var/cache/apt/archives/salt-master_0.10.4-1lucid1_all.deb (--unpack):\r\n subprocess new post-removal script returned error exit status 2\r\ndpkg (subprocess): unable to execute new post-removal script: Exec format error\r\ndpkg: error while cleaning up:\r\n subprocess new post-removal script returned error exit status 2\r\nPreparing to replace salt-minion 0.10.4-1lucid1 (using .../salt-minion_0.10.4-1lucid1_all.deb) ...\r\nUnpacking replacement salt-minion ...\r\ndpkg (subprocess): unable to execute old post-removal script: Exec format error\r\ndpkg: warning: old post-removal script returned error exit status 2\r\ndpkg - trying script from the new package instead ...\r\ndpkg (subprocess): unable to execute new post-removal script: Exec format error\r\ndpkg: error processing /var/cache/apt/archives/salt-minion_0.10.4-1lucid1_all.deb (--unpack):\r\n subprocess new post-removal script returned error exit status 2\r\ndpkg (subprocess): unable to execute new post-removal script: Exec format error\r\ndpkg: error while cleaning up:\r\n subprocess new post-removal script returned error exit status 2\r\nProcessing triggers for man-db ...\r\nProcessing triggers for ureadahead ...\r\nErrors were encountered while processing:\r\n /var/cache/apt/archives/salt-master_0.10.4-1lucid1_all.deb\r\n /var/cache/apt/archives/salt-minion_0.10.4-1lucid1_all.deb\r\nE: Sub-process /usr/bin/dpkg returned an error code (1)\r\n</pre>\r\n\r\n\r\nI manually put a shebang into these 3 files and it magically worked:\r\n<pre>\r\n/var/lib/dpkg/info/salt-common.postrm\r\n/var/lib/dpkg/info/salt-master.postrm\r\n/var/lib/dpkg/info/salt-minion.postrm\r\n</pre>\r\n\r\nCAN HAS #!/bin/bash SHEBANG!?"
2402,'techhat','Orphaned minion in 10.4\nI have been monitoring this for a few days. I previously had an issue with minions dropping off but a master restart got them back on. Now only a portion of minions drop off (including the localhost minion)\r\n\r\nI have looked at the machines when this happens and they have several salt-minion processes. \r\n\r\nI stop the salt-minion service and 1 of the salt-minion processes stops but the others remain. No stopping or starting will kill them (obviously) nor get the minion back online with the new process.\r\n\r\nA kill command on the orphaned processes is the only way, followed by a service start to get the minion back online.\r\nOK for now with 7 minions, but with a lot more, this process could not be done everyday.\r\n\r\nI have an strace of the orphaned salt-minion processes on the master machine if that helps.\r\n\r\nThanks.\r\n'
2401,'s0undt3ch',"file.comment unexpected behavior\nI'm using salt 0.10.4 (from the Ubuntu daily ppa).\r\n\r\nI'm trying to comment a line in a file but I couldn't get the file.comment state to work.\r\n\r\nFor instance, if we would like to comment the second line in a /tmp/comment file with the content:\r\n\r\n    hello\r\n    world\r\n\r\nI tried with the following state:\r\n\r\n    /tmp/comment:\r\n      file.comment:\r\n        - regex: ^world\r\n\r\n\r\nBut the world line isn't commented.\r\n\r\nHowever, it works when commenting the first line with the regex: ^hello\r\n    "
2397,'UtahDave','Failed to authenticate - 0.10.4\nHello again,\r\n I had everything working under 0.10.3 and yesterday I upgraded to the 0.10.4 Ubuntu packages from the saltstate ppa. Now, I cannot get a user or root on the salt master to authenticate and send commands to the minion. I start the salt master using sudo and have it configured to run as user salt. I\'ve even done a full purge and re-install of both salt-master and salt-minion packages. I have the master on host "salt" and the minion on host "scanworker".\r\n\r\nI\'ve tried it with defining these in the master config file:\r\n\r\nclient_acl:\r\n  root:\r\n    - .*\r\n\r\nor\r\n\r\nclient_acl:\r\n  salt:\r\n    - .*\r\n\r\nBut it still gives the same result:\r\n\r\nroot@salt:/etc/salt# salt \'scanworker\' state.highstate\r\nFailed to authenticate, is this user permitted to execute commands?\r\n\r\nor\r\n\r\nsalt@salt:~/srv$ salt \'scanworker\' state.highstate\r\nFailed to authenticate, is this user permitted to execute commands?\r\n\r\nI\'ve also tried it without defining a client_acl entry in the master config and it doesn\'t work either.\r\n\r\n\r\nWhen I run the salt-master in debug mode, here is the output:\r\n\r\nroot@salt:/etc/salt# salt-master -l debug\r\n[INFO    ] Loaded configuration file: /etc/salt/master\r\n[WARNING ] Setting up the Salt Master\r\n[DEBUG   ] Loaded master key: /etc/salt/pki/master.pem\r\n[INFO    ] Preparing the root key for local communication\r\n[DEBUG   ] Removing stale keyfile: /var/cache/salt/.root_key\r\n[INFO    ] Preparing the salt key for local communication\r\n[DEBUG   ] Removing stale keyfile: /var/cache/salt/.salt_key\r\n[INFO    ] salt-master is starting as user \'root\'\r\n[INFO    ] Current values for max open files soft/hard setting: 1024/4096\r\n[WARNING ] The value for the \'max_open_files\' setting, 100000, is higher than what the user running salt is allowed to raise to, 4096. Defaulting to 4096.\r\n[WARNING ] Raising max open files value to 4096\r\n[WARNING ] New values for max open files soft/hard values: 4096/4096\r\n[INFO    ] Setting up the master communication server\r\n[INFO    ] Starting Salt worker process 0\r\n[INFO    ] Starting the Salt Publisher on tcp://0.0.0.0:4505\r\n[INFO    ] Starting Salt worker process 1\r\n[INFO    ] Starting Salt worker process 2\r\n[INFO    ] Starting Salt worker process 3\r\n[INFO    ] Starting Salt worker process 4\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[INFO    ] Worker binding to socket ipc:///var/run/salt/workers.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[INFO    ] Worker binding to socket ipc:///var/run/salt/workers.ipc\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[INFO    ] Worker binding to socket ipc:///var/run/salt/workers.ipc\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[INFO    ] Worker binding to socket ipc:///var/run/salt/workers.ipc\r\n[DEBUG   ] Missing configuration file: /root/.salt\r\n[DEBUG   ] MasterEvent PUB socket URI: ipc:///var/run/salt/master_event_pub.ipc\r\n[DEBUG   ] MasterEvent PULL socket URI: ipc:///var/run/salt/master_event_pull.ipc\r\n[INFO    ] Worker binding to socket ipc:///var/run/salt/workers.ipc\r\n[INFO    ] Clear payload received with command publish\r\n[INFO    ] Clear payload received with command publish\r\n\r\nAt the beginning, it says that it is starting as user root, even though I\'ve configured it to start as user salt.\r\n\r\nThe last 2 INFO messages are what it displays after I attempted to run 2 salt commands on the master.\r\n\r\nWhen I restart the salt-minion, these additional messages are displayed by the salt-master:\r\n[INFO    ] Clear payload received with command _auth\r\n[DEBUG   ] This salt-master instance has accepted 1 minion keys.\r\n[INFO    ] Authentication request from scanworker\r\n[INFO    ] Authentication accepted from scanworker\r\n[INFO    ] Clear payload received with command _auth\r\n[DEBUG   ] This salt-master instance has accepted 1 minion keys.\r\n[INFO    ] Authentication request from scanworker\r\n[INFO    ] Authentication accepted from scanworker\r\n[INFO    ] AES payload received with command _pillar\r\n[DEBUG   ] Jinja search path: \'/var/cache/salt/files/base\'\r\n[INFO    ] AES payload received with command _minion_event\r\n[INFO    ] Clear payload received with command publish\r\n\r\nThe minion never indicates that there is any attempt by the master to send it a command:\r\ndev@scanworker:~$ sudo salt-minion -l debug\r\n[INFO    ] Loaded configuration file: /etc/salt/minion\r\n[WARNING ] Setting up the Salt Minion "scanworker"\r\n[DEBUG   ] Attempting to authenticate with the Salt Master at 10.8.0.13\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[INFO    ] Authentication with master successful!\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\r\n[INFO    ] Minion is starting as user \'root\'\r\n[DEBUG   ] Minion "scanworker" trying to tune in\r\n[DEBUG   ] Minion PUB socket URI: ipc:///var/run/salt/minion_event_773d46d227dca1fbd4d47130a9439418_pub.ipc\r\n[DEBUG   ] Minion PULL socket URI: ipc:///var/run/salt/minion_event_773d46d227dca1fbd4d47130a9439418_pull.ipc\r\n\r\n\r\nThe odd thing is that I can go onto the minion and run salt-call commands and they work just fine. They pull the files from the salt master and I can run state.highstate to set the state.\r\n\r\nIs there some new config in 0.10.4 that I have to setup? If so, can you point me to the documentation for it?\r\n\r\nP.S other than this issue, salt is very cool!\r\n\r\nThanks!\r\nNick'
2395,'UtahDave',"Failed to remove master pidfile - 0.10.4\nHello,\r\n I'm running 0.10.4 on Ubuntu Precise.\r\n\r\nI am user root when starting the salt-master, but I have it set to run as user salt.\r\n\r\nWhen I shutdown or restart the salt-master, I get this in the logs:\r\n\r\n2012-10-30 15:12:18,550 [salt.master         ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,551 [salt.master         ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n2012-10-30 15:12:18,550 [salt.master         ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,551 [salt.master         ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n2012-10-30 15:12:18,551 [salt.master                        ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,551 [salt.master                        ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n2012-10-30 15:12:18,552 [salt.master                        ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,552 [salt.master                        ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n2012-10-30 15:12:18,553 [salt.master                        ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,554 [salt.master                        ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n2012-10-30 15:12:18,553 [salt.master         ][WARNING ] Caught signal 15, stopping the Salt Master\r\n2012-10-30 15:12:18,655 [salt.master         ][WARNING ] Failed to remove master pidfile: /var/run/salt-master.pid\r\n\r\n\r\nThe perms on the pid file are:\r\n\r\n-rw-r--r-- 1 root root    5 Oct 30 10:17 salt-master.pid\r\n\r\nI'm guessing that it is logging multiples of this error because there are several processes. Here is the ps listing:\r\n\r\n$ ps axuf\r\n<.... snip ....>\r\nsalt     14367  0.2  1.7 491336 17920 ?        Ssl  15:17   0:00 /usr/bin/python /usr/bin/salt-master\r\nsalt     14370  0.0  1.3 114480 13928 ?        S    15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14377  0.0  1.3 229192 14224 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14378  0.0  1.4 229192 14300 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14381  0.1  1.6 532304 17268 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14384  0.1  1.6 532304 17268 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14385  0.1  1.6 532304 17272 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14388  0.1  1.6 532304 17272 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\nsalt     14389  0.1  1.6 532304 17276 ?        Sl   15:17   0:00  \\_ /usr/bin/python /usr/bin/salt-master\r\n"
2379,'s0undt3ch','file.append does not append if part of an existing line matches\nThis state:\r\n\r\n    /etc/ssh/sshd_config:\r\n      file.append:\r\n        - text: "PermitRootLogin {{ \'yes\' if params.permit_root_login else \'no\' }}"\r\n\r\nwill not cause the line to be added if a — commented – line like\r\n\r\n    #PermitRootLogin yes\r\n\r\nexists in the file.\r\n\r\nI think that file.append should check for a match of the entire line.'
2377,'thatch45',"Does not fail on unknown states\nWhen top.sls or an ext_nodes references an unknown state, i.e. a state without\r\na corresponding SLS file, no error is displayed, and there isn't even anything\r\nlogged about it. This can mean that errors can go undetected for a very long\r\ntime and pop up when you least need them."
2376,'UtahDave',"master_tops/ext_nodes needs top.sls to define at least one state\n[https://groups.google.com/d/msg/salt-users/h3-t8G5GYwU/h1kMLhIG8p4J]\r\n\r\nUsing an external nodes classifier, e.g. with the following snippet in the\r\nmaster config:\r\n\r\n    master_tops:\r\n      ext_nodes: /path/to/script\r\n\r\nonly works, if top.sls also defines at least one state. An empty top.sls will\r\n*not* work.\r\n\r\nMy solution was to create a top.sls file like this:\r\n\r\n    base:\r\n      '*':\r\n        - salt.minion\r\n\r\nwhich is kind of a no-op since every minion is a minion is a minion.\r\n"
2355,'UtahDave','Updates to Windows installation docs\nAdded info about an unattended bootstrap script for installing salt on Windows hosts. \r\n'
2353,'UtahDave',"state.highstate test=true does not agree with state.highstate for user.present state\nI just upgraded to 0.10.4 yesterday, and now when I run state.highstate with test=true, I get a ton of (false) output about changes that don't actually need to be made.  The one I'm noticing the most is with user.present states.\r\n\r\nIf I do salt-call user.getent, it includes my user like so:\r\n\r\n            [...]\r\n           {'fullname': 'Mike Chesnut',\r\n            'gid': 10001,\r\n            'groups': ['mikec', 'wheel'],\r\n            'home': '/home/mikec',\r\n            'homephone': '',\r\n            'name': 'mikec',\r\n            'passwd': 'x',\r\n            'roomnumber': '',\r\n            'shell': '/bin/bash',\r\n            'uid': 10001,\r\n            'workphone': ''},\r\n            [...]\r\n\r\nsalt-call group.getent is fine and dandy too:\r\n\r\n            [...]\r\n           {'gid': 10,\r\n            'members': ['root', 'mikec', <other users>, ...],\r\n            'name': 'wheel',\r\n            'passwd': 'x'},\r\n            [...]\r\n           {'gid': 10001, 'members': [], 'name': 'mikec', 'passwd': 'x'},\r\n            [...]\r\n\r\nThis is all well and good, and corresponds to my simple state definition for my user:\r\n\r\n               mikec:\r\n                 user.present:\r\n                   - fullname: Mike Chesnut\r\n                   - password: <hash>\r\n                   - shell: /bin/bash\r\n                   - home: /home/mikec\r\n                   - uid: 10001\r\n                   - gid: 10001\r\n                   - groups:\r\n                     - wheel\r\n                 group.present:\r\n                   - gid: 10001\r\n\r\nAnd when I run state.highstate, there are no changes that need to be made.  I agree:\r\n\r\n            [mikec@host ~]$ id\r\n            uid=10001(mikec) gid=10001(mikec) groups=10001(mikec),10(wheel)\r\n\r\nHowever, when I add 'test=true' to my highstate call, it now wants to add my user to the wheel group:\r\n\r\n    ----------\r\n    State: - user\r\n    Name:      mikec\r\n    Function:  present\r\n        Result:    None\r\n        Comment:   The following user attributes are set to be changed:\r\ngroups: ['wheel']\r\n\r\nAnd of course when I run state.highstate, it makes no changes.  In fact, if I set state_verbose to True, it says:\r\n\r\n    ----------\r\n    State: - user\r\n    Name:      mikec\r\n    Function:  present\r\n        Result:    True\r\n        Comment:   User mikec is present and up to date\r\n        Changes:   \r\n\r\n(Note that I'm not sure how recently this was introduced, as I skipped 0.10.3.)"
2342,'UtahDave','Eauth throws a keyerror  for missing/misspelled username & password via LocalClient\n'
2325,'UtahDave','Cron.present continues to add a job with \'&>/dev/null\' on every highstate run - 0.10.3\nThe sls stanza in question:\r\n\r\n```\r\ncd /srv/salt/ ; git add * &>/dev/null; git commit -a -m" Automated nightly commit." &>/dev/null  ; git push &>/dev/null:\r\n  cron.present:\r\n    - hour: 3\r\n    - minute: 34\r\n```\r\n\r\nIt was not doing this when the sls was:\r\n\r\n```\r\ncd /srv/salt/ ; git add * ; git commit -a -m" Automated nightly commit."  ; git push &>/dev/null:\r\n  cron.present:\r\n    - hour: 3\r\n    - minute: 34\r\n```\r\n\r\nThe patch from https://github.com/saltstack/salt/commit/4177087097439f08961b847c782e22064a798be6 was added back into my copy of 0.10.3 but to no avail.\r\n\r\nThanks.\r\n'
2300,'s0undt3ch','changing log_level not working for log file\nChanging log_level, does not  affect log_file, in only change log level for stdout. In log_file there are still only warning messages. \r\n\r\nversion 0.10.3 \r\n\r\n'
2287,'s0undt3ch',"Log tracebacks\nPython tracebacks don't make it into the log, even with log_level: debug. Please log them... when something really wonky happens I have to shut down the service and run it without daemon mode to see those tracebacks.\r\n\r\nLogs would be 327% more useful (+/- 3%) if tracebacks were in there. Both master and minion, please.\r\n\r\nI suggest that they should always be logged if there is any logging at all, but at *least* in debug mode."
2277,'rallytime','CLI argument issues.\nA couple of points regarding arguments on the CLI.\r\n\r\nConsider the following test function:\r\n\r\ndef clitest(arg1, arg2, arg3=None):\r\n    return [arg1, arg2, arg3]\r\n\r\nIf we do:\r\n$ sudo salt \'*\' test.clitest arg3=456 abc 123\r\nwe get:\r\n{\'host\': [\'abc\', 123, \'456\']}\r\n\r\nHowever if instead we do:\r\n$ sudo salt \'*\' test.clitest arg3=456 arg2=abc 123\r\nwe get:\r\n{\'m1330\': [\'arg2=abc\', 123, \'456\']}\r\n\r\nSo in other words key=val only works if the argument in the function already has a value.\r\n\r\nA second point, if we do:\r\n$ sudo salt \'*\' test.clitest\r\n\r\nwe get a nasty traceback and a TypeError. I\'ve actually written a small patch which catches the TypeError and gives some more helpful output along the lines of "the following arguments are required: ..." Let me know if anyone is interested, I\'m not really sure it is safe to catch a TypeError at this point?\r\n\r\nCheers,\r\nMatt'
2215,'thatch45','\'list\' object has no attribute \'difference\' in static command\n[root@master ~]# salt -sv target cmd.run \'sleep 600\'\r\n\r\nExecuting job with jid 20121010204527828885\r\n-------------------------------------------\r\n\r\nThe following minions did not return:\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt", line 9, in <module>\r\n    load_entry_point(\'salt==0.10.1\', \'console_scripts\', \'salt\')()\r\n  File "/usr/lib/python2.7/site-packages/salt/scripts.py", line 102, in salt_main\r\n    client.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/cli/__init__.py", line 92, in run\r\n    full_ret = local.cmd_full_return(**kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/client.py", line 351, in cmd_full_return\r\n    verbose))\r\n  File "/usr/lib/python2.7/site-packages/salt/client.py", line 662, in get_cli_static_event_returns\r\n    fail = sorted(list(minions.difference(found)))\r\nAttributeError: \'list\' object has no attribute \'difference\'\r\n[root@master ~]# \r\n'
2198,'thatch45',"Client interfaces to Salt components needed\nMuch of Salt's internals is bent toward CLI use. As programmatic use of Salt grows we will need client interfaces to more Salt components (think LocalClient).\r\n\r\nWe will also need much more thorough docs on how to use each.\r\n\r\nHere are some components that should be easier to use programmatically:\r\n\r\n* ~~Runners~~\r\n* ~~salt-key~~\r\n* ~~file_root~~ (not sure how close ``fileclient.LocalClient`` gets us here)\r\n* ~~pillar_root~~\r\n* Batch execution\r\n* The event system\r\n* ~~Outputters~~ (after #2269 and #2351; think using the JSON outputter for salt-api; think using an outputter to format data for a returner)\r\n* Returners (after #2268; we will need an interface to get values)\r\n* ~~Renderers~~ (this is state.show_sls)"
2167,'s0undt3ch','Traceback when using exsel/ipcidr match in SLS\nThe Matcher object for some reason does not get the "functions" attribute added to it, so a call to exsel_match in salt/minion.py fails. Exsel matching works on the CLI in compound matching and with -X, however.\r\n\r\nI just implemented #653 and the same problem exists. See the following pastebin for example SLS as well as tracebacks: http://pastebin.com/REe9jaJn'
2132,'thatch45','Fix module import errors\nvirt\r\nkvm_hyper\r\nca\r\n\r\nThrow import errors'
2087,'s0undt3ch',"salt should be more explicit about not finding the pip binary. \nCurrently, when the pip module is used, if the python pip binary is not found, salt is not explicit about the error.\n\n*I'll extend this explanation in a few hours.*"
2068,'thatch45','salt.state.call_template_str() is broken\n[salt.state.call_template_str()](https://github.com/saltstack/salt/blob/develop/salt/state.py#L1067) is broken.'
2064,'UtahDave','salt minion should continue to try resolving dns\nAs mentioned in this thread:\n\nhttps://groups.google.com/forum/#!topic/salt-users/D1Qnwm4R84M\n\n"The problem here is that the minion gets started before the network is up, it is a problem with the dns resolver failing and not trying again successfully. if you set the master value to the ip it should work. The problem does need to be fixed, the minion needs to be set to keep trying dns. If I recall, the main problem was a question of when to daemonize, since the dns resolver would hold before the minion would daemonize. So far the only serious issue we have seen here is with Upstart not waiting until the network is really up like it should before starting the minion."\n'
2028,'s0undt3ch','pip.installed - IndexError: list index out of range\n**pip.installed state throws a traceback:**\n\n```\n    State: - pip\n    Name:      supervisor\n    Function:  installed\n        Result:    False\n        Comment:   An exception occured in this state: Traceback (most recent call last):\n  File "/usr/local/lib/python2.6/dist-packages/salt/state.py", line 823, in call\n    ret = self.states[cdata[\'full\']](*cdata[\'args\'])\n  File "/usr/local/lib/python2.6/dist-packages/salt/states/pip.py", line 100, in installed\n    version = list(pkg_list.values())[0]\nIndexError: list index out of range\n```\n\nThis is the actual state:\n```\nsupervisord-pip:\n    pip.installed:\n        - name: supervisor\n        - require:\n            - pkg: python-dev\npython-dev:\n    pkg.installed\n\n```\n\n\n**OS**: Ubuntu 10.10\n**Python**: 2.6.6\n**salt**: 0.10.2 (installed with pip)'
2007,'seanchannel','define ACL\'s in a pillar\nThis is an enhancement request I\'m passing along following some IRC chat.\n\nthere seems to be some pretty well reasoned desire for the ability to control the user ACL\'s from a DB or other external source, like having a runner create a pillar that the master can load and refresh.\n\nIf we could "just" define ACL\'s in a pillar.  Does that make sense?'
1992,'s0undt3ch',"add '-d' also to command salt\n\nthis might be an alias to \n\n```salt \\*  sys.doc ```\n\nmost salt commands display useful info on ```-d```, why not salt itself?"
1991,'s0undt3ch','"Raising max open files value to 9223372036854775807"\nsalt-master seems to get a bit greedy with open files on Mac OS X:\n\n```\n(virtualenv)blast_hardcheese@x220t:~/Projects/salt$ salt-master -c etc/salt/\n[WARNING ] Starting the Salt Master\n[WARNING ] Raising max open files value to 9223372036854775807\nTraceback (most recent call last):\n  File "/Users/blast_hardcheese/Projects/salt/virtualenv/bin/salt-master", line 10, in <module>\n    salt_master()\n  File "/Users/blast_hardcheese/Projects/salt/virtualenv/lib/python2.7/site-packages/salt/scripts.py", line 18, in salt_master\n    master.start()\n  File "/Users/blast_hardcheese/Projects/salt/virtualenv/lib/python2.7/site-packages/salt/__init__.py", line 66, in start\n    master.start()\n  File "/Users/blast_hardcheese/Projects/salt/virtualenv/lib/python2.7/site-packages/salt/master.py", line 222, in start\n    self.__set_max_open_files()\n  File "/Users/blast_hardcheese/Projects/salt/virtualenv/lib/python2.7/site-packages/salt/master.py", line 208, in __set_max_open_files\n    resource.setrlimit(resource.RLIMIT_NOFILE, (mof_c, mof_h))\nValueError: current limit exceeds maximum limit\n```\n\nA functioning workaround is to set max_open_files to 2048:\n\n```\n(virtualenv)blast_hardcheese@x220t:~/Projects/salt$ salt-master -c etc/salt/\n[WARNING ] Starting the Salt Master\n[WARNING ] Raising max open files value to 2048\n[WARNING ] New values for max open files soft/hard values: 2048/9223372036854775807\n```'
1989,'s0undt3ch','"root_dir" configuration option is prepended to other paths in config, including absolute ones\nThe following configuration directives:\n\n```\nroot_dir: /Users/blast_hardcheese/Projects/salt/\nlog_file: /Users/blast_hardcheese/Projects/salt/var/log/salt/minion\n```\n\nresult in the following log file:\n\n```\n/Users/blast_hardcheese/Projects/salt/Users/blast_hardcheese/Projects/salt/var/log/salt/minion\n```'
1987,'s0undt3ch',"Refactor the jobs cache\nRefactor the jobs cache in such a way that is generic and uses salt's loader.\n\nThe idea behind this is to allow salt to have more than the file based jobs cache. Maybe an SQLite3 jobs cache, a memcached jobs cache, etc.\n\nFeel free to add ideas regarding the direction of this issue."
1983,'s0undt3ch','Keep configuration variables to the minimum possible. Refs #1981.\n'
1981,'s0undt3ch',"date column missing in log file\nI've resently upgraded several minions (qty: 16) to salt-minion 0.10.2. I am finding with every one that I upgraded, I no longer have the date column in /var/log/salt/minion log file. All older salt-minion versions are logging the date correctly.\n\nNotice the logging format after 2012-08-22, the log no longer has the date column. Any fresh installs of salt-minion 0.10.2 will not log the date, as well.\n\n```\n2012-08-08 16:22:48,727 [salt.minion    ][WARNING ] Starting the Salt Minion\n2012-08-08 16:22:50,353 [salt.crypt     ][ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\n2012-08-08 16:23:00,363 [salt.crypt     ][ERROR   ] The Salt Master has cached the public key for this node, this salt minion will wait for 10 seconds before attempting to re-authenticate\n2012-08-08 16:23:20,428 [salt           ][WARNING ] Stopping the Salt Minion\n2012-08-08 16:23:29,444 [salt.minion    ][WARNING ] Starting the Salt Minion\n2012-08-20 10:54:30,237 [salt.crypt     ][WARNING ] Failed to authenticate message\n2012-08-22 13:28:02,239 [salt.crypt     ][WARNING ] Failed to authenticate message\n14:55:19,075 [salt.minion      ][WARNING ] Starting the Salt Minion\n14:56:51,912 [salt.minion      ][WARNING ] Starting the Salt Minion\n15:17:26,054 [salt.minion      ][WARNING ] Starting the Salt Minion\n10:03:23,005 [salt.crypt       ][WARNING ] Failed to authenticate message\n12:00:30,404 [salt.crypt       ][WARNING ] Failed to authenticate message\n14:02:23,878 [salt.crypt       ][WARNING ] Failed to authenticate message\n14:03:56,469 [salt.crypt       ][WARNING ] Failed to authenticate message\n15:27:07,244 [salt.minion      ][WARNING ] Starting the Salt Minion\n16:34:13,547 [salt.crypt       ][WARNING ] Failed to authenticate message\n```"
1980,'s0undt3ch',"Raise max open files when starting master. Refs #1964\nWhen `salt-master` starts, it checks the soft and hard limits for max open files, it then raises it's process allowed max open files to the maximum if `max_open_files` is unset on the configuration file or to the set value."
1964,'s0undt3ch',"Add support for ulimit throtling/scaling on the master\nCurrently, when the max open files setting(`ulimit -n`) is reached, zmq exits.\n```\nToo many open files (tcp_listener.cpp:335)\nAborted (core dumped)\n```\n\nSalt should be allowed to raise this value if it's too low.\n\nPython allows us to get and set that value:\n```python\n>>> max_open_files_soft, max_open_files_hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n>>> max_open_files_soft, max_open_files_hard\n... (1024L, 4096L)\n>>> resource.setrlimit(resource.RLIMIT_NOFILE, (2048, max_open_files_hard))\n>>> max_open_files_soft, max_open_files_hard\n... (2048L, 4096L)\n```\n"
1959,'whiteinge','Virtualenv run-as and requirements file fails\nWhen using virtualenv.manage in state files with runas set to an otherwise unprivileged user and you specify a requirements file with a salt:// url, to be passed to pip in the newly installed virtualenv, it will fail silently when not run with a local file_client.\nThis is because the dir /var/cache/salt (and all subdirs and files) is set to only allow root access. You get no error message, but the packages pip was supposed to install, don\'t get installed in the virtualenv. I observed the actual error message by running "salt-call -l debug state.highstate" on the minion. The error message given is:\n[DEBUG   ] output: Could not open requirements file: [Errno 13] Permission denied: \'/var/cache/salt/files/base/servername/REQUIREMENTS.txt\'\nIf run with a local file_client, there will generally be universal read-only access to the state files, which means that testing locally with no master won\'t produce this problem under most circumstances.\nThis problem was observed on a master and minions running Ubuntu 12.04 with salt 0.10.2.'
1905,'terminalmage','pkg.latest (apt-get) state \'failed to install\' an already installed package.\nI have a strange error I can\'t quite figure out.  To simplify the variables I paired this down to only the individual affected .sls file (and of course top.sls)  I\'m substituting the system ids to not give out my FQDN and trimming the list of all installed packages, but otherwise I won\'t modify things.\n\nSo the problem is that I\'m trying to setup some policy surrounding SSH usage.  One part of that is to install the \'mosh\' program normally from Ubuntu repos and keep it up to date.  The first time I run highstate mosh is installed and everything seems fine.  subsequent highstate runs produce an error "Package mosh failed to install" even though it is already installed.  It\'s strange because I have other pkg.latest states that run just fine which would seem to suggest a problem with the name \'mosh\' perhaps?\n\ntop.sls:\n```yaml\nbsae:\n    \'*.mydomain.com\':\n        - ssh\n```\n\nssh/init.sls:\n```yaml\nopenssh-server:\n    pkg:\n        - latest\n\nmosh:\n    pkg:\n        - latest\n        - require:\n            - pkg.latest: openssh-server\n\nssh-users:\n    group:\n        - present\n        - require:\n            - pkg.latest: openssh-server\n\n/etc/ssh/sshd_config:\n    file.managed:\n        - group: root\n        - mode: 644\n        - source: salt://ssh/files/sshd_config\n        - user: root\n        - require:\n            - group.present: ssh-users\n            - pkg.latest: openssh-server\n\nssh:\n    service.running:\n        - enable: True\n        - require:\n            - file.managed: /etc/ssh/sshd_config\n        - watch:\n            - file.managed: /etc/ssh/sshd_config\n```\n\nI ran the following on 2 Ubuntu desktop minions I use for testing to capture the logs for you and get the displayed result:\n```\n$ sudo rm /var/log/salt/minion && sudo salt \'pc*.mydomain.com\' -v state.highstate; sudo cp /var/log/salt/minion ~/minion.log\nExecuting job with jid 20120828215822848415\n-------------------------------------------\n\npc1.mydomain.com:\n----------\n    State: - pkg\n    Name:      mosh\n    Function:  latest\n        Result:    False\n        Comment:   Package mosh failed to install\n        Changes:   \npc2.mydomain.com:\n----------\n    State: - pkg\n    Name:      mosh\n    Function:  latest\n        Result:    False\n        Comment:   Package mosh failed to install\n        Changes:   \n```\n\nAnd finally here are the captured logs:\n```\n21:58:22,851 [salt.minion          ][INFO    ] User myuser Executing command state.highstate with jid 20120828215822848415\n21:58:22,851 [salt.minion          ][DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20120828215822848415\', \'tgt\': \'*\', \'ret\': \'\', \'user\': \'myuser\', \'arg\': [], \'fun\': \'state.highstate\'}\n21:58:22,858 [salt.crypt           ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:22,861 [salt.crypt           ][DEBUG   ] Decrypting the current master AES key\n21:58:22,862 [salt.crypt           ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:22,955 [salt.crypt           ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:22,958 [salt.crypt           ][DEBUG   ] Decrypting the current master AES key\n21:58:22,959 [salt.crypt           ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,034 [salt.state           ][INFO    ] Loading fresh modules for state activity\n21:58:23,087 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,088 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,088 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,129 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,131 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,132 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,175 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,177 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,177 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,230 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,233 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,233 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,281 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,283 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,283 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,329 [salt.state               ][INFO    ] Loading fresh modules for state activity\n21:58:23,362 [salt.state               ][INFO    ] Executing state pkg.latest for openssh-server\n21:58:23,362 [cmdmod_module            ][INFO    ] Executing command dpkg-query --showformat=\'${Status} ${Package} ${Version}\n\' -W openssh-server in directory /root\n21:58:23,424 [cmdmod_module            ][DEBUG   ] stdout: install ok installed openssh-server 1:5.9p1-5ubuntu1\n21:58:23,425 [cmdmod_module            ][INFO    ] Executing command apt-cache -q show openssh-server | grep ^Version in directory /root\n21:58:23,438 [cmdmod_module            ][DEBUG   ] stdout: Version: 1:5.9p1-5ubuntu1\n21:58:23,438 [salt.state               ][INFO    ] No changes made for openssh-server\n21:58:23,439 [salt.state               ][INFO    ] Executing state group.present for ssh-users\n21:58:23,440 [salt.state               ][INFO    ] No changes made for ssh-users\n21:58:23,440 [salt.state               ][INFO    ] Executing state file.managed for /etc/ssh/sshd_config\n21:58:23,441 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,443 [salt.crypt               ][DEBUG   ] Decrypting the current master AES key\n21:58:23,444 [salt.crypt               ][DEBUG   ] Loaded minion key: /etc/salt/pki/minion.pem\n21:58:23,482 [salt.state               ][INFO    ] No changes made for /etc/ssh/sshd_config\n21:58:23,482 [salt.state               ][INFO    ] Executing state pkg.latest for mosh\n21:58:23,483 [cmdmod_module            ][INFO    ] Executing command dpkg-query --showformat=\'${Status} ${Package} ${Version}\n\' -W mosh in directory /root\n21:58:23,566 [cmdmod_module            ][DEBUG   ] stdout: install ok installed mosh 1.1.3-1\n21:58:23,566 [cmdmod_module            ][INFO    ] Executing command apt-cache -q show mosh | grep ^Version in directory /root\n21:58:23,580 [cmdmod_module            ][DEBUG   ] stdout: Version: 1.2.2-1~ubuntu12.04.1\nVersion: 1.1.3-1\n21:58:23,581 [cmdmod_module            ][INFO    ] Executing command apt-get -q update in directory /root\n21:58:31,200 [cmdmod_module            ][DEBUG   ] stdout: Ign http://us.archive.ubuntu.com precise InRelease\nIgn http://us.archive.ubuntu.com precise-updates InRelease\nIgn http://us.archive.ubuntu.com precise-backports InRelease\nIgn http://security.ubuntu.com precise-security InRelease\nIgn http://ppa.launchpad.net precise InRelease\nIgn http://extras.ubuntu.com precise InRelease\nIgn http://download.ebz.epson.net lsb3.2 InRelease\nHit http://us.archive.ubuntu.com precise Release.gpg\nHit http://us.archive.ubuntu.com precise-updates Release.gpg\nHit http://security.ubuntu.com precise-security Release.gpg\nHit http://ppa.launchpad.net precise Release.gpg\nHit http://extras.ubuntu.com precise Release.gpg\nIgn http://download.ebz.epson.net lsb3.2 Release.gpg\nHit http://us.archive.ubuntu.com precise-backports Release.gpg\nHit http://security.ubuntu.com precise-security Release\nHit http://ppa.launchpad.net precise Release\nHit http://us.archive.ubuntu.com precise Release\nHit http://extras.ubuntu.com precise Release\nIgn http://download.ebz.epson.net lsb3.2 Release\nHit http://us.archive.ubuntu.com precise-updates Release\nHit http://security.ubuntu.com precise-security/main Sources\nHit http://ppa.launchpad.net precise/main Sources\nHit http://extras.ubuntu.com precise/main Sources\nIgn http://download.ebz.epson.net lsb3.2/main amd64 Packages/DiffIndex\nHit http://us.archive.ubuntu.com precise-backports Release\nHit http://us.archive.ubuntu.com precise/main Sources\nHit http://us.archive.ubuntu.com precise/restricted Sources\nHit http://us.archive.ubuntu.com precise/universe Sources\nHit http://us.archive.ubuntu.com precise/multiverse Sources\nHit http://us.archive.ubuntu.com precise/main amd64 Packages\nHit http://security.ubuntu.com precise-security/restricted Sources\nHit http://security.ubuntu.com precise-security/universe Sources\nHit http://security.ubuntu.com precise-security/multiverse Sources\nHit http://security.ubuntu.com precise-security/main amd64 Packages\nHit http://security.ubuntu.com precise-security/restricted amd64 Packages\nHit http://security.ubuntu.com precise-security/universe amd64 Packages\nHit http://security.ubuntu.com precise-security/multiverse amd64 Packages\nHit http://security.ubuntu.com precise-security/main i386 Packages\nHit http://security.ubuntu.com precise-security/restricted i386 Packages\nHit http://security.ubuntu.com precise-security/universe i386 Packages\nHit http://us.archive.ubuntu.com precise/restricted amd64 Packages\nHit http://us.archive.ubuntu.com precise/universe amd64 Packages\nHit http://us.archive.ubuntu.com precise/multiverse amd64 Packages\nHit http://us.archive.ubuntu.com precise/main i386 Packages\nHit http://us.archive.ubuntu.com precise/restricted i386 Packages\nHit http://us.archive.ubuntu.com precise/universe i386 Packages\nHit http://us.archive.ubuntu.com precise/multiverse i386 Packages\nHit http://us.archive.ubuntu.com precise/main TranslationIndex\nHit http://us.archive.ubuntu.com precise/multiverse TranslationIndex\nHit http://ppa.launchpad.net precise/main amd64 Packages\nHit http://ppa.launchpad.net precise/main i386 Packages\nIgn http://ppa.launchpad.net precise/main TranslationIndex\nHit http://extras.ubuntu.com precise/main amd64 Packages\nHit http://extras.ubuntu.com precise/main i386 Packages\nIgn http://download.ebz.epson.net lsb3.2/main i386 Packages/DiffIndex\nIgn http://download.ebz.epson.net lsb3.2/main TranslationIndex\nHit http://us.archive.ubuntu.com precise/restricted TranslationIndex\nHit http://us.archive.ubuntu.com precise/universe TranslationIndex\nHit http://us.archive.ubuntu.com precise-updates/main Sources\nHit http://us.archive.ubuntu.com precise-updates/restricted Sources\nHit http://us.archive.ubuntu.com precise-updates/universe Sources\nHit http://us.archive.ubuntu.com precise-updates/multiverse Sources\nHit http://security.ubuntu.com precise-security/multiverse i386 Packages\nHit http://us.archive.ubuntu.com precise-updates/main amd64 Packages\nHit http://us.archive.ubuntu.com precise-updates/restricted amd64 Packages\nHit http://us.archive.ubuntu.com precise-updates/universe amd64 Packages\nHit http://us.archive.ubuntu.com precise-updates/multiverse amd64 Packages\nIgn http://extras.ubuntu.com precise/main TranslationIndex\nHit http://us.archive.ubuntu.com precise-updates/main i386 Packages\nHit http://us.archive.ubuntu.com precise-updates/restricted i386 Packages\nHit http://us.archive.ubuntu.com precise-updates/universe i386 Packages\nHit http://us.archive.ubuntu.com precise-updates/multiverse i386 Packages\nHit http://us.archive.ubuntu.com precise-updates/main TranslationIndex\nHit http://us.archive.ubuntu.com precise-updates/multiverse TranslationIndex\nHit http://security.ubuntu.com precise-security/main TranslationIndex\nHit http://security.ubuntu.com precise-security/multiverse TranslationIndex\nHit http://security.ubuntu.com precise-security/restricted TranslationIndex\nHit http://security.ubuntu.com precise-security/universe TranslationIndex\nHit http://us.archive.ubuntu.com precise-updates/restricted TranslationIndex\nHit http://us.archive.ubuntu.com precise-updates/universe TranslationIndex\nHit http://us.archive.ubuntu.com precise-backports/main Sources\nHit http://us.archive.ubuntu.com precise-backports/restricted Sources\nHit http://us.archive.ubuntu.com precise-backports/universe Sources\nHit http://us.archive.ubuntu.com precise-backports/multiverse Sources\nHit http://us.archive.ubuntu.com precise-backports/main amd64 Packages\nHit http://us.archive.ubuntu.com precise-backports/restricted amd64 Packages\nHit http://us.archive.ubuntu.com precise-backports/universe amd64 Packages\nHit http://us.archive.ubuntu.com precise-backports/multiverse amd64 Packages\nHit http://us.archive.ubuntu.com precise-backports/main i386 Packages\nHit http://us.archive.ubuntu.com precise-backports/restricted i386 Packages\nHit http://us.archive.ubuntu.com precise-backports/universe i386 Packages\nHit http://us.archive.ubuntu.com precise-backports/multiverse i386 Packages\nHit http://security.ubuntu.com precise-security/main Translation-en\nHit http://security.ubuntu.com precise-security/multiverse Translation-en\nHit http://security.ubuntu.com precise-security/restricted Translation-en\nHit http://us.archive.ubuntu.com precise-backports/main TranslationIndex\nHit http://us.archive.ubuntu.com precise-backports/multiverse TranslationIndex\nHit http://us.archive.ubuntu.com precise-backports/restricted TranslationIndex\nHit http://us.archive.ubuntu.com precise-backports/universe TranslationIndex\nHit http://us.archive.ubuntu.com precise/main Translation-en\nHit http://us.archive.ubuntu.com precise/multiverse Translation-en\nHit http://us.archive.ubuntu.com precise/restricted Translation-en\nHit http://us.archive.ubuntu.com precise/universe Translation-en\nHit http://us.archive.ubuntu.com precise-updates/main Translation-en\nHit http://us.archive.ubuntu.com precise-updates/multiverse Translation-en\nHit http://security.ubuntu.com precise-security/universe Translation-en\nHit http://us.archive.ubuntu.com precise-updates/restricted Translation-en\nHit http://us.archive.ubuntu.com precise-updates/universe Translation-en\nHit http://us.archive.ubuntu.com precise-backports/main Translation-en\nHit http://us.archive.ubuntu.com precise-backports/multiverse Translation-en\nHit http://us.archive.ubuntu.com precise-backports/restricted Translation-en\nHit http://us.archive.ubuntu.com precise-backports/universe Translation-en\nIgn http://ppa.launchpad.net precise/main Translation-en\nIgn http://extras.ubuntu.com precise/main Translation-en\nHit http://download.ebz.epson.net lsb3.2/main amd64 Packages\nHit http://download.ebz.epson.net lsb3.2/main i386 Packages\nIgn http://download.ebz.epson.net lsb3.2/main Translation-en\nReading package lists...\n21:58:31,205 [cmdmod_module            ][INFO    ] Executing command dpkg-query --showformat=\'${Status} ${Package} ${Version}\n\' -W  in directory /root\n21:58:31,281 [cmdmod_module            ][DEBUG   ] stdout: install ok installed accountsservice 0.6.15-2ubuntu9.3\ninstall ok installed acl 2.2.51-5ubuntu1\n\n...  (Trimming package list for reader\'s sanity, original available if needed) ...\n\ninstall ok installed module-init-tools 3.16-1ubuntu2\ninstall ok installed mosh 1.1.3-1\ninstall ok installed mount 2.20.1-1ubuntu3\n\n...\n\ninstall ok installed zlib1g 1:1.2.3.4.dfsg-3ubuntu4\n21:58:31,284 [cmdmod_module            ][INFO    ] Executing command apt-get -q -y  -o DPkg::Options::=--force-confold install mosh in directory /root\n21:58:31,955 [cmdmod_module            ][DEBUG   ] output: Reading package lists...\nBuilding dependency tree...\nReading state information...\nmosh is already the newest version.\n0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n21:58:31,955 [cmdmod_module            ][INFO    ] Executing command dpkg-query --showformat=\'${Status} ${Package} ${Version}\n\' -W  in directory /root\n21:58:32,130 [cmdmod_module            ][DEBUG   ] stdout: install ok installed accountsservice 0.6.15-2ubuntu9.3\ninstall ok installed acl 2.2.51-5ubuntu1\n\n...\n\ninstall ok installed module-init-tools 3.16-1ubuntu2\ninstall ok installed mosh 1.1.3-1\ninstall ok installed mount 2.20.1-1ubuntu3\n\n...\n\ninstall ok installed zlib1g 1:1.2.3.4.dfsg-3ubuntu4\n21:58:32,137 [salt.state               ][ERROR   ] No changes made for mosh\n21:58:32,137 [salt.state               ][INFO    ] Executing state service.running for ssh\n21:58:32,138 [cmdmod_module            ][INFO    ] Executing command service ssh status in directory /root\n21:58:32,146 [cmdmod_module            ][DEBUG   ] output: ssh start/running, process 29502\n21:58:32,147 [salt.state               ][INFO    ] No changes made for ssh\n21:58:32,148 [salt.minion              ][INFO    ] Returning information for job: 20120828215822848415\n```'
1888,'UtahDave',"Add daemonize_if to service.restart for the minion\nOn 08/25/2012 10:20 AM, Thomas S Hatch wrote:\r\n\r\nI don't think that we have the daemonization happening to allow for \r\n\r\n```\r\nsalt * service.restart salt-minion\r\n```\r\n\r\nto run cleanly.\r\nseems to be the case on ubuntu, using the command line 'salt service.restart salt-minion' terminates the minion and does not spawn a new minion. tested with develop and 0.10.2\r\n\r\n/var/log/salt/minion:\r\n15:35:05,916 [salt.minion ][WARNING ] Starting the Salt Minion\r\n16:02:50,380 [salt.minion ][WARNING ] Starting the Salt Minion\r\n19:53:48,398 [salt.minion ][WARNING ] Starting the Salt Minion\r\n19:53:48,398 [salt.minion ][DEBUG ] Attempting to authenticate with the Salt Master\r\n19:53:48,403 [salt.crypt ][DEBUG ] Loaded minion key: /etc/salt/pki/minion.pem\r\n19:53:48,412 [salt.crypt ][DEBUG ] Decrypting the current master AES key\r\n19:53:48,414 [salt.crypt ][DEBUG ] Loaded minion key: /etc/salt/pki/minion.pem\r\n19:53:49,006 [salt.minion ][INFO ] Authentication with master successful!\r\n19:53:49,011 [salt.crypt ][DEBUG ] Loaded minion key: /etc/salt/pki/minion.pem\r\n19:53:49,020 [salt.crypt ][DEBUG ] Decrypting the current master AES key\r\n19:53:49,021 [salt.crypt ][DEBUG ] Loaded minion key: /etc/salt/pki/minion.pem\r\n19:54:03,019 [salt.minion ][INFO ] User sean Executing command test.ping with jid 20120824195403012050\r\n19:54:03,020 [salt.minion ][DEBUG ] Command details {'tgt_type': 'glob', 'jid': '20120824195403012050', 'tgt': 'pearl', 'ret': '', 'user': 'sean', 'arg': [], 'fun': 'test.ping'}\r\n19:54:03,031 [salt.minion ][INFO ] Returning information for job: 20120824195403012050\r\n19:54:18,984 [salt.minion ][INFO ] User sean Executing command service.restart with jid 20120824195418977287\r\n19:54:18,985 [salt.minion ][DEBUG ] Command details {'tgt_type': 'glob', 'jid': '20120824195418977287', 'tgt': 'pearl', 'ret': '', 'user': 'sean', 'arg': ['salt-minion'], 'fun': 'service.restart'}\r\n19:54:19,001 [cmdmod_module ][INFO ] Executing command service salt-minion restart in directory /root\r\n\r\n/var/log/messages:\r\nAug 24 19:54:19 pearl kernel: [141949.486465] init: salt-minion main process (30058) killed by TERM signal\r\n"
1883,'UtahDave','root_dir is prepended to tempdir on Windows\nroot_dir is getting prepended to what I assume is a call to mktemp().\n\nWith ```root_dir: C:\\salt``` in Windows I get the following (after adding some debug logging)\n\n```\n[DEBUG   ] Loaded minion key: C:\\salt\\etc\\salt\\pki\\minion.pem\n[DEBUG   ] epub_uri: \'ipc://C:\\salt\\c:\\users\\admini~1\\appdata\\local\\temp\\2\\.salt\n-unix\\minion_event_pub.ipc\'\nTraceback (most recent call last):\n  File "c:\\Python27\\Scripts\\salt-minion", line 10, in <module>\n```\n\nThis is likely not an issue depending on how #1882 is resolved.'
1881,'whiteinge','state pip.removed does not work\nIt assumes the pip.list module will accept a \'packages\' kwarg, which it does not....\n\n    root@blueprint:~# salt-call state.single pip.removed name=pycrypto\n    [INFO    ] Loading fresh modules for state activity\n    [INFO    ] Executing state pip.removed for pycrypto\n    [ERROR   ] No changes made for pycrypto\n    local:\n    ----------\n        State: - pip\n        Name:      pycrypto\n        Function:  removed\n            Result:    False\n            Comment:   An exception occured in this state: Traceback (most recent call last):\n      File "/usr/lib/pymodules/python2.7/salt/state.py", line 823, in call\n        ret = self.states[cdata[\'full\']](*cdata[\'args\'])\n      File "/usr/lib/pymodules/python2.7/salt/states/pip.py", line 132, in removed\n        runas=user, cwd=cwd):\n    TypeError: list() got an unexpected keyword argument \'packages\'\n    \n            Changes:   \n        \nIt also tries to call pip.uninstall with an argument named "packages" which should be spelled "pkgs".\n\nPatch follows...\n\n    --- pip.py.orig 2012-08-02 10:52:31.000000000 -0700\n    +++ pip.py      2012-08-25 22:57:52.426181049 -0700\n    @@ -128,10 +128,9 @@\n         """\n     \n         ret = {\'name\': name, \'result\': None, \'comment\': \'\', \'changes\': {}}\n    -    if name not in __salt__["pip.list"](packages=name, bin_env=bin_env,\n    -                                        runas=user, cwd=cwd):\n    +    if name not in __salt__["pip.list"](bin_env=bin_env, runas=user, cwd=cwd):\n             ret["result"] = True\n    -        ret["comment"] = "Pacakge is not installed."\n    +        ret["comment"] = "Package is not installed."\n             return ret\n     \n         if __opts__[\'test\']:\n    @@ -139,7 +138,7 @@\n             ret[\'comment\'] = \'Package {0} is set to be removed\'.format(name)\n             return ret\n     \n    -    if __salt__["pip.uninstall"](packages=name,\n    +    if __salt__["pip.uninstall"](pkgs=name,\n                                      requirements=requirements,\n                                      bin_env=bin_env,\n                                      log=log,\n'
1879,'thatch45','state.file.append minimal and problematic file.contains\nIf you try appending using a state file, as examples, first:\n```\n# set variable identifying the chroot you work in (used in the prompt below)\nif [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then\n    debian_chroot=$(cat /etc/debian_chroot)\nfi\n```\nand then:\n```\n# enable bash completion in interactive shells\nif [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n    . /etc/bash_completion\nfi\n```\nfor which the expected output would be:\n```\n# set variable identifying the chroot you work in (used in the prompt below)\nif [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then\n    debian_chroot=$(cat /etc/debian_chroot)\nfi\n# enable bash completion in interactive shells\nif [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n    . /etc/bash_completion\nfi\n```\nyou actually end up with the following output:\n```\n# set variable identifying the chroot you work in (used in the prompt below)\nif [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then\n    debian_chroot=$(cat /etc/debian_chroot)\nfi\n# enable bash completion in interactive shells\nif [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n    . /etc/bash_completion\n```\n```diff\n  # set variable identifying the chroot you work in (used in the prompt below)\n  if [ -z "$debian_chroot" ] && [ -r /etc/debian_chroot ]; then\n      debian_chroot=$(cat /etc/debian_chroot)\n  fi\n  # enable bash completion in interactive shells\n  if [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n      . /etc/bash_completion\n- fi\n```\n\nThis problem is triggered by [this](https://github.com/saltstack/salt/blob/develop/salt/states/file.py#L1721), currently too simple check to avoid duplications in the file.\n\nI\'m checking how we can handle this better.'
1615,'whiteinge','virtualenv.managed is not installing requirements\nAt some point, my state tree stopped installing my virtualenv correctly. On the first highstate iteration, it creates the virtualenv, but will not populated it with the requirements file. On second run of highstate, it complies.\n\nHere is the relevant state tree snippet: https://gist.github.com/3099183\n\nI am using the Ubuntu ppa on Precise.'
1590,'whiteinge','docs for modules.mysql not showing query\nin: https://github.com/saltstack/salt/blob/develop/salt/modules/mysql.py\nThe function "query" I wrote has a doc, but that\'s not showing up in the auto-generated docs: http://salt.readthedocs.org/en/latest/ref/modules/all/salt.modules.mysql.html \n\n(It similarly didn\'t show up on my machine, when I regenerated docs)\n\nI have no clue why this would be. Can someone clue me in please?'
1415,'whiteinge',"Broken Link in documentation\nthis link:\n https://github.com/saltstack/salt/blob/v1.0.0.pre/salt/grains/core.py\n\ncurrently 404's on this page:\n http://salt.readthedocs.org/en/latest/topics/targeting/grains.html"
1265,'thatch45','Fixes for bugs causing minion to crash on Windows.\nThe first commit fixes a small bug that I believe appears on all operating systems, due to a missing paren.\n\nThe last two commits skip some checks that fail on Windows.'
1264,'cachedout','file.managed with \'makedirs: true\' has owner/group of root\nBelow is the state and the result I am getting. This is using version 0.9.9.1. The expected behavior should be to create the parent dir(s) with the same user/group as the file, unless it\'s declared differently elsewhere, like in a file.directory state declaration.\n\n    /home/kojiadmin/.koji/config:\n    file.managed:\n    - source: salt://koji/files/config\n    - user: kojiadmin\n    - group: kojiadmin\n    - mode: 644\n    - makedirs: true\n    - template: jinja\n    - context:\n    kojiweb_url: "http://koji2.egavas.org/koji/"\n    kojihub_url: "http://kojihub2.egavas.org/kojihub"\n    kojipkg_url: "http://kojihub2.egavas.org/mnt/koji/packages"\n    - require:\n    - user: kojiadmin\n     \n    ---------\n    [root@koji2 ~]# ls -ld /home/kojiadmin/.koji/\n    drwxr-xr-x. 2 root root 4096 May 11 17:50 /home/kojiadmin/.koji/\n    [root@koji2 ~]# ls -l /home/kojiadmin/.koji/\n    total 4\n    -rw-r--r--. 1 kojiadmin kojiadmin 540 May 11 17:50 config\n\n'
997,'seanchannel','salt-minion not installing properly on Ubuntu 10.04 x86 from PPA\nWhen installing 0.9.8 from PPA on Ubuntu 10.04 Server (x86) the following error occurs towards the end of the installation processs:\r\n\r\n```\r\nSetting up salt-minion (0.9.8-0ppa1ubuntu1) ...\r\nStarting salt-minion daemon: /usr/lib/pymodules/python2.6/salt/__init__.py:18: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\r\n  if e.message != \'No module named _msgpack\':\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-minion", line 7, in <module>\r\n    import salt\r\n  File "/usr/lib/pymodules/python2.6/salt/__init__.py", line 14, in <module>\r\n    import salt.config\r\n  File "/usr/lib/pymodules/python2.6/salt/config.py", line 23, in <module>\r\n    import salt.crypt\r\n  File "/usr/lib/pymodules/python2.6/salt/crypt.py", line 20, in <module>\r\n    import zmq\r\n  File "/usr/lib/pymodules/python2.6/zmq/__init__.py", line 35, in <module>\r\n    from zmq.utils import initthreads # initialize threads\r\nImportError: No module named utils\r\n```\r\n\r\nAfter the installation, if running "salt --version," salt is not showing as installed.'
923,'thatch45','salt-minion crashes when recursive requirement is present.\nToday I accidentally introduced a recursive requirement in my state files. This caused the salt-minion to crash:\r\n\r\n```python\r\n2012-03-16 15:43:09,301 [salt.minion    ][WARNING ] The minion function caused an exception: Traceback (most recent call last):\r\n File "/usr/lib/pymodules/python2.7/salt/minion.py", line 231, in _thread_return\r\n   ret[\'return\'] = self.functions[data[\'fun\']](*data[\'arg\'])\r\n File "/usr/lib/pymodules/python2.7/salt/modules/state.py", line 79, in highstate\r\n   return st_.call_highstate()\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 1049, in call_highstate\r\n   return self.state.call_high(high)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 712, in call_high\r\n   ret = self.format_verbosity(self.call_chunks(chunks))\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 559, in call_chunks\r\n   running = self.call_chunk(low, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n\r\n<Insert a lot of the same lines here>\r\n\r\n  File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 668, in call_chunk\r\n   running = self.call_chunk(chunk, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 634, in call_chunk\r\n   status = self.check_requisite(low, running, chunks)\r\n File "/usr/lib/pymodules/python2.7/salt/state.py", line 595, in check_requisite\r\n   if chunk[\'__id__\'] == req[req.keys()[0]] or \\\r\nRuntimeError: maximum recursion depth exceeded in cmp\r\n```\r\n\r\nHow to reproduce: Create a recursive requirement\r\n\r\nUnfortunatly, this is not detected and as such difficult to find if you have an extensive set of state files.'
922,'thatch45',"Document new split salt-{master,minion,syndic} packages in 0.9.8 release notes\nReferences #711\r\n\r\nUpgrades won't work properly. Users will have to manually install the new packages and purge the old ppa in favor of the official saltstack ppa."
786,'SEJeff','Make apt.py set a few more env vars\nhttps://github.com/puppetlabs/puppet/blob/master/lib/puppet/provider/package/apt.rb#L17\r\n\r\n'
612,'whiteinge','function file.sed not building command properly\nThe following yaml file segment:\r\n\r\n/etc/yum.repos.d/epel.repo:\r\n  file:\r\n    - sed\r\n    - before: \'enabled=1\'\r\n    - after: \'enabled=0\'\r\n    - backup: \'.bak\'\r\n\r\n\r\n...results in this command execution:\r\n\r\n[INFO    ] Executing command sed -n -r -e \'s/enabled=0/&/gp\' /etc/yum.repos.d/epel.repo in directory /root\r\n[DEBUG   ] enabled=0\r\nenabled=0\r\n\r\nThe sed command should be built as per the following code:\r\n\r\ndef sed(path, before, after, limit=\'\', backup=\'.bak\', options=\'-r -e\',\r\n        flags=\'g\'):\r\n    before = _sed_esc(before)\r\n    after = _sed_esc(after)\r\n    cmd = r"sed {backup}{options} \'{limit}s/{before}/{after}/{flags}\' {path}".format(\r\n            backup = \'-i{0} \'.format(backup) if backup else \'\',\r\n            options = options,\r\n            limit = \'/{0}/ \'.format(limit) if limit else \'\',\r\n            before = before,\r\n            after = after,\r\n            flags = flags,\r\n            path = path)\r\n    return __salt__[\'cmd.run\'](cmd)\r\n\r\nSo it should have resulted in:\r\nsed -i.bak -r -e \'s/enabled=1/enabled=0/g\' /etc/yum.repos.d/epel.repo\r\n\r\nIt looks like it\'s not seeing anything I define in the {backup} variable.  It\'s putting the value I designated as {after} in the {before} slot, and putting a literal "&" in the {after} slot.  I can\'t figure out where the -n is coming from in the command.\r\n\r\nIs this a bug in the construction of the command or is this me using the command improperly?\r\n\r\nCentOS57[root@ivwm21 ~]# salt-call --version\r\nsalt-call 0.9.6'
472,'whiteinge','Print error when trying to use states with no `file_roots` set\nIf you leave `file_roots` commented, for example, and try to call `state.highstate` it executes fine but returns no output.'
419,'thatch45','Allow `cmd` state to only run when triggered by another state\nThis would be to allow definition of `cmd` states that would only be run when another state changes.\r\n\r\nUse case: APT updating\r\n\r\n`apt.sls`\r\n\r\n    apt-update:\r\n      cmd:\r\n        - trigger\r\n        - name: /usr/bin/apt-get update\r\n\r\n`my.sls`\r\n\r\n    include:\r\n      - apt\r\n\r\n    /etc/apt/sources.list.d/fatbox.list:\r\n      file:\r\n        - managed\r\n        - source: salt://fatbox/debian/apt/fatbox.list\r\n        - owner: root\r\n        - group: root\r\n        - mode: 644\r\n\r\n    extend:\r\n      apt-update:\r\n        cmd:\r\n          - watch:\r\n            - file: /etc/apt/sources.list.d/fatbox.list'
403,'thatch45','add_host() does not add line break when the host already exists in /etc/hosts\nWhen adding a new alias to a known host in /etc/hosts, the function add_host() within modules/hosts.py does not append a line break the the newline variable. So in the new /etc/hosts file, multiple lines will be cancatenated together.\r\n\r\nAlso there are other issues (or needed improvement) on this function:\r\n\r\n* It does not check if the new alias already exist for the host, therefore can create duplicated aliases.\r\n* It would be better to make sure the FQDN name is listed as the first entry after IP, then hostname aliases. I remember I have read somewhere that some FQDN tools need this to be the case.'
389,'thatch45',"recurse doesn't transfer 0 byte files\nWhen pushing a recursion state with zero byte files in it, the zero byte files are not sent to the minion cache and the recursion fails with a path not found error when copying from cache to the target.  In this case the files failing were __init__.py files in python module folders, I did not perform tests with any other.  Removing the files allowed the recursion to complete."
344,'thatch45','Dynamic module downloads\nAs Develop stands Salt modules added during state loading will be automatically added to the state environment and made available for future state executions.\r\n\r\nBut, a simple dynamic module distribution system needs to be constructed - I am not quite sure how to best do this yet and am open so suggestions, as usual.'
330,'whiteinge','Make vcs module and state\nA module used to manage svn, git, hg, bzr etc and a state that follows suit'
184,'thatch45','Getting bad return data\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.7/multiprocessing/process.py", line 232, in _bootstrap\r\n    self.run()\r\n  File "/usr/lib/python2.7/site-packages/salt/master.py", line 286, in run\r\n    self.__bind()\r\n  File "/usr/lib/python2.7/site-packages/salt/master.py", line 249, in __bind\r\n    ret = salt.payload.package(self._handle_payload(payload))\r\n  File "/usr/lib/python2.7/site-packages/salt/master.py", line 259, in _handle_payload\r\n    \'clear\': self._handle_clear}[payload[\'enc\']](payload[\'load\'])\r\n  File "/usr/lib/python2.7/site-packages/salt/master.py", line 280, in _handle_aes\r\n    return self.aes_funcs.run_func(data[\'cmd\'], data)\r\nKeyError: \'cmd\'\r\n'
59,'thatch45','state system\nFrontends to state enforcement need to be constructed, under the states pacage we need frontend modules that call state modules in subdirs:\r\n\r\nsalt.states.pkg\r\n  loads:\r\n    salt.states.pkg.{pacman,yum,apt,etc...}\r\n\r\nthe states.pkg state module needs to set the default functions that need to be implemented in the distro specific modules'
45,'thatch45','Make threading configurable for the minion\nRight now in the minion when a publication is matched the execution happens in a thread. This should be configurable to execute in a multiprocess, so we need to add the option to the config, import multiprocessing in a try block and fall back to threading is multiprocessing is not available'
36,'thatch45','json output\nmake salt be able to make json output'
18359,'jfindlay','2014.7: Failed to authenticate message logging flood\nWhile debugging #18316 I noticed a log flood on the master:\r\n```\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n[DEBUG   ] Failed to authenticate message\r\n```\r\n\r\nAfter restarting this is fine for some time but then it happens again and never seems to be ending.'
18306,'jfindlay','salt-master should not be log error when not config gitfs_provider\n**OS:** CentOS\r\n**version-report**\r\n```\r\nsalt --versions-report\r\n           Salt: 2014.7.0\r\n         Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 2.2.0.1\r\n           RAET: Not Installed\r\n            ZMQ: 3.2.4\r\n           Mako: 0.5.0\r\n```\r\n\r\n**/etc/salt/master**\r\n\r\n```\r\nfileserver_backend:\r\n  - git\r\n  - roots\r\n# gitfs_provider: gitpython\r\ngitfs_remotes:\r\n   - https://github.com/saltstack-formulas/salt-formula.git\r\n```\r\n\r\nWhen start the salt-master, it will log a error info like below:\r\n> 2014-11-20 15:21:45,595 [salt.loaded.int.fileserver.gitfs           ][ERROR   ] Git fileserver backend is enabled in master config file, but could not be loaded, is GitPython installed?\r\n\r\nBut it is working actually. I think we should change the log message or the change the logical to not log such message.'
18302,'rallytime','pkgrepo ppa is not updating /etc/apt repositories, in Salt 2014.7\nI am running Salt 2014.7 (from ppa:saltstack/salt) on Ubuntu 14.04.\r\n\r\nUsing the following state, the PPA repository never gets added (as verified by looking under /etc/apt).  And as a result, you end up with an old version of Node (0.10.25).\r\n```\r\nnodejs:\r\n  pkgrepo:\r\n    - managed\r\n    - ppa: chris-lea/node.js\r\n    - require_in:\r\n      - pkg: nodejs\r\n  pkg:\r\n    - latest\r\n```\r\n\r\nUnsurprisingly, it\'s the same version of Node you get without using the PPA, by simply running:\r\n```\r\nsudo apt-get install nodejs\r\n```\r\n\r\nI know the PPA itself is not the problem, because manually adding the repository works fine, resulting in the latest version of Node (0.10.33):\r\n```\r\nsudo apt-get remove nodejs\r\nsudo apt-get autoremove\r\nsudo add-apt-repository ppa:chris-lea/node.js \r\nsudo apt-get update\r\nsudo apt-get install nodejs\r\nnode --version\r\n```\r\n\r\nI have other pkgrepo states that don\'t use `ppa` -- and instead use `name` / `file` / `key_url` -- and those correctly update the repositories under /etc/apt.\r\n\r\nAdding a `file` statement to the `ppa` version did not fix the problem.\r\n\r\nI verified this Ubuntu 14.04 machine has python-software-properties 0.92 (and not 0.94) -- by running `dpkg -l python-software-properties` -- so this seems different from Issue #17105.\r\n\r\nThe above `ppa` based state file worked fine in Salt 2014.1.\r\n\r\nFor completeness: I\'m executing Salt using `salt-call --local ...` though I doubt that\'s the root problem.\r\n\r\nMy /var/log/salt/minion shows the following in Salt 2014.7:\r\n```\r\n[salt.state       ][INFO    ] Running state [nodejs] at time 23:25:09.003979\r\n[salt.state       ][INFO    ] Executing state pkgrepo.managed for nodejs\r\n[salt.state       ][INFO    ] {\'repo\': \'ppa:chris-lea/node.js\'}\r\n[salt.state       ][INFO    ] Completed state [nodejs] at time 23:25:09.089735\r\n[salt.state       ][INFO    ] Running state [nodejs] at time 23:25:09.090318\r\n[salt.state       ][INFO    ] Executing state pkg.latest for nodejs\r\n[salt.loaded.int.module.cmdmod][INFO    ] Executing command \'apt-get -q update\' in directory \'/home/ubuntu\'\r\n[salt.loaded.int.module.cmdmod][INFO    ] Executing command [\'apt-cache\', \'-q\', \'policy\', \'nodejs\'] in directory \'/home/ubuntu\'\r\n[salt.loaded.int.module.cmdmod][INFO    ] Executing command [\'apt-get\', \'-q\', \'-y\', \'-o\', \'DPkg::Options::=--force-confold\', \'-o\', \'DPkg::Options::=--force-confdef\', \'install\', \'nodejs\'] in directory \'/home/ubuntu\'\r\n[salt.utils       ][ERROR   ] DNS lookup of \'salt\' failed.\r\n[salt.minion      ][ERROR   ] Master hostname: \'salt\' not found. Retrying in 30 seconds\r\n[salt.loaded.int.module.cmdmod][INFO    ] Executing command "dpkg-query --showformat=\'${Status} ${Package} ${Version} ${Architecture}\\n\' -W" in directory \'/home/ubuntu\'\r\n[salt.state       ][INFO    ] Installed Packages:\r\nlibc-ares2 changed from absent to 1.10.0-2\r\nnodejs changed from absent to 0.10.25~dfsg2-2ubuntu1\r\nlibv8-3.14.5 changed from absent to 3.14.5.8-5ubuntu2                                                                                                                                                                                                                       \r\n[salt.state       ][INFO    ] Loading fresh modules for state activity\r\n[salt.state       ][INFO    ] Completed state [nodejs] at time 23:25:21.380615\r\n```\r\n'
18286,'cachedout','Runner early-exit warning\nException AttributeError: "\'NoneType\' object has no attribute \'LINGER\'" in <bound method RunnerEvent.__del__ of <salt.utils.event.RunnerEvent object at 0x7f29627fd110>> ignored\r\n'
18199,'thatch45','OpenHub Rank by Commits\nNot too far away from Google:\r\nhttps://www.openhub.net/explore/orgs?filter=commercial\r\n![screen shot 2014-11-18 at 4 06 25 am](https://cloud.githubusercontent.com/assets/3374962/5084749/50ca95d6-6ed8-11e4-91ed-eebcd66e6f79.png)\r\n\r\n\r\nMost Hot Project:\r\nhttps://www.openhub.net/explore/projects\r\n![screen shot 2014-11-18 at 4 04 41 am](https://cloud.githubusercontent.com/assets/3374962/5084735/29d2af54-6ed8-11e4-990c-2415ed4e98f1.png)\r\n\r\n@thatch45 @rallytime @basepi @whiteinge @s0undt3ch @cro @techhat '
18131,'thatch45','fileclient.py#get_url ignores HTTP Auth (2014.1 -> 2014.7 regression)\nIn Salt 2014.1 the following would successfully fetch a file:\r\n```\r\nsalt-call cp.get_file_str \'https://salt:password2@example.com/blah.xml\'\r\n```\r\n\r\nUpon upgrading to Salt 2014.7 a 401 error is encountered with the same command:\r\n```\r\n$ salt-call cp.get_file_str \'https://salt:password2@example.com/blah.xml\'\r\n[INFO    ] Starting new HTTPS connection (1): example.com\r\nlocal:\r\n    {\r\n      "errors" : [ {\r\n        "status" : 401,\r\n        "message" : "Authentication is required."\r\n      } ]\r\n    }\r\n\r\n```\r\n\r\n```\r\n$ salt-call --versions-report\r\n           Salt: 2014.7.0\r\n         Python: 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.3.1\r\n           RAET: Not Installed\r\n            ZMQ: 3.2.4\r\n           Mako: Not Installed\r\n```\r\n\r\nI initially thought this might be related to #14359, but on closer inspection the server is not performing a redirect and the same configuration worked on 2014.1 without issues unlike what is described there.'
18116,'cachedout','__progress__ breaks runners/state.py tests\nThe changes made by PR #17725 are causing failures with the tests for `salt/runners/state.py` on develop. The test failures are:\r\n- integration.runners.state.ManageTest.test_over_parse_req_fail\r\n```\r\n       Traceback (most recent call last):\r\n         File "/root/SaltStack/salt/tests/integration/runners/state.py", line 43, in test_over_parse_req_fail\r\n           self.assertTrue(any(item in ret for item in items))\r\n       AssertionError: False is not true\r\n```\r\n- integration.runners.state.ManageTest.test_over_req_fail\r\n```\r\n       Traceback (most recent call last):\r\n         File "/root/SaltStack/salt/tests/integration/runners/state.py", line 31, in test_over_req_fail\r\n           self.assertTrue(any(item in ret for item in items))\r\n       AssertionError: False is not true\r\n```\r\nping @cachedout '
18089,'jfindlay','salt.states.sysctl.present Exception\nOperating System: **Centos 7** running **salt-master-2014.7.0-3.el6.noarch**\r\n\r\nState File:\r\n\r\n```\r\nkeepalived:\r\n  pkg:\r\n    - installed\r\n\r\nhaproxy:\r\n  pkg:\r\n    - installed\r\n\r\nnet.ipv4.ip_nonlocal_bind:\r\n  sysctl.present:\r\n    - value: 1\r\n```\r\n\r\n```\r\n[root@monitoring01 salt]# salt --versions-report\r\n           Salt: 2014.7.0\r\n         Python: 2.6.6 (r266:84292, Jan 22 2014, 09:42:36)\r\n         Jinja2: 2.2.1\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.3.1\r\n           RAET: Not Installed\r\n            ZMQ: 3.2.4\r\n           Mako: Not Installed\r\n[root@monitoring01 salt]#\r\n```\r\n\r\n```\r\n----------\r\n          ID: net.ipv4.ip_nonlocal_bind\r\n    Function: sysctl.present\r\n      Result: False\r\n     Comment: An exception occurred in this state: Traceback (most recent call last):\r\n                File "/usr/lib/python2.7/site-packages/salt/state.py", line 1533, in call\r\n                  **cdata[\'kwargs\'])\r\n                File "/usr/lib/python2.7/site-packages/salt/states/sysctl.py", line 56, in present\r\n                  configured = __salt__[\'sysctl.show\'](config_file=True)\r\n                File "/usr/lib/python2.7/site-packages/salt/modules/linux_sysctl.py", line 86, in show\r\n                  for line in salt.utils.fopen(config_file_path):\r\n                File "/usr/lib/python2.7/site-packages/salt/utils/__init__.py", line 1065, in fopen\r\n                  fhandle = open(*args, **kwargs)\r\n              IOError: [Errno 2] No such file or directory: \'/etc/sysctl.d/99-salt.conf\'\r\n     Started: 21:41:13.185549\r\n    Duration: 73.698 ms\r\n     Changes:\r\n```\r\n\r\nI currently have a work around for the problem by adding\r\n\r\n```\r\n/etc/sysctl.d/99-salt.conf:\r\n  file.touch\r\n```\r\n\r\nIn this instance I think it would be in the modules best interest to ensure the file it is trying to access already exists. I have looked at the code in the develop branch and it appears as though the issue still exists. *I have not run the above state against the develop branch.*\r\n\r\nI am not very strong with Python, but I am happy to give a fix a crack and submit a pull request if it would help.\r\n\r\nCheers,\r\n\r\nJon'
18083,'jfindlay','salt-ssh commands are mostly broken after SaltStack update\nI recently updated SaltStack and Salt-SSH on an Oracle Linux server (with the yum install commands for both).  Now salt-ssh commands are mostly broken.  When I run this:\r\nsalt-ssh \'*\' test.ping\r\nI get "retcode: 1" and this is the most recent part of the traceback:\r\n\r\n    File "/tmp/.root_778742__salt/urllib3/connectionpool.py", line 63, in <module> from     \r\n    .packages.ssl_match_hostname import match_hostname, CertificateError\r\n    ImportError: No module named packages.ssl_match_hostname stdout:\r\n\r\nI therefore did this:\r\n    pip install backports.ssl_match_hostname\r\n\r\nBut that requirement was already satisfied.  I\'m using Python 2.6.6.\r\n\r\nWhy won\'t the salt-ssh commands work now?  A test.ping is very simple.  The salt-ssh -r (raw shell commands) still work.  It all broke after I updated the Salt Master and salt-ssh.'
18058,'ssgward',"Salt-SSH: Error when trying to run a state file that contains file.recurse\nI'm getting an error when trying to run a state file that contains file.recurse. Here is the state file:\r\n```\r\n/opt/junk/:\r\n  file:\r\n    - recurse\r\n    - source: salt://kibana\r\n    - clean: True\r\n```\r\n\r\nThis is the command I'm running: ``salt-ssh \\* state.sls recurse``\r\n\r\nHere is the error:\r\n\r\n![screen shot 2014-11-12 at 5 22 05 pm](https://cloud.githubusercontent.com/assets/8439595/5034097/f2cf6356-6b28-11e4-810c-fe8fd3732dce.png)\r\n\r\nHere is the Salt Versions report:\r\n```\r\n salt --versions-report\r\n           Salt: 2014.7.0\r\n         Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.10\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.0.1\r\n           RAET: Not Installed\r\n            ZMQ: 4.0.4\r\n           Mako: Not Installed\r\n```\r\n"
18047,'ssgward','windows pkg.install fails\nUnable to get windows pkg.install to work and after command sent windows minion loops.\r\n\r\nAfter the failure I have to restart the minion.\r\n\r\n**Windows**\r\nsalt-minion.exe 2014.1.13 (Hydrogen)\r\n\r\n**Linux**\r\nsalt-master 2014.1.13 (Hydrogen)\r\n\r\n**Test command**\r\n```bash\r\nsalt \'AO1-818190\' pkg.refresh_db\r\nsalt \'AO1-818190\' pkg.install \'Firefox\'\r\n```\r\n**/etc/salt/master**\r\n```bash\r\nwin_repo: \'/srv/salt/base/win/repo\'\r\nwin_repo_mastercachefile: \'/srv/salt/base/win/repo/winrepo.p\'\r\nfile_roots:\r\n  base:\r\n    - /srv/salt/base\r\n```\r\nChanges to win_repo* was made after finding https://github.com/saltstack/salt/issues/10894\r\n\r\n**File location**\r\n```bash\r\nroot@control:/srv/salt/base/win/repo/firefox# ls -l\r\ntotal 59816\r\n-rw-r--r-- 1 root root 29014160 Nov 13 12:34 Firefox_Setup_29_0_1.exe\r\n-rw-r--r-- 1 root root 32229000 Nov 13 12:34 Firefox_Setup_31_0esr.exe\r\n-rw-r--r-- 1 root root      565 Nov 13 12:49 init.sls\r\n```\r\n\r\n```bash\r\nroot@control:/srv/salt/base/win/repo/firefox# cat init.sls\r\nFirefox:\r\n  29.0.1:\r\n    installer: \'salt://win/repo/firefox/Firefox_Setup_29_0_1.exe\'\r\n    full_name: \'Mozilla Firefox 29.0.1 (x86 en-US)\'\r\n    reboot: False\r\n    install_flags: \' /s \'\r\n    uninstaller: \'C:\\Program Files (x86)\\Mozilla Firefox\\uninstall\\helper.exe\'\r\n    uninstall_flags: \' /S\'\r\n  31.0esr:\r\n    installer: \'salt://win/repo/firefox/Firefox_Setup_31_0esr.exe\'\r\n    full_name: \'Mozilla Firefox 31.0 (x86 en-US)\'\r\n    reboot: False\r\n    install_flags: \' /s \'\r\n    uninstaller: \'C:\\Program Files (x86)\\Mozilla Firefox\\uninstall\\helper.exe\'\r\n    uninstall_flags: \' /S\'\r\n```\r\n**Windows Minion Output**\r\n```bash\r\nC:\\salt>salt-minion.exe -l debug\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[INFO    ] Using cached minion ID from c:\\salt\\conf\\minion_id: AO1-818190\r\n[DEBUG   ] Configuration file path: c:\\salt\\conf\\minion\r\n[INFO    ] Setting up the Salt Minion "AO1-818190"\r\n[DEBUG   ] Created pidfile: c:\\salt\\var\\run\\salt-minion.pid\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[DEBUG   ] Attempting to authenticate with the Salt Master at 10.156.47.20\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[INFO    ] Authentication with master at 10.156.47.20 successful!\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Reading configuration from c:\\salt\\conf\\minion\r\n[DEBUG   ] Loaded win_shadow as virtual shadow\r\n[DEBUG   ] Loaded win_ip as virtual ip\r\n[DEBUG   ] Loaded win_disk as virtual disk\r\n[DEBUG   ] Loaded win_status as virtual status\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded win_autoruns as virtual autoruns\r\n[DEBUG   ] Loaded win_system as virtual system\r\n[DEBUG   ] Loaded win_firewall as virtual firewall\r\n[DEBUG   ] Loaded zcbuildout as virtual buildout\r\n[DEBUG   ] Loaded win_pkg as virtual pkg\r\n[DEBUG   ] Loaded win_network as virtual network\r\n[DEBUG   ] Loaded win_ntp as virtual ntp\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded win_file as virtual file\r\n[DEBUG   ] Loaded virtualenv_mod as virtual virtualenv\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] Loaded win_service as virtual service\r\n[DEBUG   ] Loaded win_useradd as virtual user\r\n[DEBUG   ] Loaded win_timezone as virtual timezone\r\n[DEBUG   ] Loaded win_repo as virtual winrepo\r\n[DEBUG   ] Loaded win_groupadd as virtual group\r\n[DEBUG   ] Loaded couchdb_return as virtual couchdb\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] Loaded sqlite3_return as virtual sqlite3\r\n[DEBUG   ] schedule.clean_proc_dir: checking job {\'tgt_type\': \'glob\', \'jid\': \'20141113134306452414\', \'tgt\': \'AO1-818190\', \'pid\': 2456, \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'Firefox\', {\'version\': \'31.0esr\', \'__kwarg__\': True}], \'fun\':\r\n\'pkg.install\'} for process existence\r\n[DEBUG   ] schedule.clean_proc_dir: checking job {\'tgt_type\': \'glob\', \'jid\': \'20141113134355829971\', \'tgt\': \'AO1-818190\', \'pid\': 2456, \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134306452414\'], \'fun\': \'saltutil.find_job\'} for proce\r\nss existence\r\n[DEBUG   ] I am AO1-818190 and I am not supposed to start any proxies.\r\n[INFO    ] Minion is starting as user \'Network Services\'\r\n[DEBUG   ] Minion \'AO1-818190\' trying to tune in\r\n[DEBUG   ] Minion PUB socket URI: tcp://127.0.0.1:4510\r\n[DEBUG   ] Minion PULL socket URI: tcp://127.0.0.1:4511\r\n[DEBUG   ] Generated random reconnect delay between \'1000ms\' and \'11000ms\' (6170)\r\n[DEBUG   ] Setting zmq_reconnect_ivl to \'6170ms\'\r\n[DEBUG   ] Setting zmq_reconnect_ivl_max to \'11000ms\'\r\n[DEBUG   ] Running scheduled job: __mine_interval\r\n[DEBUG   ] schedule: This job was scheduled with jid_include, adding to cache (jid_include defaults to True)\r\n[DEBUG   ] schedule: This job was scheduled with a max number of 2\r\n[DEBUG   ] Loaded win_shadow as virtual shadow\r\n[DEBUG   ] Loaded win_ip as virtual ip\r\n[DEBUG   ] Loaded win_disk as virtual disk\r\n[DEBUG   ] Loaded win_status as virtual status\r\n[DEBUG   ] Loaded sysmod as virtual sys\r\n[DEBUG   ] Loaded win_autoruns as virtual autoruns\r\n[DEBUG   ] Loaded win_system as virtual system\r\n[DEBUG   ] Loaded win_firewall as virtual firewall\r\n[DEBUG   ] Loaded zcbuildout as virtual buildout\r\n[DEBUG   ] Loaded win_pkg as virtual pkg\r\n[DEBUG   ] Loaded win_network as virtual network\r\n[DEBUG   ] Loaded win_ntp as virtual ntp\r\n[DEBUG   ] Loaded cmdmod as virtual cmd\r\n[DEBUG   ] Loaded win_file as virtual file\r\n[DEBUG   ] Loaded virtualenv_mod as virtual virtualenv\r\n[DEBUG   ] Loaded djangomod as virtual django\r\n[DEBUG   ] Loaded win_service as virtual service\r\n[DEBUG   ] Loaded win_useradd as virtual user\r\n[DEBUG   ] Loaded win_timezone as virtual timezone\r\n[DEBUG   ] Loaded win_repo as virtual winrepo\r\n[DEBUG   ] Loaded win_groupadd as virtual group\r\n[DEBUG   ] Loaded couchdb_return as virtual couchdb\r\n[DEBUG   ] Loaded carbon_return as virtual carbon\r\n[DEBUG   ] Loaded sqlite3_return as virtual sqlite3\r\n[DEBUG   ] schedule.handle_func: adding this job to the jobcache with data {\'fun\': \'mine.update\', \'jid\': \'20141113134446\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[INFO    ] User sudo_andrewc Executing command pkg.refresh_db with jid 20141113134501207965\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134501207965\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [], \'fun\': \'pkg.refresh_db\'}\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[DEBUG   ] Decrypting the current master AES key\r\n[DEBUG   ] Loaded minion key: c:\\salt\\conf\\pki\\minion\\minion.pem\r\n[INFO    ] Returning information for job: 20141113134501207965\r\n[INFO    ] User sudo_andrewc Executing command pkg.install with jid 20141113134512065179\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134512065179\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'Firefox\'], \'fun\': \'pkg.install\'}\r\n[DEBUG   ] Initializing COM library\r\n[DEBUG   ] Uninitializing COM library\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134517077987\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134517077987\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134517077987\r\n[DEBUG   ] Fetching file from saltenv \'base\', ** attempting ** \'salt://win/repo/firefox/Firefox_Setup_31_0esr.exe\'\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134522357408\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134522357408\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134522357408\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134528279019\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134528279019\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134528279019\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134534148276\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134534148276\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134534148276\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134539957519\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134539957519\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134539957519\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134545904964\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134545904964\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134545904964\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134551367876\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134551367876\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134551367876\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134556898132\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134556898132\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134556898132\r\n[INFO    ] User sudo_andrewc Executing command saltutil.find_job with jid 20141113134602290116\r\n[DEBUG   ] Command details {\'tgt_type\': \'glob\', \'jid\': \'20141113134602290116\', \'tgt\': \'AO1-818190\', \'ret\': \'\', \'user\': \'sudo_andrewc\', \'arg\': [\'20141113134512065179\'], \'fun\': \'saltutil.find_job\'}\r\n[INFO    ] Returning information for job: 20141113134602290116\r\n```'
17960,'rallytime',"Write tests to verify the existence of dunder variables\nWe should write a test state that verified the existence of all dunder vars. (i.e. __salt__, __context__, __pillar__ etc) to prevent problems when things change in the loader. We don't want to break the injection logic."
17958,'jfindlay',"unpriv salt.client.LocalClient.get_cache_returns spins forever\nA non-privileged user trying to retrieve results of a job will quietly spin forever on a CPU.\r\n\r\nThe following code reproduces the problem on 2014.1.13:\r\n```\r\nimport salt.client\r\nlocal = salt.client.LocalClient()\r\nvalid_jobid = '20141112100557387760'\r\nlocal.get_cache_returns(valid_jobid)\r\n```\r\n\r\nLooks like we're just blindly [ignoring the permissions error](https://github.com/saltstack/salt/blob/2014.1/salt/client/__init__.py#L775).\r\n"
17957,'pitatus',"salt always fails the first time after a restart\nI am testing salt master and minion on the same machine.\r\nEvery time I run : \r\n```\r\n   >eonSpider ~ # service salt-master restart; service salt-minion restart;\r\n   salt-master stop/waiting\r\n   salt-master start/running, process 21317\r\n   salt-minion stop/waiting\r\n   salt-minion start/running, process 21327\r\n```\r\nthe next command after restarting will not print anything, I should call the command 2 or 3 times to show a result : \r\n```\r\nExample\r\n   > eonSpider ~ # salt eonSpider state.highstate\r\n```\r\nWill show nothing.\r\n\r\nThe second time (sometimes the third) the command works.\r\n\r\nAnother important thing : \r\nSometimes the number of :\r\n\r\n     ps -ef|grep salt|grep -v 'grep'|wc -l\r\n\r\nwill increase everytime I make a restart to master and minion.\r\n\r\n\r\n"
17956,'jfindlay','Salt-minion consuming all memory and not freeing it when running twadmin\nWhen using salt to run twadmin to generate a key when a key already exists all memory is consumed by the minion and not released even when the twadmin process is killed.\r\n\r\nWhen a key already exists twadmin prompts for an overwrite:\r\n```\r\n/usr/sbin/twadmin --generate-keys -P twpassword --local-keyfile /etc/tripwire/myhost-local.key\r\nThe local key file: "/etc/tripwire/myhost-local.key" exists. Overwrite (Y/N)?\r\n```\r\nWhen run outside of tripwire twadmin looks like this while waiting:\r\n```\r\nread(0,\r\n```\r\nIt just stays at that prompt forever, not consuming any extra memory.\r\n\r\nWhen run via salt it does this non-stop:\r\n```\r\nread(0, "", 1024)                       = 0\r\nwrite(1, "The local key file: \\"/etc/tripwi"..., 85) = 85\r\nread(0, "", 1024)                       = 0\r\nwrite(1, "The local key file: \\"/etc/tripwi"..., 85) = 85\r\n```\r\nTwadmin doesn\'t increase in memory, but the salt-minion process does. Killing the twadmin process does not free the salt-minion memory:\r\n\r\nBefore killing twadmin, after 45 seconds of running:\r\n```\r\n24441 root      15   0 2256m 1.9g 2360 S 71.9 94.5   0:31.12 salt-minion\r\n24467 root      18   0 20772 1236  600 R 68.9  0.1   0:26.86 twadmin\r\n```\r\nOne minute after killing twadmin, after 60 seconds of running:\r\n```\r\n24441 root      16   0 3105m 1.9g 1332 S  6.2 95.7   0:48.26 salt-minion\r\n```\r\nIt will stay that way until the salt-minion is killed.\r\n```\r\n           Salt: 2014.7.0\r\n         Python: 2.6.8 (unknown, Nov  7 2012, 14:47:45)\r\n         Jinja2: 2.5.5\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.12\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.3\r\n        libnacl: Not Installed\r\n         PyYAML: 3.08\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.3.1\r\n           RAET: Not Installed\r\n            ZMQ: 4.0.4\r\n           Mako: Not Installed\r\n```\r\nAlso tested with the official version from the EPEL 5 repo 2014.1.13.'
17286,'rallytime','ValueError: unknown url type: top.sls while running saltutil.sync_all\nWhile running saltutil.sync_all, I often hit the following issue:\r\n\r\n    root@ip-172-31-42-67:/srv/salt# salt \'*\' saltutil.sync_all\r\n    cchung-idx:\r\n        ----------\r\n        grains:\r\n        modules:\r\n        outputters:\r\n        renderers:\r\n        returners:\r\n        states:\r\n    cchung-sh:\r\n        The minion function caused an exception: Traceback (most recent call last):\r\n          File "salt/minion.py", line 809, in _thread_return\r\n          File "salt/modules/saltutil.py", line 343, in sync_all\r\n          File "salt/modules/saltutil.py", line 228, in sync_modules\r\n          File "salt/modules/saltutil.py", line 73, in _sync\r\n          File "salt/modules/saltutil.py", line 55, in _get_top_file_envs\r\n          File "salt/state.py", line 2042, in get_top\r\n          File "salt/state.py", line 1915, in get_tops\r\n          File "salt/fileclient.py", line 143, in cache_file\r\n          File "salt/fileclient.py", line 537, in get_url\r\n          File "urllib2.py", line 127, in urlopen\r\n          File "urllib2.py", line 396, in open\r\n          File "urllib2.py", line 258, in get_type\r\n        ValueError: unknown url type: top.sls\r\n     cchung-spl86889-idx:\r\n        ----------\r\n        grains:\r\n        modules:\r\n        outputters:\r\n        renderers:\r\n        returners:\r\n        states:\r\n    cchung-spl86889-sh:\r\n        The minion function caused an exception: Traceback (most recent call last):\r\n          File "salt/minion.py", line 809, in _thread_return\r\n          File "salt/modules/saltutil.py", line 344, in sync_all\r\n          File "salt/modules/saltutil.py", line 247, in sync_states\r\n          File "salt/modules/saltutil.py", line 73, in _sync\r\n          File "salt/modules/saltutil.py", line 55, in _get_top_file_envs\r\n          File "salt/state.py", line 2042, in get_top\r\n          File "salt/state.py", line 1915, in get_tops\r\n          File "salt/fileclient.py", line 143, in cache_file\r\n          File "salt/fileclient.py", line 537, in get_url\r\n          File "urllib2.py", line 127, in urlopen\r\n          File "urllib2.py", line 396, in open\r\n          File "urllib2.py", line 258, in get_type\r\n        ValueError: unknown url type: top.sls\r\n\r\nsalt versions:\r\n\r\n    root@ip-172-31-42-67:/srv/salt# salt --versions-report\r\n               Salt: 2014.1.10\r\n             Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n             Jinja2: 2.7.2\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.3.0\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6.1\r\n             PyYAML: 3.10\r\n              PyZMQ: 14.0.1\r\n                ZMQ: 4.0.4\r\n\r\n    root@ip-172-31-42-67:/srv/salt# salt \'*\' test.versions_report\r\n    cchung-spl86889-idx:\r\n                   Salt: 2014.1.11\r\n                 Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n                 Jinja2: 2.7.1\r\n               M2Crypto: 0.21.1\r\n         msgpack-python: 0.4.2\r\n           msgpack-pure: Not Installed\r\n               pycrypto: 2.6\r\n                 PyYAML: 3.11\r\n                  PyZMQ: 14.1.1\r\n                    ZMQ: 4.0.4\r\n\r\nmaster is running on ubuntu 14.04, and minions are running on windows2008R2, I dont get why top.sls sometimes get passed as an url?\r\nCould someone help me out? We need a stabler behavior before we can move our deployment to next step, any help will be really appreciated.\r\nThanks!\r\n'
17227,'rallytime','During summary report UnicodeEncodeError: \'latin-1\' codec can\'t encode\nSteps to reproduce:\r\n\r\n```\r\nsudo salt-call cmd.run \'echo "Je peux manger du verre, ça ne me fait pas mal."\'\r\n```\r\n\r\nor\r\n\r\n```\r\nsudo salt-call cmd.run \'echo "私はガラスを食べられます。それは私を傷つけません。"\'\r\n\r\n[ERROR   ] An un-handled exception was caught by salt\'s global exception handler:\r\nUnicodeEncodeError: \'latin-1\' codec can\'t encode characters in position 26798-26800: ordinal not in range(256)\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 82, in salt_call\r\n    client.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 319, in run\r\n    caller.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/caller.py", line 148, in run\r\n    self.opts)\r\n  File "/usr/lib/pymodules/python2.7/salt/output/__init__.py", line 49, in display_output\r\n    print(display_data)\r\nUnicodeEncodeError: \'latin-1\' codec can\'t encode characters in position 26798-26800: ordinal not in range(256)\r\nTraceback (most recent call last):\r\n  File "/usr/bin/salt-call", line 11, in <module>\r\n    salt_call()\r\n  File "/usr/lib/pymodules/python2.7/salt/scripts.py", line 82, in salt_call\r\n    client.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/__init__.py", line 319, in run\r\n    caller.run()\r\n  File "/usr/lib/pymodules/python2.7/salt/cli/caller.py", line 148, in run\r\n    self.opts)\r\n  File "/usr/lib/pymodules/python2.7/salt/output/__init__.py", line 49, in display_output\r\n    print(display_data)\r\nUnicodeEncodeError: \'latin-1\' codec can\'t encode characters in position 26798-26800: ordinal not in range(256)\r\nvagrant@precise64:~$ locale\r\nLANG=en_US.UTF-8\r\nLANGUAGE=\r\nLC_CTYPE="en_US"\r\nLC_NUMERIC="en_US"\r\nLC_TIME="en_US"\r\nLC_COLLATE="en_US"\r\nLC_MONETARY="en_US"\r\nLC_MESSAGES="en_US"\r\nLC_PAPER="en_US"\r\nLC_NAME="en_US"\r\nLC_ADDRESS="en_US"\r\nLC_TELEPHONE="en_US"\r\nLC_MEASUREMENT="en_US"\r\nLC_IDENTIFICATION="en_US"\r\nLC_ALL=en_US\r\n\r\nvagrant@precise64:~$ salt-call --version\r\nsalt-call 2014.1.13 (Hydrogen)\r\n```\r\nI downgraded to 2014.1.10 and it works fine. Seems like states that print unicode are causing a problem when salt tries to report the final state summary'
17164,'cachedout','Explore using linbacl keys on zmq\nWhich would substantially speed up the master and lower many deps'
17162,'cachedout','Raet Local Client Finish\nMake it not have all the problems of salt-zmq'
17161,'basepi','Pass metadata into jobs from all entry points\nWe may need to talk to @garethgreenaway about his'
17160,'cachedout','Add Django callback to returners\nWe need to sit down and figure out the best way to do this with @pass-by-value '
17159,'basepi','Extend salt-ssh scan roster\nSome of the stuff is the scan roster needs to be more configurable, please see @thatch45'
17158,'cro','Remove non namespaced events\nthey are past deprecation warnings'
17040,'cachedout','Run execution modules as invoking user\nWe want the minion to be smart enough to execute any remote execution function call as the user sending the command if specified in the pub'
16901,'rallytime',"Request for state run from the minion\nThe idea here is to allow for the master to send a minion a state run but not run it, instead run it with test=True, and save the output data and the compiled data. Then the state run could be executed by locally via salt-call at the local admin's digression.\r\n\r\nWe will also need to allow the local admin to view the proposed states and the expected changes, re-run with test=True and exclude any IDs or names in the accrual application of the states."
16847,'basepi',"salt-ssh hangs on some remote hosts and does not timeout\nHi, I have a weird issue with salt-ssh where it hangs some Amazon Linux instances. On some machines I have to disable requiretty and then it works, on others requiretty is present (I also have the option set to true in the roster file).\r\n\r\nI am stuck on a couple of instances where I get no response from this command\r\n    \r\n        sudo salt-ssh '*' test.ping\r\n\r\nI get some debug with lots of output and the last lines are:\r\n\r\n        > done\r\n        WARNING: Unable to locate current thin version.\r\n        _edbc7885e4f9aac9b83b35999b68d015148caf467b78fa39c05f669c0ff89878\r\n        deploy\r\n\r\nMy salt version are:\r\n\r\n    Salt: 2014.7.0rc5\r\n    Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n    Jinja2: 2.7.3\r\n    M2Crypto: 0.21.1\r\n    msgpack-python: 0.3.0\r\n    msgpack-pure: Not Installed\r\n    pycrypto: 2.6.1\r\n    libnacl: Not Installed\r\n    PyYAML: 3.10\r\n    ioflo: Not Installed\r\n    PyZMQ: 14.3.1\r\n    RAET: Not Installed\r\n    ZMQ: 4.0.4\r\n    Mako: 0.9.1\r\n\r\nWhat should I do to debug this further?"
16835,'techhat',"[salt.cloud] Switch to Salt SSH\nThis issue replaces saltstack/salt-cloud#853.\r\n\r\nWe need to switch Salt Cloud from its own internal SSH code to Salt SSH. This will allow both programs to take advantage of one another's feature sets.\r\n\r\nNot everything that Salt Cloud does with SSH is available yet in Salt SSH. Additionally, some other features need to be added to Salt SSH to round out the experience. We need:\r\n\r\n* Full support for sudo users and passwords (both API and command line)\r\n* Full support for passing SSH and sudo users/password and SSH keys (API and command line)\r\n* Ability to change the path to the sudo command (API and command line)\r\n* Support for passing additional options to sudo (API and command line)\r\n* Support for using SSH agents (see saltstack/salt#15338)\r\n\r\nCC: @thatch45"
16780,'terminalmage','There is no control for hard link in states.file\nA temporary solution is to add two files and use the same source.\r\n\r\nBut it is better to have to direct control for hard link.'
16751,'cro',' "salt.utils.fopen(path, \'rb\')" may cause error\nhere is my scene:\r\n     in salt\\modules\\saltutil.py line  421 ,\r\n     function _read_proc_file\r\n     line:  with salt.utils.fopen(path, \'rb\') as fp_:\r\n                     buf = fp_.read()\r\n                     fp_.close()\r\n                     if buf:\r\n                           data = serial.loads(buf)   #mspack load here\r\n                     else:\r\n----------------------------------------------------code finish-------------------------------------------------\r\n if  path\'s file\'content is: \r\n\r\n劊fun玬ine.update\ue5b0id?0141017165429682000\ue5b6id?\r\n寓id\ue719VR6175\r\n\r\n      if use \'rb\' mode to read the file,buf will be:\r\n\r\n\r\n劊fun玬ine.update\ue5b0id?0141017165429682000\ue5b6id?\r\n\r\n寓id\ue719VR6175\r\n\r\n\r\nThe content readed contain a \'\\r\' more than the orginal content(on windows 7 plat-form, \'\\n\' will change to \'\\r\\n\' through the  open(file,\'rb\').read()  ), and this cause a error when the file is mspack\'s format,because  an additional \'\\r\' causes mspack loading failed.\r\n\r\n\r\nBut I try to modify the \'rb\' mode to \'r\' mode, the problem miss. (\'\\n\' still to be \'\\n\' through the open(file,\'r\').read() )\r\nSo is it  a  real problem?\r\n'
16679,'whiteinge',"Inconsistencies in function names for things that send commands down to minions\nThe ``cmd`` prefix for sending commands to minions in the reactor doesn't match the same thing in ``salt-api`` (which goes by ``local``). There's a related problem in the Orchestrate runner with the generic ``function`` and ``state`` function names. It would be nice to have a consistent way to refer to these.\r\n\r\nPerhaps add an alias to the reactor for ``local`` -> ``cmd``?"
16607,'cro','Race running aptitude update at the same time as list_pkgs\nI haven\'t dived into the code, but we get the exception below _once in a while_ when pkg.list_pkgs is running at the same time as an aptitude update.\r\n\r\n    An exception occurred in this state: Traceback (most recent call last):\r\n      File "/usr/lib/python2.7/dist-packages/salt/state.py", line 1379, in call\r\n        **cdata[\'kwargs\'])\r\n      File "/usr/lib/python2.7/dist-packages/salt/states/pkg.py", line 468, in installed\r\n        **kwargs)\r\n      File "/usr/lib/python2.7/dist-packages/salt/states/pkg.py", line 130, in _find_install_targets\r\n        cur_pkgs = __salt__[\'pkg.list_pkgs\'](versions_as_list=True, **kwargs)\r\n      File "/usr/lib/python2.7/dist-packages/salt/modules/aptpkg.py", line 703, in list_pkgs\r\n        virtpkgs_all = _get_virtual()\r\n      File "/usr/lib/python2.7/dist-packages/salt/modules/aptpkg.py", line 132, in _get_virtual\r\n        apt_cache = apt.cache.Cache()\r\n      File "/usr/lib/python2.7/dist-packages/apt/cache.py", line 102, in __init__\r\n        self.open(progress)\r\n      File "/usr/lib/python2.7/dist-packages/apt/cache.py", line 145, in open\r\n        self._cache = apt_pkg.Cache(progress)\r\n     SystemError: E:Problem renaming the file /var/cache/apt/srcpkgcache.bin.w9sHov to /var/cache/apt/srcpkgcache.bin - rename (2: No such file or directory), E:Problem renaming the file /var/cache/apt/pkgcache.bin.2wWm3u to /var/cache/apt/pkgcache.bin - rename (2: No such file or directory), E:The package lists or status file could not be parsed or opened.\r\n\r\nWe\'re running Wheezy (7.6) with python-apt 0.8.8.2, and Hydrogen. This problem seems to have cropped up since we started rolling out Hydrogen.'
16592,'cro',"salt-cp fails with large files, cp.get_file succeeds \nsalt-cp of files larger than 1MB fails no matter how big a --timeout value is used. The minion never gets a job/jid, small files copy just fine. \r\n\r\nOn the same host and minion combination salt 'minion' cp.get_file salt://file /tmp/file succeeds. \r\n\r\n    $ sudo  salt-call test.versions_report\r\nlocal:\r\n               Salt: 2014.1.11\r\n             Python: 2.7.8 (default, Oct  2 2014, 17:06:07)\r\n             Jinja2: 2.7.3\r\n           M2Crypto: 0.22\r\n     msgpack-python: 0.4.2\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6.1\r\n             PyYAML: 3.11\r\n              PyZMQ: 14.3.1\r\n                ZMQ: 4.0.4\r\n"
16588,'UtahDave','State tomcat.wait unavailable under windows\nMinion OS: Windows Server 2008 R2\r\nMaster OS: CentOS 6\r\nSalt version: 2014.1.10\r\n\r\nIt seems the tomcat states (http://docs.saltstack.com/en/latest/ref/states/all/salt.states.tomcat.html#salt.states.tomcat.wait) are not available under windows.\r\n\r\n"State tomcat.wait found in sls test is unavailable."'
16285,'terminalmage','Nested gitfs_* options are not always strings\nIn some cases nested gitfs_remotes options are read as non-string values from the master config.\r\n\r\n**Example**\r\n```yaml\r\ngitfs_remotes:\r\n  - git@github.com:Vye/gitfstest.git:\r\n    - base: 2014.7\r\n```\r\n\r\nThe above config will not work because YAML converts ```2014.7``` to a float which will get you a nasty exception on the master every time it tries to retrieve that revision.\r\n\r\n```\r\n[ERROR   ] Exception \'float\' object has no attribute \'startswith\' occurred in file server update\r\nTraceback (most recent call last):\r\n  File "/usr/lib/python2.6/site-packages/salt/daemons/masterapi.py", line 216, in fileserver_update\r\n    fileserver.update()\r\n  File "/usr/lib/python2.6/site-packages/salt/fileserver/__init__.py", line 312, in update\r\n    self.servers[fstr]()\r\n  File "/usr/lib/python2.6/site-packages/salt/fileserver/gitfs.py", line 993, in update\r\n    find_file\r\n  File "/usr/lib/python2.6/site-packages/salt/fileserver/__init__.py", line 237, in reap_fileserver_cache_dir\r\n    ret = find_func(filename, saltenv=saltenv)\r\n  File "/usr/lib/python2.6/site-packages/salt/fileserver/gitfs.py", line 1153, in find_file\r\n    tree = _get_tree_gitpython(repo, tgt_env)\r\n  File "/usr/lib/python2.6/site-packages/salt/fileserver/gitfs.py", line 359, in _get_tree_gitpython\r\n    commit = repo[\'repo\'].rev_parse(tgt_env)\r\n  File "/usr/lib/python2.6/site-packages/git/repo/fun.py", line 124, in rev_parse\r\n    if rev.startswith(\':/\'):\r\nAttributeError: \'float\' object has no attribute \'startswith\'\r\n```\r\nA simple work around I\'ve used is to wrap the value in single quotes to force YAML to return a string.\r\n\r\nWhen troubleshooting I noticed that nested gitfs config options are not being type checked. gitfs_remotes is verified to be ```list``` but other values within like gitfs_base and gitfs_mountpoint are not verified.\r\n\r\nIt may make sense to add type checking to these nested options so that a sane error is produced. It may also be appropriate for salt to force these values to be strings?\r\n\r\n**salt version report**\r\n```\r\n           Salt: 2014.7.0-378-g0780ba4\r\n         Python: 2.7.5 (default, Jun 17 2014, 18:11:42)\r\n         Jinja2: 2.7.3\r\n       M2Crypto: 0.22\r\n msgpack-python: 0.4.2\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n        libnacl: Not Installed\r\n         PyYAML: 3.11\r\n          ioflo: Not Installed\r\n          PyZMQ: 14.3.1\r\n           RAET: Not Installed\r\n            ZMQ: 4.0.4\r\n```'
16223,'rallytime','Test salt-call without minion running\nWe discovered a bug whereby ```salt-call``` run where it communicates with a master without a salt minion also running was broken. We need to test this functionality and ensure that this works properly, even without a populated PKI dir.\r\n\r\ncc: @rallytime '
16124,'cro','Allow EAuth authorization to live in an external db\n'
16123,'cro','Extend eauth \nAllow authorization to be read from Django and eauth.'
15995,'techhat',"Allow manage.bootstrap to accept keys on master\nCurrently the manage.bootstrap runner only installs salt on a remote machine. It should be able to generate and lay down keys for the machine, as well as config files. There's vast amounts of functionality available in salt-bootstrap that we're not taking advantage of."
15994,'techhat','Change manage.bootstrap from subprocess.call()\nCurrently the manage.bootstrap runner uses `subprocess.call()`. We should be able to change it to use salt-ssh, or even just salt.utils.vt directly.'
15764,'cachedout','Python client API for access to Salt SSH\nSalt-API access to Salt SSH (REST API)'
15759,'cachedout','Cache runner and wheel jobs\nRunner and wheel execution results need to end up in the job cache and be available to returners.'
15741,'cachedout',"Optimize cloud-cache\nThere's cache code in salt.utils.cloud that needs a review with an eye toward optimization."
14872,'UtahDave',"Add an installer hash option to verify installer integrity\nPer @UtahDave in https://github.com/saltstack/salt-winrepo/issues/59 I'm reporting this here as well.\r\n\r\nAllow a sha or md5 hash to be specified as part of the package definition and then ensure the downloaded installer matches that hash.\r\n\r\nThis is for the Windows Package Management specification."
14659,'cachedout',"Exception handling and failed bootstrapping in salt-cloud\nWhen you are running salt-cloud with a map file, tons of these exceptions cause the entire script to abort. If you are running in parallel mode, it bails out on installing salt on any machines that were being spun up, and you end up with nodes that have booted up, and no way of knowing which of them have salt installed (other than re-running your map, comparing the list of running nodes to the list of nodes that respond to a test.ping).\r\n\r\nIn fact, there are tons of reasons why salt-cloud will boot a minion, but the salt install will fail, and it would be really nice if there were a way to easily get that list. As it stands, I have to go manually run the list through something like excel, and manually delete and then re-run my map file. It's either that or manually install salt on those boxes. A more elegant solution would be VERY welcome.\r\n\r\nIs it possible to have an interface to make_salty on minions that have launched in salt-cloud but failed to bootstrap salt?\r\n\r\nThe following is a list of exceptions that have caused my map file to abort, resulting in a confusing configuration of my nodes.\r\n\r\n- scp is not installed on my minion, and the bootstrap script fails to transfer (resolved with the new sftp option, but the exception can still occur)\r\n- I hit the node limit on one of my providers. \r\n- I included a provider in my map file that does not parallelize well (GoGrid, for example, however the gogrid.py file can and should be rewritten to support parallelization), a GoGrid minion pitches an exception, and the command aborts\r\n- One of my api requests to boot a minion hangs and times out\r\n\r\nI can provide additional detail if necessary. I encounter this issue on a regular basis."
14612,'cachedout','Catch provider errors in salt cloud\n```\r\nmp@silver /etc/salt % sudo salt-cloud -p linode_1024 d1  \r\n[INFO    ] salt-cloud starting\r\n[INFO    ] Creating Cloud VM d1\r\n[ERROR   ] Error creating d1 on LINODE\r\n\r\nThe following exception was thrown by libcloud when trying to run the initial deployment: \r\nA Linode label may only contain ASCII letters or numbers, dashes, and underscores, must begin and end with letters or numbers, and be at least three characters in length.\r\nTraceback (most recent call last):\r\n  File "/home/mp/Devel/salt/salt/cloud/clouds/linode.py", line 194, in create\r\n    data = conn.create_node(**kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/libcloud/compute/drivers/linode.py", line 313, in create_node\r\n    self.connection.request(API_ROOT, params=params)\r\n  File "/usr/local/lib/python2.7/dist-packages/libcloud/common/base.py", line 675, in request\r\n    response = responseCls(**kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/libcloud/common/linode.py", line 105, in __init__\r\n    raise self.errors[0]\r\nLinodeException: (8) A Linode label may only contain ASCII letters or numbers, dashes, and underscores, must begin and end with letters or numbers, and be at least three characters in length.\r\nError: There was a profile error: Failed to deploy VM\r\n```\r\n\r\nCatch and format in a friendlier way.'
14150,'UtahDave','saltutil.kill_job fails on Windows minion\nOn a Windows minion, trying to kill a hung job fails:\r\n\r\n```\r\n$ salt \'web013\' saltutil.kill_job 20140711221222434111\r\nweb013:\r\n    The minion function caused an exception: Traceback (most recent call last):\r\n      File "salt/minion.py", line 793, in _thread_return\r\n      File "salt/modules/saltutil.py", line 499, in kill_job\r\n    AttributeError: \'module\' object has no attribute \'SIGKILL\'\r\n```\r\n\r\nThe minion is Windows Server 2012 R2. Minion version is 2014.1.3. Master version is 2014.1.5.'
13898,'techhat','[salt.cloud] Make ephemeral volumes available to non-spot instances\nThis is in ec2, with the BlockDeviceMapping setting (set in the ex_blockdevicemappings variable).'
13857,'basepi',"git revisions should be expanded before comparing against current\npassing a partial sha as a `rev` to `git.latest` results in an unnecessary checkout on each run after the full sha at `HEAD` is compared to the `rev` argument. if it is determined that `rev` is not a branch or tag name, it should be expanded for comparison against the full sha at `head`. i'm not setup for local salt development (yet) but i thought i would throw this up in the event it needed to be patched before helium. here's the patch i threw in to make sure that the error was fixable:\r\n\r\n```diff\r\ndiff --git a/salt/states/git.py b/salt/states/git.py\r\nindex d0029ee..22f854b 100644\r\n--- a/salt/states/git.py\r\n+++ b/salt/states/git.py\r\n@@ -174,10 +174,13 @@ def latest(name,\r\n             if len(branch) > 0 and branch == rev:\r\n                 remote_rev = __salt__['git.ls_remote'](target,\r\n                                                        repository=name,\r\n                                                        branch=branch, user=user,\r\n                                                        identity=identity)\r\n+            # expand partial sha's\r\n+            elif len(rev) < 40:\r\n+              rev = __salt__['git.revision'](target, rev=rev, user=user)\r\n\r\n             # only do something, if the specified rev differs from the\r\n             # current_rev and remote_rev\r\n             if current_rev in [rev, remote_rev]:\r\n                 new_rev = current_rev\r\n```\r\ni'll open a proper pull request whenever i get a chance to setup a working development environment locally."
13756,'basepi',"Use --trivial-only when removing/purging packages with apt\nI am very excited about the saltstack and i am using it on  approximately  one thousand machines. \r\nHowever couple of days ago i was faced with a problem. \r\nSalt when running on machine and have such lines in sls formula:  \r\n \t\r\n       pkg. purged:\r\n\t\t- pkgs:\r\n\t\t\t- somepkgname \r\n   \r\n\r\n  will purge all packages which depends on somepkgname.  Sometimes it could be crucial when purging some package having  such  reverse  dependencies. \r\nAnd  assming salt is a  CMS, which could be run on ALL hosts simultaneously it could be even fatal.  \r\n\r\nIn our practice and in our scripts we always use one option for apt-get utility which is called  --trivial-only\r\n\r\n       --trivial-only\r\n           Only perform operations that are 'trivial'. Logically this can be considered related to --assume-yes, where --assume-yes will answer yes to any prompt, --trivial-only will answer no.\r\n           Configuration Item: APT::Get::Trivial-Only.\r\n\r\nUsing this option we NEVER purge packages which depend on  somepkgname  and thus have more confidence that we can not ruin production at once. \r\n\r\nThe reason i write to you is that i haven’t found where this option (or maybe similar) could be written in configs or sls files ( or maybe some patch is required ) ? \r\n\r\nI mean is there any way of  editing default options (flags) to install/purge packages ?   \r\nFor example default install command  is the following one\r\n      \r\n           apt-get -q -y -o DPkg::Options::=--force-confold -o DPkg::Options::=--force-confdef install somepkgname\r\n\r\nHow could I change / add new options in this command ?  \r\nСould you please make this point clear for me ? \r\n "
13755,'whiteinge','Latest offline pdf documentation is broken\nLink https://media.readthedocs.org/pdf/salt/latest/salt.pdf leaves to file, containing only saltstack logo.'
13714,'terminalmage',"Add support for custom delimiter when targeting\nThis should be targeted for the Lithium release.\r\n\r\nThere are a number of different aspects of Salt which use the targeting system. They include (but are not limited to) the following:\r\n\r\n* CLI\r\n* Reactor\r\n* Orchestration\r\n* Top file (both states and pillar)\r\n* API ?\r\n\r\nIf I've left any out, let me know and I'll add them.\r\n\r\nI have begun work on this and will be submitting a RFC pull request shortly."
13698,'whiteinge','Add tests to serve as a minimum specification for REST netapi modules\nWrite integration tests that can be used as as specification for minimum functionality in a REST API. The minimum specification should be:\r\n\r\n1. The root URL (``/``) lowstate interface of the rest_wsgi, rest_cherrypy, and rest_tornado modules.\r\n2. The event SSE stream.\r\n\r\nnetapi modules are encouraged to experiment with appropriate interfaces around that spec.'
13658,'thatch45','libvirt keys non functional\nFor context see: https://groups.google.com/forum/#!topic/salt-users/vvG-XFj0ST0\r\n\r\nWhen using the documentation found here: http://docs.saltstack.com/en/latest/topics/tutorials/cloud_controller.html to push out libvirt keys to each minion a silent failure happens. The keys are never pushed out to the minions nor are they on the salt master any place. This causes the libvirt keys to not function as it should.'
13558,'s0undt3ch','Minion sometimes stops logging after logrotate\nIt appears that after a logrotate has occurred the minion stops logging:\r\n```\r\nroot@shared01:/var/log/salt# ls -alh\r\ntotal 360K\r\ndrwxr-xr-x  2 root root   4,0K jun 15 06:42 .\r\ndrwxrwxr-x 14 root syslog 4,0K jun 19 06:37 ..\r\n-rw-------  1 root root      0 jun 15 06:42 minion\r\n-rw-------  1 root root    75K jun 11 09:58 minion.1.gz\r\n-rw-------  1 root root    13K jun 10 10:47 minion.2.gz\r\n-rw-------  1 root root   2,4K jun  5 16:01 minion.3.gz\r\n-rw-------  1 root root    43K mei 26 10:58 minion.4.gz\r\n-rw-------  1 root root    133 mei 14 01:40 minion.5.gz\r\n-rw-------  1 root root    72K mei  7 10:05 minion.6.gz\r\n-rw-------  1 root root   134K mei  2 14:50 minion.7.gz\r\n```\r\nI have checked another minion and there it is logging properly though. \r\n\r\nLogging has not explicitly been configured in the minion configuration so it uses the default values. \r\n\r\nVersion report:\r\n```\r\n           Salt: 2014.1.5\r\n         Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.0.1\r\n            ZMQ: 4.0.4\r\n```'
13532,'terminalmage',"ssh_auth.present corrupts SSH1 RSA key type in authorized_keys\nWhen trying to create authorized_keys file with multiple key types using ssh_auth.present, some key formats are successfully added to authorized_keys and others are corrupted.  Here is an example salt/users.sls file:\r\n\r\n    # Add ssh keys to .ssh/authorized_keys file\r\n    ssh_auth.present:\r\n      - user: {{ username }}\r\n      - names:\r\n      {% for pub_ssh_key in details.get('authorized_keys', []) %}\r\n        - {{ pub_ssh_key }}\r\n      {% endfor %}\r\n\r\n\r\nAnd here is snippet from pillar/users.sls file with user data:\r\n\r\n    users:\r\n      testuser:\r\n        fullname: Test User\r\n        authorized_keys:\r\n          - '1024 33 144422644861843994149619420106137732514008548930774773431048102615325103713228623632622293816792497851685026704241232686514999440638829956630606671633729814637279217493779748030844279649003942654466692515888266764791316825159621676076980607350129143822822848093331893791313143593925530373430183166331165329709 test@test.com'\r\n          - ssh-dss AAAAB3NzaC1kc3MAAACBAKmhDem1AYZXVZH+t3bY9h2xVjoQ0XutRhdZEnbjfy71eTjiPiAnyGuFUIouhSQMapwe6zf7zAYrhOyirr3Aak+wFGavo1+NdkWni7FInrRdN80VXg4JKA+/6c1P9lvn6xyypHcSTJNzTgDEJG5kwu0mSZ/Q5Hbcq3R0Yh+GIevnAAAAFQCCq66LQdNRw/5KeFU0w7DL7NBqBwAAAIA3P+CrSMALHRkeLrQhELxaWj1+VW8PXua61KMJwgc1lgZmlpEpGSXrXCi6HeGRTQTQIwMHx5QesT1/DRhab8lIlZ8+RMMg/kjjGqCYcvtH7unFZsbUBqnzPeuw8jPnJycAes92frY6onrMtrHLxNuOY4jeFhPTy5J9QCihUYbc2AAAAIA2/i1AkMYlazmgbsyGfjWWELhF91LqD0zbsKNDu+7rPBFpB9ucS5ayQRC9q1uh9aGxp95CuxPkVd4Kw44+PQAcNu7606ta1lhFo0EXLopz+C5WbA5or2QhDEdZIRfgwVq1x0su0jZXQ9FgZw+XtCG3O8fklHO+COXvfQEFSsaqKQ== Test User\r\n          - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDOHt6EgWyeKQ3tRUjvQ1LzSmq0on6HVRBC6RxpjRm7qgfGlNlSO5EtHz7kUsjUwdJEdpp7UPzj6jIL2Rvwii00hHdVjy9izESIpprpWZf8ItI0/SMIJz6uRTeupIHnEaNiLp0BNaNAjy7KtwlIg5PN82CSFqzGuuYGTGqrK6DxMYz3mRWNPcrDYH2baHiBtAqhAIt1eu7tG59kKbK2YtROldIj7CHOruhQdR36K6wTc6Jv53wAZGRQP7Hxx9C8TSBxon/hJQ7QTsZCs6CKPW7d55b+/kZxLyla9wPaATjJz8Cp4Ivq3XRVZKWtA13UIF1IUntrsxxMqjqiwWARdtlj test@test.com\r\n\r\n\r\nRunning this results in the following ~/.ssh/authorized_keys file:\r\n\r\n    ssh-dss AAAAB3NzaC1kc3MAAACBAKmhDem1AYZXVZH+t3bY9h2xVjoQ0XutRhdZEnbjfy71eTjiPiAnyGuFUIouhSQMapwe6zf7zAYrhOyirr3Aak+wFGavo1+NdkWni7FInrRdN80VXg4JKA+/6c1P9lvn6xyypHcSTJNzTgDEJG5kwu0mSZ/Q5Hbcq3R0Yh+GIevnAAAAFQCCq66LQdNRw/5KeFU0w7DL7NBqBwAAAIA3P+CrSMALHRkeLrQhELxaWj1+VW8PXua61KMJwgc1lgZmlpEpGSXrXCi6HeGRTQTQIwMHx5QesT1/DRhab8lIlZ8+RMMg/kjjGqCYcvtH7unFZsbUBqnzPeuw8jPnJycAes92frY6onrMtrHLxNuOY4jeFhPTy5J9QCihUYbc2AAAAIA2/i1AkMYlazmgbsyGfjWWELhF91LqD0zbsKNDu+7rPBFpB9ucS5ayQRC9q1uh9aGxp95CuxPkVd4Kw44+PQAcNu7606ta1lhFo0EXLopz+C5WbA5or2QhDEdZIRfgwVq1x0su0jZXQ9FgZw+XtCG3O8fklHO+COXvfQEFSsaqKQ== \r\n\r\n    ssh-rsa 1024 \r\n\r\n    ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDOHt6EgWyeKQ3tRUjvQ1LzSmq0on6HVRBC6RxpjRm7qgfGlNlSO5EtHz7kUsjUwdJEdpp7UPzj6jIL2Rvwii00hHdVjy9izESIpprpWZf8ItI0/SMIJz6uRTeupIHnEaNiLp0BNaNAjy7KtwlIg5PN82CSFqzGuuYGTGqrK6DxMYz3mRWNPcrDYH2baHiBtAqhAIt1eu7tG59kKbK2YtROldIj7CHOruhQdR36K6wTc6Jv53wAZGRQP7Hxx9C8TSBxon/hJQ7QTsZCs6CKPW7d55b+/kZxLyla9wPaATjJz8Cp4Ivq3XRVZKWtA13UIF1IUntrsxxMqjqiwWARdtlj test@test.com\r\n\r\n\r\nAs you can see in the resulting authorized_keys file ssh-rsa, and ssh-dss keys are in good format, although ssh-dss is missing the comment string.  The ssh1-rsa type key is invalid, and contains only the first fragment from the original data.  Also, I'm not sure why there are blank lines between the keys in the resulting authorized_keys file."
13477,'UtahDave',"win_repo location is ignored.\nSo it seems in the base environment it's using /srv/salt/base/win/repo, out of \r\n\r\n    file_roots: \r\n      base:\r\n        - /srv/salt/base \r\n\r\n rather than the win_repo variable down the bottom.\r\n\r\n    win_repo: '/srv/salt/win/repo/'         "
13476,'pass-by-value','LDAP and eauth fails for non-root users, tries to read ldap configuration file (thus, bind password)\nLocal machine is both a salt master and openldap server.\r\nI\'ve setup LDAP & pam via ldap on local machine (ubuntu 14.04)\r\n\r\nRunning\r\n````\r\nroot@saltmaster:~ # salt -a ldap \'owl.gr1*\' test.ping\r\n```\r\n\r\nworks fine. But, running as user "sivann" (an ldap user), we get this:\r\n```\r\nsivann@saltmaster:~$ salt -a ldap \'owl.gr1*\' test.ping\r\nUsage: salt [options] \'<target>\' <function> [arguments]\r\nsalt: error: Failed to load configuration: [Errno 13] Permission denied: \'/etc/salt/master.d/ldap.conf\'\r\n```\r\n\r\nThe problem is ldap.conf contains the LDAP Bind password and should not be read by normal users.\r\n\r\n'
13329,'techhat',"EC2 list_nodes_min is too invasive\nThis looks to be an issue with any provider that has `list_nodes_min()`. When running any generic query (`-Q`, `-F` or `-S`), the default function called is `list_nodes_min()`. This doesn't happen when calling `-f list_nodes*()` directly.\r\n\r\nHopefully an easy fix, looking directly at command line args to see if a generic query was called, and if so, run the appropriate function."
13258,'s0undt3ch','Logging Internals Doc Missing\nThe "Logging Internals" only have the words "TODO":\r\nhttp://docs.saltstack.com/en/latest/topics/development/logging.html\r\n\r\n@s0undt3ch ping - looks like you did the logging implementation in #6953. Can you take a look?'
13151,'terminalmage','Better error reporting for file.mknod states\nSee https://github.com/saltstack/salt/issues/13045#issuecomment-44651337 for background info. The state should be modified to fail with a more descriptive error message when the desired type is different than the existing type.'
13044,'UtahDave','Firewall module not available in windows\nI\'m trying to open ports in the windows firewall, I\'ve tried both:\r\nhttp://docs.saltstack.com/en/latest/ref/states/all/salt.states.win_firewall.html#module-salt.states.win_firewall\r\n\r\nwith the following config:\r\n```\r\nelasticsearch.install:\r\n  firewall.add_rule:\r\n    - name: "Elasticsearch"\r\n    - localport: \'8081,9300-9310\' # 10 ports for 10 nodes, use pillar data here later on\r\n    - protocol: \'tcp\'\r\n    - action: \'allow\'\r\n    - dir: \'in\'\r\n```\r\nResult is:\r\n``` Comment: State firewall.add_rule found in sls elasticsearch.install is unavailable```\r\n\r\nand with the command:\r\nhttp://docs.saltstack.com/en/latest/ref/modules/all/salt.modules.win_firewall.html\r\n```\r\nsalt rs-sm1 firewall.add_rule "Elasticsearch" "tcp" "8081,9300-9310"\r\n```\r\nBut here I get the error: ``` \'firewall.add_rule\' is not available. ```\r\n\r\nThe minion is running Windows 2012 x64 server in Azure and the versions report is:\r\n```\r\nroot@saltmaster:/srv# salt --versions-report\r\n           Salt: 2014.1.4\r\n         Python: 2.7.6 (default, Mar 22 2014, 22:59:56)\r\n         Jinja2: 2.7.2\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.0.1\r\n            ZMQ: 4.0.4\r\nroot@saltmaster:/srv# salt rs-sm1 test.versions_report\r\nrs-sm1:\r\n               Salt: 2014.1.4\r\n             Python: 2.7.5 (default, May 15 2013, 22:44:16) [MSC v.1500 64 bit (AMD64)]\r\n             Jinja2: 2.7.1\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.4.2\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.11\r\n              PyZMQ: 14.1.1\r\n                ZMQ: 4.0.4\r\nroot@saltmaster:/srv#\r\n```'
12935,'cachedout',"No data/highstate event returned to salt-master when highstated via scheduler/salt-call on salt-minion\nIf someone issues `salt 'hostname' state.highstate` from the master it is executed and the results are sent back to the master. With the event system you can do nice stuff (like salt-eventsd) such as storing when a highstate was executed and what the results were.\r\n\r\nHowever, if you call `salt-call state.highstate` from the minion it *is* communicating with the master (since it needs states & files) but no data gets sent back and no events are being raised. As a result no events are being logged (e.g. with salt-eventsd).\r\n\r\nThis opens op the possibility that the minion was highstated without the master's knowledge.\r\n\r\nI'm not sure if this is a known fact (couldn't find it in the issues), a known limitation or by-design but it would be nice if there was a way for a master to know when something happened on a minion (e.g. a highstate)"
12880,'terminalmage','Apache2 reload doesn\'t seem to work\nMy state, running on Ubuntu Quantal:\r\n```\r\napache2:\r\n  service:\r\n    - running\r\n    - reload: True\r\n```\r\n\r\ngenerates this output:\r\n```\r\n          ID: apache2\r\n    Function: service.running\r\n      Result: True\r\n     Comment: The service apache2 is already running\r\n     Changes:   \r\n```\r\n\r\n(And fails to reload the server.)\r\n\r\nIt\'s not a question of the init script not supporting reload, as this works just fine:\r\n\r\n```\r\nreload_apache:\r\n  cmd.run:\r\n    - name: "service apache2 reload"\r\n```'
12855,'cro','Create a Portfile for the latest Salt in MacPorts\nThe Portfile for MacPorts desperately needs to be updated.'
12446,'whiteinge','Salt Documentaion Sprint(s)\nThis issue should serve as a single place to collect notes and link other issues that should be focused on during one of the upcoming documentation-focused sprints.\r\n\r\nIn general the focus of each documentation sprint should be improving the existing documentation and not adding new documentation.\r\n\r\nEach sprint should have a focus and a list of issues as the goal for that sprint. Those issues _can_ be taken from the giant list below.'
12248,'cachedout',"14.04 / salt '*' test.ping Failed to authenticate, is this user permitted to execute commands?\nSince my upgrade to 14.04 and sync of @makinacorpus fork to last develop, something is weird.\r\n\r\nI get intermittent total unfunctionnality of the saltmaster , on a period of 1 at 2 minutes, i cant ping any minion, and the next 2 minutes ping is working, and 2 minutes after this does not work.\r\n```\r\n mastersalt '*' test.ping\r\nFailed to authenticate, is this user permitted to execute commands?\r\n\r\n```\r\n\r\nI rebuilded all the eggs locally installed to be sure of the linking, but this seems not to be sufficient.\r\n\r\nIll check tomorow to double check if this is one of our maintainance crons, but i would suspect yet another regression."
12173,'cachedout',"salt-call --local state.highstate very slow due to event\nExample:\r\n\r\n```\r\n[DEBUG   ] SaltEvent PUB socket URI: ipc:///var/run/salt/minion/minion_event_56c02c9f39234bae85c5910942f731ec_pub.ipc\r\n[DEBUG   ] SaltEvent PULL socket URI: ipc:///var/run/salt/minion/minion_event_56c02c9f39234bae85c5910942f731ec_pull.ipc\r\n[DEBUG   ] Sending event - data = {'_stamp': '2014-04-21T22:52:52.031194'}\r\n```\r\nThis call occurs twice and each call takes 3-5 seconds, adding roughly 10 seconds to salt-call, even when using --local, with no minion or master daemon. This makes salt-call very slow to start."
12111,'cachedout','External Authentication System does not work for groups of users\nI have the following external authentication system config in the master:\r\n\r\n```\r\nexternal_auth:\r\n  pam:\r\n    saltusers%:\r\n      - \'*\'\r\n        - \'test.*\'\r\n        - \'grains.get\'\r\n        - \'grains.items\'\r\n        - \'grains.item\'\r\n        - \'grains.has_value\'\r\n        - \'grains.ls\'\r\n```\r\n\r\nAccording to the example at - http://docs.saltstack.com/en/latest/topics/eauth/index.html - this is valid.\r\n\r\nHowever, on restarting the master, there are errors:\r\n\r\n```\r\n# /etc/init.d/salt-master restart\r\n[....] Restarting salt master control daemon: salt-master[ERROR   ] Error parsing configuration file: /etc/salt/master - while parsing a block collection\r\n  in "<string>", line 188, column 7:\r\n          - \'*\'\r\n          ^\r\nexpected <block end>, but found \'<block sequence start>\'\r\n  in "<string>", line 189, column 9:\r\n            - \'test.*\'\r\n            ^\r\n. ok \r\n\r\n```'
12064,'UtahDave',"Comment: Unable to manage file: global name '__salt__' is not defined \nsystem :windows \r\nversion:2014.1.1\r\ncmd: state.highstate\r\nerror:\r\n192.168.1.1:\r\n----------\r\n          ID: D:\\\\Server-Tools\\\\Rar.exe\r\n    Function: file.managed\r\n      Result: False\r\n     Comment: Unable to manage file: global name '__salt__' is not defined\r\n     Changes:\r\n----------\r\n          ID: D:\\\\Server-Tools\\\\addIpsec.exe\r\n    Function: file.managed\r\n      Result: True\r\n     Comment: File D:\\\\Server-Tools\\\\addIpsec.exe is in the correct state\r\n     Changes:\r\n----------\r\n          ID: C:\\\\Windows\\\\System32\\\\iisset.exe\r\n    Function: file.managed\r\n      Result: True\r\n     Comment: File C:\\\\Windows\\\\System32\\\\iisset.exe is in the correct state\r\n     Changes:\r\n----------\r\n          ID: D:\\\\Script\\\\salt\\\\ipsecset.exe\r\n    Function: file.managed\r\n      Result: True\r\n     Comment: File D:\\\\Script\\\\salt\\\\ipsecset.exe updated\r\n     Changes:\r\n              ----------\r\n              diff:\r\n                  New file\r\n\r\nSummary\r\n------------\r\nSucceeded: 3\r\nFailed:    1\r\n------------\r\nTotal:     4\r\n"
12030,'whiteinge',"Add a short-URL redirect service for documentation\nThis will allow us to provide links to the documentation from external sources more easily.\r\n\r\nE.g.: ``http://docs.saltstack.com/r/ref:Mine  -> http://docs.saltstack.com/en/latest/topics/mine/index.html\r\n\r\nIt also provides a permanent entry-point to certain docs pages that will survive documentation renames and reorganization. (See also: the fiasco with the Formula Conventions docs URL change.) This will use Sphinx's existing refs index (bcb66f1).\r\n\r\nRefs #12446."
12025,'cro','Improve logging in proxy minions\n'
12024,'cro','Improve process management in proxy minion\n'
12023,'cro','Update MacPorts package to latest Salt release\n'
12018,'whiteinge','Build PDF / ePub versions of the docs\nWe need to add building the PDF (and ePub?) versions of the docs to docs.saltstack.com.\r\n\r\nRefs: #12015.'
12017,'whiteinge',"Add links to different versions of the docs\nWe're currently building multiple versions of the Salt docs on docs.saltstack.com but those versions are linked from anywhere.\r\n\r\nRefs: #12015"
12015,'whiteinge',"Remove readthedocs documentation\nGoogle searches often include readthedocs, which is incredibly confusing. Is there any reason that can't simply be deleted?"
11980,'UtahDave','file.managed backup feature does not work for standalone windows minion?\n```\r\nC:\\Program Files (x86)\\Java\\jre7\\lib\\security\\US_export_policy.jar:\r\n  file:\r\n    - managed\r\n    - source: salt://STIG/US_export_policy.jar\r\n    - makedirs: False\r\n    - backup: minion\r\n```\r\nresults in this error:\r\n\r\n```\r\n    State: - file←[0m\r\n    ←[0;31mName:      C:\\Program Files (x86)\\Java\\jre7\\lib\\security\\US_export_po\r\nlicy.jar←[0m\r\n    ←[0;31mFunction:  managed←[0m\r\n        ←[0;31mResult:    False←[0m\r\n        ←[0;31mComment:   An exception occurred in this state: Traceback (most r\r\necent call last):\r\n  File "salt/state.py", line 1278, in call\r\n  File "salt/states/file.py", line 1157, in managed\r\n  File "salt/modules/file.py", line 1998, in manage_file\r\n  File "salt/utils/__init__.py", line 579, in copyfile\r\n  File "salt/utils/__init__.py", line 611, in backup_minion\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 150, in makedirs\r\n  File "os.py", line 157, in makedirs\r\nWindowsError: [Error 123] The filename, directory name, or volume label syntax i\r\ns incorrect: \'c:\\\\salt\\\\var\\\\cache\\\\salt\\\\minion\\\\file_backup\\\\:\'\r\n←[0m\r\n```'
11960,'s0undt3ch','confusing warning message "\'use\' is an invalid keyword argument ..." when running state.sls\nas a continue to this [thread](https://github.com/saltstack/salt/pull/11958) \r\n\r\nwhen I"m applying the state below\r\n```\r\napplication.wait.4.manager:\r\n  tomcat.wait:\r\n    - timeout: 600\r\n    - url: http://localhost:8080/manager\r\n\r\nwait.application.restart.tomcat.after.deploy:\r\n  tomcat.wait:\r\n    - use:\r\n      - tomcat: application.wait.4.manager\r\n```\r\nI\'m getting a response back from the minion\r\n```\r\n                  tomcat_|-wait.application.restart.tomcat.after.deploy_|-wait.application.restart.tomcat.after.deploy_|-wait:\r\n                      ----------\r\n                      __run_num__:\r\n                          66\r\n                      changes:\r\n                          ----------\r\n                      comment:\r\n                          tomcat manager is ready\r\n                      name:\r\n                          wait.application.restart.tomcat.after.deploy\r\n                      result:\r\n                          True\r\n                      warnings:\r\n                          - \'use\' is an invalid keyword argument for \'tomcat.wait\'. If you were trying to pass additional data to be used in a template context, please populate \'context\' with \'key: value\' pairs. Your approach will work until Salt Lithium is out. Please update your state files.\r\n```\r\n`use` indeed is not a valid argument of `tomcat.wait` but it is used as a requisite so the error message here is not relevant...\r\n\r\na similar issue occurs when applying the below state\r\n```\r\npostfix.config:\r\n  file.managed:\r\n    - name: /etc/postfix/master\r\n    - source: salt://somewhere..\r\n\r\npostfix:\r\n  service.running:\r\n    - enable: True\r\n    - require:\r\n      - pkg: mailpkgs\r\n    - watch:\r\n      - file: postfix.config\r\n```\r\nwhen the `postfix.config` changes it triggers the `postfix` `service.mod_watch` function\r\nbut the state returns the error message below\r\n```\r\n                  service_|-postfix_|-postfix_|-running:\r\n                      ----------\r\n                      __run_num__:\r\n                          29\r\n                      changes:\r\n                          ----------\r\n                          postfix:\r\n                              True\r\n                      comment:\r\n                          Service restarted\r\n                      name:\r\n                          postfix\r\n                      result:\r\n                          True\r\n                      warnings:\r\n                          - \'enable\' is an invalid keyword argument for \'service.mod_watch\'. If you were trying to pass additional data to be used in a template context, please populate \'context\' with \'key: value\' pairs. Your approach will work until Salt Lithium is out. Please update your state files.\r\n```\r\n\r\n@s0undt3ch, you can now recreate this in a test :smile: '
11769,'UtahDave','Support Microsoft "OneGet" repositories on WIndows minions\nSeems useful for salt to [support this](http://blogs.technet.com/b/windowsserver/archive/2014/04/03/windows-management-framework-v5-preview.aspx)'
11631,'cachedout','Minion errors are not reported to master\nThe most recent instance of this problem was a YAML idiosyncrasy described in #11630.\r\n\r\nBasically, a non-msgpack-serializable value ended up in the minion opts.  When a highstate was then called from the master, the minion would try to send its opts to the master, msgpack would traceback, salt would swallow the traceback (and log it in the minion logs) and then would return nothing to the master.  On the master end, all that would be seen was an empty return for each minion.  No errors were returned to the master.\r\n\r\nThe solution I propose is two-fold.\r\n\r\n1. The first thing we should do is implement an event listener on the master which looks for error events.  Then, when an un-handled error occurs on the minion, we should fire an event to the master containing the traceback for the error.  The master should log this traceback to its own log, specifying the minion that threw the error, and any other information relevant.  This will allow all major minion errors to be reported to the master.\r\n\r\n2. When an error like this occurs, we need to find a way to tell that master that an error occurred in the job.  The only way I can think of to semi-consistently report these errors to the master is to leave a breadcrumb for find_job to find and report to the master when the master queries about the status of a given job.  Other proposed solutions are welcome.  Our forking model on the minion in conjunction with the asynchronous nature of salt makes this a rather difficult problem to solve.\r\n\r\n/cc @pjscott @balacuit @vdhillonjpl'
11548,'terminalmage',"Allow gitfs config to specify an ssh key\nCurrently it's necessary to add a key to ~/.ssh/id_rsa, but for many systems this key is used for other things as well. It would be ideal to be able to specify an ssh key to use."
11492,'s0undt3ch','Grains do not inherit\nThis is duplicated from salt-cloud issue here: https://github.com/saltstack/salt-cloud/issues/908\r\n\r\nWhen one profile extends another - only the applied profiles grains will be added to the instance, rather than merging the grains.\r\n\r\nFor example:\r\n\r\n```\r\nubuntu_13.10_az_1:\r\n  provider: ec2-ap-southeast-2\r\n  image: ami-XXXXXXXX\r\n  ssh_username: ubuntu\r\n  location: ap-southeast-2\r\n  availability_zone: ap-southeast-2a\r\n  subnetid: subnet-XXXXXXXX\r\n  script: /srv/salt/infrastructure-profiles/platform.sh\r\n  minion:\r\n    master: X.X.X.X\r\n  grains:\r\n    location: ap-southeast-2a\r\n```\r\n\r\nand\r\n\r\n```\r\nAIO_zk_1_profile:\r\n  size: m3.xlarge\r\n  grains:\r\n    aio_role: zookeeper\r\n  extends: ubuntu_13.10_az_1\r\n```\r\n\r\nThen invoking `# salt-call grains.items | grep location` on the new instance will not return anything.\r\n\r\nChecking for the grain from the sub-node will return the correct value:\r\n\r\n```\r\n# salt-call grains.items | grep aio_role\r\n  aio_role: zookeeper\r\n```'
11373,'thatch45','- exclude does not exclude a watch_in\nIn this example:\r\ncat /srv/salt/test.sls\r\n```\r\nmypkgs:\r\n  pkg.installed:\r\n    - pkgs:\r\n      - erlnode\r\n      - erlang\r\n    - watch_in:\r\n      - service: erlnode\r\n\r\nerlnode:\r\n  service.running\r\n```\r\n\r\nThen in another sls file we include and exclude:\r\ncat /srv/salt/test2.sls\r\n```\r\ninclude:\r\n  - test\r\n\r\n/etc/foo:\r\n  file.managed:\r\n    - source: salt://foo\r\n\r\nexclude:\r\n - id: mypkgs\r\n\r\nextend:\r\n  erlnode:\r\n    service:\r\n      - require:\r\n        - file: /etc/foo\r\n```\r\n\r\nIf you run\r\n```\r\nsalt-call state.sls test2\r\n```\r\nThen you get an error because the watch_in gets converted to a watch before the  mypkgs state gets excluded.\r\n'
11093,'UtahDave','Unable to add a windows user password - user state and windows  useradd  module (v2014.1.0-5)\nMy environment consists of standalone-windows-minions.\r\n\r\n>c:\\salt\\salt-call --version\r\nsalt-call.exe 2014.1.0-5-g7400343\r\n\r\nFrom the documentation, "password" does not appear to be supported for the user state on windows:\r\n\r\n"password\r\n    A password hash to set for the user. This field is only supported on Linux, FreeBSD, NetBSD, OpenBSD, and Solaris."\r\n\r\nYet, documentation concerning win_useradd states this:\r\n\r\n "salt.modules.win_useradd.setpassword(name, password)\r\n    Set a user\'s password"\r\n\r\nAnd, in order to truly add a windows user, I need to be able to include a password along with the username.  I\'ve tried adding this parameter within the user.state like below, but am having no luck.\r\n\r\nIs there a solution for adding users via the user state for windows like below?\r\n```\r\nJenkinsGroup:\r\n  group:\r\n   - present\r\n\r\nsshGroup:\r\n  group:\r\n   - present\r\n\r\nmletest:\r\n  user:\r\n    - present\r\n    - fullname:  "mletest Account"\r\n    - password: {{ pillar[\'Jauth\'] }}\r\n    - groups:\r\n      - JenkinsGroup\r\n      - Users\r\n      - sshGroup\r\n```\r\n\r\nThanks.'
11046,'s0undt3ch',"_syspaths.py not generated when salt installed using pip install --editable=/path/to/git/checkout\n@s0undt3ch per our conversation in #salt-devel, I've opened this issue."
10877,'cachedout',"salt-call does not respect/honor CLI `test=False` while minion uses config `test: True`\nWhen test mode is set by default (in minion config `test: True`), at least `salt-call --local` command does not respect override given in command line.\r\n\r\n### Actual Results\r\n\r\n```\r\n> grep 'test:' /etc/salt/minion\r\ntest: True\r\n\r\n> salt-call --local state.highstate test=False\r\n...\r\nSummary\r\n-------------\r\nSucceeded: 31\r\nFailed:     0\r\nNot Run:    3\r\n-------------\r\nTotal:     34\r\n```\r\nSome states were not run.\r\n\r\n### Expected Result\r\n\r\nThe state should be executed as if config file doesn't have `test: True` setting:\r\n```\r\n> grep 'test:' /etc/salt/minion\r\n# no output\r\n\r\n> salt-call --local state.highstate test=False\r\n....\r\nSummary\r\n-------------\r\nSucceeded: 34\r\nFailed:     0\r\n-------------\r\nTotal:     34\r\n```\r\nAll states were run.\r\n\r\n### Note\r\n\r\nThis issue correlates with: #7788, #5411, #3363.\r\n\r\nThe combination where config `test: False` and CLI `test=True` is fine: CLI overrides config and runs all states in test mode as expected.\r\n\r\n```\r\n> grep 'test:' /etc/salt/minion\r\ntest: False\r\n\r\n> salt-call --local state.highstate test=True\r\n...\r\nSummary\r\n-------------\r\nSucceeded: 31\r\nFailed:     0\r\nNot Run:    3\r\n-------------\r\nTotal:     34\r\n```\r\n### Version\r\n```\r\n> salt --versions-report\r\n           Salt: 2014.1.0\r\n         Python: 2.7.5 (default, Feb 19 2014, 13:47:28)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.2\r\n            ZMQ: 3.2.4\r\n\r\n> rpm -qf $(which salt-call)\r\nsalt-minion-2014.1.0-1.fc20.noarch\r\n```\r\n"
10540,'techhat','DNSUtil parse_zone out of range error\nI started writing a new test called ```test_parse_zone``` in tests/unit/modules/dnsutil_test to test the ```parse_zone``` functionality in salt/modules/dnsutil and the function is giving an index error on the mock dns file that I am passing into the test. \r\n\r\nI got the dns zone file from http://www.tldp.org/HOWTO/DNS-HOWTO-7.html in section 7.3. \r\n\r\n@techhat can you take a look at the parse_zone function? See pull req #10539.'
10455,'UtahDave','Cannot restart a service based on the watch condition on windows minion (v17.2)\nWhen a watched file is changed, service.running successfully detects that and stops the service, however, it has never successfully restarted the service. I\'ve tried both "reload: True" and "fully_restart: True".  But, no matter what service I\'ve tried this on, it\'s never worked - salt-call seems to hang.  I\'ve waited many, many minutes after service is down and must finally use ctrl-c to stop salt-call process.  Here is one of my state files:\r\n```\r\nsshGroup:\r\n  group:\r\n   - present\r\n\r\nssh-pkg-install:\r\n  pkg:\r\n    - installed\r\n    - names:\r\n      - sshClient\r\n      - sshClientPatch\r\n      - sshServer\r\n\r\nE:\\Attachmate\\RSecureServer\\Logs:\r\n  file.directory:\r\n    - makedirs: True\r\n\r\nC:\\ProgramData\\Attachmate\\RSecureServer\\ssh-banner.txt:\r\n  file:\r\n    - managed\r\n    - source: salt://STIG/ssh-banner.txt\r\n    - makedirs: False\r\n\r\nC:\\ProgramData\\Attachmate\\RSecureServer\\rsshd_config.xml:\r\n  file:\r\n    - managed\r\n    - source: salt://STIG/rsshd_config.xml\r\n    - makedirs: False\r\n \r\nAttachmate Reflection for Secure IT Server:\r\n  service.running:\r\n    - enable: True\r\n    - full_restart: True\r\n    - watch:\r\n      - file: C:\\ProgramData\\Attachmate\\RSecureServer\\rsshd_config.xml\r\n```\r\n\r\n'
10418,'cro',"Should we not hard code these values?\nWhile researching an annoying bug on the Windows minion I realized that 'os.umask()' was being used with hard coded values in many locations.\r\n\r\n```\r\n[boucha@dasalt salt (develop u=)]$ grep -rn 'os.umask' *\r\ncrypt.py:72:    mask = os.umask(191)\r\ncrypt.py:84:    os.umask(mask)\r\ncrypt.py:96:    cumask = os.umask(191)\r\ncrypt.py:98:    os.umask(cumask)\r\ndaemons/masterapi.py:131:        cumask = os.umask(191)\r\ndaemons/masterapi.py:134:        os.umask(cumask)\r\nfileclient.py:99:        cumask = os.umask(63)\r\nfileclient.py:107:        os.umask(cumask)\r\n__init__.py:186:                    current_umask = os.umask(0077)\r\n__init__.py:188:                    os.umask(current_umask)\r\nloader.py:1048:            cumask = os.umask(077)\r\nloader.py:1062:            os.umask(cumask)\r\nmodules/file.py:2646:                current_umask = os.umask(077)\r\nmodules/file.py:2668:                os.umask(current_umask)\r\nmodules/file.py:2693:            mask = os.umask(0)\r\nmodules/file.py:2694:            os.umask(mask)\r\nmodules/virt.py:572:                mask = os.umask(0)\r\nmodules/virt.py:573:                os.umask(mask)\r\nmodules/cmdmod.py:131:        os.umask(umask)\r\nmodules/state.py:283:    cumask = os.umask(077)\r\nmodules/state.py:293:    os.umask(cumask)\r\nmodules/state.py:391:    cumask = os.umask(077)\r\nmodules/state.py:402:    os.umask(cumask)\r\nstate.py:2498:        cumask = os.umask(077)\r\nstate.py:2513:        os.umask(cumask)\r\ntransport/table/__init__.py:163:        current = os.umask(191)\r\ntransport/table/__init__.py:166:        os.umask(current)\r\nutils/__init__.py:248:    os.umask(18)\r\nutils/verify.py:208:                cumask = os.umask(18)  # 077\r\nutils/verify.py:213:                os.umask(cumask)\r\n```\r\n\r\nI'm thinking these values should not be hard coded. I imagine that many of these entries are setting the umask for the same reason and could use the same config variable.  os.umask(191) is problematic on Windows because it sets files to read only, even for the owner.   077 might be a better option. But these should probably all be set in the minion or master's config file.\r\n\r\nThoughts?"
10407,'s0undt3ch',"Extending profile in EC2 cloud profile with tags does not extend tags\nYou could argue that this is functioning as designed, but I think it's not very helpful the way it is.\r\n\r\nIf I have something like:\r\n\r\n```\r\nsome_profile:\r\n  provider: some_configured_ec2_provider\r\n  image: ami-12345678\r\n  script: my_deploy_script.sh\r\n  tag:\r\n    Management: Salt\r\n    Manager: some_host.somewhere.com\r\n\r\nmy_special_server.company.com:\r\n  extends: some_profile\r\n  size: m1.small\r\n  location: us-east-1\r\n  availability_zone: us-east-1c\r\n  securitygroup:\r\n    - my_sg\r\n    - my_other_sg\r\n  tag:\r\n    Stack: Production\r\n    Purpose: Whatever\r\n```\r\n\r\nWhat I really want is for the resulting tag setup of ```my_special_server.company.com``` to look like this:\r\n\r\n```\r\ntag:\r\n  Management: Salt\r\n  Manager: some_host.somewhere.com\r\n  Stack: Production\r\n  Purpose: Whatever\r\n```\r\n\r\nSame deal with the security groups, for that matter (assuming I was extending something that already had them). Instead of appending to the dictionaries, we're clobbering them. I don't think that's ideal as it leads to a lot of duplication when using the ```extend``` option a lot."
10406,'basepi','Tag on instance launch not documented\nWe launch instances using salt-cloud and our EC2 provider. After launching instances, we sometimes need to manually add tags we want ourselves (if we remember). eg.\r\n\r\n```# salt-cloud -a set_tags some-instance tag0=value0 tag1=value1```\r\n\r\netc. It would be much nicer to just list the tags each instance is to have applied to it within the salt-cloud EC2 profile config file.\r\n\r\nIn fact... looking through the source just now, it appears that the "tag" profile configuration option already exists! It\'s just missing from the documentation. Haha.'
10327,'whiteinge','Audit ePub builds\nePub : iBooks report error \r\n```\r\n“error on line 74 at column 10: Opening and ending tag mismatch: link line 0 and head\r\n”\r\n\r\n```\r\nand cannot render it.'
10207,'techhat','Feature request: Add config option in salt-cloud ec2 driver for termination protection\nCurrently the ec2 driver only has actions defined to change termination protection on an instance (see http://docs.saltstack.com/topics/cloud/aws.html#id10 ).  Please add a config option in the cloud profile configuration section to set termination protection when an instance is created.  Something along the lines of "enable_term_protect: True".'
10075,'techhat','[salt.cloud] No warning when libcloud is not installed\nSalt Cloud used to warn when libcloud was not present. Now the execution modules just stacktrace. They need to provide more meaningful warnings.'
9998,'whiteinge','Add testing advice to state module docs\nWe need a larger narrative on how to test states/formula, however some of that functionality is built-in with ``test=true``. It would be helpful if the docs for each state module would hint at how best to verify the result of running that state.'
9996,'s0undt3ch','Automate Doc Translation Process\nCurrently, we need to manually run a command to upload the documentation to [transifiex](https://www.transifex.com/organization/saltstack/dashboard/salt).\r\n\r\nWe need to automate this process to allow the translators to always have the latest version available'
9969,'techhat','Improve documentation of saltify "cloud" provider\nI\'m trying out saltify in salt-cloud.\r\n\r\nThe documentation needs improving: \r\n\r\n```\r\nmake_salty:\r\n  myinstance:\r\n    ssh_host: 192.168.0.19\r\n    ssh_username: root\r\n    #ssh_passwd: root\r\n    key_filename: /etc/salt/passphrase-less-salt-key.pem\r\n```\r\n\r\nssh_keyfile does not work. and ssh_password doesn\'t really seem to be implemented (would it use something like sshpass ? )\r\n\r\nThe documentation should also mention that one needs /etc/salt/cloud.providers.d/saltify.conf with \r\n\r\n```\r\n\r\nsaltify:\r\n    provider : saltify\r\n```'
9955,'terminalmage','freebsdports state relies on /var/db/ports/*/options not pkgng database for installed port options\nThe freebsdports state in git currently relies on the options configured in /var/db/ports/*/options or similar. The options specified in that file only contain information about the ports build configuration not information about currently installed ports (also known as packages).\r\n\r\nIf one installs a package without the IPV6 option for example, then runs "make config" and enables the IPV6 option, but does not build and install the package the ports state will incorrectly think that the port is already installed with the IPV6 option when it is in fact not.\r\n\r\nOn systems with pkgng (FreeBSD 10 by default - earlier versions pkgng is not the default but can be used) the command "pkg info -R <package/port name>" will output json with a dictionary "options" that will show the options of the currently installed package. I assume this information can be obtained on older versions of FreeBSD not using pkgng as well via the pkg_ tools, but I don\'t have any such systems handy to determine the correct way to obtain this information.\r\n\r\nOtherwise a very handy state. :)'
9929,'UtahDave','pkg.removed and version problems on windows minion.\nIssue 4492  was closed about 9 months ago concerning pkg.removed problems on a windows minion; however I am experiencing similar problems.  In my standalone windows environment, I cannot specify a version or get "removed" to work within a state.file.\r\n\r\nMy version is:\r\n```\r\nC:\\salt>salt --versions-report\r\n           Salt: 0.17.2\r\n         Python: 2.7.5 (default, May 15 2013, 22:43:36) [MSC v.1500 32 bit (Inte\r\nl)]\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.3.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.1.0\r\n            ZMQ: 3.2.2\r\n``` \r\nEach java upgrade is added to the init.sls in the winrepo like so:\r\n```\r\njavasdk:\r\n  1.7.0.510:\r\n    installer: \'D:\\\\srv\\salt\\file_roots\\winrepo\\javasdk\\jdk-7u51-windows-i586.exe\'\r\n    full_name: \'Java SE Development Kit 7 Update 51\'\r\n    reboot: False\r\n    install_flags: \' /s /v"/norestart AUTOUPDATECHECK=0 JAVAUPDATE=0 JU=0 WEBSTARTICON=0 EULA=0 SYSTRAY=0" \'\r\n    msiexec: False\r\n    uninstaller: \'msiexec.exe\'\r\n    uninstall_flags: \' /X{32A3A4F4-B792-11D6-A78A-00B0D0170510} /qn\'\r\n  1.7.0.250:\r\n    installer: \'D:\\\\srv\\salt\\file_roots\\winrepo\\javasdk\\jdk-7u25-windows-i586.exe\'\r\n    full_name: \'Java SE Development Kit 7 Update 25\'\r\n    reboot: False\r\n    install_flags: \' /s /v"/norestart AUTOUPDATECHECK=0 JAVAUPDATE=0 JU=0 WEBSTARTICON=0 EULA=0 SYSTRAY=0" \'\r\n    msiexec: False\r\n    uninstaller: \'msiexec.exe\'\r\n    uninstall_flags: \' /X{32A3A4F4-B792-11D6-A78A-00B0D0170250} /qn\'\r\n```\r\n...which allows me to manually remove or install a specific version of the package from the command line like so:\r\n\r\nsalt-call pkg.remove javasdk version=1.7.0.510\r\n\r\nBut, I wish to manage the pkg installations and removals via states rather than command line. Unfortunately, I\'m unable to successfully install or remove a specific version of the pkg like so:\r\n```\r\ncore-install:\r\n  pkg:\r\n    - installed\r\n    - names:\r\n      - firefox\r\n      - tortoisesvn\r\n      - javasdk: \'1.7.0.510\'\r\n      - 7zip\r\n\r\ncore-remove:\r\n  pkg:\r\n    - removed\r\n    - names:\r\n      - javasdk: \'1.7.0.250\'\r\n```\r\n\r\nThe state file is never parsed correctly, but chokes on the use of "removed" and the version number.\r\n'
9912,'whiteinge','Improve the "flow" of Salt documentation\nMany of the Salt docs are "dead-ends". Meaning you land on a page that explains something and there\'s rarely links to deeper explanations or related topics. When there are links they are typically embedded in the text and not friendly to skimming.\r\n\r\nHere is an example. Say you land on the main States documentation <http://docs.saltstack.com/ref/states/>. How do you get from that main page to any of the sub-pages (and there are many)?\r\n\r\nFor example, that main page has a section on the top file but no link to this page:\r\nhttp://docs.saltstack.com/ref/states/top.html\r\n\r\nI have no idea how to get to this page without going through the Full Table of Contents:\r\nhttp://docs.saltstack.com/ref/states/vars.html\r\n\r\nI think a big part this problem was (my) decision to have a monolithic, stand-alone table of contents (``contents.rst``). Although that makes it easy to globally reorder the entire documentation it avoids smaller section-local table of contents. There\'s probably a good middle-ground; let\'s find it.'
9634,'terminalmage','Add support for a list of sources to cron.file state\nThis came up in #9620. Since ``file.managed`` allows one to specify a list of possible sources, ``source.list`` should do so as well.'
9616,'terminalmage',"Provide CLI notification for conflicting environment entries when compiling highstate\nThe other day I noticed that some of my formulas were not being applied in highstate. From what I can tell the problem occurs when I use gitfs as the backend. I checked the cache directory and found a couple formula directories missing. \r\n\r\nI've searched all over for an open bug report but couldn't find one. Yet I can't help but wonder why no one else seems to be having this issue.\r\n\r\nI created a test repo (https://github.com/Vye/gitfstest/tree/dev) so this can be easily reproduced. There were no errors or odd warnings in the log files during the highstate.\r\n\r\nI first noticed the issue with 0.17.4 but have reproduced it in the develop branch as well.\r\n```\r\n$ cat /etc/redhat-release \r\nCentOS release 6.3 (Final)\r\n$ salt --versions-report\r\n           Salt: 0.17.4\r\n         Python: 2.6.6 (r266:84292, Sep 11 2012, 08:34:23)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.4\r\n$ rpm -q GitPython\r\nGitPython-0.3.2-0.1.RC1.el6.noarch\r\n```\r\nbased on 'develop' branch:\r\n```\r\n$ cat /etc/redhat-release \r\nCentOS release 6.5 (Final)\r\n$ salt --versions-report\r\nSalt: 2014.1.0-147-g3152e26\r\n         Python: 2.6.6 (r266:84292, Nov 22 2013, 12:16:22)\r\n         Jinja2: 2.7.1\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.4.0\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.6.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 14.0.1\r\n            ZMQ: 4.0.3\r\n$ pip freeze | grep Git\r\nGitPython==0.3.2.RC1\r\n```\r\n\r\nI added each formula one at a time and gitfs updated just fine until the fourth one, then it stopped populating the rest (there are five at this point). I have deleted the gitfs cache and restarted the salt-master half a dozen times with the same results.\r\n\r\n```\r\nls var/cache/salt/master/gitfs/refs/dev\r\nformula1  formula2  formula3  top.sls\r\n```\r\n\r\nThe above should have formula4 and formula5 folders as well.\r\n\r\nhighstate output:\r\n```\r\n$ salt -c ./etc/salt '*' state.highstate test=True\r\nsaltdev:\r\n----------\r\n          ID: /formula2\r\n    Function: file.managed\r\n      Result: None\r\n     Comment: The following values are set to be changed:\r\n              newfile: /formula2\r\n     Changes:   \r\n              ----------\r\n----------\r\n          ID: /formula3\r\n    Function: file.managed\r\n      Result: None\r\n     Comment: The following values are set to be changed:\r\n              newfile: /formula3\r\n     Changes:   \r\n              ----------\r\n----------\r\n          ID: /formula1\r\n    Function: file.managed\r\n      Result: None\r\n     Comment: The following values are set to be changed:\r\n              newfile: /formula1\r\n     Changes:   \r\n              ----------\r\n\r\nSummary\r\n------------\r\nSucceeded: 0\r\nFailed:    0\r\nNot Run:   3\r\n------------\r\nTotal:     3\r\n```\r\n\r\nI've gisted my 'develop' master and minion config here: https://gist.github.com/Vye/359c1db233901d80da4f\r\n\r\nCan someone else try and see if they are able to reproduce this issue? "
9066,'UtahDave',"Expand Windows documentation regarding what can and can't be accomplished\n@UtahDave Someone in the chat today asked about the functionality of the Windows Salt minions. Other than not being a master, I wasn't really able to answer a lot about this since I haven't messed with the Windows version. I think it would be great if the existing docs were expanded specifying anything unique that the Windows version is unable to do past being a master."
8880,'cro',"salt-ssh fails on test methods b/c sha1 binary is not available\ni wasn't able to get this salt-ssh working:\r\n<pre>\r\npille@salt-master ~ [1] % sudo salt-ssh '*' test.version \r\nlive3:\r\n    /bin/sh: 60: sha1: not found\r\n    /bin/sh: 60: [: =: unexpected operator\r\n    /usr/bin/python2.7: can't open file '/tmp/.salt/salt-call': [Errno 2] No such file or directory\r\n    \r\npille@salt-master ~ % sudo salt-ssh '*' test.ping   \r\nlive3:\r\n    /bin/sh: 60: sha1: not found\r\n    /bin/sh: 60: [: =: unexpected operator\r\n    /usr/bin/python2.7: can't open file '/tmp/.salt/salt-call': [Errno 2] No such file or directory\r\n</pre>\r\n\r\nwhen i provide a symlink for <code>sha1</code> pointing to <code>sha1sum</code>.\r\ni'm on ubuntu-12.04 and this works.\r\n\r\nworking around this, leads to #8882."
8815,'s0undt3ch','In a runner, unable to set log level other than "warn"\nIn a runner, if the log level is set to something other than "warn", then nothing is logged.\r\n\r\nThis was seen in a custom runner.'
8809,'thatch45',"salt-call publish.runner timeouts\nOn 0.17.0 frequent timeouts are being seen when executing a custom runner like this:\r\n\r\n```\r\nsalt-call publish.runner\r\n```\r\n\r\nWhen using salt-run these timeouts don't occur. "
8754,'rallytime',"[salt.cloud] Need test cases for salt.cloud providers\nAs noted in #8753, the API for Joyent has changed. We need to be able to catch these changes as they happen, to make sure any required updates make it into the next release.\r\n\r\nI'm thinking we would want to have these tests check for the existence of a cloud.providers and cloud.profiles file, which would be documented but not present in the repository (can't be publishing that, you know)."
8747,'s0undt3ch','[salt.cloud] global name \'__active_provider_name__\' is not defined\nI can\'t currently seem to use `salt-cloud` (master as of now) as it is documented. [Following the YAML here](http://salt.readthedocs.org/en/latest/ref/clouds/all/salt.cloud.clouds.botocore_aws.html#the-aws-cloud-module) and copying it into `cloud.providers.d/aws.conf` verbatim I get the following:\r\n\r\n```\r\n$ salt-cloud --list-providers\r\n[INFO    ] salt-cloud starting\r\n[ERROR   ] Failed to read the virtual function for cloud: botocore_aws\r\nTraceback (most recent call last):\r\n  File "/home/pwaller/.local/src/salt/salt/loader.py", line 779, in gen_functions\r\n    virtual = mod.__virtual__()\r\n  File "/home/pwaller/.local/src/salt/salt/cloud/clouds/botocore_aws.py", line 149, in __virtual__\r\n    conn = get_conn(**{\'location\': get_location()})\r\n  File "/home/pwaller/.local/src/salt/salt/cloud/clouds/libcloud_aws.py", line 291, in get_location\r\n    vm_ or get_configured_provider(), __opts__,\r\n  File "/home/pwaller/.local/src/salt/salt/cloud/clouds/libcloud_aws.py", line 173, in get_configured_provider\r\n    __active_provider_name__ or \'aws\',\r\nNameError: global name \'__active_provider_name__\' is not defined\r\n[WARNING ] The cloud driver, \'aws\', configured under the \'my-aws-botocore-config\' cloud provider alias was not loaded since \'aws.get_configured_provider()\' could not be found. Removing it from the available providers list\r\nError: There was an error listing providers: There are no cloud providers configured.\r\n```\r\n\r\n```\r\n$ salt-cloud --versions-report\r\n            Salt: 0.17.0-4901-g8cbdd9c\r\n          Python: 2.7.3 (default, Sep 26 2013, 20:03:06)\r\n          Jinja2: 2.7.1\r\n        M2Crypto: 0.21.1\r\n  msgpack-python: 0.4.0\r\n    msgpack-pure: Not Installed\r\n        pycrypto: 2.4.1\r\n          PyYAML: 3.10\r\n           PyZMQ: 14.0.0\r\n             ZMQ: 4.0.1\r\n Apache Libcloud: 0.13.2\r\n```\r\n\r\nI originally was trying to make my own configuration but I had no luck. It also seems to happen if I don\'t use `cloud.providers.d/aws.conf` but `cloud.providers` directly.\r\n\r\nNote: I\'m using `--providers-config`.'
8744,'s0undt3ch',"SALT_CLOUD_PROVIDERS_CONFIG can't be set to a directory but --providers-config can\n```\r\n$ export SALT_CLOUD_PROVIDERS_CONFIG=${PWD}/cloud/cloud.providers\r\n```\r\n\r\nWorks as expected, picks up `${PWD}/cloud/cloud.providers`\r\n\r\n```\r\n$ salt-cloud --providers-config=${PWD}/cloud/cloud.providers.d\r\n[DEBUG   ] Missing configuration file: /home/pwaller/sw/salt/cloud/cloud.providers.d\r\n[DEBUG   ] Including configuration from '/home/pwaller/sw/salt/cloud/cloud.providers.d/aws.conf'\r\n[DEBUG   ] Reading configuration from /home/pwaller/sw/salt/cloud/cloud.providers.d/aws.conf\r\n```\r\n\r\nWorks as expected.\r\n\r\n```\r\n$ export SALT_CLOUD_PROVIDERS_CONFIG=${PWD}/cloud/cloud.providers.d\r\n$ salt-cloud\r\n...\r\n[DEBUG   ] Missing configuration file: /etc/salt/cloud.providers\r\n...\r\n```\r\n\r\nDoesn't work. I would expect the same behaviour as if I had written `--providers-config`."
8719,'techhat','states.iptables does not support iptables version in RHEL / CentOS 6\nRHEL 6 and clones use IPTables 1.4.7.  This version does not support the -C (check) option.  Would it be possible to create a version of states.iptables that does not require this?\r\n\r\nIf not then at the least the docs should be updated to show that there is an IPTables version requirement.\r\n\r\nThanks'
8684,'terminalmage','gitfs backend error related to SSH host key checking\nAfter setting up a gitfs backend for a (private) GitHub repo, the following appeared in the master log:\r\n\r\n    [salt.loaded.int.fileserver.gitfs][WARNING ] GitPython exception caught while\r\n    fetching: len(["c179aa10976f1152d37e1deca0736698da8744bd\\tnot-for-\r\n    merge\\tbranch \'master\' of git+ssh://xxx\\n"]) != len(["Warning: Permanently added the \r\n    RSA host key for IP address \'xxx\' to the list of known hosts.", \'\', \' = [up to date]      \r\n    master     -> origin/master\'])\r\n\r\nWorkaround was to add the following to `.ssh/config`:\r\n\r\n    Host github.com\r\n    User salt\r\n    StrictHostKeyChecking no\r\n    UserKnownHostsFile=/dev/null\r\n\r\nSalt 0.17.2, Ubuntu Raring'
8521,'UtahDave','Active Directory Service Integration / Improvements\nAs requested by @basepi in https://github.com/saltstack/salt/issues/8119, I\'m re-opening this issue as a feature request.\r\n\r\nI am currently adding/updating/deleting entries from DNS and DHCP using netsh and dnscmd using the remote command execution framework and run states. The salt-minion executing these commands requires the minion service to be started as a privileged A/D user. \r\n\r\nUpdates are complicated since netsh/dnscmd do not support "updates" directly, so I need to first determine if an update is required and then delete the current entry in DNS and DHCP before adding its replacement. This requires some set logic to compare the current state of all of my hosts/VMs with what\'s in DNS and DHCP. I will likely do this in memory from within python to start with (using dictionaries as hash tables perhaps) but will eventually move the set subtraction to redis probably.\r\n\r\nIt would be very helpful if we had functions in salt that provided add, delete and update/modify semantics for both DNS and DHCP in A/D.\r\n\r\nOther A/D integration features like user management, user lookup (with member-of group lookups - including nested groups), etc.. would also be helpful but are secondary to anything I need at the moment.'
8405,'UtahDave',"pip state cannot install packages on Windows minions\nWith python/mako.sls file being:\r\n```\r\nmako:\r\n    pip.installed\r\n```\r\n```\r\nsalt -G 'os:Windows' state.sls python.mako\r\n```\r\ngives\r\n```\r\n----------\r\n    State: - pip\r\n    Name:      mako\r\n    Function:  install\r\n        Result:    False\r\n        Comment:   State pip.installed found in sls python.mako is unavailable\r\n\r\n        Changes:\r\n                   ----------\r\n\r\n```\r\n\r\nAlso,\r\n\r\n```\r\nsalt -G 'os:Windows' pip.list\r\n```\r\ngives\r\n```\r\n    ----------\r\n    pip:\r\n        1.4.1\r\n    pywin32:\r\n        218\r\n```\r\n\r\nI tested all of this on a linux minion and it all works well."
8273,'cachedout',"`salt-ssh` prompts to overwrite (!) private keyfile passed via `--priv` \nAs I had problems getting `salt-ssh` to work, I tried the `--priv` command line argument, in addition to the `priv:` roster setting. When I ran `salt-ssh` with `--priv ~/.ssh/id_rsa`, I got the following message (first line may be unrelated):\r\n\r\n    [WARNING ] Warning:  sshpass is not present, so password-based authentication is not available.\r\n    /Users/foo/.ssh/id_rsa already exists.\r\n    Overwrite (y/n)?\r\n\r\nI don't know what the purpose of `--priv` is, and how it relates to `priv:`. (`I wish salt-ssh` had more docs.) But I assume something is going wrong here. Mac OS 10.9, Salt 0.17.1"
8240,'cachedout',"non optimal behaviour salt.modules.ssh.set_known_host\nI have an issue with salt.modules.ssh.set_known_host.  If there is already an entry in the known_hosts file, but it is incorrect, it's flagged as 'exists' and not set.  In order to ensure the entry is always current, it would need to always be removed first, then always set.  This seems like it is going to generate a lot of unnecessary noise.\r\n\r\nCould we either have an additional parameter update-if-incorrect, or change the behaviour so that the end result is that the host's entry is actually set.\r\n\r\nTo reproduce this problem:\r\n\r\nsaltmaster# cat ssh-user-keys.sls \r\nroot_pub_keys:\r\n  module.run:\r\n    - name: ssh.set_known_host\r\n    - user: root\r\n    - hostname: 1.2.3.4\r\n\r\nExecute this state once.  After, an attempt to ssh into 1.2.3.4 goes straight to login prompt.\r\nChange the pub key on 1.2.3.4, then execute once more.  Now an attempt to ssh into 1.2.3.4 gives the fingerprint mismatch warning as the pubkey was not changed.\r\n"
8009,'thatch45',"Salt CLI can't communicate via AF_UNIX socket with master\nSalt's CLI tools (e.g. `salt -a ldap 'some-minion' test.ping`) aren't able to communicate via `AF_UNIX` sockets with the master, but instead only talk via `AF_INET` sockets with the master's ZeroMQ.\r\n\r\nThey use the `interface` option from the master's options to determine the communication socket for `salt.payload.SREQ()`, so when executing any Salt CLI tools from the same host as on which the master daemon is running, everything is fine as this will open an `AF_INET` socket to `tcp://0.0.0.0:4506`, but it will fail in environments where only the socket files are shared between the environment in which the master daemon runs and where the CLI tools are executed, e.g.:\r\n\r\n  * a Docker container to run the daemon\r\n  * a Docker container which shares directories/volumes with the 1st container (`/etc/salt`, `/run/salt/master`, `/srv/salt`, `/var/cache/salt/master`) and is used to work with the CLI tools\r\n\r\nTo make it a bit more short:\r\nThe methods `token_cli`, `mk_token` and `get_token` in `salt/auth/__init__.py` should not only allow to to initialize `sreq` with `tcp://{0[interface]}:{0[ret_port]}'.format(self.opts)` but should also be able to use IPC sockets in `/run/salt/master` when they're existent/writeable.\r\n\r\nThe same applies probably to many other places where `SREQ()` is [used in a similar way](https://eliasprobst.eu/~elias/stuff/salt.payload.SREQ.html)."
7903,'basepi',"salt-ssh: add state.single, cp.list_master, and similar to wrapper\nIn a continue to this [salt-users thread](https://groups.google.com/forum/#!topic/salt-users/3gzXOMHXpy8)\r\n\r\nWhen using salt-ssh to copy files from the ssh-master to the ssh-minion it won't find the file in  the master.\r\nfor example:\r\n\r\n```\r\n/etc/salt/master\r\n------------------------\r\nfile_roots:\r\n  base:\r\n    - /srv/salt\r\n\r\nls -l /srv/salt/ran.sls \r\n-rw-r--r-- 1 root root 24 Oct 16 10:47 /srv/salt/ran.sls\r\n```\r\n\r\n```\r\n[root@master salt]# salt-ssh saltdev state.single file.managed name='/root/ran.xxx' source='salt://ran.sls'\r\nsaltdev:\r\n    ----------\r\n    local:\r\n        ----------\r\n        file_|-/root/ran.xxx_|-/root/ran.xxx_|-managed:\r\n            ----------\r\n            __run_num__:\r\n                0\r\n            changes:\r\n                ----------\r\n            comment:\r\n                Source file salt://ran.sls not found\r\n            name:\r\n                /root/ran.xxx\r\n            result:\r\n                False\r\n```\r\n\r\nalso, when I issue salt-ssh saltdev cp.list_master it returns empty ...\r\n\r\nSalt 0.17.0 RHEL 6.4 64bit (from epel)\r\n\r\nThanks !\r\n"
7780,'s0undt3ch','Need test case for df sanitization\nAs requested, this is a reminder for @s0undt3ch. Relevant to #7774, #7776, #7779.'
7772,'terminalmage','Version support for "sources" arg in pkg.installed states\nExample config:\r\n\r\n```\r\nerlang:\r\n  pkg.installed:\r\n    - sources:\r\n      - esl-erlang: salt://files/packages/esl-erlangR15B02.deb\r\n```\r\n\r\nIf I were to change the above config to use a deb with the version R16B02, salt does not upgrade the package because esl-erlang is already installed, and pkg.latest does not work with a "sources" parameter.'
7747,'cachedout',"salt/auth/pam.py outdated\nThe upstream (from salt) library that has been included (almost verbatim) for pam has released a new version, presumably fixing some bugs/issues.\r\n\r\nIt appears that the currently included code is based on version 0.1.3 and 0.1.4 has been released.\r\n\r\nThis should be updated or removed and added as a dependency.\r\n\r\nI would be in favour of removing it, as the package is available in debian unstable and should be easy enough to backport to the required releases. I don't know about other distros though.\r\n\r\nI also think that the less copy/paste code we have lying around the better.\r\n\r\nThe actual salty bit required for pam authentication is tiny!\r\n\r\nThanks,\r\n\r\nJoe"
7545,'thatch45','Codernity Database\nProvide api and runner for codenity database to enable UI to persist workflow task configurations of users, minions, formula and pillars\r\n'
7544,'thatch45','Dynamic Eauth\nProvide the ability to create eauth permissions dynamically without restarting the master to enable workflow management from the UI'
7543,'thatch45','Cache events associate with jobs\nNeed to cache events so that we can preload the UI with the events associate with jobs so that in monitoring of infrastructure can have detailed drill down persistent view of what actually happened.'
7271,'basepi',"Wildcard Excludes\nwildcard excludes don't seem to be working. They were added in commit:\r\nhttps://github.com/saltstack/salt/commit/27e4f2da5eb652d9fdcd1f1738b28532f3fb7c90\r\n\r\nI would like to be able to do the following in an init.sls and have it exclude everything which matches user_home*\r\n```\r\nexclude:\r\n  - sls: user_home*\r\n```\r\n\r\nInstead, at the moment I have to do something like this:\r\n```\r\nexclude:\r\n  -sls: user_home.user1\r\n  -sls: user_home.user2\r\n  -sls: user_home.user3\r\n```\r\n\r\nFrom my minion:\r\n```\r\nsalt --versions-report \r\n           Salt: 0.16.2\r\n         Python: 2.6.6 (r266:84292, Jul 10 2013, 22:48:45)\r\n         Jinja2: unknown\r\n       M2Crypto: 0.20.2\r\n msgpack-python: 0.1.13\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.0.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 2.2.0.1\r\n            ZMQ: 3.2.3\r\n```\r\n\r\nFrom the master:\r\n```\r\nsalt --versions-report \r\n           Salt: 0.16.3\r\n         Python: 2.7.3 (default, Aug  1 2012, 05:14:39)\r\n         Jinja2: 2.6\r\n       M2Crypto: 0.21.1\r\n msgpack-python: 0.1.10\r\n   msgpack-pure: Not Installed\r\n       pycrypto: 2.4.1\r\n         PyYAML: 3.10\r\n          PyZMQ: 13.0.0\r\n            ZMQ: 3.2.2\r\n```\r\n\r\nThanks"
7217,'terminalmage',"package_module.upgrade() API inconsistent\nThe API for the `upgrade()` function of package manager modules is inconsistent.\r\n\r\nThe following modules don't allow any argument to be passed to `upgrade()`:\r\n* `freebsdpkg`\r\n* `pacman`\r\n* `pkgin`\r\n\r\nThe following modules implement (at least) a `refresh` parameter:\r\n* `apt`\r\n* `ebuild`\r\n* `pkgutil`\r\n* `win_pkg`\r\n* `yumpkg`\r\n* `yumpkg5`\r\n* `zypper`\r\n\r\nThe following modules have a `**kwargs` parameter, which they completely ignore:\r\n* `apt`\r\n* `pkgutil`\r\n\r\nAnd `pkgng` is a real oddball: `upgrade(jail=None, chroot=None, force=False, local=False, dryrun=False):`\r\n\r\nI would ask that the API be made more uniform."
7212,'terminalmage',"some_service_module.enable()/disable() should not take **kwargs\nThe `enable()` & `disable()` functions in several service modules accept `**kwargs`, *but ignore them*.\r\n[According to my research](https://docs.google.com/spreadsheet/ccc?key=0AtlcnvZE4HSKdDMzZGRxWEdVMzBkSVFvRWZwOWM4akE&usp=sharing) (and feel free to double-check me), only `freebsdservice` actually uses additional optional arguments to these functions (specifically, it can use `config`), so accepting `**kwargs` [edit: per se] isn't necessary for interface compliance.\r\n\r\n[edit: So, could the `**kwargs` be deprecated/removed?]\r\n~~Since they are ignored and aren't needed for API cross-compatibility, do these `**kwargs` parameters need to go through a deprecation process, or can they be removed immediately?~~"
7140,'thatch45','Avoid multiple downloads of a same file during single salt job run\nThis refers to a discussion around https://mail.google.com/mail/u/0/?shva=1#inbox/140f00ae891fb37c\r\n\r\n\r\nOne thought is that salt-minion process, keeps track of a file download attempts during single salt job. If it was already attempted and file is in the cache then do not attempt to get it again. \r\n\r\nNot sure how involved and tricky is this to implement. \r\n\r\nThis improvement will help in salt speedup during large salt jobs such as state.highstate, where salt master is hammered with redundant downloads request (and checksum calculations).'
7124,'thatch45',"salt extends not working as expected, fails to extend lists\nI checked the behavior of extends in salt, and it turns out that the following use case just won't work:\r\n\r\nhttps://gist.github.com/Rudd-O/6491847\r\n\r\nexpected is that rudd-o will have adm, wheel, plone3 and plone4 groups.\r\n\r\nthe reality is that rudd-o only has plone3 or plone4 depending on the order of inclusion in top.sls of plone3.sls or plone4.sls\r\n\r\nThis is problematic as I expected that extending would add to existing lists rather than replacing lists.\r\n\r\nIdeas?"
7119,'terminalmage',"[FreeBSD] status module doesn't work as expected\nThe only data returned by the status module is ``loadavg`` and ``uptime``:\r\n\r\n```yaml\r\n[root@starbuck ~]# salt 'ariel' status.all_status\r\nariel:\r\n    ----------\r\n    cpuinfo:\r\n        ----------\r\n    cpustats:\r\n        ----------\r\n    diskstats:\r\n        ----------\r\n    diskusage:\r\n        ----------\r\n    loadavg:\r\n        ----------\r\n        1-min:\r\n            0.85400390625\r\n        15-min:\r\n            0.89111328125\r\n        5-min:\r\n            0.92724609375\r\n    meminfo:\r\n        ----------\r\n    netdev:\r\n        ----------\r\n    netstats:\r\n        ----------\r\n    uptime:\r\n         8:23AM  up 17 days, 59 mins, 0 users, load averages: 0.85, 0.93, 0.89\r\n    vmstats:\r\n        ----------\r\n    w:"
7114,'UtahDave','Feature request: Windows Update\nA nice feature would be a management of the windows update, a way to check if there is some updates to be applied.\r\n'
7014,'terminalmage','Add corresponding state for modules.cron.set_env()\nIt would be nice to be able set e.g. `MAILTO` in the same SLS where I do my cronjob setup.'
6925,'s0undt3ch',"Properly mock path join test\nIt's being skipped, [here](https://github.com/s0undt3ch/salt/commit/5bc3491c0a33d38c48a291fade96f000faae5954)"
6859,'UtahDave','Rework Windows package manager to allow for rendering\nThis will allow for jinja and mako templating and the use of Pillar'
6853,'whiteinge','Salt formulas should be way more visible in the documentation\nHello,\r\n\r\n[Salts formulas](http://docs.saltstack.com/topics/conventions/formulas.html) are very cool but totally absent from the documentation: they aren\'t on the [main page](http://docs.saltstack.com/index.html) nor in the introductions tutorials and this is very uncool.\r\n\r\nI\'ve discovered them randomly browsing [this page](http://docs.saltstack.com/contents.html). Ironically, I\'ve been looking for a way to distribute and reuse salt states/modules/others and haven\'t been able to find it using the current documentation a search for "distribute salt states".\r\n\r\nSo salt formulas must be made way more visible and maybe mentioned in the introduction tutorials.\r\n\r\nKind regards,'
6823,'terminalmage',"propose: pkg.wait\nThis would be analogous to the `cmd.wait` but for pkg. It would install (or reinstall) the package based on a `watch`.\r\n\r\nThe motivation for this come from the following senario:\r\n\r\nUnder Gentoo, sudo is desired with the `ldap` use flag. Thus the following state is crafted:\r\n\r\n    app-admin/sudo:\r\n      portage_config.flags:\r\n        - use:\r\n          - ldap\r\n      pkg.installed:\r\n        - require:\r\n          - portage_config: app-admin/sudo\r\n\r\nThis would work if sudo is not already installed. However, if sudo is installed, the use flags are set but sudo is not rebuilt to include `ldap`.\r\n\r\nHowever, if the following state could be created:\r\n\r\n    app-admin/sudo:\r\n      portage_config.flags:\r\n        - use:\r\n          - ldap\r\n      pkg.wait:\r\n        - watch:\r\n          - portage_config: app-admin/sudo\r\n\r\nIt would rebuild sudo when the use flags change.\r\n\r\n\r\nI'm not sure if this helps any other distros, but it would be very useful for managing Gentoo."
6799,'basepi','cmd.run env= needs syntax warnings\nMarkedly different output when you don\'t give correct yaml to the env for cmd/run:\r\n```\r\nroot@salt-master:/home/salt/conf/php/conf.d# salt idd* cmd.run env="EDITOR:sed" cmd=\'echo $EDITOR\'\r\nidd0012:\r\n\r\nidd0014:\r\n\r\n```\r\nvs.\r\n```\r\nroot@salt-master:/home/salt/conf/php/conf.d# salt idd* cmd.run env="EDITOR: sed" cmd=\'echo $EDITOR\'\r\nidd0012:\r\n    sed\r\nidd0014:\r\n    sed\r\n```\r\nThere\'s a space missing in the first example, but there\'s no error warning you for invalid yaml.'
6759,'terminalmage','Add a runner or function to clear cached files\nPer a request on the mailing list, we should add a means of clearing cached files on the minion. This can likely be done using fnmatch to facilitate wildcards, and should also include a test mode (or separate function, ``check_cache`` perhaps?) to allow the user to do information gathering prior to blowing away the cache data.\r\n\r\n@thatch45 are there any caveats of which you can think, cache files that should be exempted from removal, stuff like that?'
6721,'terminalmage','"mode" is a required parameter for "network.managed: type: bond" state.\nI am trying to use a state to disable Linux (CentOS 5) bonded interfaces that was configured during kickstart.\r\n\r\n```\r\nbond1:\r\n  network.managed:\r\n    - type: bond\r\n    - enabled: False\r\n```\r\n\r\nthrows the following exception:\r\n\r\n```\r\n    State: - network\r\n    Name:      bond1\r\n    Function:  managed\r\n        Result:    False\r\n        Comment:   An exception occurred in this state: Traceback (most recent call last):\r\n  File "/usr/lib/python2.7/site-packages/salt/state.py", line 1201, in call\r\n    *cdata[\'args\'], **cdata[\'kwargs\'])\r\n  File "/usr/lib/python2.7/site-packages/salt/states/network.py", line 175, in managed\r\n    new = __salt__[\'ip.build_interface\'](name, type, enabled, kwargs)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/rh_ip.py", line 824, in build_interface\r\n    opts = _parse_settings_eth(settings, iface_type, enabled, iface)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/rh_ip.py", line 560, in _parse_settings_eth\r\n    bonding = _parse_settings_bond(opts, iface)\r\n  File "/usr/lib/python2.7/site-packages/salt/modules/rh_ip.py", line 204, in _parse_settings_bond\r\n    if opts[\'mode\'] in [\'balance-rr\', \'0\']:\r\nKeyError: \'mode\'\r\n```\r\n\r\nDocumentation does not list "mode" as a required parameter for network.managed - nor should it really be needed if the state is being used to disable something.\r\n\r\nThe workaround is to add "- mode: 0" or any other valid value.'
6660,'terminalmage','salt-call --local with gitfs\nI would like to propose the gitfs support for salt-call.\r\n\r\nThis would enable a lot of cool stuff: like easier deployment from saltstack-formulas\r\nusing docker.io or packer.io without the overhead of a complete salt-master installation.\r\n\r\nRequirements as I think of is that salt-call needs to parse /etc/salt/master.d/ or\r\n/etc/salt/minion.d/ files for correct information about what gitfs remotes there are..\r\n\r\n'
6567,'thatch45','Method to access the requesting user from module/runner\nRIght now the only way to get the username of the requestor is to instantiate the client and do client.salt_user (which only works in the runner). I know that the requestor is passed around in the payload_kwargs. Can we make that accessible in some sort of __payload__ dict or something?'
6442,'terminalmage','Add  - pkgs   option to pip.installed state.\nPlease add a - pkgs option to pip.installed state. This would work the same as the - pkgs option in the pkg state.'
6420,'thatch45','Differentiate between permissions and authentication failure\nIf the user has successfully authenticated and the token is still valid, raise an authorization exception not an authentication exception. The authorization exception is a subclass of the authentication exception so no salt code has to change but the salt-api can detect the difference.'
6207,'thatch45','Issue with Salt-Syndic and Salt-Minion Responding to Higher Master\nBackground info: I was originally running Salt 0.15.3 on Ubuntu Server 12.04. I have upgraded to Salt 0.16.0, but this did not seem to change anything. I can reproduce this issue consistently on my environment.\r\n\r\nI have a simple 3 node topology:\r\nA root master, running only salt-master, with the "order_masters" options set in the configuration file\r\nA sub-master, running salt-master, salt-syndic, and salt-minion. The salt-syndic and salt-minion are connected to the root master (ie. Their configuration files have the root master\'s IP set)\r\nA minion, running only salt-minion, connected to the sub-master\r\n\r\nMy problem is, that whenever I run test.ping (or manage.up) from the root master, I can only ever get a reply from the sub-master\'s salt-minon OR the minion\'s salt-minion, never both. It seems like the root master only communicates with either the salt-minion process on the sub-master or the salt-syndic process, but not both. From running the syndic and submaster minion in debug mode, I can see that the one that doesn\'t respond never even receives the command from the root master.\r\n\r\nI can switch which one receives the command by restarting the service that is not responding and then restarting the root master\'s salt-master service. When the minion is the one receiving commands, the commands will pause as if waiting for the sub-master salt-minion to respond, but it never does. (Simply restarting the services on the sub-master does nothing, I have to restart the root master\'s service before I see any changes.) Both the salt-syndic and the salt-minion on the submaster seem to exchange keys with the root master just fine when they start, so they are communicating with the root master. This makes me suspect that the problem is with the salt-master service on the root master.\r\n\r\nObviously the salt-syndic is working, because I can communicate just fine with the minion (the one that is connected to the submaster) when that one is the one that is last restarted. Also, this is a simple setup for testing purposes - there is nothing running on these machines except for SaltStack. (I have a Python script I am using to deploy salt-syndic topologies that I can include if anyone thinks it would be helpful.)\r\n\r\nI originally posted this as a discussion on the mailing list here: https://groups.google.com/forum/#!topic/salt-users/_dr_OOIzzJY'
6202,'s0undt3ch',"salt logging aggregator\nHi,\r\n\r\nI don't know if the feature was discussed or not, but ...\r\nI'd love to see something like https://pypi.python.org/pypi/ScribeHandler in Salt.\r\n\r\neg:\r\n\r\nimport logging\r\nimport salt\r\nmylogger = logging.getLogger('SaltLoggingHandler')\r\nmylogger.setLevel(logging.DEBUG)\r\nhandler = salt.SaltLoggingHandler(category='%(hostname)s-%(levelname)s')\r\nmylogger.addHandler(handler)\r\nmylogger.debug('stuff happens')\r\n\r\n\r\nOf course SaltLoggingHandler should send logs using ZMQ socket to the minion on the local machine.\r\nThen minion should send logs to the master and there should be collected. The whole should be Internet connection fault tolerant, etc.\r\n\r\nIt would be ideal to have the ability to view logs to real-time on master.\r\n\r\n\r\nMaybe someone has done something like this?\r\nI would like to replace https://github.com/facebook/scribe"
5999,'thatch45','libvirt.keys does not work\ntried setting up libvirt according to http://docs.saltstack.com/topics/tutorials/cloud_controller.html\r\non ubuntu-10.04 salt-master and ubuntu 12.04 salt-minions.\r\nsalt is 0.16.0 on all systems.\r\n\r\nit somehow failed. i don\'t know what should happen excactly, since documentation on this topic could be better, but at least the pki-structure created on the master in /etc/salt/pki/libvirt seems damaged:\r\n<pre>\r\nsalt-master libvirt # ls -lhR\r\n.:\r\ntotal 4.0K\r\n-rw-r--r-- 1 root root  31 2013-07-06 13:05 ca.info\r\n-rw-r--r-- 1 root root   0 2013-07-06 13:05 cakey.pem\r\ndrwxr-xr-x 2 root root 176 2013-07-06 13:05 target\r\n\r\n./target:\r\ntotal 8.0K\r\n-rw-r--r-- 1 root root 173 2013-07-06 13:05 client.info\r\n-rw-r--r-- 1 root root   0 2013-07-06 13:05 clientkey.pem\r\n-rw-r--r-- 1 root root 121 2013-07-06 13:05 server.info\r\n-rw-r--r-- 1 root root   0 2013-07-06 13:05 serverkey.pem                                                                                                                                                                                                              \r\n</pre>\r\n\r\nnotice that all certificates a zero-byte sized files.\r\n\r\n\r\nfrom /var/log/salt/master:\r\n<pre>\r\n2013-07-06 13:05:16,237 [salt.pillar      ][ERROR   ] Failed to load ext_pillar libvirt: [Errno 2] No such file or directory: \'/etc/salt/pki/master/libvirt/cacert.pem\'                                                                                                   \r\nTraceback (most recent call last):                                                                                                                                                                                                                                        \r\n  File "/usr/lib/pymodules/python2.6/salt/pillar/__init__.py", line 349, in ext_pillar                                                                                                                                                                                    \r\n    ext = self.ext_pillars[key](pillar, val)                                                                                                                                                                                                                              \r\n  File "/usr/lib/pymodules/python2.6/salt/pillar/libvirt.py", line 32, in ext_pillar                                                                                                                                                                                      \r\n    with open(cacert, \'r\') as fp_:                                                                                                                                                                                                                                        \r\nIOError: [Errno 2] No such file or directory: \'/etc/salt/pki/master/libvirt/cacert.pem\'                                                                                                                                                                                   \r\n</pre>\r\n\r\nthis file really does not exist.\r\ni guess somehow the creation of the CA failed.\r\nother errors follow as a dependency...\r\n\r\n/etc/pki which is mentioned in the tutorial does not exist on any host involved.'
5752,'thatch45','Add a salt key subsystem\nIn my environment, we use puppet for config management, and salt for remote execution, deployment, and a number of other things. We want to avoid having two key systems, though. Salt\'s key system is fairly compatible with puppet\'s.\r\n\r\nOn the minion, it\'s possible to just reuse puppet\'s public and private keys, assuming salt knows their path. On the master, puppetca stores a PEM encoded X.509 file. It should be possible for the salt master to directly use these files as well, by just extracting the public key from the X.509 file.\r\n\r\nTo test this, on the minion:\r\n\r\n ln -s /var/lib/puppet/ssl/public_keys/$(hostname -f).pem  /etc/salt/pki/minion/minion.pub\r\n ln -s /var/lib/puppet/ssl/private_keys/$(hostname -f).pem /etc/salt/pki/minion/minion.pem\r\n\r\nAnd on the master:\r\n\r\n openssl x509 -pubkey -noout -in /var/lib/puppet/server/ssl/ca/signed/<minion>.pem  -out /etc/salt/pki/master/minions/<minion>\r\n\r\nThen, restart the minion, and everything works.\r\n\r\nIdeally we\'d be able to tell salt to use a "puppet key subsystem", then point it at puppet\'s ssl locations and salt would automatically work when puppet signs keys.'
5715,'whiteinge',"HTTP redirects should respect URL scheme\nBecause you list PGP keys and other URLs on those pages; URLs that contain secure elements behind them. e.g. The Debian installation page with information about the package signature. I feel the entire documentation site should be served over https. This will give paranoid new users (like myself) the confidence that we are dealing with the real SaltStack docs. I'm almost tempted to build the docs myself from the Github sources just to be safe.\r\n\r\nI know you guys are busy, what with your explosive growth and what not :+1: I just wanted to point out a potential attack vector that has not yet been exploited and is fairly easy to fix. "
5692,'thatch45',"Mine not working when cache is disabled\nI wanted to configure the master to not cache the pillar data of the minion.\r\nSo I've configured in the master config:\r\n```yaml\r\nminion_data_cache: false\r\n```\r\n\r\nafter that I cleaned the /var/cache/salt/master/minions directory and restarted the master & minion service.\r\nthe problem was that from that moment all the mine functions returned None, if I switched back to minion_data_cache: True everything will work as usual.\r\n\r\nHow can I disable the pillar cache and still have the mine operational ?\r\n(RHEL 6.4 salt 0.15.3)\r\n\r\nThanks"
5634,'UtahDave',"Upgrade option for Windows Minions\nA cleaner upgrade path for Windows minions would be nice. I just installed 0.15.3 over a 0.15.1 install and it clobbered the config information (c:\\salt\\conf). I suspected that was the behavior and saved my configs before upgrading, but I can see less wary users being sad from losing their configs.\r\n\r\nI checked the docs (http://docs.saltstack.com/topics/installation/windows.html) and didn't find any mention of an upgrade option so I assume this isn't available yet."
5575,'jfindlay',"Deb package compile instructions removed.. wtf?\nIn [1], the deb package compile instructions were included. However in the latest [2], they have been removed. Is there any legit reason to not have this info included? If not, then I'll get a patch done to put the info back in.\r\n\r\nThanks\r\n\r\nCal\r\n\r\n[1] http://salt.readthedocs.org/en/v0.12.1/topics/installation/debian.html\r\n[2] http://docs.saltstack.com/topics/installation/debian.html\r\n"
5542,'thatch45','master should not use two top level directories in /srv\nCurrently, by default, salt uses/expects two top level directories in /srv, /srv/salt and /srv/pillar.  While the two directories are logically separate, and pillar\'s can contain sensitive data and so on, large portions of the documents refer to and recommend using pillar data for templating states.  This makes it very appealing to have both the /srv/salt directory and the /srv/pillar directory under version control.  To this end, and to limit clutter in the system "shared" /srv top level directory, I propose that the pillar and salt directories move down a level _by default_.  Something like /srv/salt/states and /srv/salt/pillar.\r\n\r\nI\'m well aware that this is not something that would be easy to migrate from old installations.  I\'m also well aware that I can configure this myself in the salt master config.  However, I feel that the _default_ setup should only use a single top level directory.'
5504,'terminalmage',"Add state support for pkg.group_install\nAt the request of @terminalmage I'm entering this ticket based on my query in #salt.  Please add state support for pkg.group_install (in my case, particularly for yum).\r\n\r\n"
5424,'thatch45',"Salt as orchestrator with arguments\nHello,\r\n\r\nAs discussed with UtahDave, my goal is to use salt as orchestrator to deploy many cluster based on a deployment template across multiple minion.\r\n\r\nSo I’ve looked into OverStates. But my problem about it is that overstate doesn’t take arguments so I’ll have to change the match in the overstate each time I want to deploy a new cluster.\r\n\r\nI would like to have a template on how deploy a MongoDB, a hadoop cluster or a update process of an full features app (Load balancer, DB schema update … ) and feed this template with specific data.\r\n\r\nPerhaps something like that:  \r\n(Please excuse the crudity of this model. I didn't have time to build it to scale or paint it)\r\n\r\n```\r\n1 install_mongod:\r\n2.\t  match: {{ args.mongod.members }}\r\n3.\t  sls:\r\n4.\t   - mongo.server\r\n5.\tinstall_mongocs:\r\n6.\t  match: {{ args.mongocs.members }}\r\n7.\t  sls:\r\n8.\t   - mongo.config_server\r\n9.\tcreate_replica_set:\r\n10.\t  match: G@shard_name:{{ args.shard_name }}\r\n11.\t  sls:\r\n12.\t   - create_replica_set\r\n13.\t  require:\r\n14.\t   - install_mongod\r\n15.\t   - install_mongocs\r\n16.\t\r\n17.\tspawn_mongos_process:\r\n18.\t  match: {{ args.mongocs.members[0] }}\r\n19.\t  sls:\r\n20.\t   - spawn_process\r\n21.\t  require:\r\n22.\t   - create_replica_set\r\n23.\tcreate_user:\r\n24.\t  match: {{ args.mongocs.members[0] }}\r\n25.\t  sls:\r\n26.\t   - create_user\r\n27.\t  require:\r\n28.\t   - spawn_mongos_process\r\n29.\t\r\n30.\tdestroy_mongos_process:\r\n31.\t  match: {{ args.mongocs.members[0] }}\r\n32.\t  sls:\r\n33.\t   - destroy_process\r\n34.\t  require:\r\n35.\t   - create_user\r\n```\r\n\r\nBut it’s very not the best approach, since at the end I just want a list of salt ‘*’ command and perhaps not even sls. The utimate goal would be something like:\r\n\r\n```\r\nsalt 'G@mongo:*' and 'G@mongo:cluster_name:mycluster1' mongo.setup_cluster\r\n```\r\n\r\nor\r\n\r\n``` \r\nsalt-run state.over base /root/mongo_overstate.sls args {cluster_name: mycluster1, members: .... }\r\n```\r\n\r\nWell I hope my english is good enough, thank you very much for all your work !\r\n"
5373,'UtahDave','dns module for Windows\n I would likea dns module for Windows. This would be the first foray into Windows server infrastructure management'
5372,'terminalmage','SSH config affecting/breaking salt gitfs\nI always have a special conf in _~/.ssh/config_ for ssh repos, ie. github.com.\r\n\r\n    Host github.com\r\n        LogLevel DEBUG\r\n        IdentityFile ~/.ssh/whatever\r\n\r\nThe LogLevel part breaks salt-master when running (eg.) _salt \\\\* saltutil.sync_all_:\r\n\r\n    2013-06-02 11:58:20,872 [salt.master                                 ][ERROR   ] Exception len(["6311b6aa0d81bccd530167397eb29aefda170c97\\tnot-for-merge\\tbranch \'master\' of git+ssh://github.com/XXX/XXXXXXXXXX\\n"]) != len([\'debug1: Connecting to github.com [204.232.175.90] port 22.\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \' = [up to date]      master     -> origin/master\', \'debug1: client_input_channel_req: channel 0 rtype exit-status reply 0\', \'\', \'\', \'\', \'\', \'\', \'\', \'\']) occurred in file server update\r\n\r\nOr the above split for readability:\r\n\r\n    2013-06-02 11:58:20,872 [salt.master                                 ][ERROR   ] Exception\r\n    len(["6311b6aa0d81bccd530167397eb29aefda170c97\\tnot-for-merge\\tbranch \'master\' of git+ssh://github.com/XXX/XXXXXXXXXX\\n"]) \r\n        !=\r\n    len([\'debug1: Connecting to github.com [204.232.175.90] port 22.\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\', \'\r\n        = [up to date]      master     -> origin/master\', \'debug1: client_input_channel_req: channel 0 rtype exit-status reply 0\', \'\', \'\', \'\', \'\', \'\', \'\', \'\'])\r\n    occurred in file server update\r\n\r\nThe setup in question is Debian Wheezy with salt 0.15.2 built and packaged from the Github tag. Should break on anything, though.\r\n\r\n    salt@ip-10-34-204-69:~$ salt --versions-report | sed \'s/^/    /g\'\r\n               Salt: 0.15.2\r\n             Python: 2.7.3 (default, Jan  2 2013, 13:56:14)\r\n             Jinja2: 2.6\r\n           M2Crypto: 0.21.1\r\n     msgpack-python: 0.1.10\r\n       msgpack-pure: Not Installed\r\n           pycrypto: 2.6\r\n             PyYAML: 3.10\r\n              PyZMQ: 2.2.0\r\n                ZMQ: 2.2.0\r\n\r\n(It should also be noted the documentation says the _id_rsa_ file must be used with gitfs. This is obviously not true ;))'
5309,'UtahDave','Add Watchdog execution module and state\nA Watchdog module and state would allow for some interesting things.\r\nhttps://pypi.python.org/pypi/watchdog\r\n\r\n1. Notify the master if certain files were modified or deleted unexpectedly or without\r\n    authorization.\r\n2. Take action when certain files are dropped in a certain directory.'
5219,'terminalmage','improve error reports for failed states\nin short: when salt errors out when processing states for a minion, salt should better report _where_ the failure occurred.\r\n\r\nIt may be difficult to be specific about certain details, such as when jinja/etc are involved, but salt SHOULD know what state it is processing when it fails, and salt SHOULD tell the user where to start looking (the state file/etc) \r\n\r\nas a _small_ example, though there are so, _so_ many other examples:\r\n\r\n     [salt.loaded.int.module.pkg_resource][WARNING ] "name" parameter will be ignored in favor of "pkgs"\r\n\r\nif I have 30 state files that use pkgs, how do I know where to start looking?'
4994,'s0undt3ch','salt.client mess with logging\ntested on 0.14.0\r\n\r\n```\r\nimport logging\r\nimport sys\r\nimport salt.client\r\nlogger = logging.getLogger()\r\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format="Debug: %(message)s")\r\nlogger.debug("Test")\r\n```\r\n\r\nNothing show, but if I move ``salt.client`` import just after the logging is initialized:\r\n\r\n```\r\nimport logging\r\nimport sys\r\n\r\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG, format="Debug: %(message)s")\r\nimport salt.client\r\n\r\nlogger = logging.getLogger()\r\nlogger.debug("Test")\r\n```\r\n\r\nI get:\r\n\r\n```\r\nDebug: Test\r\n```'
4834,'UtahDave','Feature request: Make salt-minion interact with desktop on Windows\nThe salt-minion windows service does not interact with desktop. So, when you start a program through salt-minion, you will never see the GUI window.\r\n\r\nI tried to set the salt-minion service interacted with desktop by `sc config salt-minion type= own type= interact`. It worked. But the salt-minion displayed a console window on the desktop and this was a little annoying.\r\n\r\nIs there a way to hide the console window?'
4633,'UtahDave','salt-minion service not running after install\nAfter running the salt-minion 0.14.1 installer on Windows Server 2012 64bit; the service is not started automatically (as I thought it would as per the last step offering to run the minion).\r\n\r\nIf I visit Services I can see it installed fine, its just in a Stopped state.'
4614,'UtahDave',"Silent option for Salt Minion Windows uninstaller\nIt's immensely helpful that the Windows installer for Salt Minion has a /S switch for unattended install and I think it would be equally helpful to have a /S switch for c:\\salt\\uninst.exe so that we could do unattended uninstalls. \r\n\r\nCurrently the only method to automatically upgrade Salt Minion on Windows is to install over the previous version. Which has worked for me for 0.14.0 -> 0.14.1, but a cleaner upgrade path should be preferable.\r\n\r\nThank you!"
4598,'UtahDave','Windows Minion can\'t use multiprocessing\nwith ```multiprocessing: True``` set I get the following at the cli when trying to start the Windows Minion.\r\n\r\n```\r\n[ERROR   ] Exception Can\'t pickle <function sync_modules at 0x0000000005841748>:\r\n it\'s not found as salt.loaded.int.module.saltutil.sync_modules occurred in sche\r\nduled job\r\nTraceback (most recent call last):\r\n  File "<string>", line 1, in <module>\r\n  File "C:\\salt\\python27\\lib\\multiprocessing\\forking.py", line 373, in main\r\n    prepare(preparation_data)\r\n  File "C:\\salt\\python27\\lib\\multiprocessing\\forking.py", line 482, in prepare\r\n    file, path_name, etc = imp.find_module(main_name, dirs)\r\nImportError: No module named salt-minion\r\n[DEBUG   ] Running scheduled job: __mine_interval\r\n[ERROR   ] Exception Can\'t pickle <function sync_modules at 0x0000000005841748>:\r\n it\'s not found as salt.loaded.int.module.saltutil.sync_modules occurred in sche\r\nduled job\r\nTraceback (most recent call last):\r\n  File "<string>", line 1, in <module>\r\n  File "C:\\salt\\python27\\lib\\multiprocessing\\forking.py", line 373, in main\r\n    prepare(preparation_data)\r\n  File "C:\\salt\\python27\\lib\\multiprocessing\\forking.py", line 482, in prepare\r\n    file, path_name, etc = imp.find_module(main_name, dirs)\r\nImportError: No module named salt-minion\r\n```'
4502,'UtahDave','cannot set custom installation path in windows installer\nBasiclly i do not allways want to install salt at the current default path c:\\salt and it should bepossible to set the path in the installer. Also do not forget the ability to set the path when installing salt silent/unattended.\r\n\r\nOne extra thing is that the default paths could should be rendered from this set path so that all paths in the minion config is working proper directly after installing. Related to issue https://github.com/saltstack/salt/issues/4122'
4449,'terminalmage','State to untar a file downloaded from master\nIt would be nice/helpful to be able to untar a file downloaded from a salt:// URL using file.managed or something similar directly. Right now you need to copy to a temp file, watch the temp file, and then untar using cmd.wait or something similar.'
4169,'thatch45','No problem here..\nNo problem here, just wanted to say Salt is absolutely amazing, and thank everyone involved for their hard work... You guys rock!\r\n\r\nCal'
4136,'UtahDave',"'Run Salt Minion 0.13.2' checkbox is ignored.\nEvery time I install it it automatically adds and starts the service, no matter whether I've got the box ticked or not."
3991,'thatch45','Feature request: `extend` functionality in pillar .sls\nCurrently defining defining pillar key the second time will overwrite the contents of the first definition, i.e.:\r\n```yaml\r\nfirst.sls:\r\ndata:\r\n  love\r\n\r\nsecond.sls:\r\ndata:\r\n  hate\r\n\r\ntop.sls:\r\nbase:\r\n "*":\r\n   - first\r\n   - second\r\n```\r\nThe result pillar:\r\n```yaml\r\ndata:\r\n  hate\r\n```\r\n\r\n## Proposition A\r\nAs the pillar definitions are pure data, there is no need to have unique IDs, or unique keys at all. \r\n\r\n* If there are overlapping keys defined, merge their contents by default:\r\n\r\n```yaml\r\nfirst.sls:\r\ndata:\r\n  love\r\n\r\nsecond.sls:\r\ndata:\r\n  hate\r\n```\r\nresulting pillar:\r\n```yaml\r\ndata:\r\n  love\r\n  hate\r\n```\r\n\r\n\r\n* Have keyword reserved to override default behavior and have second definition replace the first one\r\n\r\n* Have keyword reserved to have a second definition exclude data from the first one\r\n\r\n## Proposition B\r\nPillar is data, data is a set. Implement basic set operations: union, intersection, difference for overlapping data definitions.'
3904,'terminalmage','OS X / launchd service module missing.\nToday you can\'t manage services on a Mac.  This is problematic, let\'s fix it!\r\n\r\nhttps://github.com/puppetlabs/puppet/blob/master/lib/puppet/provider/service/launchd.rb proves it\'s doable, at least.\r\n\r\nAnd in additional to the global plists in \r\n    "/Library/LaunchAgents",\r\n     "/Library/LaunchDaemons",\r\n     "/System/Library/LaunchAgents",\r\n     "/System/Library/LaunchDaemons"\r\nbe sure to also include, as we\'ve got homebrew support in other aspects of salt,\r\n/Users/{{ salt[\'file.stats\'](\'/usr/local/bin/brew\')[\'user\'] }}/Library/LaunchDaemons and LaunchAgents.'
3888,'terminalmage','FreeBSD pkg installs fail when using origin names\nWhen you install a package on a FreeBSD server using an origin name, such as `sysutils/dmidecode` salt installs the package, but fails to detect it.\r\n\r\nThis is probably more of a gotcha than an issue, but using origin names is sometimes more convenient, especially for things like python ports which get "pyXX-" prefixed to the name where XX is the python version.\r\n\r\nHere is an example SLS pair\r\n```\r\n# top.sls\r\nbase:\r\n  \'*\':\r\n    - core\r\n\r\n# core.sls\r\ndmidecode:\r\n  - pkg:\r\n    - installed\r\n    - name: sysutils/dmidecode\r\n```\r\n\r\nThis is partly due to the way in which `pkg info` and `pkg_info` will list installed ports, they don\'t include the origin name. Using `pkg query` its possible to print the installed ports by origin name along with the version.\r\n\r\nWhen the `pkg_resource.find_changes` method is called its obviously looking for `sysutils/dmidecode` and not `dmidecode` and so fails.\r\n\r\nI\'ve no idea how you\'d want to handle this if I\'m perfectly honest.\r\n\r\nSome of the relevant parts from `salt-call -l debug` shown below\r\n```\r\n[INFO    ] Executing command \'/usr/sbin/pkg info\' in directory \'/root\'\r\n[DEBUG   ] output: ca_root_nss-3.14.3             The root certificate bundle from the Mozilla Project\r\ncurl-7.24.0_2                  Non-interactive tool to get files from FTP, GOPHER, HTTP(S) servers\r\ncvsps-2.1_1                    Create patchset information from CVS\r\ndmidecode-2.11                 A tool for dumping DMI (SMBIOS) contents in human-readable format\r\nexpat-2.0.1_2                  XML 1.0 parser written in C\r\ngettext-0.18.1.1               GNU gettext package\r\ngit-1.8.1.3                    Distributed source code management tool\r\ngmp-5.1.1                      A free library for arbitrary precision arithmetic\r\nlibiconv-1.14                  A character set conversion library\r\nopenssl-1.0.1_8                SSL and crypto library\r\np5-Error-0.17019               Perl module to provide Error/exception support for perl: Error\r\np5-Net-SMTP-SSL-1.01_1         An SMTP client supporting SSL\r\nperl-5.14.2_2                  Practical Extraction and Report Language\r\npkg-1.0.8                      New generation package manager\r\npy27-Babel-0.9.6               A collection of tools for internationalizing Python applications\r\npy27-Jinja2-2.6_1              Fast and easy to use stand-alone template engine\r\npy27-MarkupSafe-0.15           Implements a XML/HTML/XHTML Markup safe string for Python\r\npy27-m2crypto-0.21.1           Crypto and SSL toolkit for Python\r\npy27-msgpack-0.2.2             MessagePack (de)serializer for Python\r\npy27-pycrypto-2.6_1            The Python Cryptography Toolkit\r\npy27-pyzmq-2.2.0               Python bindings for ZeroMQ\r\npy27-salt-0.13.1               Distributed remote execution and configuration management system\r\npy27-setuptools-0.6c11_3       Download, build, install, upgrade, and uninstall Python packages\r\npy27-yaml-3.10                 Python YAML parser\r\npython27-2.7.3_6               An interpreted object-oriented programming language\r\nswig-1.3.40                    Simplified Wrapper and Interface Generator\r\nzmq-2.2.0                      Open source message queue optimised for performance\r\n[ERROR   ] Installed Packages:\r\ndmidecode changed from absent to 2.11\r\n\r\n... snipped ...\r\n\r\n----------\r\n    State: - pkg\r\n    Name:      sysutils/dmidecode\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   The following package(s) failed to install/update: sysutils/dmidecode.\r\n        Changes:   dmidecode: {\'new\': \'2.11\', \'old\': \'\'}\r\n```\r\n\r\n'
3872,'terminalmage','Specify more than one version of a package\nThis depends on #2783 being implemented.\r\n\r\nOnce multiple versions of packages are properly tracked by list_pkgs, you should be able to do the following in SLS:\r\n\r\n```\r\nsomepkg:\r\n  pkg:\r\n    - installed\r\n    - version:\r\n      - 1.2.3-4\r\n      - 1.2.3-5\r\n\r\nmypkgs:\r\n  pkg:\r\n    - installed\r\n    - pkgs:\r\n      - somepkg:\r\n        - 1.2.3-4\r\n        - 1.2.3-5\r\n      - some_other_pkg\r\n```\r\n'
3817,'terminalmage','ssh_auth.absent should have a "source" option as well.\nssh_auth.present has a "source" option that lets you install ssh keys from a file. It would be nice if ssh_auth.absent had a similar option to remove ssh keys.'
3802,'terminalmage',"debconf.set always changes setting\nI have a very simple config to enable unattended upgrades for Debian/Ubuntu machines:\r\n\r\n```yaml\r\nsecurity: \r\n  debconf.set: \r\n  - name: unattended-upgrades \r\n  - data: \r\n        'unattended-upgrades/enable_auto_updates': {'type': ' boolean', 'value':  True} \r\n  cmd.run: \r\n  - name: dpkg-reconfigure -fnoninteractive unattended-upgrades \r\n  - watch: \r\n    - debconf: unattended-upgrades \r\n  pkg.installed: \r\n  - name: unattended-upgrades \r\n```\r\n\r\nWhen I run salt the state is applied correctly:\r\n\r\n```\r\n    State: - debconf\r\n    Name:      unattended-upgrades\r\n    Function:  set\r\n        Result:    True\r\n        Comment:   \r\n        Changes:   unattended-upgrades/enable_auto_updates: True\r\n```                   \r\nHowever when I run highstate a second (or third, fourth, etc.) time it keeps changing the state every time. Checking the current setting manually shows it being set correctly:\r\n\r\n```\r\nroot@services:~# debconf-get-selections | grep unattended-upgrades\r\nunattended-upgrades\tunattended-upgrades/enable_auto_updates\tboolean\tTrue\r\n```\r\n\r\nNot having looked at the source I can think of two possible problems:\r\n\r\n* the spelling of the boolean value is incorrect: debconf uses the lowercase `true` and `false` instead of `True` and `False`. Perhaps that is causing confusion.\r\n* the state does not correctly test the current setting."
3790,'thatch45','Allow pillar to point to a file inside pillar roots.\nAllow pillar to point to a file inside pillar roots. The contents of that file then become the value of that pillar key.\r\n\r\nExample use case: deploy ssh keys'
3597,'techhat',"Implement iptables state\nManaging iptables is going to be difficult at best. This issue is here to converse about the proper way to handle it.\r\n\r\nIn iptables, there are always at least two elements that are being handled: a table (such as filter) and a chain (such as INPUT). Every chain will have a default policy (such as ACCEPT), and an ordered list of rules. Because the default policy is not always enough, it is frequently set to ACCEPT, and the final rule of the chain applies the actual policy.\r\n\r\nChains are evaluated by iptables in order. In a stateless chain, order is generally functionally unimportant: you block what you block, accept what you accept, and then apply a default policy to anything else. In a stateful chain, order is crucial: you usually have a header (a set of rules to set up the state), the set of rules to be evaluated (order is frequently unimportant here) and a footer (effectively, a default chain policy, evaluated before the actual default chain policy).\r\n\r\niptables itself knows nothing about rules on disk; iptables-apply and iptables-save are tools used to persist rules from disk to memory. The execution module addresses this to some extent. I think the correct action here is to persist the rules to disk, and not apply them to memory until all of the rules have been persisted. I also think that the user should have the option to not do this (persist=True vs persist=False).\r\n\r\nThe biggest problem we have is that as it stands, the current iptables functionality is non-deterministic: no matter what we do, every single highstate run will cause the iptables rules to be refreshed. I would never want to do this in production.\r\n\r\nWe also have a problem with abstraction. What if a rule is specified that is functionally identical to an existing rule, but is generated with matches in a different order? There would be a conflict. It seems to me that we'll need to abstract each rule.\r\n\r\nLet me lay out a simple data structure to start the conversation:\r\n\r\n``` yaml\r\nhttp:\r\n  iptables:\r\n    - managed\r\n    - persist: True\r\n    - table: filter\r\n    - chain: INPUT\r\n    - target: ACCEPT\r\n    - matches:\r\n        - state\r\n        - dport: 80\r\n        - proto: tcp\r\n        - sports: 1025-65535\r\n```\r\n\r\nI fully expect this structure to be beaten up and torn apart during this conversation."
3543,'thatch45',"Default returners config option\nThere should be a config value, that would define default set of returners, that would be used, in case user haven't specified --returner= option.\r\n\r\nThat would be a great feature for a users that are using non default returners (mysql, redis) to make sure nobody would forget to add --retuner=mysql to every salt call."
3499,'thatch45',"Add master returner\nCreate a returner such that a ```salt-call``` can be returned to the master.\r\n\r\nAbstract out the current internal master returner to make it a general purpose returner.\r\n\r\nThis will allow minions to return to the master without having to go through any other network ports or applications.\r\n\r\nThe master can then store the returns in its job cache, whether that's a regular job cache or an external job cache."
3433,'terminalmage',"No error Reporting for pkg.install \nWith salt version 0.12, comit id g671374a, (both client and server), running the following on the master:\r\nsalt stageapi01.us1.foo.com pkg.install name=bci-static version=9\r\n\r\nyields the following output on the server:\r\nstageapi01.us1.foo.com:\r\n\r\nThe version number supplied, '9' was intentionally wrong. That package does not exist to the client. The client logs an error in /var/log/salt/minion indicating as such (which is good). However, the servers output does not contain any of this error output, and there was nothing to indicate (not even $?) that anything went wrong.\r\n\r\n"
3365,'thatch45','Display watch/watch_in actions in "test mode" output\nCurrently, the changes dict is not inspected by the state renderer, and thus the actions to be taken based on watch/watch_in directives are not displayed in test mode (i.e. test=True passed to state.highstate or state.show_sls).'
3158,'thatch45','pub_silos for multi-tenant situations\nAdd pub_silos so that in a multi-tenant situation each tenant only has access to their assigned files on the file server. There are probably other issues to consider in addition to the file server.'
3022,'terminalmage','Add support for entropy package manager (Sabayon)\nNot urgent, but would be useful for the future as a number of VPS hosting services provide Sabayon as an OS option.\r\n\r\nhttp://wiki.sabayon.org/index.php?title=En:Entropy\r\n\r\n\r\nNote that this would also require that core grains be updated to distinguish between Sabayon and Gentoo. Currently, Sabayon is recognized as Gentoo due to their inclusion of both an /etc/gentoo-release and an /etc/sabayon-release.'
2747,'basepi','file.recurse exclude_pat improvements (accept a list)\nI tried to use exclude_pat but it seems not working for folders, only files. \r\nI think it will be very useful if we can ignore a subtree by matching a folder.\r\nAnd i believe that exclude_pat can be an array, so we can match more than one file by glob.\r\n\r\nSo...\r\n\r\n```yaml\r\n/home/josmar/tree:\r\n  file.recurse:\r\n    - source: salt://test/tree\r\n    - include_empty: True\r\n    - clean: True\r\n    # not sure if this regex is right, but my intention here is \r\n    # to match all files inside these folders.\r\n    - exclude_pat: \'E@(^excluded/*)|(^holder/excluded/*)\'\r\n    - user: josmar\r\n    - group: josmar\r\n    - dir_mode: 550\r\n    - file_mode: 440\r\n\r\n\r\n/home/josmar/tree/holder/excluded/managed:\r\n  file.managed:\r\n    - user: josmar\r\n    - group: josmar\r\n    - require:\r\n      - file: /home/josmar/tree\r\n```\r\n\r\nCan be improved like:\r\n\r\n```yaml\r\n\r\n/home/josmar/tree:\r\n  file.recurse:\r\n    - source: salt://test/tree\r\n    - include_empty: True\r\n    - clean: True\r\n    - exclude_pat:\r\n      - excluded\r\n      - holder/excluded\r\n    - user: josmar\r\n    - group: josmar\r\n    - dir_mode: 550\r\n    - file_mode: 440\r\n```\r\n\r\nAnd the whole "/home/josmar/tree/excluded" folder will be ignored by recurse. No copying and no cleaning.\r\n'
2735,'techhat','Add Salt-Introspection\nAdd ability for Salt to introspect an already installed and configured server and build a state file that will recreate that server as best it can.\n\nModules could be created to deal with things such as recreating a MySQL database with schema and data, etc.'
2615,'whiteinge',"Fix salt.utils.required_module_list\nCommit dc6fdd4e4b doesn't work with the salt.utils.required_module_list function."
2385,'thatch45','per-module grains/states/modules/returners\nIn the interest of sharing and code reuse, I am trying to write self-contained\r\nstate modules (i.e. `mystate/init.sls` and e.g. `mystate/templatej2`).\r\n\r\nSome of my more complex modules need custom grains, states, modules or even\r\nreturners.\r\n\r\nCurrently, these need to be put into `/_grains` etc., which is outside the\r\nstate module. I would appreciate if custom grains etc. would also be copied\r\nfrom `_grains` directories found in subdirectories.\r\n'
2276,'thatch45','cmd_yaml pillar can\'t be used on a per minion basis\nIt seems like a cmd_yaml pillar can only setup "global" data as it doesn\'t get the minion id passed as a parameter like the ext_nodes command does.\r\n\r\n(Unless I missed something in which case maybe the documentation could be expanded?)  :-)'
2011,'terminalmage',"0.10.2-2 yumpkg fails to correctly skip_verify when custom repo has gpgcheck set\nWe've got various local repo's defined in /etc/yum.repos.d - some of these had gpgcheck enabled as a local config setting.\n\nAttempting to install unsigned packages from these custom repos failed whether skip_verify was True or not.\n\nlooking at the code (https://github.com/saltstack/salt/blob/develop/salt/modules/yumpkg.py#L235), I'm guessing its setting the global yum config variable (i.e. /etc/yum.conf), which I'm double guessing is then being overridden by specific repo config. i.e.  /etc/yum.repos.d/my-repo.repo also sets gpgcheck=1. \n\nRemoving the gpgcheck=1 from the custom repo definition resolved the issue\n\nNot sure if it's worth the coding to handle individual repo definitions, if not, probably worth documenting somewhere/somehow and pointing out that skip_verify only impacts the global setting.\n\nCheers"
1985,'s0undt3ch','Minions heartbeat\nAdd the ability to know what minions are connected, the last time they connected, etc.\n\nIf we issue a test.ping we can know which minions are currently connected.\nSometimes we want to know when minion X was last connected, etc.\n\nAdding the ability to the master to know this kind of stuff might be a great addition to salt.'
1038,'thatch45',"[FR] add wait timer to batch mode\nFeature Request.\r\n\r\nThere are times when an application can restart almost immediately, but doesn't necessarily start responding for x seconds. In these cases, I'd like the ability to add a wait timer in the batch call.\r\n\r\ne.g.\r\n    salt -G 'role:webserver' -b 2 --batch-wait 30 cmd.run 'service tomcat restart'\r\n\r\nWhere it would iterate through all servers matching the grain, performing 2 at a time, with a 30 second wait between each.\r\n\r\nMany thanks"
896,'terminalmage','pkg needs to state why a package cannot be installed\n```    \r\nState: - pkg\r\n    Name:      mariadb-client-5.3\r\n    Function:  installed\r\n        Result:    False\r\n        Comment:   Package mariadb-client-5.3 failed to install\r\n        Changes:\r\n```\r\nAn output like this is not very helpful, a reason would solve a lot of headaches :)'
