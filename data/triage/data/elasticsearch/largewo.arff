@relation large.json
@attribute id integer
@attribute owner string
@attribute content string
@data
8597,'jpountz','Fix wrong error messages in MultiMatchQueryParser\nThere is an issue with the error messages of `multi_match`:\r\n\r\n```json\r\n{\r\n    "multi_match": {\r\n        "query":    "full text search",\r\n        "fields":   [ "title", "body" ]\r\n    }\r\n}\r\n```\r\n\r\nIf I do not have any "title" or "body" fields in my mapping, it threw a parse error: `No fields specified for match_all query`. Which is wrong both ways:\r\n\r\n- There are fields in my query\r\n- my query is not a "match_all" :)\r\n\r\nThis PR fix this last issue and a small typo in the doc block.'
8594,'s1monw','Added utility method\n'
8587,'s1monw',"Recovery detects false corruption if legacy checksums are present for a new written segment\nBWC tests run into a failure this morning which is caused by the verification of the old adler 32 checksums we added recently. \r\n\r\nhttp://build-us-00.elasticsearch.org/job/es_bwc_1x/5047/CHECK_BRANCH=tags%2Fv1.2.4,jdk=JDK7,label=bwc/\r\n\r\nthe problem here is the following\r\n * index was created with es `1.2.4` which still records Adler32 checksums in the legacy checksum file\r\n * we flush but apparently the last segment `_h` didn't make it into the commit but was recorded in the checksums file. \r\n * we uprade the node to `1.4.1-SNAPSHOT` - the index opens just fine\r\n * we apply the transaction log and IndexWriter starts writing a segment `_h`\r\n  * note: now we have a Adler32 checksum for `_h` in the checksum file but the files are actually not the once that where checksummed.\r\n * since we have a replica we initiate a recovery and in our `Store.java` code line `#638` we prefer the adler checksum even though we could get the original checksum from lucene.\r\n * on recovery we now compare the checksums and they obviously don't match - in turn fail the primary :-1: \r\n\r\nI think the fix here is to prefer new checksums since they are taken from the file if we know we have them...."
8585,'dakrone','Update DiskThresholdDecider javadoc\nSince v1.3.0, and issue #6201, the default values in code and documentation have been updated to 85% and 90% for low and high watermarks. However, the related javadoc still contains the initial values : this commit fix this.'
8556,'martijnvg','DateMath: Fix semantics of rounding with inclusive/exclusive ranges.\nDate math rounding currently works by rounding the date up or down based\r\non the scope of the rounding.  For example, if you have the date\r\n`2009-12-24||/d` it will round down to the inclusive lower end\r\n`2009-12-24T00:00:00.000` and round up to the non-inclusive date\r\n`2009-12-25T00:00:00.000`.\r\n\r\nThe range endpoint semantics work as follows:\r\n* `gt` - round D down, and use > that value\r\n* `gte` - round D down, and use >= that value\r\n* `lt` - round D down, and use <\r\n* `lte` - round D up, and use <=\r\n\r\nThere are 2 problems with these semantics:\r\n* `lte` ends up including the upper value, which should be non-inclusive\r\n* `gt` only excludes the beginning of the date, not the entire rounding scope\r\n\r\nThis change makes the range endpoint semantics symmetrical.  First, it\r\nchanges the parser to round up and down using the first (same as before)\r\nand last (1 ms less than before) values of the rounding scope.  This\r\nmakes both rounded endpoints inclusive. The range endpoint semantics\r\nare then as follows:\r\n* `gt` - round D up, and use > that value\r\n* `gte` - round D down, and use >= that value\r\n* `lt` - round D down, and use < that value\r\n* `lte` - round D up, and use <= that value\r\n\r\ncloses #8424'
8550,'dadoonet','Fix example in logging daily rotate configuration\nPR #8464 come with a bug in the example provided.\r\n\r\nFirst, the current log file is not compressed so it should not end with `.gz`.\r\nSecond, conversion pattern was removing all the log content but was printing only the log date.\r\nThen, the current log filename was hardcoded to `elasticsearch` instead of the cluster name.'
8539,'tlrx',"ElasticsearchIntegrationTest circular dependency from Client\nWe're having a big issue upgrading plugins to 1.4.0. We use `ElasticsearchIntegrationTest` to run our integration tests. In 1.4.0, if any class in your plugin injects `Client` it creates a circular dependency. @dr2014 put together a simple example with 4 `.java` files that demonstrates the issue.\r\n\r\nhttps://groups.google.com/forum/?fromgroups#!topic/elasticsearch/KHXzKLWoBrQ\r\n\r\nhttps://github.com/dr2014/ES1.4_IntegrationTest_Issue\r\n\r\nIf you switch back to 1.3.x everything works fine."
8535,'dakrone','[CI Failure] MemoryCircuitBreakerTests.testThreadedUpdatesToChildBreakerWithParentLimit\nhttp://build-us-1.elasticsearch.org/job/es_core_master_regression/867/\r\n\r\n\ttestThreadedUpdatesToChildBreakerWithParentLimit(org.elasticsearch.common.breaker.MemoryCircuitBreakerTests)\r\n\tREPRODUCE WITH  : mvn clean test -Dtests.seed=6124793851234CFA -Dtests.class=org.elasticsearch.common.breaker.MemoryCircuitBreakerTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=Europe/Berlin -Dtests.method="testThreadedUpdatesToChildBreakerWithParentLimit" -Des.logger.level=DEBUG -Dtests.nightly=false -Dtests.heap.size=512m -Dtests.processors=8\r\n\tThrowable:\r\n\tcom.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=1982, name=Thread-39, state=RUNNABLE, group=TGRP-MemoryCircuitBreakerTests]\r\n\tCaused by: java.lang.AssertionError: tripped too many times: 3\r\n\tExpected: <false>\r\n\t\t got: <true>\r\n\r\n\t\t__randomizedtesting.SeedInfo.seed([6124793851234CFA]:0)\r\n\t\t[...org.junit.*]\r\n\t\torg.elasticsearch.common.breaker.MemoryCircuitBreakerTests$5.run(MemoryCircuitBreakerTests.java:186)\r\n\t\tjava.lang.Thread.run(Thread.java:745)'
8534,'s1monw','Resolve `now` in date ranges in percolator and alias filters at search time instead of parse time\nPR for #8474. Percolator queries and index alias filters are parsed once and reused as long as they exist on a node. If they contain time based range filters with a `now` expression then the alias filters and percolator queries are going to be incorrect from the moment these are constructed (depending on the date rounding).\r\n\r\n If a `range` filter or `range` query is constructed as part of adding a percolator query or a index alias filter then these get wrapped in special query or filter wrappers that defer the resolution of now at last possible moment instead of during parse time. In the case of the range filter a special Resolvable Filter makes sure that `now` is resolved when the DocIdSet is pulled and in the case of the range query `now` is resolved at query rewrite time. Both occur at the time the range filter or query is used as apposed when the query or filter is constructed during parse time.\r\n\r\nNote this PR is against the 1.x branch.'
8528,'mikemccand','[CI Failure] IndexStatsTests.throttleStats "Delete Index failed - not acked"\nhttp://build-us-1.elasticsearch.org/job/es_core_1x_metal/4765\r\n\r\n```\r\n  1> Throwable:\r\n  1> java.lang.AssertionError: Delete Index failed - not acked\r\n  1> Expected: <true>\r\n  1>      but: was <false>\r\n  1>     __randomizedtesting.SeedInfo.seed([567E75887CCCE111:EB7D7A75ECB82CA5]:0)\r\n  1>     org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n  1>     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:114)\r\n  1>     org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:110)\r\n  1>     org.elasticsearch.test.TestCluster.wipeIndices(TestCluster.java:138)\r\n  1>     org.elasticsearch.test.TestCluster.wipe(TestCluster.java:75)\r\n  1>     org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:616)\r\n  1>     org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1788)\r\n```\r\n\r\nThis failure has plagued us for some time.  It happens during test cleanup, when we ask ES to delete the index, but that delete takes > 30 seconds (times out) and causes the not acked.\r\n\r\nThe question is why the delete would take so long.\r\n\r\nWe recently (#7730) changed MockFSDirectoryService to do a flush when an index is closed, in order to run check index, and for this test that flush sometimes (rarely) takes > 30 seconds.\r\n\r\nOn the most recent iteration, I added "dump all threads" when this assert trips, so we got all threads in this failure, but ... it does not reveal much (no deadlock).  I see one thread stuck in fsync:\r\n\r\n```\r\n  1>   421) Thread[id=5152, name=elasticsearch[node_s0][generic][T#2], state=RUNNABLE, group=TGRP-IndexStatsTests]\r\n  1>         at sun.nio.ch.FileDispatcherImpl.force0(Native Method)\r\n  1>         at sun.nio.ch.FileDispatcherImpl.force(FileDispatcherImpl.java:76)\r\n  1>         at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:386)\r\n  1>         at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:316)\r\n  1>         at org.apache.lucene.store.FSDirectory.fsync(FSDirectory.java:415)\r\n  1>         at org.apache.lucene.store.FSDirectory.sync(FSDirectory.java:310)\r\n  1>         at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)\r\n  1>         at org.elasticsearch.index.store.DistributorDirectory.sync(DistributorDirectory.java:115)\r\n  1>         at org.apache.lucene.store.MockDirectoryWrapper.sync(MockDirectoryWrapper.java:233)\r\n  1>         at org.apache.lucene.store.FilterDirectory.sync(FilterDirectory.java:74)\r\n  1>         at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:4519)\r\n  1>         at org.apache.lucene.index.IndexWriter.prepareCommitInternal(IndexWriter.java:2994)\r\n  1>         at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:3097)\r\n  1>         at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3064)\r\n  1>         at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:924)\r\n  1>         at org.elasticsearch.index.shard.service.InternalIndexShard.flush(InternalIndexShard.java:628)\r\n  1>         at org.elasticsearch.test.store.MockFSDirectoryService$1.beforeIndexShardClosed(MockFSDirectoryService.java:89)\r\n```\r\n\r\nAnd then, because MockDirectoryWrapper is heavily sync\'d, I see other threads in Lucene (one merging, one trying to do refresh) blocked in MDW\'s methods.\r\n\r\nNet/net this seems to indicate that fsync of the many small files created by this test is just too slow.'
8527,'markharwood','Parser throws NullPointerException when Filter aggregation clause is empty\nAdded Junit test that recreates the error and fixed FilterParser to default to using a MatchAllDocsFilter if the requested filter clause is left empty.\r\nCloses #8438'
8519,'dakrone','[TEST] Register data.path for all nodes on close in InternalTestCluster\nWe need to register those data paths otherwise we might miss path that\r\nneed to get cleaned when using local gatway etc. which can otherwise\r\ncause imports of dangeling indices.'
8507,'jpountz','Exception from geohash_grid aggregation with array of points (ES 1.4.0)\nUsing a geohash_grid aggregation used to work on arrays of points, but with ES 1.4.0 an exception occurs instead. The following curl commands reproduce the issue on a clean installation of ES:\r\n```\r\n# create index with geo_point mapping\r\ncurl -XPUT localhost:9200/test -d \'{\r\n  "mappings": {\r\n    "test": {\r\n      "properties": {\r\n        "points": {\r\n          "type": "geo_point",\r\n          "geohash_prefix": true\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n# insert documents\r\ncurl -XPUT localhost:9200/test/test/1?refresh=true -d \'{ "points": [[1,2], [2,3]] }\'\r\ncurl -XPUT localhost:9200/test/test/2?refresh=true -d \'{ "points": [[2,3], [3,4]] }\'\r\n\r\n# perform aggregation\r\ncurl -XGET localhost:9200/test/test/_search?pretty -d \'{\r\n  "size": 0,\r\n  "aggs": {\r\n    "a1": {\r\n      "geohash_grid": {\r\n        "field": "points",\r\n        "precision": 3\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nOn Elasticsearch 1.3.5 this produces the expected result:\r\n```\r\n...\r\n  "aggregations" : {\r\n    "a1" : {\r\n      "buckets" : [ {\r\n        "key" : "s09",\r\n        "doc_count" : 2\r\n      }, {\r\n        "key" : "s0d",\r\n        "doc_count" : 1\r\n      }, {\r\n        "key" : "s02",\r\n        "doc_count" : 1\r\n      } ]\r\n    }\r\n  }\r\n...\r\n```\r\n\r\nHowever on Elasticsearch 1.4.0 this triggers a failure:\r\n```\r\n{\r\n  "took" : 76,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 3,\r\n    "failed" : 2,\r\n    "failures" : [ {\r\n      "index" : "test",\r\n      "shard" : 2,\r\n      "status" : 500,\r\n      "reason" : "QueryPhaseExecutionException[[test][2]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[1]; "\r\n    }, {\r\n      "index" : "test",\r\n      "shard" : 3,\r\n      "status" : 500,\r\n      "reason" : "QueryPhaseExecutionException[[test][3]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]]; nested: ArrayIndexOutOfBoundsException[1]; "\r\n    } ]\r\n  },\r\n  "hits" : {\r\n    "total" : 0,\r\n    "max_score" : 0.0,\r\n    "hits" : [ ]\r\n  },\r\n  "aggregations" : {\r\n    "a1" : {\r\n      "buckets" : [ ]\r\n    }\r\n  }\r\n}\r\n```\r\nThe log contains this exception:\r\n```\r\n[2014-11-17 17:28:25,121][DEBUG][action.search.type       ] [Franklin Storm] [test][3], node[-S9ijRKKQH-IEAr3sUW14Q], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1fa40d49]\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [test][3]: query[ConstantScore(cache(_type:test))],from[0],size[0]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:275)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:231)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:228)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridParser$GeoGridFactory$CellValues.setDocument(GeoHashGridParser.java:154)\r\n\tat org.elasticsearch.search.aggregations.bucket.geogrid.GeoHashGridAggregator.collect(GeoHashGridAggregator.java:73)\r\n\tat org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:161)\r\n\tat org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)\r\n\tat org.apache.lucene.search.Weight$DefaultBulkScorer.scoreAll(Weight.java:193)\r\n\tat org.apache.lucene.search.Weight$DefaultBulkScorer.score(Weight.java:163)\r\n\tat org.apache.lucene.search.BulkScorer.score(BulkScorer.java:35)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:117)\r\n\t... 7 more\r\n```'
8506,'markharwood','Bulk indexing issue - missing parent routing causes NullPointerException\nNow each error is reported in bulk response rather than causing entire bulk to fail.\r\nAdded a Junit test but the use of TransportClient means the error is manifested differently to a REST based request - instead of a NullPointer the whole of the bulk request failed with a RoutingMissingException. Changed TransportBulkAction to catch this exception and treat it the same as the existing logic for a ElasticsearchParseException - the individual bulk request items are flagged and reported individually rather than failing the whole bulk request.\r\n\r\nCloses #8365'
8500,'dakrone','[CI Failure] UpdateTests.testContextVariables failure\nhttp://build-us-1.elasticsearch.org/job/es_core_master_metal/5306/\r\n\r\n```\r\norg.elasticsearch.ElasticsearchIllegalArgumentException: failed to execute script\r\n\tat __randomizedtesting.SeedInfo.seed([C76216C8FF38338C:62E6C0522E52A40E]:0)\r\n\tat org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:201)\r\n\tat org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:176)\r\n\tat org.elasticsearch.action.update.TransportUpdateAction.shardOperation(TransportUpdateAction.java:170)\r\n\tat org.elasticsearch.action.support.single.instance.TransportInstanceSingleOperationAction$AsyncSingleAction$1.run(TransportInstanceSingleOperationAction.java:187)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: org.elasticsearch.script.groovy.GroovyScriptExecutionException: AssertionError[ttl is not within acceptable range. Expression: ((111211211 - ctx._ttl) <= (now - 1416211700028)). Values: now = 1416211700146]\r\n\tat org.elasticsearch.script.groovy.GroovyScriptEngineService$GroovyScript.run(GroovyScriptEngineService.java:253)\r\n\tat org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:197)\r\n```'
8496,'colings86','[BWC] BWC layer for GetIndex should not block in a listener\nToday we execute BWC calls against nodes that have not GetIndex API\r\nin a action listeners #onFailure method. These calls are blocking today\r\nand might be executed on a bounded thread-pool which might deadlock the\r\ncall depending on how many threads are in the pool and how the pool is\r\nsetup. These calls should run async as well.'
8495,'clintongormley',"Add Wireshark protocol dissection support\nHi guys, I've finished my Wireshark dissector for Elasticsearch. It is now merged into their master. See https://code.wireshark.org/review/#/c/4948/"
8477,'clintongormley','Elasticsearch is constantly falling\nHello,\r\n\r\nWe are running on Elasticsearch Version 1.3.4 having 3 dedicated Master nodes , 3 Nodata nodes , 15 Data nodes. Since the upgrade to 1.3* series we have suffered from a major instability issues with our ES cluster.\r\n\r\nBasically what happens is that one of the master nodes gets  a JavaOutOfMemory ,then the clusters elects a new master and he gets  a JavaOutOfMemory too,in parallel one of the data nodes gets a JavaOutOfMemory. \r\n\r\nThe Strange thing is that each node that gets JavaOutOfMemory,the process of Elasticsearch is immediately killed and restarted(After the restart the node rejoins the cluster) but the cluster is unresponsive from this point(The point where a Master and a Data node get  JavaOutOfMemory).\r\nNothing Helps to make the cluster recover from it\'s unresponsive status except a Full cluster restart.\r\nMoreover, when looking at the cluster health using Es api "curl -XGET localhost:9200/_cluster/health?pretty" it shows that the cluster\'s health is yellow whereas when i take a look at the marvel dashboard it shows that all the indexes are not reporting for X time.\r\n\r\nHaving said that i can\'t understand how can it be that The health api shows the cluster is in yellow status, whereas marvel shows that all the indexes are not reporting.\r\n\r\nAttached the log from today crash:\r\n[2014-11-13 09:19:44,374][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n\tat java.util.HashMap$KeySet.iterator(HashMap.java:912)\r\n\tat java.util.HashSet.iterator(HashSet.java:172)\r\n\tat java.util.Collections$UnmodifiableCollection$1.<init>(Collections.java:1099)\r\n\tat java.util.Collections$UnmodifiableCollection.iterator(Collections.java:1098)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.processConnectTimeout(NioClientBoss.java:119)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:83)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:19:45,292][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 09:19:45,292][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 09:20:16,184][WARN ][index.merge.scheduler    ] [elasticsearch-prod-hist06] [2014_11][3] failed to merge\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 09:20:20,503][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search\r\norg.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed\r\n\tat org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:508)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:20:20,530][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search\r\norg.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed\r\n\tat org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:508)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:20:20,522][ERROR][index.engine.internal    ] [elasticsearch-prod-hist06] [2014_11][3] failed to acquire searcher, source search\r\norg.apache.lucene.store.AlreadyClosedException: this ReferenceManager is closed\r\n\tat org.apache.lucene.search.ReferenceManager.acquire(ReferenceManager.java:98)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:711)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:647)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:508)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:688)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:677)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:28:41,136][INFO ][action.admin.cluster.node.shutdown] [elasticsearch-prod-hist06] shutting down in [200ms]\r\n[2014-11-13 09:28:41,350][INFO ][action.admin.cluster.node.shutdown] [elasticsearch-prod-hist06] initiating requested shutdown...\r\n[2014-11-13 09:28:41,351][INFO ][node                     ] [elasticsearch-prod-hist06] stopping ...\r\n[2014-11-13 09:28:41,468][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [transport disconnected (with verified connect)]\r\n[2014-11-13 09:28:41,488][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [failed to perform initial connect [[elasticsearch-prod-hist-master02][inet[/10.179.174.119:9300]] connect_timeout[30s]]]\r\n[2014-11-13 09:28:41,488][INFO ][cluster.service          ] [elasticsearch-prod-hist06] master {new [elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, previous [elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}}, removed {[elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master01][DWwFU-GDRsWOS4SiS8ja7Q][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})\r\n[2014-11-13 09:28:41,497][WARN ][discovery.ec2            ] [elasticsearch-prod-hist06] not enough master nodes after master left (reason = failed to perform initial connect [[elasticsearch-prod-hist-master02][inet[/10.179.174.119:9300]] connect_timeout[30s]]), current nodes: {[elasticsearch-prod-hist11][SD_0-pzqRV6Nd5sLEWklKg][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][l0oz1_vRQOuM2tgv9jlBVQ][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][NMHy5dyvTZeyZEaODIrr7A][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist10][2MKPMbOtTraoVgP-tz_60Q][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist06][Ekp72e2-Sl2wXZWIP7L22w][elasticsearch-prod-hist06.totango.com][inet[ip-10-63-144-12.ec2.internal/10.63.144.12:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][HuHAIiKWR4-r_-Y01y7uyQ][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][vuHFjWwoR46_7Mbu-hwJ_g][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist09][y6eEqkI_Tqmw-CI9UH3zCw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][PHir_YKIQMu_TWzT9XfL_Q][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd02][dtiEiaMmQgGYgcFiFsJpTA][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][uhrR_7uAT2ml1pYeWdQLKw][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist01][aTACc8Y6StGq5tLEQwPTVw][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist04][1FMiCjVOSXSSdowYKClEIA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][HhAGRdUaSpKKV4VxjfVymg][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd01][SARDt4MpRv2nXSBZ3IbB3Q][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist02][NjMPR4fRSUi0mIkgfuaoJw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist13][5c4jQFWiSyKGeVLneeiqXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][BcPwD4AtRd-gNdw3UHBXEA][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}\r\n[2014-11-13 09:28:41,498][INFO ][cluster.service          ] [elasticsearch-prod-hist06] removed {[elasticsearch-prod-hist11][SD_0-pzqRV6Nd5sLEWklKg][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][l0oz1_vRQOuM2tgv9jlBVQ][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][NMHy5dyvTZeyZEaODIrr7A][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist10][2MKPMbOtTraoVgP-tz_60Q][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][HuHAIiKWR4-r_-Y01y7uyQ][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][vuHFjWwoR46_7Mbu-hwJ_g][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist09][y6eEqkI_Tqmw-CI9UH3zCw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][PHir_YKIQMu_TWzT9XfL_Q][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][dtiEiaMmQgGYgcFiFsJpTA][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][uhrR_7uAT2ml1pYeWdQLKw][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist01][aTACc8Y6StGq5tLEQwPTVw][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist04][1FMiCjVOSXSSdowYKClEIA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][HhAGRdUaSpKKV4VxjfVymg][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd01][SARDt4MpRv2nXSBZ3IbB3Q][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist02][NjMPR4fRSUi0mIkgfuaoJw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist13][5c4jQFWiSyKGeVLneeiqXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][BcPwD4AtRd-gNdw3UHBXEA][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master02][jyV18jyuRu2tk0fkRnPetA][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})\r\n[2014-11-13 09:28:41,505][WARN ][action.bulk              ] [elasticsearch-prod-hist06] Failed to perform bulk/shard on remote replica [elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false}[2014_11][1]\r\norg.elasticsearch.transport.NodeDisconnectedException: [elasticsearch-prod-hist15][inet[/10.203.179.73:9300]][bulk/shard/replica] disconnected\r\n[2014-11-13 09:28:41,505][WARN ][action.bulk              ] [elasticsearch-prod-hist06] Failed to perform bulk/shard on remote replica [elasticsearch-prod-hist15][5KzNeqYLQuqLIVYdhVgc8Q][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false}[2014_11][1]\r\norg.elasticsearch.transport.NodeDisconnectedException: [elasticsearch-prod-hist15][inet[/10.203.179.73:9300]][bulk/shard/replica] disconnected\r\n[2014-11-13 09:28:41,506][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist06] can\'t send shard failed for [2014_11][1], node[5KzNeqYLQuqLIVYdhVgc8Q], [R], s[STARTED]. no master known.\r\n[2014-11-13 09:28:41,507][WARN ][cluster.action.shard     ] [elasticsearch-prod-hist06] can\'t send shard failed for [2014_11][1], node[5KzNeqYLQuqLIVYdhVgc8Q], [R], s[STARTED]. no master known.\r\n[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-c361d029-0][elasticsearch-prod-hist06.totango.com][inet[/10.7.144.161:9300]]]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist03][inet[/10.7.144.161:9300]][discovery/zen/unicast]\r\nCaused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-e76adb0d-0][elasticsearch-prod-hist06.totango.com][inet[/10.30.193.43:9300]]]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist05][inet[/10.30.193.43:9300]][discovery/zen/unicast]\r\nCaused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-3a6edfd0-0][elasticsearch-prod-hist06.totango.com][inet[/10.69.17.49:9300]]]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist14][inet[/10.69.17.49:9300]][discovery/zen/unicast]\r\nCaused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:28:41,984][WARN ][discovery.zen.ping.unicast] [elasticsearch-prod-hist06] failed to send ping to [[#cloud-i-bd51e057-0][elasticsearch-prod-hist06.totango.com][inet[/10.179.146.242:9300]]]\r\norg.elasticsearch.transport.RemoteTransportException: [elasticsearch-prod-hist13][inet[/10.179.146.242:9300]][discovery/zen/unicast]\r\nCaused by: org.elasticsearch.ElasticsearchIllegalStateException: received ping request while stopped/closed\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.handlePingRequest(UnicastZenPing.java:392)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing.access$2400(UnicastZenPing.java:59)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:430)\r\n\tat org.elasticsearch.discovery.zen.ping.unicast.UnicastZenPing$UnicastPingRequestHandler.messageReceived(UnicastZenPing.java:414)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleRequest(MessageChannelHandler.java:217)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:111)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:28:42,207][INFO ][node                     ] [elasticsearch-prod-hist06] stopped\r\n[2014-11-13 09:28:42,207][INFO ][node                     ] [elasticsearch-prod-hist06] closing ...\r\n[2014-11-13 09:28:42,221][INFO ][node                     ] [elasticsearch-prod-hist06] closed\r\n[2014-11-13 09:34:14,165][INFO ][node                     ] [elasticsearch-prod-hist06] version[1.3.4], pid[27430], build[a70f3cc/2014-09-30T09:07:17Z]\r\n[2014-11-13 09:34:14,165][INFO ][node                     ] [elasticsearch-prod-hist06] initializing ...\r\n[2014-11-13 09:34:14,380][INFO ][plugins                  ] [elasticsearch-prod-hist06] loaded [cloud-aws, marvel], sites [marvel]\r\n[2014-11-13 09:34:18,354][INFO ][node                     ] [elasticsearch-prod-hist06] initialized\r\n[2014-11-13 09:34:18,354][INFO ][node                     ] [elasticsearch-prod-hist06] starting ...\r\n[2014-11-13 09:34:18,479][INFO ][transport                ] [elasticsearch-prod-hist06] bound_address {inet[/0.0.0.0:9300]}, publish_address {inet[/10.63.144.12:9300]}\r\n[2014-11-13 09:34:18,542][INFO ][discovery                ] [elasticsearch-prod-hist06] totango_prod_hist/AdEjYlsIRw6W0p-NahHGKA\r\n[2014-11-13 09:34:30,557][INFO ][cluster.service          ] [elasticsearch-prod-hist06] detected_master [elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, added {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])\r\n[2014-11-13 09:34:30,809][INFO ][http                     ] [elasticsearch-prod-hist06] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/10.63.144.12:9200]}\r\n[2014-11-13 09:34:30,810][INFO ][node                     ] [elasticsearch-prod-hist06] started\r\n[2014-11-13 09:34:31,550][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])\r\n[2014-11-13 09:34:32,559][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])\r\n[2014-11-13 09:34:33,569][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])\r\n[2014-11-13 09:34:34,580][INFO ][cluster.service          ] [elasticsearch-prod-hist06] added {[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-receive(from master [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}])\r\n[2014-11-13 09:57:47,420][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n\tat org.elasticsearch.common.netty.buffer.ChannelBuffers.buffer(ChannelBuffers.java:134)\r\n\tat org.elasticsearch.common.netty.buffer.HeapChannelBufferFactory.getBuffer(HeapChannelBufferFactory.java:68)\r\n\tat org.elasticsearch.common.netty.buffer.AbstractChannelBufferFactory.getBuffer(AbstractChannelBufferFactory.java:48)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:80)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n[2014-11-13 09:59:03,811][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 09:59:04,613][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 09:59:04,643][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:30,821][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x7eaa6ee8, /10.179.174.119:48642 => /10.63.144.12:9300]], closing connection\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:40,859][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:40,860][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:40,859][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:49,551][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:02:51,862][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x7eaa6ee8, /10.179.174.119:48642 => /10.63.144.12:9300]], closing connection\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:04:16,168][WARN ][transport.netty          ] [elasticsearch-prod-hist06] exception caught on transport layer [[id: 0x1b5ad6c8, /10.63.144.12:53190 => /10.179.146.242:9300]], closing connection\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:04:17,117][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:04:16,167][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:06:42,143][WARN ][netty.channel.socket.nio.AbstractNioSelector] Unexpected exception in the selector loop.\r\njava.lang.OutOfMemoryError: Java heap space\r\n[2014-11-13 10:07:13,079][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [failed to ping, tried [3] times, each with  maximum [30s] timeout]\r\n[2014-11-13 10:07:13,755][INFO ][cluster.service          ] [elasticsearch-prod-hist06] master {new [elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}, previous [elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}}, removed {[elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master02][Ln7bOuJrS_akmcrMQ5QB8Q][elasticsearch-prod-hist-master02.totango.com][inet[/10.179.174.119:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})\r\n[2014-11-13 10:07:18,907][INFO ][discovery.ec2            ] [elasticsearch-prod-hist06] master_left [[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true}], reason [no longer master]\r\n[2014-11-13 10:07:18,908][WARN ][discovery.ec2            ] [elasticsearch-prod-hist06] not enough master nodes after master left (reason = no longer master), current nodes: {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist06][AdEjYlsIRw6W0p-NahHGKA][elasticsearch-prod-hist06.totango.com][inet[ip-10-63-144-12.ec2.internal/10.63.144.12:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}\r\n[2014-11-13 10:07:18,919][INFO ][cluster.service          ] [elasticsearch-prod-hist06] removed {[elasticsearch-prod-hist13][7iNunaxMQdGB8dic_Y2lXg][elasticsearch-prod-hist13.totango.com][inet[/10.179.146.242:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist05][M2XgBd9JSmWHqRQZ5cgj5Q][elasticsearch-prod-hist05.totango.com][inet[/10.30.193.43:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist12][mNgZpVfrT5-IrtDGPWfURQ][elasticsearch-prod-hist12.totango.com][inet[/10.67.142.94:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist03][lAj9n3NDQUOv9IbWtlLq4A][elasticsearch-prod-hist03.totango.com][inet[/10.7.144.161:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-nd03][Xp6HPZorQJKsZM4PjnXBog][elasticsearch-prod-hist-nd03.totango.com][inet[/10.69.53.78:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-master03][v_8fEEtiRSW70Uj4PKJJvg][elasticsearch-prod-hist-master03.totango.com][inet[/10.154.233.247:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist09][ZcjG4XaeQr6gh-a_DkNIWw][elasticsearch-prod-hist09.totango.com][inet[/10.231.52.214:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist10][E4y5yg4YRCiEkgiiz2t9HQ][elasticsearch-prod-hist10.totango.com][inet[/10.144.216.229:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true},[elasticsearch-prod-hist-nd02][DauYY04MQei6CfVw6Xvkqw][elasticsearch-prod-hist-nd02.totango.com][inet[/10.181.32.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist-nd01][Ch6aYSnDQkO3H_-aXXDckw][elasticsearch-prod-hist-nd01.totango.com][inet[/10.153.214.223:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=false},[elasticsearch-prod-hist04][oSanqbQPQA2Oi0yxLEa9hA][elasticsearch-prod-hist04.totango.com][inet[/10.182.54.85:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist15][qAn5cAEGSLS9YvcI7pDoOA][elasticsearch-prod-hist15.totango.com][inet[/10.203.179.73:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist01][HfAEifKqS86YIuTy0yLyxQ][elasticsearch-prod-hist01.totango.com][inet[/10.143.223.48:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist07][BqAauJRMS0yDcSgXkdrGIQ][elasticsearch-prod-hist07.totango.com][inet[/10.146.186.210:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist08][WXMjYXo-QbKE1eWOQebciw][elasticsearch-prod-hist08.totango.com][inet[/10.101.151.169:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist11][VbuIH5Y3THKLeJlXO2kIug][elasticsearch-prod-hist11.totango.com][inet[/10.218.139.4:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist14][e9hJgHCnQQuhdZPu5ImWjw][elasticsearch-prod-hist14.totango.com][inet[/10.69.17.49:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},[elasticsearch-prod-hist02][ZRjhMm4QShu8EEuR8Q-HVw][elasticsearch-prod-hist02.totango.com][inet[/10.41.173.149:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, master=false},}, reason: zen-disco-master_failed ([elasticsearch-prod-hist-master01][Soj39jZ-RJ-r-k3-okNhGw][elasticsearch-prod-hist-master01.totango.com][inet[/10.144.199.124:9300]]{max_local_storage_nodes=1, aws_availability_zone=us-east-1b, data=false, master=true})\r\n    \r\n\r\nPlease advise?\r\n\r\nThanks,\r\nCostya.'
8475,'clintongormley','[GEO] Fix for ArithmeticException[/ by zero] when parsing a polygon\nWhile this commit is primariy a fix for issue/8433 it adds more rigor to ShapeBuilder for parsing against the GeoJSON specification. Specifically, this adds LinearRing and LineString validity checks as defined in http://geojson.org/geojson-spec.html to ensure valid polygons are specified. The benefit of this fix is to provide a gate check at parse time to avoid any further processing if an invalid GeoJSON is provided.  More parse checks like these will be necessary going forward to ensure full compliance with the GeoJSON specification.\r\n\r\nCloses #8433'
8461,'martijnvg','ClassCastException in ParentIdsFilter\nSimilar to #8390, \r\n\r\n    ParentIdsFilter.java#144\r\n    nonNestedDocs = (FixedBitSet) nonNestedDocsFilter.getDocIdSet(context, acceptDocs);\r\n\r\nIf you pass acceptDocs, then the result may not be a FixedBitSet.  Not sure if the solution in #8390 works here too.'
8452,'colings86','Indices API: Fix GET index API always running all features\nPrevious to this change all features (_alias,_mapping,_settings,_warmer) are run regardless of which features are actually requested. This change fixes the request object to resolve this bug.\r\n\r\nThis change is for 1.4 and a.x branches see https://github.com/elasticsearch/elasticsearch/pull/8392 for equivalent change in master'
8447,'clintongormley','[DOCS] Corrected syntax error in search curl cmd\n'
8438,'markharwood',"Parse Failure: NullPointerException\nQuery is pretty big and I have no idea what's causing it. I don't think that's relevant anyway, IMHO the code should check for nulls and throw a more specific exception, otherwise we clients are left in the dark about what we're doing wrong.\r\nHere's the stack trace:\r\n\r\n```\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:660)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:516)\r\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:488)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:257)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.search.aggregations.bucket.filter.FilterParser.parse(FilterParser.java:42)\r\n        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:130)\r\n        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:120)\r\n        at org.elasticsearch.search.aggregations.AggregatorParsers.parseAggregators(AggregatorParsers.java:77)\r\n        at org.elasticsearch.search.aggregations.AggregationParseElement.parse(AggregationParseElement.java:60)\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:644)\r\n        ... 9 more\r\n```"
8437,'dakrone','Docs: note about confusing disk threshold settings\n'
8433,'nknize','"ArithmeticException[/ by zero]" when parsing a "polygon" "geo_shape" query with one pair of coordinates\nFor a query like this one:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "geo_shape": {\r\n      "location": {\r\n        "shape": {\r\n          "type": "polygon",\r\n          "coordinates": [[["4.901238","52.36936"]]]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nAn "ArithmeticException: / by zero" is returned enclosed in a SearchParseException:\r\n```\r\nCaused by: java.lang.ArithmeticException: / by zero\r\n\tat org.elasticsearch.common.geo.builders.ShapeBuilder$Edge.ring(ShapeBuilder.java:460)\r\n\tat org.elasticsearch.common.geo.builders.BasePolygonBuilder.createEdges(BasePolygonBuilder.java:442)\r\n\tat org.elasticsearch.common.geo.builders.BasePolygonBuilder.coordinates(BasePolygonBuilder.java:129)\r\n\tat org.elasticsearch.common.geo.builders.BasePolygonBuilder.buildGeometry(BasePolygonBuilder.java:170)\r\n\tat org.elasticsearch.common.geo.builders.BasePolygonBuilder.build(BasePolygonBuilder.java:146)\r\n\tat org.elasticsearch.index.query.GeoShapeQueryParser.getArgs(GeoShapeQueryParser.java:173)\r\n\tat org.elasticsearch.index.query.GeoShapeQueryParser.parse(GeoShapeQueryParser.java:155)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:252)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:382)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:276)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)\r\n```\r\nIf I increase the number of coordinates for the polygon in the query to two, a more acceptable and meaningful exception is being thrown: "IllegalArgumentException[Invalid number of points in LinearRing (found 2 - must be 0 or >= 4)]". Probably, the same exception should be thrown and returned in case of just one set of coordinates.'
8431,'martijnvg','index.query.parse.allow_unmapped_fields setting does not seem to allow unmapped fields in alias filters\nShould I be able to create a alias filter on an unmapped field if I have index.query.parse.allow_unmapped_fields set to true?\r\n\r\nHere is a gist of the problem:\r\nhttps://gist.github.com/loren/cde1aded2f86950f2cc4'
8430,'s1monw',"Add static index based backcompat tests\nI'm looking for initial feedback on this approach to solve #8065. I refactored in such a way that the upgrade tests can use the same framework.  I am currently still trying to get 0.90 indexes to build correctly (explicit flush doesn't seem to work?), but I think the basics are all here.\r\n\r\ncloses #8065"
8428,'gmarz','Added quotes to allow spaces in installation path\nCloses #8441'
8424,'rjernst','Rounded date ranges with `lte` should set `include_upper` to `false`\nAs discussed in https://github.com/elasticsearch/elasticsearch/issues/7203 the current behaviour of `lt`/`lte` with date rounding is inconsistent.  For instance, with the following docs indexed:\r\n\r\n    \r\n    DELETE /t\r\n    \r\n    POST /t/t/_bulk\r\n    {"index":{"_id":1}}\r\n    {"date":"2014/11/07 00:00:00"}\r\n    {"index":{"_id":2}}\r\n    {"date":"2014/11/07 01:00:00"}\r\n    {"index":{"_id":3}}\r\n    {"date":"2014/11/08 00:00:00"}\r\n    {"index":{"_id":4}}\r\n    {"date":"2014/11/08 01:00:00"}\r\n    {"index":{"_id":5}}\r\n    {"date":"2014/11/09 00:00:00"}\r\n    {"index":{"_id":6}}\r\n    {"date":"2014/11/09 01:00:00"}\r\n    \r\nThis query with `lt`:\r\n\r\n    GET /_search?sort=date\r\n    {\r\n      "query": {\r\n        "range": {\r\n          "date": {\r\n            "lt": "2014/11/08||/d"\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\ncorrectly returns `2014/11/07 00:00:00` and `2014/11/07 01:00:00`, but this query with `lte`:\r\n\r\n    GET /_search?sort=date\r\n    {\r\n      "query": {\r\n        "range": {\r\n          "date": {\r\n            "lte": "2014/11/08||/d"\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nincorrectly returns:\r\n\r\n* `2014/11/07 00:00:00` \r\n* `2014/11/07 01:00:00`\r\n* `2014/11/08 00:00:00` \r\n* `2014/11/08 01:00:00`\r\n* `2014/11/09 00:00:00` \r\n\r\nIt should not include that last document.  The `lte` parameter, when used with date rounding, should use `ceil()` to round the date up, as it does today, but `include_upper` should be set to `false`.'
8423,'brwe','Mapping update fails if _all.enabled was set to false - MergeMappingException\nIn ES 1.4.0 it is no longer possible to update the mapping if _all.enabled has been set to false and _all isn\'t present in the updating request\r\n\r\ncreate index t:\r\n\r\n    http put localhost:9200/t/\r\n\r\ncreate mapping:\r\n\r\n    http put localhost:9200/t/_mapping/default default:=\'{"dynamic": "strict", "_all": {"enabled": false}}\'\r\n\r\nupdate mapping:\r\n\r\n    http put localhost:9200/t/_mapping/default default:=\'{"dynamic": "true"}\r\n\r\nFails with:\r\n\r\n     "error": "MergeMappingException[Merge failed with failures {[mapper [_all] enabled is false now encountering true]}]", \r\n\r\n\r\nIf I include _all in the request it works as expected:\r\n\r\n    http put localhost:9200/t/_mapping/default default:=\'{"dynamic": "true", "_all": {"enabled": false}}\'\r\n    {\r\n         "acknowledged": true\r\n    }\r\n\r\n\r\nIn 1.3.5 that was possible. '
8422,'dakrone','Query: add option for analyzing wildcard/prefix to simple_query_strinq\nThe query_string query has an option for analyzing wildcard/prefix (#787) by a best effort approach.\r\n\r\nThis adds `analyze_wildcard` option also to simple_query_string.\r\n\r\nThe default is set to `false` so the existing behavior of simple_query_string is unchanged.'
8392,'colings86','Indices API: Fix GET index API always running all features\nPrevious to this change all features (_alias,_mapping,_settings,_warmer) are run regardless of which features are actually requested. This change fixes the request object to resolve this bug'
8387,'colings86','Indices API: Fixed backward compatibility issue\nIf a 1.4 node needs to forward a GET index API call to a <1.4 master it cannot use the GET index API endpoints as the master has no knowledge of it. This change detects that the master does not understand the initial request and instead tries it again using the old APIs. If these calls also do not work, an error is returned\r\n\r\nCloses #8364'
8377,'brwe','1.4.0 failed to compile and run native script score function\nHi y\'all, \r\n\r\nAfter upgrading to 1.4.0, I failed to compile my native(Java) score functions which worked well with 1.3.4.\r\nThe first issue I ran into is that my function extended `AbstractDoubleSearchScript` and call `score()` to get document score from Lucene, apparently it won\'t work now with 1.4.0.\r\n\r\nI wonder if any of you could tell me which method should be called to get the original score of one document. An example of my function looks like:\r\n```\r\npublic class BasicScorer extends AbstractDoubleSearchScript {\r\n\r\n  public static final String NAME = "basic-scorer";\r\n\r\n  public static class Factory implements NativeScriptFactory {\r\n    @Override\r\n    public ExecutableScript newScript(@Nullable Map<String, Object> params) {\r\n      return new BasicScorer();\r\n  }\r\n\r\n  private BasicScorer() {\r\n  }\r\n\r\n  @Override\r\n  public double runAsDouble() {\r\n      return score() * ((ScriptDocValues.Doubles) doc().get("rank")).getValue();\r\n  }\r\n}\r\n````\r\n\r\nThe second issue is when I just remove the \'score()\' method and query ElasticSearch with following parameter:\r\n```\r\n...\r\n     "script_score": {\r\n        "script": "basic-scorer",\r\n        "lang": "native"\r\n      }\r\n...\r\n```\r\nfollowing error was logged:\r\n```\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:665)\r\n\t... 9 more\r\nCaused by: java.lang.UnsupportedOperationException\r\n\tat org.elasticsearch.script.AbstractSearchScript.setScorer(AbstractSearchScript.java:104)\r\n\tat org.elasticsearch.common.lucene.search.function.ScriptScoreFunction.<init>(ScriptScoreFunction.java:86)\r\n\r\n```\r\nI noticed there were some groovy script function changes, but I don\'t think that should affect the interface of native score function.\r\n\r\nRegards,\r\nAndy'
8371,'bleskes',"1.4.0 tribe node fails to initialize if Marvel is installed\nWith Marvel installed, a 1.4.0 tribe node will fail with the following:\r\n\r\n[2014-11-06 14:09:34,680][ERROR][bootstrap                ] {1.4.0}: Initialization Failed ...\r\n1) Tried proxying org.elasticsearch.discovery.DiscoveryService to support a circular dependency, but it is not an interface.2) IllegalStateException[This is a proxy used to support circular references involving constructors. The object we're proxying is not constructed yet. Please wait until after injection has completed to use this object.]\r\n\r\nThis can be reproduced by setting up a working 1.4.0 tribe node, then installing Marvel.  On the next restart the node will fail with this error.  If Marvel is removed, the node will start properly."
8368,'dakrone','Raise log level on DiskThresholdDecider\nHaving not enough disk space to allocate the shard is worth warning about.\r\n\r\nCloses #8367'
8367,'dakrone','Disk free space threshold - at least a Warning message in the log file\nHi,\r\nToday i had the issue, that all my replica shards were not starting.\r\nAfter 3 hours i enabled DEBUG in logging.yml and finally spotted:\r\nLess than the required 15.0% free disk threshold (11.348983564561003% free) on node [blahblah], preventing allocation.\r\n\r\nSo i freed some space and everything is back online.\r\n\r\nIt would be great if ES emits a (at least)warning in the log file.'
8365,'markharwood','Bulk update child doc, NPE error message when parent is not specified\nIf you do a _bulk that contains an update to a child doc (parent/child) and you don\'t (or forget to) specify the parent id, you will get an NPE error message in the item response. It would be good to adjust the error message to RoutingMissingException (just like when you do a single update (not _bulk) to the same doc but forget to specify parent id.\r\n\r\nSteps to reproduce:\r\n\r\n```\r\ncurl -XDELETE localhost:9200/test1\r\n\r\ncurl -XPUT localhost:9200/test1 -d \'{\r\n  "mappings": {\r\n    "p": {},\r\n    "c": {\r\n      "_parent": {\r\n        "type": "p"\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XPUT localhost:9200/test1/c/1?parent=1 -d \'{\r\n}\'\r\n\r\ncurl -XPOST localhost:9200/test1/c/_bulk -d \'\r\n{ "update": { "_id": "1" }}\r\n{ "doc": { "foo": "bar" } }\r\n\'\r\n```\r\n\r\nResponse:\r\n\r\n```\r\n{"error":"NullPointerException[null]","status":500}\r\n```'
8364,'colings86','Issues during rolling update from 1.3.4 to 1.4.0\nCurrently doing a rolling update of my 3 node cluster from 1.3.4 to 1.4.0, but after upgrading the first machine (es-node-1), I noticed Kibana stopped working, digging through the logs, I can reproduce the error:\r\n```bash\r\ncurl es-node-1:9200/main-2014.39/_aliases\r\n```\r\nThis returns:\r\n```json\r\n{"error":"RemoteTransportException[[es-node-3][inet[/xxx.xxx.xxx.xxx:9300]][indices:admin/get]]; nested: ActionNotFoundTransportException[No handler for action [indices:admin/get]]; ","status":500}\r\n```\r\nand in the log of es-node-3:\r\n```\r\n[es-node-3] Message not fully read (request) for [128466] and action [indices:admin/get], resetting\r\n```\r\nHowever the same query on the other two nodes works as expected (tried for all of my indicies), eg:\r\n```bash\r\ncurl es-node-2:9200/main-2014.39/_aliases\r\n```\r\nIndexing seems to be happening as normal, and the _search endpoint also seems to work as expected.\r\n\r\nI\'ve now shut down es-node-3 and everything seems to work as expected, continuing with update...'
8361,'clintongormley','OOM when indexing large volumes on a single 1.4b1 node \nWe\'re importing compressed logs (about 7G uncompressed per day and testing with 30 days total) on a single elastic search node with 8G of heap of 16G total on the machine. I\'m stress-testing the single ES node (centos 7 + latest openjdk7, ES installed via the RPM) with a maximum of 70 concurrent curl processes feeding ES started from a ruby script, using the bulk API, to insert about 1000 log lines (wildfly client request logs) per curl proces. Data is added to a daily index and I optimize the index after each day\'s import is completed.\r\n\r\nAfter several hours ES 1.4b1 starts logging excessive gc activity like so:\r\n\r\n[2014-11-06 00:00:02,079][INFO ][monitor.jvm              ] [Gorr] [gc][old][32172][4535] duration [8.3s], collections [1]/[8.8s], total [8.3s]/[2.8h], memory [7.5gb]->[7.3gb]/[7.7gb], all_pools {[young] [360.8mb]->[160.5mb]/[532.5mb]}{[survivor] [0b]->[0b]/[66.5mb]}{[old] [7.1gb]->[7.1gb]/[7.1gb]}\r\n[2014-11-06 00:00:15,211][WARN ][monitor.jvm              ] [Gorr] [gc][old][32173][4536] duration [12.8s], collections [1]/[13.2s], total [12.8s]/[2.8h], memory [7.3gb]->[7.3gb]/[7.7gb], all_pools {[young] [160.5mb]->[190.1mb]/[532.5mb]}{[survivor] [0b]->[0b]/[66.5mb]}{[old] [7.1gb]->[7.1gb]/[7.1gb]}\r\n[2014-11-06 00:00:28,292][WARN ][monitor.jvm              ] [Gorr] [gc][old][32174][4537] duration [12.5s], collections [1]/[13s], total [12.5s]/[2.8h], memory [7.3gb]->[7.2gb]/[7.7gb], all_pools {[young] [190.1mb]->[101.4mb]/[532.5mb]}{[survivor] [0b]->[0b]/[66.5mb]}{[old] [7.1gb]->[7.1gb]/[7.1gb]}\r\n\r\nDuring previous runs with 2G heap I would get heap space exhausted exceptions which I initially attributed to query load coming from Kibana. However, I\'m still seeing growing memory use with a 8GB heap for ES and no queries during indexing from Kibana.\r\nI would expect the memory use to be roughly constant over time during indexing, but I only notice logged GC activity after importing > 90 days of logs (about 90GB). Last night I had gc lines like the above during more than 10 hours after which I restarted the node to improve the situation. Imports continued at their usual pace after this node restart.\r\n\r\nI\'m aware that this issue description is vague. Please advice me on what metrics you need to debug this issue.\r\n\r\n----\r\njava version "1.7.0_71"\r\nOpenJDK Runtime Environment (rhel-2.5.3.1.el7_0-x86_64 u71-b14)\r\nOpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)\r\n'
8357,'mikemccand','Add option to increase maxDeterminizedStates in Lucene\nWith https://issues.apache.org/jira/browse/LUCENE-6046 (to be in 4.10.3, once it\'s eventually released) we added protection to the various APIs (AutomatonQuery, RegexpQuery, etc.) that attempt to determinize an automaton, to give up when too many states would be created (e.g. the "exponential" worst-cases) instead of trying to consume tons of RAM/CPU.\r\n\r\nThe default limit is 10,000 states, but there could be real use cases that need a higher limit.\r\n\r\nOnce 4.10.3 is released and we\'ve upgraded we need to expose control for this new limit in Elasticsearch\'s rest/client APIs when creating automaton queries.'
8355,'jpountz',"Tab characters in YAML should throw an exception\nBetter handling of tabs vs spaces in elasticsearch.yml\r\n\r\n - Throw an exception if there is a 'tab' character in the elasticsearch.yml file\r\n\r\nCloses #8259"
8346,'mrsolo','Tests: queryThenFetch failures\n[core es master medium] (http://build-us-1.elasticsearch.org/job/es_core_master_medium/505/) fails twice testing TransportTwoNodesSearchTests.\r\n\r\n```\r\nMockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1}\r\n```\r\nand \r\n```\r\nUnclosed Searchers instance for shards: [[test][0],]\r\n```\r\n'
8344,'jpountz','Tests: RoutingBackwardCompatibilityUponUpgradeTests: SimpleHashFunction but was:<null>\nFailing on [es_core_master_small] (http://build-us-1.elasticsearch.org/job/es_core_master_small/882/testReport/org.elasticsearch.cluster.routing/RoutingBackwardCompatibilityUponUpgradeTests/testCustomRouting/)\r\n\r\n```\r\njava.lang.AssertionError: expected:<org.elasticsearch.cluster.routing.operation.hash.simple.SimpleHashFunction> but was:<null>\r\n    __randomizedtesting.SeedInfo.seed([9BB079AAA1CBCBD:6958E48DC1F33431]:0)\r\n    [...org.junit.*]\r\n    org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityUponUpgradeTests.test(RoutingBackwardCompatibilityUponUpgradeTests.java:63)\r\n    org.elasticsearch.cluster.routing.RoutingBackwardCompatibilityUponUpgradeTests.testCustomRouting(RoutingBackwardCompatibilityUponUpgradeTests.java:49)\r\n    [...sun.*, java.lang.reflect.*, com.carrotsearch.randomizedtesting.*]\r\n```\r\n\r\nAlso, see the ["DjbHashFunction" failing] (http://build-us-1.elasticsearch.org/job/es_core_master_small/882/testReport/org.elasticsearch.cluster.routing/RoutingBackwardCompatibilityUponUpgradeTests/testDefaultRouting/) in the same build.\r\n\r\nI am not able to reproduce locally.  And this doesn\'t occur routinely.'
8343,'s1monw','Gateway: Prefer recovering the state file that uses the latest format.\nCurrently MetaDataStateFormat loads the first available state file that has\r\nthe latest version. In case several files are available and some of them use\r\nthe new format while other ones use the legacy format, it should also prefer\r\nthe new format. This is typically useful when we upgrade the metadata when\r\nrecovering from the gateway: we might write the upgraded state with the new\r\nformat while the previous state used the legacy format, so we end up with\r\ntwo files having the same version but using different formats.'
8337,'javanna','[TEST] move ClusterDiscoveryConfiguration to org.elasticsearch.test.discovery\n`ClusterDiscoveryConfiguration` is part of the test infra and should get exported as part of the test jar. This is achieved by moving the class to org.elasticsearch.test.discovery'
8323,'clintongormley','typo fixes - https://github.com/vlajos/misspell_fixer\n'
8319,'clintongormley','Add elastics-rb to the list of community clients\n'
8310,'imotov',"NPE on Snapshots\nI'm seeing NPE's occur on random shards while performing a snapshot. Cluster Health throughout the process is green with very little CPU usage and heap. I usually try and run another snapshot after it finishes and the previous failed shards will complete and a new set will fail. \r\n\r\n```\r\nhost                 ip        heap.percent ram.percent load node.role master name\r\n<redacted>    127.0.1.1           55          59 0.38 d         *      Caldari\r\n<redacted> 127.0.1.1           40          56 0.01 d         m      Amarr\r\n<redacted> 127.0.1.1           44          56 0.01 d         m      Minmatar\r\n<redacted> 127.0.1.1           30          66 0.00 d         m      Gallente\r\n```\r\n\r\n```\r\n[2014-10-31 15:35:31,771][WARN ][snapshots                ] [Amarr] [[<idx_name>][2]] [s3_backups:1414767797] failed to create snapshot\r\norg.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [<idx_name>][2] null\r\n\tat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:141)\r\n\tat org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)\r\n\tat org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:456)\r\n\tat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)\r\n\t... 5 more\r\n```"
8306,'dadoonet','Logs: Change log level for mpercolate\nWhen using _mpercolate API we log by default a lot of DEBUG `Percolate shard response`.\r\nThey should be in TRACE level instead of DEBUG.'
8305,'clintongormley',"Docs: Add mention of `hyphenation_patterns_path`\nRefs ElasticSearch's HyphenationCompoundWordTokenFilterFactory.java."
8304,'jpountz','Internal: Use a 1024 byte minimum weight for filter cache entries\nThis changes the weighing function for the filter cache to use a\r\nconfigurable minimum weight for each filter cached. This value defaults\r\nto 1kb and can be configured with the\r\n`indices.cache.filter.minimum_entry_weight` setting.\r\n\r\nThis also fixes an issue with the filter cache where the concurrency\r\nlevel of the cache was exposed as a setting, but not used in cache\r\nconstruction.\r\n\r\nRelates to #8268\r\nFixes #8249'
8302,'clintongormley','Add elastic, a client for Google Go.\nI worked on another client for Elasticsearch for Google Go. It is used in production for more than two years now.\r\n'
8295,'colings86','[TEST] Remove redundant call to setTemplateType()\n'
8287,'clintongormley','_recovery command returns "No handler found" error\nI tried to query a snapshot restore status by querying "GET _recovery" with the Sense.\r\n(See http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-recovery.html#indices-recovery)\r\nThis query returns "No handler found for uri [/_recovery] and method [GET]".\r\n\r\n![screenshot_10](https://cloud.githubusercontent.com/assets/6837849/4841430/a49d6872-601a-11e4-9f4b-976bf842e467.jpg)\r\n\r\nAny idea why is it happening?'
8286,'colings86','Core: Fix location information for loggers\nThis change corrects the location information gathered by the loggers so that when printing class name, method name, and line numbers in the log pattern, the information from the class calling the logger is used rather than a location within the logger itself.\r\n\r\nA reset method has also been added to the LogConfigurator class which allows the logging configuration to be reset. This is needed because if the LoggingConfigurationTests and Log4jESLoggerTests are run in the same JVM the second one to run needs to be able to override the log configuration set by the first\r\n\r\nCloses #5130, #8052'
8281,'colings86','Add the ability to sort aggregations by allowing arithmetic operations between sub aggregations\nI want to get the top X event senders for a time range but sorted not by doc count but rather by percentage increase/decrease to a different time range.\r\nI know I can do this:\r\n\r\n      "query": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "range": {\r\n                "sentTimestamp": {\r\n                  "gte": "2014-10-07T00:01:00.0Z",\r\n                  "lt": "2014-10-07T00:01:50.0Z"\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "size": 0,\r\n      "aggs": {\r\n        "topSenders": {\r\n          "terms": {\r\n            "field": "sender",\r\n            "size": 5,\r\n            "order": {\r\n              "t2>tu.value": "desc"\r\n            }\r\n          },\r\n          "aggs": {\r\n            "t1": {\r\n              "filter": {\r\n                "range": {\r\n                  "sentTimestamp": {\r\n                    "gte": "2014-10-07T00:01:00.0Z",\r\n                    "lt": "2014-10-07T00:01:40.0Z"\r\n                  }\r\n                }\r\n              },\r\n              "aggs": {\r\n                "tu": {\r\n                  "cardinality": {\r\n                    "field": "sender"\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            "t2": {\r\n              "filter": {\r\n                "range": {\r\n                  "sentTimestamp": {\r\n                    "gte": "2014-10-07T00:01:40.0Z",\r\n                    "lt": "2014-10-07T00:01:50.0Z"\r\n                  }\r\n                }\r\n              },\r\n              "aggs": {\r\n                "tu": {\r\n                  "cardinality": {\r\n                    "field": "sender"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nYou can see I am sorting by "t2>tu.value". If I was able to sort by something like "(t2>tu.value-t2>tu.value)/t1>tu.value" I would be able to get that all in one query'
8280,'tlrx',"Read time out errors when recovering a snapshot from S3 repository lets elasticsearch hanging\nDuring a snapshot restore process for a snapshot which is stored in a S3 repository, chances are that a Read Timeout error occurs when communicating to S3. Don't know whether S3 or Elasticsearch are to blame.\r\n\r\nThe issue is that that timeout error should be translated in some kind of error response from elasticsearch, but elasticsearch just keeps waiting for an answer that never comes. In consequence, besides dead points in the application code, the cluster remains in a fake recovery process, preventing it from allowing other recoveries.\r\n\r\nthis is the TimeoutError, as seen in elasticsearch logs\r\n\r\nthe endpoint is\r\n\r\n```\r\nhttp://elasticsearch.server.com/_snapshot/repository_name/snapshot_name/_restore?wait_for_completion=true\r\n```\r\n```\r\n\r\n```\r\n[2014-10-23 16:37:50,005][INFO ][snapshots                ] [Whiplash] snapshot [backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730] is done\r\n[2014-10-23 16:37:50,218][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] deleting index\r\n[2014-10-23 16:37:50,236][INFO ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] deleting index\r\n[2014-10-23 16:44:28,557][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632] re-syncing mappings with cluster state for types [[product]]\r\n[2014-10-23 16:44:28,558][WARN ][cluster.metadata         ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_phonetic_ts20141023163747520269] re-syncing mappings with cluster state for types [[product]]\r\n[2014-10-23 16:46:24,886][WARN ][indices.cluster          ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed\r\n        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)\r\n        ... 3 more\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:158)\r\n        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)\r\n        ... 4 more\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:741)\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:155)\r\n        ... 5 more\r\nCaused by: java.net.SocketTimeoutException: Read timed out\r\n        at java.net.SocketInputStream.socketRead0(Native Method)\r\n        at java.net.SocketInputStream.read(SocketInputStream.java:152)\r\n        at java.net.SocketInputStream.read(SocketInputStream.java:122)\r\n        at sun.security.ssl.InputRecord.readFully(InputRecord.java:442)\r\n        at sun.security.ssl.InputRecord.readV3Record(InputRecord.java:554)\r\n        at sun.security.ssl.InputRecord.read(InputRecord.java:509)\r\n        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:927)\r\n        at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:884)\r\n        at sun.security.ssl.AppInputStream.read(AppInputStream.java:102)\r\n        at org.apache.http.impl.io.AbstractSessionInputBuffer.read(AbstractSessionInputBuffer.java:204)\r\n        at org.apache.http.impl.io.ContentLengthInputStream.read(ContentLengthInputStream.java:182)\r\n        at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)\r\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\r\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\r\n        at java.security.DigestInputStream.read(DigestInputStream.java:161)\r\n        at com.amazonaws.services.s3.internal.DigestValidationInputStream.read(DigestValidationInputStream.java:59)\r\n        at com.amazonaws.internal.SdkFilterInputStream.read(SdkFilterInputStream.java:71)\r\n        at java.io.FilterInputStream.read(FilterInputStream.java:107)\r\n        at org.elasticsearch.cloud.aws.blobstore.AbstractS3BlobContainer$1.run(AbstractS3BlobContainer.java:99)\r\n        ... 3 more\r\n[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] sending failed shard for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]\r\n[2014-10-23 16:46:24,887][WARN ][cluster.action.shard     ] [Whiplash] [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] received shard failed for [4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0], node[4dZyhkKySw-HpxhLjDYp4A], [P], restoring[backups-6:4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730], s[INITIALIZING], indexUUID [P6kfnE_UTt6Caae5KAIXHg], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed recovery]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] restore failed]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] failed to restore snapshot [4af7ac9316f168daccf8af5f42b6271e_ts20141023163749251730]]; nested: IndexShardRestoreFailedException[[4af7ac9316f168daccf8af5f42b6271e_ts20141023163747416632][0] Failed to recover index]; nested: SocketTimeoutException[Read timed out]; ]]\r\n```\r\n\r\n"
8269,'jpountz','Inconsistent integer numbers with exponential part\n```\r\nweb245 ~ # curl \'http://web245:9200/statistics-super-fast-201406/_search?pretty\' -d \'{"size": 0, "aggs": { "filtered": { "filter": { "query": { "query_string": { "query": "_type:events AND @timestamp:2014-06-01" } } }, "aggs": { "sum": { "sum": { "field": "@value" } } } } } }\'\r\n{\r\n  "took" : 355,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 38864898,\r\n    "max_score" : 0.0,\r\n    "hits" : [ ]\r\n  },\r\n  "aggregations" : {\r\n    "filtered" : {\r\n      "doc_count" : 1326965,\r\n      "sum" : {\r\n        "value" : 1.20356041461966131E18\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n```\r\nweb245 ~ # curl \'http://web245:9200/statistics-super-fast-201406/_search?pretty\' -d \'{"size": 0, "aggs": { "filtered": { "filter": { "query": { "query_string": { "query": "_type:events AND @timestamp:2014-06-01" } } }, "aggs": { "sum": { "sum": { "field": "@value" } } } } } }\'\r\n{\r\n  "took" : 465,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 38864898,\r\n    "max_score" : 0.0,\r\n    "hits" : [ ]\r\n  },\r\n  "aggregations" : {\r\n    "filtered" : {\r\n      "doc_count" : 1326965,\r\n      "sum" : {\r\n        "value" : 1.20356041461968358E18\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nTo compare them easily:\r\n\r\n`1.20356041461966131E18`\r\n`1.20356041461968358E18`\r\n\r\nThe index is completely cold, no updates happened between queries. Same happens with facets.\r\n\r\nI am doing reindexing with compaction and my checks failed for some indices, this is the reason.\r\n\r\nBelow is the diff for 180 days (first and last day should be discarded)\r\n\r\n```diff\r\n--- slow.txt\t2014-10-29 11:59:19.000000000 +0300\r\n+++ fast.txt\t2014-10-29 12:30:19.000000000 +0300\r\n@@ -1,5 +1,4 @@\r\n "time","_type:events"\r\n-"2014-05-02T03:00:00",148035486204\r\n "2014-05-03T03:00:00",206274109000\r\n "2014-05-04T03:00:00",208608806764\r\n "2014-05-05T03:00:00",217063816673\r\n@@ -26,12 +25,12 @@\r\n "2014-05-26T03:00:00",226269192968\r\n "2014-05-27T03:00:00",218775134426\r\n "2014-05-28T03:00:00",215284624266\r\n-"2014-05-29T03:00:00",1015987720526389800\r\n-"2014-05-30T03:00:00",156505613309490800\r\n+"2014-05-29T03:00:00",1015987720541722500\r\n+"2014-05-30T03:00:00",156505613321714850\r\n "2014-05-31T03:00:00",12004020735428\r\n-"2014-06-01T03:00:00",1203560414599458800\r\n+"2014-06-01T03:00:00",1203560414619775500\r\n "2014-06-02T03:00:00",112970958246073\r\n-"2014-06-03T03:00:00",150547467159098050\r\n+"2014-06-03T03:00:00",150547467167122800\r\n "2014-06-04T03:00:00",201503104830\r\n "2014-06-05T03:00:00",193058631009\r\n "2014-06-06T03:00:00",175652820687\r\n@@ -179,4 +178,3 @@\r\n "2014-10-26T03:00:00",195305183271\r\n "2014-10-27T03:00:00",205336038145\r\n "2014-10-28T03:00:00",18642642544668\r\n-"2014-10-29T03:00:00",18489805001076\r\n```\r\n\r\nThose days are actually outliers, they have the biggest sums:\r\n\r\n```\r\nλ cat slow.txt | awk -F, \'{ print $2, $1 }\' | sort -n | tail\r\n37077344518816 "2014-09-22T03:00:00"\r\n37079209536035 "2014-10-01T03:00:00"\r\n37082482578643 "2014-10-25T03:00:00"\r\n37086732785585 "2014-10-23T03:00:00"\r\n37087873971618 "2014-09-19T03:00:00"\r\n112970958246073 "2014-06-02T03:00:00"\r\n150547467159098050 "2014-06-03T03:00:00"\r\n156505613309490800 "2014-05-30T03:00:00"\r\n1015987720526389800 "2014-05-29T03:00:00"\r\n1203560414599458800 "2014-06-01T03:00:00"\r\n```\r\n\r\n1203560414599458800 log 2 is 60.06201426993902, so there are some bits available :)'
8267,'rjernst','"random_score", acceptable values for "seed" changed between 1.3 and 1.x\nhi there,\r\n\r\nwe have successfully been using integer representations of javascript Date objects for the "seed" property of the "random_function" scoring function, i.e. output of the following function:\r\n\r\n```\r\nfunction seedval(max_age) \r\n{\r\n    var ma = max_age || 60000;\r\n    return Math.ceil(+new Date() / ma) * ma;\r\n}\r\n```\r\n\r\nwhich results in numbers like:\r\n\r\n> 1414573200000\r\n\r\nyesterday a colleague built and installed ES from source using the 1.x branch and stumbled upon the following error (it was definitely referring to the value set for "random_score.seed"!)\r\n\r\n```\r\nJsonParseException[Numeric value (1414573200000) out of range of int\r\n```\r\n\r\n> NB: I am not looking for a fix as such (we decided to simply divide our generated seed values by 1000 thereby solving our immediate problem) \r\n\r\nI am creating this issue because it just might be something that you would consider to be a regression, I have no idea which of the following might be true ... so by all means close this issue if you consider it to be irrelevant :) \r\n\r\n1. we are running ES as a 32bit process (not the case AFAICT)\r\n2. this is an intentional change/fix\r\n3. this is an unintentional change/bug - possibly related to upgrading (or introducing?) the Jackson lib, from which the parse error seems to originate.\r\n\r\nhope this is of some use, thanks for reading!\r\n\r\n\r\n\r\n'
8257,'colings86','Fix: If dangling_timeout was set to 0 and auto_import_dangled\nwas set to yes, dangling indices were deleted by mistake,\r\nbecause a RemoveDanglingIndices runnable was added\r\nto every dangling indices, without considering the auto_import_dangled\r\nsetting.'
8256,'bleskes',"Received ping response with no matching id\nI'm seeing a bunch of these on startup:\r\n```\r\n[2014-10-28 14:39:57,956][WARN ][discovery.zen.ping.multicast] [elastic1020] received ping response ping_response{target [[elastic1004][eq_H7EFGQ9SNTJuA6Wj9zw][elastic1004][inet[/10.64.0.111:9300]]{rack=A3, row=A, master=false}], master [[elastic1002][pEwiGcyESvSXFtUUwlh8qw][elastic1002][inet[/10.64.0.109:9300]]{rack=A3, row=A, master=true}], cluster_name[production-search-eqiad]} with no matching id [1]\r\n```\r\nand then everything works just fine.  Is this OK?"
8252,'s1monw','Internal: Make indexQueryParserService available from ParseContext\nCloses #8248 \r\n\r\nSorry, new pull request, the previous one had outdated commits in it. Cleaned up my local repo, now its correct.'
8246,'jpountz','or filter issue\nI have the following records\r\n\r\n```javascript\r\n{\r\n    "took": 0,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 5,\r\n        "successful": 5,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 9,\r\n        "max_score": 1,\r\n        "hits": [\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "V2xbgfWwTzuNyilVGxjElw",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F1",\r\n                    "lastName": "L1",\r\n                    "age": 20\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "fnuqEGNzTHSM8tWsjX2o1w",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F2",\r\n                    "lastName": "L1",\r\n                    "age": 21\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "3TUO53bDQ22rYFcuT-oY3g",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F4",\r\n                    "lastName": "L1",\r\n                    "age": 23\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "h6zZ5WaRQbaSIHanGFpTVg",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F5",\r\n                    "lastName": "L1",\r\n                    "age": 24\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "pOLxQxb_QXSMFG82GocYVQ",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F1",\r\n                    "lastName": "L2",\r\n                    "age": 31\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "4K7kTX32Qui4B3OGOgXahw",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F2",\r\n                    "lastName": "L2",\r\n                    "age": 32\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "HfbbfcBrRQueSTCSCWM1tg",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F3",\r\n                    "lastName": "L2",\r\n                    "age": 33\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "eqQzRvN9SqalZsqZRncb9A",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F3",\r\n                    "lastName": "L1",\r\n                    "age": 22\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "VRuZXRRWSO2PloBY-KUfcA",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F4",\r\n                    "lastName": "L2",\r\n                    "age": 34\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nWhen i query for all records where the lastName is "L1" and age 20 or lastName is L2 with age 31 using the the query :\r\n```javascript\r\n{\r\n    "query": {\r\n        "filtered": {\r\n          "filter" : {\r\n                "or": [{\r\n                    "query": {\r\n                        "filtered": {\r\n                            "filter": {\r\n                                "and": [{\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "lastName": "L1"\r\n                                        }\r\n                                    }\r\n                                }, {\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "age": 20\r\n                                        }\r\n                                    }\r\n                                }]\r\n                            }\r\n                        }\r\n                    }\r\n                }, {\r\n                    "query": {\r\n                        "filtered": {\r\n                            "filter": {\r\n                                "and": [{\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "lastName": "L2"\r\n                                        }\r\n                                    }\r\n                                }, {\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "age": 31\r\n                                        }\r\n                                    }\r\n                                }]\r\n                            }\r\n                        }\r\n                    }\r\n                }]\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nI get the results correct with the following response :\r\n```javascript\r\n{\r\n    "took": 1,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 5,\r\n        "successful": 5,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 2,\r\n        "max_score": 1,\r\n        "hits": [\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "V2xbgfWwTzuNyilVGxjElw",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F1",\r\n                    "lastName": "L1",\r\n                    "age": 20\r\n                }\r\n            },\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "pOLxQxb_QXSMFG82GocYVQ",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F1",\r\n                    "lastName": "L2",\r\n                    "age": 31\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nBut the moment i add an "and" filter to the same query where still its applicable for both the records, i get only one record in the response :\r\n```javascript\r\n{\r\n    "query": {\r\n        "filtered": {\r\n            "filter": {\r\n                "and": [{\r\n                    "query": {\r\n                        "terms": {\r\n                            "firstName": [\r\n                                "f1"\r\n                            ]\r\n                        }\r\n                    }\r\n                }],\r\n                "or": [{\r\n                    "query": {\r\n                        "filtered": {\r\n                            "filter": {\r\n                                "and": [{\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "lastName": "L1"\r\n                                        }\r\n                                    }\r\n                                }, {\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "age": 20\r\n                                        }\r\n                                    }\r\n                                }]\r\n                            }\r\n                        }\r\n                    }\r\n                }, {\r\n                    "query": {\r\n                        "filtered": {\r\n                            "filter": {\r\n                                "and": [{\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "lastName": "L2"\r\n                                        }\r\n                                    }\r\n                                }, {\r\n                                    "query": {\r\n                                        "match": {\r\n                                            "age": 31\r\n                                        }\r\n                                    }\r\n                                }]\r\n                            }\r\n                        }\r\n                    }\r\n                }]\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nResponse :\r\n```javascript\r\n{\r\n    "took": 0,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 5,\r\n        "successful": 5,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 1,\r\n        "max_score": 1,\r\n        "hits": [\r\n            {\r\n                "_index": "testindex",\r\n                "_type": "collection1",\r\n                "_id": "pOLxQxb_QXSMFG82GocYVQ",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "firstName": "F1",\r\n                    "lastName": "L2",\r\n                    "age": 31\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nIt appears to be clearly an issue with the OR filter. I have other cases where OR also fails. But this appears to give a picture about what is going on.\r\n\r\nI don\'t know exactly if there is any issue with how i am using the filter/query. If there is an alternative approach where it would work well, i would appreciate the help.\r\nLet me know if you need any more information as well.\r\nThanks.\r\n\r\n'
8239,'dakrone',"Don't use negative ramBytesUsed() values.\nThe accountable interface specifies that such values are illegal. When extending this interface to give more detailed information in https://issues.apache.org/jira/browse/LUCENE-5949, I know I added more checks and so on around this. \r\n\r\nWe should fix the places in e.g. fielddata returning -1."
8237,'costin','Improve \nCurrently `service.bat` and `elasticsearch.bat` duplicate the code required to trigger the variable and classpath initialization on windows. In a similar vein to `elasticsearch.in.sh`, a common file between the two would simplify the maintenance of the two going forward.'
8229,'imotov','Snapshots "records" persist after deletion of repo and corresponding snapshots - ES 1.3.2 \nSnapshots in Elasticsearch 1.3.2 seem to have an issue with regard to snapshot deletion and recreation.  If you create a repo and snapshot, then delete these objects, Elasticsearch will somehow retain a record of the snapshot indices and refuse to create the snapshots even after the repo, snapshot and backed up index files are deleted.\r\n\r\nHere are the steps to reproduce the problem:\r\n\r\n1. Create repo A:\r\ncurl -XPUT \'http://localhost:9200/_snapshot/A\' -d \'\r\n{\r\n    "type": "fs",\r\n    "settings": {\r\n        "location": "/media/snaps0",\r\n        "compress": true\r\n    }\r\n}\'\r\n\r\n2. Create snapshot B containing indices X and Y:\r\ncurl -XPUT \'http://localhost:9200/_snapshot/A/B\' -d \'\r\n{\r\n    "indices": [\r\n        "X", "Y"\r\n    ],\r\n    "ignore_unavailable": "true",\r\n    "include_global_state": false\r\n}\'\r\n\r\n3. Verify snapshot created in /media/snaps0, directory structure should look like this:\r\n/media/snaps0/index\r\n/media/snaps0/metadata-A  (might not be quite the right name for this file)\r\n/media/snaps0/snapshot-A\r\n/media/snaps0/indices/X   (index files in this directory not shown)\r\n/media/snaps0/indices/Y   (index files in this directory not shown)\r\n\r\n4. Delete the repo, snapshot and files in /media/snaps0:\r\ncurl -XDELETE \'http://localhost:9200/_snapshot/A/B\'\r\ncurl -XDELETE \'http://localhost:9200/_snapshot/A\'\r\nrm -rf /media/snaps0/*\r\n\r\n5. Recreate the same repo and snapshot by repeating steps 1 and 2.\r\n\r\n6. Repeating step 3 to verify the contents of /media/snaps0 will show that the /media/snaps0/indices folder is not created.\r\n\r\nSomehow Elasticsearch 1.3.2 seems to think that the snapshot of X and Y still exists and refuses to re-snapshot them the second time around.  I thinks this is a bug, because I followed the directions on snapshot and restore from the Elasticsearch.org online documentation.'
8228,'clintongormley','Update multi-get.asciidoc\nDuplicate word'
8227,'martijnvg','NonNestedDocsFilter.getDocIDSet() looks buggy\nIt modifies the docidset (casts to FixedBitSet and flips all the bits).\r\n\r\nIf this bitset was cached, this will be modifying the cached instance each time flipping all of its bits back and forth.'
8226,'s1monw','Empty index corrupts sort order\nI\'ve been trying to track down some strange sorting results and have narrowed it down to having an empty index.  I have multiple indexes all aliased to the same value.  Sort order works fine, but once you add an empty index, the order is corrupted.  Reproduction below:\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# Create index with no docs with alias test\r\ncurl -s -XPOST "http://localhost:9200/test-1"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\n\r\n# Then create a bunch of indices with alias test\r\ncurl -s XPOST "http://localhost:9200/test-2"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-2/doc" -d "{ \\"entry\\": 1 }"\r\n\r\ncurl -s -XPOST "http://localhost:9200/test-3"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-3/doc" -d "{ \\"entry\\": 2 }"\r\n\r\ncurl -s -XPOST "http://localhost:9200/test-4"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-4/doc" -d "{ \\"entry\\": 3 }"\r\n\r\ncurl -s -XPOST "http://localhost:9200/test-5"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-5/doc" -d "{ \\"entry\\": 4 }"\r\n\r\ncurl -s -XPOST "http://localhost:9200/test-6"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-6/doc" -d "{ \\"entry\\": 5 }"\r\n\r\ncurl -s -XPOST "http://localhost:9200/test-7"     -d \'{ "aliases" : { "test" : {} }}\' > /dev/null\r\ncurl -s -XPOST "http://localhost:9200/test-7/doc" -d "{ \\"entry\\": 6 }"\r\n\r\nsleep 2\r\n\r\n# Perform a sorted query, descending on field \'entry\'\r\ncurl -XPOST \'http://localhost:9200/test/_search?pretty\' -d \'{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "*"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "match_all": {}\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "highlight": {\r\n    "fields": {},\r\n    "fragment_size": 2147483647,\r\n    "pre_tags": [\r\n      "@start-highlight@"\r\n    ],\r\n    "post_tags": [\r\n      "@end-highlight@"\r\n    ]\r\n  },\r\n  "size": 500,\r\n  "sort": [\r\n    {\r\n      "entry": {\r\n        "order": "desc",\r\n        "ignore_unmapped": true\r\n      }\r\n    }\r\n  ]\r\n}\' | jq ".hits.hits[] | ._source.entry"\r\n```\r\n\r\nResults:\r\n\r\n```\r\n5\r\n1\r\n3\r\n2\r\n6\r\n4\r\n```\r\n\r\nI can fix sort order in two ways: 1) remove the empty index or 2) change "ignore_unmapped" to false.\r\n\r\nBut IMO an empty index should no affect sort results.  I\'m also confused on why "ignore_unmapped": false, fixes things.  I would think you would want the reverse, but it fails when true.'
8224,'imotov','Unassigned shards after restore\nES 1.3.3\r\n1. Have two nodes that form a cluster (one shard per index, no replicas).\r\n2. Insert some documents (multiple indices).\r\n3. Put repository, backup the cluster (global state = true, partial = false ).\r\n4. Stop both nodes.\r\n5. Delete the data folder of one of the nodes (path.data).\r\n6. Start the two nodes (the cluster has red status).\r\n7. Close all indices\r\n7. Put repository, restore (global state = true, partial = false).\r\n\r\nObserved : Half of the shards remain unassigned.\r\n\r\nNote : If deleting data folders of both nodes (on step 5), after restore everything is ok.'
8205,'clintongormley','fixed typo in documentation\n'
8202,'rjernst','Adds ability to specify \'named\' threadpool to searches.\nNamed threadpools needs to be configured in yml explicitly .\r\n\r\nFor eg:\r\n\r\nthreadpool:\r\n     fred:\r\n        type: fixed\r\n        size: 30\r\n\r\n[ Here fred , named threadpool is configured with default size of 30]\r\n\r\nSearchRequestBuilder and ScrollRequestBuilder takes threadpool name ,\r\nbut defaults to SEARCH. REST API accepts new http parameter \'threadpool\'\r\nwhich can take name of threadpool to run queries.\r\n\r\ncurl -XPUT "http://localhost:9200/movies/movie/1" -d\'\r\n> {\r\n>     "title": "The Godfather",\r\n>     "director": "Francis Ford Coppola",\r\n>     "year": 1972,\r\n>     "genres": ["Crime", "Drama"]\r\n> }\'\r\n\r\nCall with \'custom\' threadpool\r\n++\r\n\r\ncurl \'localhost:9200/_search?q=*&threadpool=fred\'\r\n{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"movies","_type":"movie","_id":"1","_score":1.0,"_source":\r\n{\r\n    "title": "The Godfather",\r\n    "director": "Francis Ford Coppola",\r\n    "year": 1972,\r\n    "genres": ["Crime", "Drama"]\r\n}\r\n\r\nInvalid threadpool name\r\n++\r\n\r\n curl \'localhost:9200/_search?q=*&threadpool=pollois\r\n\'\r\n{"error":"SearchPhaseExecutionException[Failed to execute phase [query],\r\nall shards failed; shardFailures {[J8vv81u1SKK5BUizD5Yppw][movies][0]:\r\nElasticsearchIllegalArgumentException[No executor found for\r\n[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][1]:\r\nElasticsearchIllegalArgumentException[No executor found for\r\n[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][2]:\r\nElasticsearchIllegalArgumentException[No executor found for\r\n[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][3]:\r\nElasticsearchIllegalArgumentException[No executor found for\r\n[pollois]]}{[J8vv81u1SKK5BUizD5Yppw][movies][4]:\r\nElasticsearchIllegalArgumentException[No executor found for\r\n[pollois]]}]","status":400}\r\n\r\ndefault call\r\n++\r\n\r\ncurl \'localhost:9200/_search?q=*\'\r\n{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"movies","_type":"movie","_id":"1","_score":1.0,"_source":\r\n{\r\n    "title": "The Godfather",\r\n    "director": "Francis Ford Coppola",\r\n    "year": 1972,\r\n    "genres": ["Crime", "Drama"]\r\n\r\n}\r\n\r\nThanks to way original code is done - _stats shows new threadpool stats'
8198,'clintongormley','Non-deterministic inclusion of empty string by exists filter\nThis happens with 1.3.4. Repeating these steps enough times I sometimes get both documents returned by the filter and sometimes only the one with the non-empty string.\r\n\r\nDelete the index\r\n\r\n```\r\n$ curl -XDELETE localhost:9200/test\r\n```\r\n\r\nInsert 2 documents using bulk API (one has empty string and one does not)\r\n``` \r\n$ cat requests\r\n    { "index" : { "_index" : "test", "_type" : "test", "_id" : "1" } }\r\n    { "title": "" }\r\n    { "index" : { "_index" : "test", "_type" : "test", "_id" : "2" } }\r\n    { "title": "Test document" }\r\n$ curl -XPOST localhost:9200/_bulk --data-binary @requests\r\n```\r\n\r\nThis filter sometimes returns only document 2 and sometimes both\r\n\r\n```\r\n$ curl -XPOST localhost:9200/test/_search?pretty -d \'\r\n{\r\n    "filter": {\r\n        "exists": {\r\n            "field": "title"\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nI suspect it may have to do with the order in which the documents are indexed by the bulk API. But in http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/_batch_processing.html it is mentioned that "The bulk API executes all the actions sequentially and in order" so I am wondering if this is a bug.'
8197,'areek','Completion Suggester: Fix CompletionFieldMapper to correctly parse weight\nAllows weight to be defined as a string representation of a positive integer\r\n\r\ncloses #8090'
8195,'clintongormley','Update getting-started.asciidoc\n'
8194,'clintongormley','Update getting-started.asciidoc\n'
8190,'s1monw','[ROUTING] Add rebalance enabled allocation decider\nThis commit adds the ability to enable / disable relocations\r\non an entire cluster or on individual indices for either:\r\n\r\n * `primaries` - only primaries can rebalance\r\n * `replica` - only replicas can rebalance\r\n * `all` - everything can rebalance (default)\r\n * `none` - all rebalances are disabled\r\n\r\nsimilar to the allocation enable / disable functionality.\r\n\r\nRelates to #7288'
8186,'johtani',"Settings: Validates bool values in yaml for node settings\n- Added parseBooleanExact in booleans which throws exception in case of\r\n  parse failure\r\n- Used ParseExact in static's to make it consistent\r\n\r\nCloses #8097 "
8180,'jpountz',"Aggregations: The `children` agg didn't take deleted document into account\nThe live docs that is passed down was ignored by the filter impl. Now the children filter gets wrapped with ApplyAcceptedDocsFilter, so live docs are actually applied.\r\n\r\nThis issue was reported via the user list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/elasticsearch/sybogZsMFso/f_UW4Zv01ZIJ"
8176,'clintongormley','missing quote\nfix missing quote'
8167,'clintongormley','Update filtered-query.asciidoc\nFix mistyping'
8165,'martijnvg','Parent/child: Reduce memory usage in top children query\nCloses https://github.com/elasticsearch/elasticsearch/issues/8160'
8157,'clintongormley','Update minimum-should-match.asciidoc\nAdd %-sign to examle in the last section'
8151,'clintongormley',"Added 'd' to the list of supported units.\nDay was missing from the list of supported units in the date math section."
8146,'dakrone','Disk allocation not periodically checking for whether thresholds/watermarks are exceeded\nRepo video below.\r\n\r\nIn short:\r\n\r\nYou will see that there is an index with shards allocated to 2 nodes.  Cluster setting has watermark.low set to 80% and high set to 90%.  Disk usage for node_local is at 76%, node_vm is at 83%.\r\n\r\nYou can skip the video from 0:30 to 2:38, that is when I incrementally add large files to the disk on node_vm so that it uses > 90% of disk.  After 2:38, You will see that the disk usage has increased to 90%+ but no relocation of shards occurred from node_vm to node_local.  But if I wait 30+s after the disk hits 90%+ usage, and then set the *same* cluster.routing.allocation.disk.* settings again, the relocation immediately happens.  This suggests that we are not currently periodically checking for whether the disk is filling up beyond the high mark without manual intervention by either:\r\n\r\n* Setting the same cluster.routing.allocation.disk.* settings again, or\r\n* Using the reroute command with no actions, eg. `POST /_cluster/reroute`\r\n\r\nhttps://drive.google.com/file/d/0BzqaicoBfqMfdXdSWnI2OFp0eXM/view?usp=sharing\r\n\r\nWorkaround is to periodically call reroute command (or reset the same settings).'
8140,'martijnvg',"non fatal npe in warmer\nI've seen the stacktrace below in our integration tests a couple of time. We're starting elasticsearch as an embedded node. The error appears to be non fatal since our integration tests pass anyway. I've seen this stacktrace twice in the past week but can't reproduce it reliably.\r\n\r\nWe are running our maven tests concurrently and in randomized order, so there are a lot of integration tests hitting our elasticsearch node all at once right after it starts and reports a green status.\r\n\r\nUsing elasticsearch 1.4.0 Beta1\r\n\r\n17-10-2014T15:56:40+0200 W warmer - [test-node-gstJI] [inbot_users_v27][2] failed to load random access for [_type:usercontact]\r\norg.elasticsearch.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:132) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:284) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.8.0]\r\nCaused by: java.lang.NullPointerException: null\r\n\tat org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:157) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:132) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\tat org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[elasticsearch-1.4.0.Beta1.jar:na]\r\n\t... 8 common frames omitted"
8138,'jpountz','Search: Reduce memory usage during fetch source sub phase\nIf includes or excludes are set\r\nXContentFactory.xcontentBuilder() allocates a new\r\nBytesStreamOutput using the default page size which is 16kb.\r\n\r\nCan be optimized to use the length of the sourceRef because\r\nthat is the maximum possible size that the streamOutput will\r\nuse.\r\n\r\nThis redcues the amount of memory allocated for a request\r\nthat is fetching 200.000 small documents (~150 bytes each)\r\nby about 300 MB'
8136,'mikemccand',"Internal: Dynamic changes to `max_merge_count` are now picked up by index throttling\nToday, index throttling won't notice any dynamic/live changes to max_merge_count.\r\n\r\nSo, I just fixed the throttle code to ask the MergeSchedulerProvider for its maxMergeCount every time a merge starts/finishes.\r\n\r\nThis means after a dynamic change, it will be the next merge that starts/finishes until the throttling notices the change.  We could also install an UpdateSettingsListener to force throttling to notice the change immediately, but that's more complex and I think this simple solution is sufficient.\r\n\r\nCloses #8132"
8133,'jpountz','Search: Remove partial fields.\nPartial fields have been deprecated since 1.0.0Beta1 in favor of _source\r\nfiltering. They will be removed in 2.0.'
8132,'mikemccand',"Core: dynamic updates to max_merge_count is ignored by index throttling\nWith #6066 we added index throttling when merges cannot keep up, which is important since this ensures index remains healthy (does not develop ridiculous number of segments).\r\n\r\nIt works by watching the number of merges that need to run, and if this exceeds max_merge_count, it starts throttling.\r\n\r\nHowever, max_merge_count is dynamically updatable, but when you update it dynamically, the index throttling doesn't notice and keeps throttling at the original max_merge_count (on ES startup)."
8125,'markharwood','Bulk request hangs when one index can be auto created an another cannot\nTo reproduce, download latest 1.3.x or 1.4 beta and update config/elasticsearch.yml to include:\r\naction.auto_create_index: +willwork*\r\n\r\nThen create a requests file that contains:\r\n\r\n    { "index" : { "_index" : "willwork", "_type" : "type1", "_id" : "1" } }\r\n    { "field1" : "value1" }\r\n    { "index" : { "_index" : "noway", "_type" : "type1", "_id" : "1" } }\r\n    { "field1" : "value1" }\r\n\r\nRun the command to bulk insert:\r\n\r\n    curl -s -XPOST localhost:9200/_bulk --data-binary @requests; echo\r\n\r\nThe command hangs and doesn\'t return. \r\n'
8121,'pickypg','Fixing SearchRequestBuilder aggregations call to facets\nThis is already fixed in master thanks to the removal of facets.\r\n\r\nCloses #8120'
8119,'imotov',"Snapshots are taking more place even if no changes happened\nI took 2 snapshots for read-only indices with curator and some indices were snapshotted again even though they didn't have any changes.\r\n\r\nLook at the first backup (50 oldest indices):\r\n\r\n```\r\n51G      s3://backups-es-statistics/indices/statistics-20131004/\r\n27G      s3://backups-es-statistics/indices/statistics-20131005/\r\n24G      s3://backups-es-statistics/indices/statistics-20131006/\r\n39G      s3://backups-es-statistics/indices/statistics-20131007/\r\n25G      s3://backups-es-statistics/indices/statistics-20131008/\r\n30G      s3://backups-es-statistics/indices/statistics-20131009/\r\n37G      s3://backups-es-statistics/indices/statistics-20131010/\r\n28G      s3://backups-es-statistics/indices/statistics-20131011/\r\n27G      s3://backups-es-statistics/indices/statistics-20131012/\r\n28G      s3://backups-es-statistics/indices/statistics-20131013/\r\n32G      s3://backups-es-statistics/indices/statistics-20131014/\r\n41G      s3://backups-es-statistics/indices/statistics-20131015/\r\n42G      s3://backups-es-statistics/indices/statistics-20131016/\r\n33G      s3://backups-es-statistics/indices/statistics-20131017/\r\n29G      s3://backups-es-statistics/indices/statistics-20131018/\r\n29G      s3://backups-es-statistics/indices/statistics-20131019/\r\n30G      s3://backups-es-statistics/indices/statistics-20131020/\r\n32G      s3://backups-es-statistics/indices/statistics-20131021/\r\n33G      s3://backups-es-statistics/indices/statistics-20131022/\r\n29G      s3://backups-es-statistics/indices/statistics-20131023/\r\n36G      s3://backups-es-statistics/indices/statistics-20131024/\r\n32G      s3://backups-es-statistics/indices/statistics-20131025/\r\n32G      s3://backups-es-statistics/indices/statistics-20131026/\r\n34G      s3://backups-es-statistics/indices/statistics-20131027/\r\n31G      s3://backups-es-statistics/indices/statistics-20131028/\r\n40G      s3://backups-es-statistics/indices/statistics-20131029/\r\n29G      s3://backups-es-statistics/indices/statistics-20131030/\r\n35G      s3://backups-es-statistics/indices/statistics-20131031/\r\n7G       s3://backups-es-statistics/indices/statistics-20131101/\r\n6G       s3://backups-es-statistics/indices/statistics-20131102/\r\n7G       s3://backups-es-statistics/indices/statistics-20131103/\r\n7G       s3://backups-es-statistics/indices/statistics-20131104/\r\n7G       s3://backups-es-statistics/indices/statistics-20131105/\r\n7G       s3://backups-es-statistics/indices/statistics-20131106/\r\n7G       s3://backups-es-statistics/indices/statistics-20131107/\r\n7G       s3://backups-es-statistics/indices/statistics-20131108/\r\n7G       s3://backups-es-statistics/indices/statistics-20131109/\r\n7G       s3://backups-es-statistics/indices/statistics-20131110/\r\n7G       s3://backups-es-statistics/indices/statistics-20131111/\r\n7G       s3://backups-es-statistics/indices/statistics-20131112/\r\n```\r\n\r\nAnd the subsequent backup, same indices:\r\n\r\n```\r\n57G      s3://backups-es-statistics/indices/statistics-20131004/\r\n30G      s3://backups-es-statistics/indices/statistics-20131005/\r\n27G      s3://backups-es-statistics/indices/statistics-20131006/\r\n44G      s3://backups-es-statistics/indices/statistics-20131007/\r\n28G      s3://backups-es-statistics/indices/statistics-20131008/\r\n33G      s3://backups-es-statistics/indices/statistics-20131009/\r\n41G      s3://backups-es-statistics/indices/statistics-20131010/\r\n31G      s3://backups-es-statistics/indices/statistics-20131011/\r\n30G      s3://backups-es-statistics/indices/statistics-20131012/\r\n31G      s3://backups-es-statistics/indices/statistics-20131013/\r\n35G      s3://backups-es-statistics/indices/statistics-20131014/\r\n46G      s3://backups-es-statistics/indices/statistics-20131015/\r\n47G      s3://backups-es-statistics/indices/statistics-20131016/\r\n37G      s3://backups-es-statistics/indices/statistics-20131017/\r\n33G      s3://backups-es-statistics/indices/statistics-20131018/\r\n33G      s3://backups-es-statistics/indices/statistics-20131019/\r\n34G      s3://backups-es-statistics/indices/statistics-20131020/\r\n36G      s3://backups-es-statistics/indices/statistics-20131021/\r\n37G      s3://backups-es-statistics/indices/statistics-20131022/\r\n32G      s3://backups-es-statistics/indices/statistics-20131023/\r\n40G      s3://backups-es-statistics/indices/statistics-20131024/\r\n36G      s3://backups-es-statistics/indices/statistics-20131025/\r\n36G      s3://backups-es-statistics/indices/statistics-20131026/\r\n38G      s3://backups-es-statistics/indices/statistics-20131027/\r\n34G      s3://backups-es-statistics/indices/statistics-20131028/\r\n45G      s3://backups-es-statistics/indices/statistics-20131029/\r\n33G      s3://backups-es-statistics/indices/statistics-20131030/\r\n39G      s3://backups-es-statistics/indices/statistics-20131031/\r\n7G       s3://backups-es-statistics/indices/statistics-20131101/\r\n6G       s3://backups-es-statistics/indices/statistics-20131102/\r\n7G       s3://backups-es-statistics/indices/statistics-20131103/\r\n7G       s3://backups-es-statistics/indices/statistics-20131104/\r\n7G       s3://backups-es-statistics/indices/statistics-20131105/\r\n7G       s3://backups-es-statistics/indices/statistics-20131106/\r\n7G       s3://backups-es-statistics/indices/statistics-20131107/\r\n7G       s3://backups-es-statistics/indices/statistics-20131108/\r\n7G       s3://backups-es-statistics/indices/statistics-20131109/\r\n7G       s3://backups-es-statistics/indices/statistics-20131110/\r\n7G       s3://backups-es-statistics/indices/statistics-20131111/\r\n7G       s3://backups-es-statistics/indices/statistics-20131112/\r\n```\r\n\r\nSegments are here:\r\n\r\n\r\n```\r\nstatistics-20131004 0 p 4.6\r\nstatistics-20131004 0 r 4.6\r\nstatistics-20131005 0 p 4.4\r\nstatistics-20131005 0 p 4.9\r\nstatistics-20131005 0 r 4.4\r\nstatistics-20131005 0 r 4.9\r\nstatistics-20131006 0 p 4.4\r\nstatistics-20131006 0 p 4.9\r\nstatistics-20131006 0 r 4.4\r\nstatistics-20131006 0 r 4.9\r\nstatistics-20131007 0 p 4.4\r\nstatistics-20131007 0 p 4.9\r\nstatistics-20131007 0 r 4.4\r\nstatistics-20131007 0 r 4.9\r\nstatistics-20131008 0 p 4.4\r\nstatistics-20131008 0 p 4.9\r\nstatistics-20131008 0 r 4.4\r\nstatistics-20131008 0 r 4.9\r\nstatistics-20131009 0 p 4.4\r\nstatistics-20131009 0 p 4.9\r\nstatistics-20131009 0 r 4.4\r\nstatistics-20131009 0 r 4.9\r\nstatistics-20131010 0 p 4.4\r\nstatistics-20131010 0 p 4.9\r\nstatistics-20131010 0 r 4.4\r\nstatistics-20131010 0 r 4.9\r\nstatistics-20131011 0 p 4.4\r\nstatistics-20131011 0 p 4.9\r\nstatistics-20131011 0 r 4.4\r\nstatistics-20131011 0 r 4.9\r\nstatistics-20131012 0 p 4.4\r\nstatistics-20131012 0 p 4.9\r\nstatistics-20131012 0 r 4.4\r\nstatistics-20131012 0 r 4.9\r\nstatistics-20131013 0 p 4.4\r\nstatistics-20131013 0 p 4.9\r\nstatistics-20131013 0 r 4.4\r\nstatistics-20131013 0 r 4.9\r\nstatistics-20131014 0 p 4.4\r\nstatistics-20131014 0 p 4.9\r\nstatistics-20131014 0 r 4.4\r\nstatistics-20131014 0 r 4.9\r\nstatistics-20131015 0 p 4.4\r\nstatistics-20131015 0 p 4.9\r\nstatistics-20131015 0 r 4.4\r\nstatistics-20131015 0 r 4.9\r\nstatistics-20131016 0 p 4.4\r\nstatistics-20131016 0 p 4.9\r\nstatistics-20131016 0 r 4.4\r\nstatistics-20131016 0 r 4.9\r\nstatistics-20131017 0 p 4.4\r\nstatistics-20131017 0 p 4.9\r\nstatistics-20131017 0 r 4.4\r\nstatistics-20131017 0 r 4.9\r\nstatistics-20131018 0 p 4.4\r\nstatistics-20131018 0 p 4.9\r\nstatistics-20131018 0 r 4.4\r\nstatistics-20131018 0 r 4.9\r\nstatistics-20131019 0 p 4.4\r\nstatistics-20131019 0 p 4.9\r\nstatistics-20131019 0 r 4.4\r\nstatistics-20131019 0 r 4.9\r\nstatistics-20131020 0 p 4.4\r\nstatistics-20131020 0 p 4.9\r\nstatistics-20131020 0 r 4.4\r\nstatistics-20131020 0 r 4.9\r\nstatistics-20131021 0 p 4.4\r\nstatistics-20131021 0 p 4.9\r\nstatistics-20131021 0 r 4.4\r\nstatistics-20131021 0 r 4.9\r\nstatistics-20131022 0 p 4.4\r\nstatistics-20131022 0 p 4.9\r\nstatistics-20131022 0 r 4.4\r\nstatistics-20131022 0 r 4.9\r\nstatistics-20131023 0 p 4.4\r\nstatistics-20131023 0 p 4.9\r\nstatistics-20131023 0 r 4.4\r\nstatistics-20131023 0 r 4.9\r\nstatistics-20131024 0 p 4.4\r\nstatistics-20131024 0 p 4.9\r\nstatistics-20131024 0 r 4.4\r\nstatistics-20131024 0 r 4.9\r\nstatistics-20131025 0 p 4.4\r\nstatistics-20131025 0 p 4.9\r\nstatistics-20131025 0 r 4.4\r\nstatistics-20131025 0 r 4.9\r\nstatistics-20131026 0 p 4.4\r\nstatistics-20131026 0 p 4.9\r\nstatistics-20131026 0 r 4.4\r\nstatistics-20131026 0 r 4.9\r\nstatistics-20131027 0 p 4.4\r\nstatistics-20131027 0 p 4.9\r\nstatistics-20131027 0 r 4.4\r\nstatistics-20131027 0 r 4.9\r\nstatistics-20131028 0 p 4.4\r\nstatistics-20131028 0 p 4.9\r\nstatistics-20131028 0 r 4.4\r\nstatistics-20131028 0 r 4.9\r\nstatistics-20131029 0 p 4.4\r\nstatistics-20131029 0 p 4.9\r\nstatistics-20131029 0 r 4.4\r\nstatistics-20131029 0 r 4.9\r\nstatistics-20131030 0 p 4.4\r\nstatistics-20131030 0 p 4.9\r\nstatistics-20131030 0 r 4.4\r\nstatistics-20131030 0 r 4.9\r\nstatistics-20131031 0 p 4.4\r\nstatistics-20131031 0 p 4.9\r\nstatistics-20131031 0 r 4.4\r\nstatistics-20131031 0 r 4.9\r\nstatistics-20131101 0 p 4.4\r\nstatistics-20131101 0 p 4.9\r\nstatistics-20131101 0 r 4.4\r\nstatistics-20131101 0 r 4.9\r\nstatistics-20131101 1 p 4.4\r\nstatistics-20131101 1 p 4.9\r\nstatistics-20131101 1 r 4.4\r\nstatistics-20131101 1 r 4.9\r\nstatistics-20131101 2 p 4.4\r\nstatistics-20131101 2 p 4.9\r\nstatistics-20131101 2 r 4.4\r\nstatistics-20131101 2 r 4.9\r\nstatistics-20131101 3 p 4.4\r\nstatistics-20131101 3 p 4.9\r\nstatistics-20131101 3 r 4.4\r\nstatistics-20131101 3 r 4.9\r\nstatistics-20131101 4 p 4.4\r\nstatistics-20131101 4 p 4.9\r\nstatistics-20131101 4 r 4.4\r\nstatistics-20131101 4 r 4.9\r\nstatistics-20131102 0 p 4.4\r\nstatistics-20131102 0 p 4.9\r\nstatistics-20131102 0 r 4.4\r\nstatistics-20131102 0 r 4.9\r\nstatistics-20131102 1 p 4.4\r\nstatistics-20131102 1 p 4.9\r\nstatistics-20131102 1 r 4.4\r\nstatistics-20131102 1 r 4.9\r\nstatistics-20131102 2 p 4.4\r\nstatistics-20131102 2 p 4.9\r\nstatistics-20131102 2 r 4.4\r\nstatistics-20131102 2 r 4.9\r\nstatistics-20131102 3 p 4.4\r\nstatistics-20131102 3 p 4.9\r\nstatistics-20131102 3 r 4.4\r\nstatistics-20131102 3 r 4.9\r\nstatistics-20131102 4 p 4.4\r\nstatistics-20131102 4 p 4.9\r\nstatistics-20131102 4 r 4.4\r\nstatistics-20131102 4 r 4.9\r\nstatistics-20131103 0 p 4.4\r\nstatistics-20131103 0 p 4.9\r\nstatistics-20131103 0 r 4.4\r\nstatistics-20131103 0 r 4.9\r\nstatistics-20131103 1 p 4.4\r\nstatistics-20131103 1 p 4.9\r\nstatistics-20131103 1 r 4.4\r\nstatistics-20131103 1 r 4.9\r\nstatistics-20131103 2 p 4.4\r\nstatistics-20131103 2 p 4.9\r\nstatistics-20131103 2 r 4.4\r\nstatistics-20131103 2 r 4.9\r\nstatistics-20131103 3 p 4.4\r\nstatistics-20131103 3 p 4.9\r\nstatistics-20131103 3 r 4.4\r\nstatistics-20131103 3 r 4.9\r\nstatistics-20131103 4 p 4.4\r\nstatistics-20131103 4 p 4.9\r\nstatistics-20131103 4 r 4.4\r\nstatistics-20131103 4 r 4.9\r\nstatistics-20131104 0 p 4.4\r\nstatistics-20131104 0 p 4.9\r\nstatistics-20131104 0 r 4.4\r\nstatistics-20131104 0 r 4.9\r\nstatistics-20131104 1 p 4.4\r\nstatistics-20131104 1 p 4.9\r\nstatistics-20131104 1 r 4.4\r\nstatistics-20131104 1 r 4.9\r\nstatistics-20131104 2 p 4.4\r\nstatistics-20131104 2 p 4.9\r\nstatistics-20131104 2 r 4.4\r\nstatistics-20131104 2 r 4.9\r\nstatistics-20131104 3 p 4.4\r\nstatistics-20131104 3 p 4.9\r\nstatistics-20131104 3 r 4.4\r\nstatistics-20131104 3 r 4.9\r\nstatistics-20131104 4 p 4.4\r\nstatistics-20131104 4 p 4.9\r\nstatistics-20131104 4 r 4.4\r\nstatistics-20131104 4 r 4.9\r\nstatistics-20131105 0 p 4.4\r\nstatistics-20131105 0 p 4.9\r\nstatistics-20131105 0 r 4.4\r\nstatistics-20131105 0 r 4.9\r\nstatistics-20131105 1 p 4.4\r\nstatistics-20131105 1 p 4.9\r\nstatistics-20131105 1 r 4.4\r\nstatistics-20131105 1 r 4.9\r\nstatistics-20131105 2 p 4.4\r\nstatistics-20131105 2 p 4.9\r\nstatistics-20131105 2 r 4.4\r\nstatistics-20131105 2 r 4.9\r\nstatistics-20131105 3 p 4.4\r\nstatistics-20131105 3 p 4.9\r\nstatistics-20131105 3 r 4.4\r\nstatistics-20131105 3 r 4.9\r\nstatistics-20131105 4 p 4.4\r\nstatistics-20131105 4 p 4.9\r\nstatistics-20131105 4 r 4.4\r\nstatistics-20131105 4 r 4.9\r\nstatistics-20131106 0 p 4.4\r\nstatistics-20131106 0 p 4.9\r\nstatistics-20131106 0 r 4.4\r\nstatistics-20131106 0 r 4.9\r\nstatistics-20131106 1 p 4.4\r\nstatistics-20131106 1 p 4.9\r\nstatistics-20131106 1 r 4.4\r\nstatistics-20131106 1 r 4.9\r\nstatistics-20131106 2 p 4.4\r\nstatistics-20131106 2 p 4.9\r\nstatistics-20131106 2 r 4.4\r\nstatistics-20131106 2 r 4.9\r\nstatistics-20131106 3 p 4.4\r\nstatistics-20131106 3 p 4.9\r\nstatistics-20131106 3 r 4.4\r\nstatistics-20131106 3 r 4.9\r\nstatistics-20131106 4 p 4.4\r\nstatistics-20131106 4 p 4.9\r\nstatistics-20131106 4 r 4.4\r\nstatistics-20131106 4 r 4.9\r\nstatistics-20131107 0 p 4.4\r\nstatistics-20131107 0 p 4.9\r\nstatistics-20131107 0 r 4.4\r\nstatistics-20131107 0 r 4.9\r\nstatistics-20131107 1 p 4.4\r\nstatistics-20131107 1 p 4.9\r\nstatistics-20131107 1 r 4.4\r\nstatistics-20131107 1 r 4.9\r\nstatistics-20131107 2 p 4.4\r\nstatistics-20131107 2 p 4.9\r\nstatistics-20131107 2 r 4.4\r\nstatistics-20131107 2 r 4.9\r\nstatistics-20131107 3 p 4.4\r\nstatistics-20131107 3 p 4.9\r\nstatistics-20131107 3 r 4.4\r\nstatistics-20131107 3 r 4.9\r\nstatistics-20131107 4 p 4.4\r\nstatistics-20131107 4 p 4.9\r\nstatistics-20131107 4 r 4.4\r\nstatistics-20131107 4 r 4.9\r\nstatistics-20131108 0 p 4.4\r\nstatistics-20131108 0 p 4.9\r\nstatistics-20131108 0 r 4.4\r\nstatistics-20131108 0 r 4.9\r\nstatistics-20131108 1 p 4.4\r\nstatistics-20131108 1 p 4.9\r\nstatistics-20131108 1 r 4.4\r\nstatistics-20131108 1 r 4.9\r\nstatistics-20131108 2 p 4.4\r\nstatistics-20131108 2 p 4.9\r\nstatistics-20131108 2 r 4.4\r\nstatistics-20131108 2 r 4.9\r\nstatistics-20131108 3 p 4.4\r\nstatistics-20131108 3 p 4.9\r\nstatistics-20131108 3 r 4.4\r\nstatistics-20131108 3 r 4.9\r\nstatistics-20131108 4 p 4.4\r\nstatistics-20131108 4 p 4.9\r\nstatistics-20131108 4 r 4.4\r\nstatistics-20131108 4 r 4.9\r\nstatistics-20131109 0 p 4.4\r\nstatistics-20131109 0 p 4.9\r\nstatistics-20131109 0 r 4.4\r\nstatistics-20131109 0 r 4.9\r\nstatistics-20131109 1 p 4.4\r\nstatistics-20131109 1 p 4.9\r\nstatistics-20131109 1 r 4.4\r\nstatistics-20131109 1 r 4.9\r\nstatistics-20131109 2 p 4.4\r\nstatistics-20131109 2 p 4.9\r\nstatistics-20131109 2 r 4.4\r\nstatistics-20131109 2 r 4.9\r\nstatistics-20131109 3 p 4.4\r\nstatistics-20131109 3 p 4.9\r\nstatistics-20131109 3 r 4.4\r\nstatistics-20131109 3 r 4.9\r\nstatistics-20131109 4 p 4.4\r\nstatistics-20131109 4 p 4.9\r\nstatistics-20131109 4 r 4.4\r\nstatistics-20131109 4 r 4.9\r\nstatistics-20131110 0 p 4.4\r\nstatistics-20131110 0 p 4.9\r\nstatistics-20131110 0 r 4.4\r\nstatistics-20131110 0 r 4.9\r\nstatistics-20131110 1 p 4.4\r\nstatistics-20131110 1 p 4.9\r\nstatistics-20131110 1 r 4.4\r\nstatistics-20131110 1 r 4.9\r\nstatistics-20131110 2 p 4.4\r\nstatistics-20131110 2 p 4.9\r\nstatistics-20131110 2 r 4.4\r\nstatistics-20131110 2 r 4.9\r\nstatistics-20131110 3 p 4.4\r\nstatistics-20131110 3 p 4.9\r\nstatistics-20131110 3 r 4.4\r\nstatistics-20131110 3 r 4.9\r\nstatistics-20131110 4 p 4.4\r\nstatistics-20131110 4 p 4.9\r\nstatistics-20131110 4 r 4.4\r\nstatistics-20131110 4 r 4.9\r\nstatistics-20131111 0 p 4.4\r\nstatistics-20131111 0 p 4.9\r\nstatistics-20131111 0 r 4.4\r\nstatistics-20131111 0 r 4.9\r\nstatistics-20131111 1 p 4.4\r\nstatistics-20131111 1 p 4.9\r\nstatistics-20131111 1 r 4.4\r\nstatistics-20131111 1 r 4.9\r\nstatistics-20131111 2 p 4.4\r\nstatistics-20131111 2 p 4.9\r\nstatistics-20131111 2 r 4.4\r\nstatistics-20131111 2 r 4.9\r\nstatistics-20131111 3 p 4.4\r\nstatistics-20131111 3 p 4.9\r\nstatistics-20131111 3 r 4.4\r\nstatistics-20131111 3 r 4.9\r\nstatistics-20131111 4 p 4.4\r\nstatistics-20131111 4 p 4.9\r\nstatistics-20131111 4 r 4.4\r\nstatistics-20131111 4 r 4.9\r\nstatistics-20131112 0 p 4.4\r\nstatistics-20131112 0 p 4.9\r\nstatistics-20131112 0 r 4.4\r\nstatistics-20131112 0 r 4.9\r\n```\r\n\r\nThose indices should be roughly the same size in snapshot.\r\n\r\nI also took dir diff from subsequent backups:\r\n\r\n```diff\r\n--- before.3.txt\t2014-10-16 21:39:05.559338129 +0400\r\n+++ after.3.txt\t2014-10-16 21:56:46.272597922 +0400\r\n@@ -152,6 +152,26 @@\r\n 2014-10-16 17:25       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part0\r\n 2014-10-16 17:25       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part1\r\n 2014-10-16 17:25        21M  s3://backups-es-statistics/indices/statistics-20140108/3/__3m.part2\r\n+2014-10-16 17:48       436   s3://backups-es-statistics/indices/statistics-20140108/3/__3n\r\n+2014-10-16 17:48       179   s3://backups-es-statistics/indices/statistics-20140108/3/__3o\r\n+2014-10-16 17:48       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part0\r\n+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part1\r\n+2014-10-16 17:48       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part2\r\n+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part3\r\n+2014-10-16 17:48        18M  s3://backups-es-statistics/indices/statistics-20140108/3/__3p.part4\r\n+2014-10-16 17:48       459k  s3://backups-es-statistics/indices/statistics-20140108/3/__3q\r\n+2014-10-16 17:48         7M  s3://backups-es-statistics/indices/statistics-20140108/3/__3r\r\n+2014-10-16 17:48        34   s3://backups-es-statistics/indices/statistics-20140108/3/__3s\r\n+2014-10-16 17:48         2k  s3://backups-es-statistics/indices/statistics-20140108/3/__3t\r\n+2014-10-16 17:48        16M  s3://backups-es-statistics/indices/statistics-20140108/3/__3u\r\n+2014-10-16 17:49        81M  s3://backups-es-statistics/indices/statistics-20140108/3/__3v\r\n+2014-10-16 17:48        57   s3://backups-es-statistics/indices/statistics-20140108/3/__3w\r\n+2014-10-16 17:48         2M  s3://backups-es-statistics/indices/statistics-20140108/3/__3x\r\n+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3y.part0\r\n+2014-10-16 17:49        87M  s3://backups-es-statistics/indices/statistics-20140108/3/__3y.part1\r\n+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part0\r\n+2014-10-16 17:49       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part1\r\n+2014-10-16 17:49        21M  s3://backups-es-statistics/indices/statistics-20140108/3/__3z.part2\r\n 2014-09-30 19:39       959k  s3://backups-es-statistics/indices/statistics-20140108/3/__4\r\n 2014-09-30 19:39        53M  s3://backups-es-statistics/indices/statistics-20140108/3/__5\r\n 2014-09-30 19:39       100M  s3://backups-es-statistics/indices/statistics-20140108/3/__6.part0\r\n@@ -205,3 +225,4 @@\r\n 2014-10-15 08:46         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-14\r\n 2014-10-16 08:41         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-15\r\n 2014-10-16 17:25         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16\r\n+2014-10-16 17:49         5k  s3://backups-es-statistics/indices/statistics-20140108/3/snapshot-statistics-2014-10-16-again\r\n```\r\n\r\nCluster consists of 5 nodes on 1.3.2.\r\n\r\ncc @imotov "
8118,'pickypg',"Fixing copy/paste mistake in SearchRequest.extraSource's exception message\nUses the `extraSource` parameter for debug instead of the `source` field.\r\n\r\nCloses #8117"
8117,'pickypg','SearchRequest - Map extraSource exception uses source field accidently\nThis is incredibly minor, and it really only helps with client debug that is unlikely to be needed. The method with the signature:\r\n\r\n```java\r\npublic SearchRequest extraSource(Map extraSource)\r\n```\r\n\r\nIncludes the re-thrown exception:\r\n\r\n```java\r\nthrow new ElasticsearchGenerationException("Failed to generate [" + source + "]", e);\r\n```\r\n\r\nwhere `source` is the internal field due to copy/paste.'
8116,'clintongormley','Percolate reference - a typo and a misused word\n'
8115,'clintongormley','Percolate `_score` reference\nAdded missing `_score` word, made the sentence less ambiguous.'
8113,'colings86',"Aggregations: Buckets can now be serialized outside of an Aggregation\nThis change means that buckets can now be serialised to JSON and serialized and deserialized to the transport API outside of the aggregation that contains them.  This is a required change for #8110 (Reducers framework) but should make sense on it's own since object should really take care of their own serialization rather than relying on their parent object."
8109,'colings86','Docs: adds note about using null_value with dynamic templates\nCloses #7874'
8103,'clintongormley',"Some Data Cant't be obtained.\nIn previous case, the first 100 data can't be obtained if it has."
8092,'s1monw',"Recovery: refactor RecoveryTarget state management\nThe PR rewrites the state controls in the RecoveryTarget family classes to make it easier to guarantee that:\r\n- recovery resources are only cleared once there are no ongoing requests\r\n- recovery is automatically canceled when the target shard is closed/removed\r\n- canceled recoveries do not leave temp files behind when canceled. \r\n\r\nHighlights of the change:\r\n1) All temporary files are cleared upon failure/cancel (see #7315 )\r\n2) All newly created files are always temporary \r\n3) Doesn't list local files on the cluster state update thread (which throw unwanted exception)\r\n4) Recoveries are canceled by a listener to IndicesLifecycle.beforeIndexShardClosed, so we don't need to explicitly call it.\r\n5) Simplifies RecoveryListener to only notify when a recovery is done or failed. Removed subtleties like ignore and retry (they are dealt with internally)"
8091,'clintongormley','fix typo in docs/reference/modules/cluster.asciidoc\n'
8090,'areek','Completion Suggester: Fix CompletionFieldMapper to correctly parse weight\nFollowup for #3977:\r\n\r\nif #3977 a check was added, that indexing completion suggester fields are rejected, if the weight is not an integer. My problem is that this check introduced there only works if the weight is a number. Unfortunately the parser has a bit strange logic: The new code is not executed if the client (creating the JSON) is passing the weight as "string", e.g. { "weight" : "10.5" }\r\n\r\nIn fact the weight is then ignored completely and not even an error is given (this is an additional bug in the parser logic). This caused me headaches yesterday, because the weight was given as JSON string in the indexing document. For other fields this makes no difference while indexing.\r\n\r\nThe parser for completion fields should be improved to have the outer check on the JSON key first and later check the types, not vice versa. This would also be consistent with indexing other fields, where the type of JSON value does not matter.'
8085,'mikemccand',"Core: changing refresh_interval to non-positive (0, -1, etc.) value might cause 100% CPU spin\nAs reported on the user's list:\r\n\r\n    https://groups.google.com/d/msg/elasticsearch/IQWvod8hq_Q/H6358j_24B0J\r\n\r\nIt looks like there is a concurrency bug when you dynamically update refresh_interval down to a value <= 0.  We cancel the scheduled future when this happens, but if the future was already executing (which we don't try to cancel because we pass false to the cancel call), EngineRefresher.run will then forever continue rescheduling itself for the immediate future."
8079,'dakrone','Change max index name length to 255 bytes\nAs per discussion in #7252, the current max index name length of 100 is too short for some users.\r\n\r\nWorth setting it to a safe (on all? most? file systems) max of 255.'
8078,'rjernst','Still use of unsafe methods in 1.3.4 - causing crashes on SPARC\nThe changes for Issue 6962 is present in 1.3.2, but there are still uses of Unsafe methods in other classes, apart from UnsafeUtils.\r\n\r\njprante signalled one occurrence on the site below:\r\nhttp://www.snip2code.com/Snippet/140415/Solaris-SPARC-JVM-64bit-crash-with-Java-\r\n\r\nbut I could not find a reference to it here at ElasticSearch.\r\n\r\nThese classes are involved.\r\n\r\nUnsafeChunkDecoder.class\r\nUnsafeChunkEncoder.class\r\nUnsafeChunkEncoderBE.class\r\nUnsafeChunkEncoderLE.class\r\nUnsafeChunkEncoders.class\r\nUnsafeDynamicChannelBuffer.class\r\n'
8066,'clintongormley','Update source-field.asciidoc\nvery minor typofix'
8065,'rjernst','[TEST] Add simple BWC tests that start from pre-build indices\nToday we only have integrated bwc tests for the current  major version. Yet there is no java-based bwc test that starts an old pre-build index. We lately added a test that starts from a prebuild `0.20` index to test if our upgrade API works. We should have simple things like tests for get by id and simple sorts'
8061,'johtani','Update cjk-bigram-tokenfilter.asciidoc\nInsert missing comma and correct typo in the source code.'
8058,'johtani',"download links / versions should be consistent across elk stack\ninconsistent punctuation ( dot vs dash preceding 'beta' ) and capitalization ( for the word beta ) between the different products download links causes usability issues for people writing chef/puppet/etc to automate installs.\r\n\r\nhttps://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.4.0.Beta1.tar.gz\r\nhttps://download.elasticsearch.org/kibana/kibana/kibana-4.0.0-BETA1.1.tar.gz\r\nhttps://download.elasticsearch.org/logstash/logstash/logstash-1.4.0.beta2.tar.gz"
8052,'colings86','Internal: Fix location information for loggers\nThis change corrects the location information gathered by the loggers so that when printing class name, method name, and line numbers in the log pattern, the information from the class calling the logger is used rather than a location within the logger itself.\r\n\r\nCloses #5130'
8046,'imotov','Snapshot of a closed index can leave snapshot hanging in initializing state\nTo reproduce close and index and start snapshot by specifying the index name explicitly in the list of indices. '
8045,'clintongormley',"Updates to threadpool docs\nSomeone questioned the existing doc on IRC as they didn't find it clear.\r\n\r\nI've done some minor updates to clarify and elaborate what the queues are in relation to, as well as pad out the explanation of the threadpool calculations."
8038,'bleskes',"Query logging\nWhen debugging some GC issues, it was really useful to correlate what queries were being run at the time of the GC's, by abusing the slowlog and some ELK magic.\r\n\r\nIt would be great to have the ability to log all queries out of the box in a more obvious way (eg. index.search.log_all: true in elasticsearch.yml). Maybe even in the future Marvel could allow viewing of queries alongside cluster stats as generally it's a query/index event that caused some kind of effect."
8037,'colings86','Aggregations: Fixes scripted metrics aggregation when used as a sub aggregation\n\r\nThe scripted metric aggregation is now a PER_BUCKET aggregation so that parent buckets are evaluated independently. Also the params and reduceParams are copied for each instance of the aggregator (each parent bucket) so modifications to the values are kept only within the scope of its parent bucket\r\n\r\nCloses #8036'
8036,'colings86','Aggregations: scripted metric agg does not separate parent buckets\nThe script metric aggregation does not separate script scopes based on the bucketOrd so when using it as a sub aggregation the results are given for all parent buckets combined rather than each parent bucket separately\r\n\r\n\r\nSee the following comment for more details:\r\nhttps://github.com/elasticsearch/elasticsearch/pull/7075#issuecomment-58403364'
8033,'mikemccand','Tests: dump all thread stacks on failure\nSometimes when a test fails, its logs are not necessarily sufficient to determine what threads were doing at the time ... this change will dump all threads whenever there is a failure either in the main test case itself, or in the cleanup we do after the test succeeds.'
8031,'martijnvg','NPE due to delete-by-query with parent/child when upgrading from 1.1.1 to 1.3.x\nAn NPE was encountered when upgrading from 1.1.1 to 1.3.4.  During the rolling upgrade, a background cron tried to execute a delete-by-query which included a parent/child query.  This was allowed in 1.1.1, but [disabled in later versions](https://github.com/elasticsearch/elasticsearch/pull/5916).\r\n\r\nThis caused a delete-by-query to queue up in the translog of a 1.1.1 node.  Before the translog was cleared, the shard tried to move to a 1.3.4 node, which caused an NPE.  The shards repeatedly failed recovery and kept bouncing around the cluster.  Because allocation filtering was being used to migrate data from old -> new, the cluster tried to recover the shards on only 1.3.4 nodes...leading to a continuous failure.\r\n\r\nThe situation eventually resolved itself, likely because a background flush cleared out the translog and allowed the recovery to finally proceed normally.\r\n\r\nStack trace (sanitized to remove sensitive names/ips):\r\n\r\n```\r\n\r\n[2014-10-08 21:43:26,881][WARN ][indices.cluster          ] [prod-1.3.4] [my_index][6] failed to start shard\r\norg.elasticsearch.indices.recovery.RecoveryFailedException: [my_index][6]: Recovery failed from [prod-1.1.1][YhcqkTzLTGSF8dyKAQPRBQ][prod-1.1.1.localdomain][inet[...]]{aws_availability_zone=us-east-1e, max_local_storage_nodes=1} into [prod-1.3.4][0cRcLbzTTAm15PMu_R_U2w][prod-1.3.4.localdomain][inet[prod-1.3.4.localdomain/...]]{aws_availability_zone=us-east-1e, max_local_storage_nodes=1}\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.doRecovery(RecoveryTarget.java:306)\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget.access$200(RecoveryTarget.java:65)\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget$2.run(RecoveryTarget.java:175)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [prod-1.1.1][inet[/...]][index/shard/recovery/startRecovery]\r\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [my_index][6] Phase[2] Execution failed\r\n    at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1109)\r\n    at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:627)\r\n    at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:117)\r\n    at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:61)\r\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)\r\n    at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:323)\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: [prod-1.3.4][inet[/...]][index/shard/recovery/translogOps]\r\nCaused by: org.elasticsearch.index.query.QueryParsingException: [my_index] Failed to parse\r\n    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:330)\r\n    at org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:449)\r\n    at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryOperation(InternalIndexShard.java:780)\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:431)\r\n    at org.elasticsearch.indices.recovery.RecoveryTarget$TranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:410)\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n    at org.elasticsearch.index.query.QueryParserUtils.ensureNotDeleteByQuery(QueryParserUtils.java:36)\r\n    at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:52)\r\n    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:302)\r\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:283)\r\n    at org.elasticsearch.index.query.NotFilterParser.parse(NotFilterParser.java:63)\r\n    at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:302)\r\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:283)\r\n    at org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:239)\r\n    at org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:342)\r\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)\r\n    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:263)\r\n    at org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:314)\r\n    ... 8 more\r\n```'
8027,'clintongormley','Inconsistency between `_percolate` and `_search`\n# Summary\r\nThere seems to be an unexplained inconsistency between the results of `_percolate` and `_search` when using the same document and query set.\r\n\r\n# Description\r\nI have an index defined with a custom analyzer set as my default and a few string fields. The analyzer tokenizes on commas, and nothing else. Curling the mapping returns the following output:\r\n```\r\n{\r\n    "news_documents_20140729": {\r\n        "mappings": {\r\n            "0000000022-nid": {\r\n                "_all": {\r\n                    "enabled": false\r\n                },\r\n                "_timestamp": {\r\n                    "enabled": true,\r\n                    "store": true\r\n                },\r\n                "_ttl": {\r\n                    "enabled": true\r\n                },\r\n                "dynamic": "false",\r\n                "analyzer": "csv_lowercase_commas",\r\n                "properties": {\r\n                     ...\r\n                     "subjects": {\r\n                        "type": "string"\r\n                     },\r\n                     "product": {\r\n                        "type": "string",\r\n                        "null_value": "DJGM"\r\n                     },\r\n                     ...\r\n```\r\n\r\nThe analyzer referenced above is defined as follows:\r\n```\r\n"csv_lowercase_commas" : {\r\n  "type" : "custom",\r\n  "filter" : [ "lowercase" ],\r\n  "tokenizer" : "commas"\r\n}\r\n```\r\n\r\nAnd the `commas` tokenizer is defined as follows:\r\n```\r\n"commas" : {\r\n    "pattern" : ",+",\r\n    "type" : "pattern"\r\n}\r\n```\r\n\r\n\r\nI\'ve indexed a very simple document to the above index, as the shown type..\r\n```\r\n{ "subjects":"N/DJMT", "product":"DJGM"}\r\n```\r\n\r\nI search for the above document using the following three queries, and the document is returned each time, as expected.\r\n\r\n## Match All Query\r\n```\r\n{"query": { "match_all":{}}}\r\n```\r\n\r\n## Term Query\r\n```\r\n{\r\n    "query": {\r\n        "filtered": {\r\n            "filter": {\r\n                "and": [\r\n                    {\r\n                        "term": {\r\n                            "product": "djgm"\r\n                        }\r\n                    },\r\n                    {\r\n                        "in": {\r\n                            "subjects": [\r\n                                "n/djmt"\r\n                            ]\r\n                        }\r\n                    }\r\n                ]\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## Phrase Query\r\n```\r\n{\r\n    "query": {\r\n        "filtered": {\r\n            "filter": {\r\n                "term": {\r\n                    "product": "djgm"\r\n                }\r\n            },\r\n            "query": {\r\n                "match_phrase" : {\r\n                    "subjects" : "n/djmt"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## Here\'s the problem\r\nI add each of the above queries to the .percolator type in the above described index, and then percolate `{"doc":{ "subjects":"N/DJMT", "product":"DJGM"}}` -- i.e., the exact same document content I indexed and searched on previously. _However, the result contains only the `Match All Query` and the `Phrase Query`_. \r\n\r\nTo be more concise...\r\n\r\n**Searching for the document using the above queries returns the document independent of which query is used to perform the search. Percolating the same document, with the above three queries indexed in the same index\'s .percolator type, returns _only the first two queries_.**\r\n\r\n**The phrase query is inexplicably missing from the percolate results.**\r\n\r\n# More Information\r\n\r\nThis was verified on a 1.1.0 instance, within a clean index. It was originally discovered in a production index through a slightly more complicated, albeit technically synonymous (by my assessment), example.\r\n\r\n~~Sorry, I don\'t have access to a 1.3 cluster at the moment, so I haven\'t verified it there.~~\r\nAlso verified on a 1.3.4 test cluster I just spooled up for testing.\r\n\r\nA quick grep through the issue tracker using `percolate is:open label:bug` didn\'t show anything similar.\r\n\r\n# (Naive) Partially-substantiated Insights\r\n\r\nRunning the same experiment, except removing the \'/\' in the string \'n/djmt\' in both all of the queries and the document, produces the expected, correct results. In other words, `_percolate` returns all three queries, and `_search` returns the document in response to all three queries. _(Did this on a 1.3.4 cluster with the same mappings as described above.)_\r\n\r\nThe combination of (1) the punctuation being a variable in the problem and (2) the fact that analysis is applied to phrase queries and not to term queries, has me believing the issue has to do with analysis, specifically it not being applied properly when percolating.\r\n\r\n_Lastly, if I\'m missing something trivial, then I apologize in advance for wasting time. It\'s very likely I\'m ignorant of some crucial fact here._'
8024,'clintongormley','Update query-string-query.asciidoc\nUpdate sample as it was using the wrong text.'
8017,'colings86','Use Case for Aggregation Metrics Calculated based on Other Metrics\nIs I mentioned in #5608 it would be great to be able to define metrics as an expression against other metrics (ideally but not necessarily including other calculated metrics) on the same bucket level and if possible able to traverse to parent/child buckets. While this calculation could be done outside of ES, **sorting** of buckets (such as terms) on these calculated values can only be done as part of ES query\r\n\r\nHere is one example:\r\n\r\nFor health care providers I want to find top 10  providers with highest % of overcharge relative to allowed charges (another variation would be % of overcharge relative to average charge) given charge amount and allowed amount fields in my document.   \r\n\r\nI would like to be able to define aggregation `overchargePct`  as `(totalCharges - totalAllowes) * 100 / totalAllowes` and do descending sort on this calculated metric'
8016,'clintongormley','Added RethinkDB River to list of community plugins\n'
8015,'clintongormley','Custom _all fields do not support not_analyzed\nIs that by design? is there a reason that a custom _all field should always be analyzed? For example, the following mapping doesnt work...\r\n\r\n```\r\n                "LocationName": {\r\n                    "type": "string",\r\n                    "copy_to": "Location",\r\n                    "include_in_all": true,\r\n                    "index": "not_analyzed",\r\n                    "store": false\r\n                },\r\n                "LocationParent": {\r\n                    "type": "string",\r\n                    "copy_to": "Location",\r\n                    "include_in_all": true,\r\n                    "index": "not_analyzed",\r\n                    "store": false\r\n                },\r\n                "Location": {\r\n                    "type": "string",\r\n                    "index": "not_analyzed"\r\n                }\r\n```'
8012,'clintongormley','Async support has already been added with Search::Elasticsearch::Async\n'
8008,'javanna',"[TEST] fix CurrentTestFailedMarker to reset its state after each test\nThe currently used method `testRunStarted` is only called before any tests have been run, we need to reset that state before each test, that's why we need to use `testStarted`."
8004,'martijnvg','Percolator gets confused when query is added as a document that is not of .percolator type\nIt appears that the percolator gets confused and does not return a match after the first percolator query\r\nif queries are unintentionally added as documents instead of percolator queries.  eg.\r\n\r\n```\r\nDELETE /twitter\r\n\r\n# register FIRST percolator query\r\nPUT /twitter/.percolator/FIRST\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "FIRST"\r\n        }\r\n    }\r\n}\r\n\r\n# create a document with the same query in the body\r\nPUT /twitter/foo/FIRST_DOC\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "FIRST"\r\n        }\r\n    }\r\n}\r\n# register SECOND percolator query\r\nPUT /twitter/.percolator/SECOND\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "SECOND"\r\n        }\r\n    }\r\n}\r\n# create a document with the same query in the body\r\nPUT /twitter/foo/SECOND_DOC\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "SECOND"\r\n        }\r\n    }\r\n}\r\n# register THIRD percolator query\r\nPUT /twitter/.percolator/THIRD\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "THIRD"\r\n        }\r\n    }\r\n}\r\n# create a document with the same query in the body\r\nPUT /twitter/foo/THIRD_DOC\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "message" : "THIRD"\r\n        }\r\n    }\r\n}\r\n\r\n# Match found\r\nGET /twitter/test/_percolate\r\n{\r\n  "doc":{\r\n    "message":"FIRST"\r\n  }\r\n}\r\n\r\n# Match not found\r\nGET /twitter/test/_percolate\r\n{\r\n  "doc":{\r\n    "message":"SECOND"\r\n  }\r\n}\r\n\r\n# Match not found\r\nGET /twitter/test/_percolate\r\n{\r\n  "doc":{\r\n    "message":"THIRD"\r\n  }\r\n}\r\n\r\n```\r\n'
8001,'dakrone','Groovy should be a mandatory dependency in ES pom\nIn 1.4 mvel was removed and groovy was made the default scripting language however it is marked as an optional dependency in the Elasticsearch pom. Unless the user manually adds groovy as a dependency, Elasticsearch will not be able to run scripts throwing an exception that groovy is not supported.\r\n\r\nBetter yet, groovy could be nested (just like mvel before it) to prevent classloading issues which will happen considering groovy is more popular than mvel.'
7987,'bleskes','Change IndexPrimaryShardNotAllocatedException from 409 to 500\nCloses #7632\r\n\r\nChange IndexPrimaryShardNotAllocatedException from 409 (RestStatus.CONFLICT) to 500 (RestStatus.INTERNAL_SERVER_ERROR)'
7980,'imotov',"Snapshot/Restore: snapshot with missing metadata file cannot be deleted\nIf snapshot metadata file disappears from a repository or it wasn't created due to network issues or master node crash during snapshot process, such snapshot cannot be deleted. Was originally reported in https://github.com/elasticsearch/elasticsearch/issues/5958#issuecomment-57136510"
7977,'colings86','Scripting: Created a parameter parser to standardise script options\n'
7973,'dadoonet',"Docs: rolling upgrade process seems incorrect\nWhen reading the [rolling upgrade process](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades), you can see that we wrote:\r\n\r\n* disable allocation\r\n* upgrade node1\r\n* upgrade node2\r\n* upgrade node3\r\n* ...\r\n* enable allocation\r\n\r\nThat won't work as after a node has been removed and restarted, no shard will be allocated anymore.\r\nSo closing node2 and remaining nodes, won't help to serve index and search request anymore.\r\n\r\nWe should write:\r\n\r\n* disable allocation\r\n* upgrade node1\r\n* enable allocation\r\n* wait for shards being recovered on node1\r\n* disable allocation\r\n* upgrade node2\r\n* enable allocation\r\n* wait for shards being recovered on node2\r\n* disable allocation\r\n* upgrade node3\r\n* enable allocation\r\n* wait for shards being recovered on node3\r\n* disable allocation\r\n* ...\r\n* enable allocation\r\n\r\nI think this documentation update should go in 1.3, 1.4, 1.x and master branches.\r\n"
7969,'colings86','Aggregations: Makes script params consistent with other APIs in scripted_metric\nThis change removes the script_type parameter form the Scripted Metric Aggregation and adds support for _file and _id suffixes to the init_script, map_script, combine_script and reduce_script parameters to make defining the source of the script consistent with the other APIs which use the ScriptService'
7967,'dakrone','Leniency makes simple_query_string query a match_all\nAs far as I can see since 1.3.3 when query is using "lenient" parameter it yelds results when we have format based failures (at least in 1.3.2 it does not return any results).\r\nWhen searching by non-existing subfields (i.e. "field.nonexisting") no errors and no results are found (which seems to be correct)\r\nThe problem comes when I combine queries from both types - with "lenient" and format failure and with non-existing fields in a "should" boolean query\r\nI\'m tested with 1.3.4 version and 1.4.0-beta1\r\nHere are the structure, test data and queries I tested with:\r\nStructure\r\nhttps://gist.github.com/shoteff/3ee9f2ad320410375c91\r\nData\r\nhttps://gist.github.com/shoteff/f5c23bd6bb85f6fecdc9\r\nQueries - here I also described what is working and what not\r\nhttps://gist.github.com/shoteff/20fa2411ede09068ce95\r\n'
7962,'jpountz','Misbehaviour when using missing filter on fields which have the same name as _type\nWhen using the missing filter on a field which has the same name as _type, it seems to get transformed to a match_all filter:\r\n\r\n```bash\r\n/tmp $ cat t\r\n#!/bin/sh\r\n\r\ncurl http://$HOST:9200/\r\ncurl -XDELETE http://$HOST:9200/foo\r\ncurl -XPOST http://$HOST:9200/foo -d \'{}\'\r\ncurl -XPOST http://$HOST:9200/foo/bar/_mapping -d \'{\r\n\t"dynamic": "strict",\r\n\t"_index": {\r\n\t\t"enabled": true\r\n\t},\r\n\t"_id": {\r\n\t\t"index": "not_analyzed",\r\n\t\t"indexed": false,\r\n\t\t"store": true\r\n\t},\r\n\t"properties": {\r\n\t\t"foo": {\r\n\t\t\t"type": "string",\r\n\t\t\t"index": "not_analyzed"\r\n\t\t},\r\n\t\t"bar": {\r\n\t\t\t"type": "string",\r\n\t\t\t"index": "not_analyzed"\r\n\t\t}\r\n\t}\r\n}\r\n\'\r\ncurl -XPOST http://$HOST:9200/foo/bar/123 -d \'{"foo": "abc", "bar": "def"}\'\r\ncurl -XPOST http://$HOST:9200/foo/bar/456 -d \'{"foo": "abc", "bar": "def"}\'\r\n\r\nsleep 1\r\n\r\ncurl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d \'\r\n{\r\n\t"query": {\r\n\t\t"filtered": {\r\n\t\t\t"filter": {\r\n\t\t\t\t"bool": {\r\n\t\t\t\t\t"must": [\r\n\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t"missing": {"field": "bar"}\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\'\r\n\r\ncurl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d \'\r\n{\r\n\t"query": {\r\n\t\t"filtered": {\r\n\t\t\t"filter": {\r\n\t\t\t\t"bool": {\r\n\t\t\t\t\t"must": [\r\n\t\t\t\t\t\t{\r\n\t\t\t\t\t\t\t"missing": {"field": "foo"}\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\'\r\n\r\ncurl -XGET http://$HOST:9200/foo/bar/123/_explain?pretty -d \'\r\n{\r\n\t"query": {\r\n\t\t"filtered": {\r\n\t\t\t"filter": {\r\n\t\t\t\t\t"missing": {"field": "bar"}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\'\r\n/tmp $ HOST=xxx ./t\r\n{\r\n  "status" : 200,\r\n  "name" : "xxx",\r\n  "version" : {\r\n    "number" : "1.3.4",\r\n    "build_hash" : "a70f3ccb52200f8f2c87e9c370c6597448eb3e45",\r\n    "build_timestamp" : "2014-09-30T09:07:17Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.9"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n{"acknowledged":true}{"acknowledged":true}{"acknowledged":true}{"_index":"foo","_type":"bar","_id":"123","_version":1,"created":true}{"_index":"foo","_type":"bar","_id":"456","_version":1,"created":true}{\r\n  "_index" : "foo",\r\n  "_type" : "bar",\r\n  "_id" : "123",\r\n  "matched" : true,\r\n  "explanation" : {\r\n    "value" : 1.0,\r\n    "description" : "ConstantScore(BooleanFilter(+*:*)), product of:",\r\n    "details" : [ {\r\n      "value" : 1.0,\r\n      "description" : "boost"\r\n    }, {\r\n      "value" : 1.0,\r\n      "description" : "queryNorm"\r\n    } ]\r\n  }\r\n}\r\n{\r\n  "_index" : "foo",\r\n  "_type" : "bar",\r\n  "_id" : "123",\r\n  "matched" : false,\r\n  "explanation" : {\r\n    "value" : 0.0,\r\n    "description" : "ConstantScore(BooleanFilter(+cache(NotFilter(cache(BooleanFilter(_field_names:foo)))))) doesn\'t match id 0"\r\n  }\r\n}\r\n{\r\n  "_index" : "foo",\r\n  "_type" : "bar",\r\n  "_id" : "123",\r\n  "matched" : true,\r\n  "explanation" : {\r\n    "value" : 1.0,\r\n    "description" : "ConstantScore(cache(_type:bar)), product of:",\r\n    "details" : [ {\r\n      "value" : 1.0,\r\n      "description" : "boost"\r\n    }, {\r\n      "value" : 1.0,\r\n      "description" : "queryNorm"\r\n    } ]\r\n  }\r\n}\r\n```'
7957,'clintongormley','Ambiguous explanation of fractional intervals for date histogram aggregation\nThe date histogram aggregation states\r\n<pre>\r\nfractional values are allowed, for example 1.5 hours\r\n</pre>\r\nand goes on to give an explanation using a value of ```1.5h```\r\n\r\nGiven that the date histogram supports bucketing by both month and minute, it is not immediately obvious what the correct abbreviation for either is.\r\n\r\nWhile easy enough to find out by testing (```M``` for month, ```m``` for minute), I think the documentation could use an improvement in this regard.\r\n\r\nDocumentation page:\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html'
7954,'jpountz','Switch to murmurhash3 to route documents to shards.\nWe currently use the djb2 hash function in order to compute the shard a\r\ndocument should go to. Unfortunately this hash function is not very\r\nsophisticated and you can sometimes hit adversarial cases, such as numeric ids\r\non 33 shards.\r\n\r\nMurmur3 generates hashes with a better distribution, which should avoid the\r\nadversarial cases.\r\n\r\nHere are some examples of how 100000 incremental ids are distributed to shards\r\nusing either djb2 or murmur3.\r\n\r\n5 shards:\r\nMurmur3: [19933, 19964, 19940, 20030, 20133]\r\nDJB:     [20000, 20000, 20000, 20000, 20000]\r\n\r\n3 shards:\r\nMurmur3: [33185, 33347, 33468]\r\nDJB:     [30100, 30000, 39900]\r\n\r\n33 shards:\r\nMurmur3: [2999, 3096, 2930, 2986, 3070, 3093, 3023, 3052, 3112, 2940, 3036, 2985, 3031, 3048, 3127, 2961, 2901, 3105, 3041, 3130, 3013, 3035, 3031, 3019, 3008, 3022, 3111, 3086, 3016, 2996, 3075, 2945, 2977]\r\nDJB:     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 900, 900, 900, 900, 1000, 1000, 10000, 10000, 10000, 10000, 9100, 9100, 9100, 9100, 9000, 9000, 0, 0, 0, 0, 0, 0]\r\n\r\nEven if djb2 looks ideal in some cases (5 shards), the fact that the\r\ndistribution of its hashes has some patterns can raise issues with some shard\r\ncounts (eg. 3, or even worse 33).\r\n\r\nSome tests have been modified because they relied on implementation details of\r\nthe routing hash function.\r\n\r\nThis change only affects indices that are created on or after elasticsearch 2.0.'
7952,'imotov','wait_for_completion=false waits for snapshot completion\nRegardless of whether I set `wait_for_completion` to true or false, the call appears to block for the duration of the snapshot creation (20-30s)--though in both cases the snapshot status still shows as IN_PROGRESS right for a second or so after the method returns.\r\n\r\n    client.admin().cluster().prepareCreateSnapshot(repositoryName, snapshotId).setWaitForCompletion(false).get();'
7951,'markharwood','Assertion failure when doing a significant terms aggregation.\nHello.\r\n\r\nI\'m running: Version: 1.3.0, Build: 1265b14/2014-07-23T13:46:36Z, JVM: 1.7.0_65\r\n\r\nI\'m trying to do a simple significant terms aggregation and I get an exception:\r\n\r\nCommand:\r\n\r\n    curl -s XGET \'localhost:9200/test_index/job/_search?pretty\' -d \'{\r\n        "query": {\r\n            "filtered": {\r\n                "filter": {\r\n                    "terms": {\r\n                        "profession": [\r\n                            "4980"\r\n                        ]\r\n                    }\r\n                }\r\n            }\r\n        },\r\n        "size": 0,\r\n        "aggs": {\r\n            "term_cloud": {\r\n                "significant_terms": {\r\n                    "field": "fulltext"\r\n                }\r\n            }\r\n        }\r\n    }\'\r\n\r\n\r\nResponse:\r\n\r\n    {\r\n      "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[w_0mi1Z-Tvm68G3dfr7rUg][test_index][2]: ElasticsearchIllegalArgumentException[supersetFreq > supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][3]: ElasticsearchIllegalArgumentException[supersetFreq > supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][4]: ElasticsearchIllegalArgumentException[supersetFreq > supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][0]: ElasticsearchIllegalArgumentException[supersetFreq > supersetSize, in JLHScore.score(..)]}{[w_0mi1Z-Tvm68G3dfr7rUg][test_index][1]: ElasticsearchIllegalArgumentException[supersetFreq > supersetSize, in JLHScore.score(..)]}]",\r\n      "status" : 400\r\n    }\r\n\r\n\r\nConsole:\r\n\r\n    [2014-10-01 18:02:55,119][DEBUG][action.search.type       ] [Joystick] [test_index][2], node[w_0mi1Z-Tvm68G3dfr7rUg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4b58136a]\r\n    org.elasticsearch.ElasticsearchIllegalArgumentException: supersetFreq > supersetSize, in JLHScore.score(..)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.heuristics.JLHScore.getScore(JLHScore.java:79)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.InternalSignificantTerms$Bucket.updateScore(InternalSignificantTerms.java:80)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.GlobalOrdinalsSignificantTermsAggregator.buildAggregation(GlobalOrdinalsSignificantTermsAggregator.java:102)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.GlobalOrdinalsSignificantTermsAggregator.buildAggregation(GlobalOrdinalsSignificantTermsAggregator.java:41)\r\n            at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:133)\r\n            at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:171)\r\n            at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:261)\r\n            at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n            at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n            at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n            at java.lang.Thread.run(Thread.java:745)\r\n\r\nThe mapping for the two fields involved:\r\n\r\n    "profession" : {"type" : "integer"},\r\n    "fulltext" : {"type" : "string"},'
7948,'clintongormley','Fix order for PUT _mapping URL in docs\nRemove backwards compatibility message.\r\n\r\n@clintongormley Anything you want to update?\r\n\r\nSee a old order for _mapping here as well, should I update it?\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/docs/reference/search/suggesters/context-suggest.asciidoc'
7947,'colings86','Add log4j-extras dependency\nTo enable log compression and automatic deletion after a set period.\r\n\r\nCloses #7927'
7943,'martijnvg','Filtered query containing has_parent filter ignores nested not filter \nI\'ve just upgraded from 1.3.0 to 1.3.3 and I\'m finding that a filtered query containing a has_parent filter with a nested not filter is no longer returning the data I expect.\r\n\r\nIn my particular scenario the ElasticSearch repository contains a child record whose parent has a field "addrNo" with the value "60".  When I run the following query that particular record is returned (which is not what I expected):\r\n\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "query": {\r\n            "match_all": {}\r\n          },\r\n          "filter": {\r\n            "has_parent": {\r\n              "filter": {\r\n                "not": {\r\n                  "filter": {\r\n                    "fquery": {\r\n                      "query": {\r\n                        "match": {\r\n                          "addrNo": {\r\n                            "query": "60",\r\n                            "type": "boolean"\r\n                          }\r\n                        }\r\n                      },\r\n                      "_cache": true\r\n                    }\r\n                  }\r\n                }\r\n              },\r\n              "parent_type": "testAddress"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nIf you want me to provide a cutdown example please let me know.\r\n\r\nEdit: Improved query layout.'
7941,'imotov','Snapshots locked up after aborting very large snapshot\nRunning 1.3.2. Have a snapshot repository using my swift plugin. The snapshot plugin has worked for us before on smaller subsets of indexes. I attempted a rather large snapshot of all our currently running indexes (about 1700 or so) with the usual:\r\n\r\n```bash\r\ncurl -s -XPUT localhost:9200/_snapshots/backups/cirrus-initial-all?wait_for_completion=false -d \'{\r\n  "ignore_unavailable": "true",\r\n  "include_global_state": false\r\n}\'\r\n```\r\n\r\nWaited for a bit, not a lot seemed to be happening. Querying the snapshot showed it as IN_PROGRESS. I didn\'t see any data getting written to Swift though. I Ctrl+C\'d out of the snapshot and now all of /_snapshots/backups/ seems hung up from GET requests. Can\'t seem to get a DELETE on the snapshot to go through, and the nuclear option of issuing a DELETE on the whole repository returned a message about it being in use. ES error logs contain nothing relevant that I see.'
7937,'clintongormley','Adding community-contributed Scout plugins\nScout plugins for reporting key node, cluster, and index health metrics.'
7936,'dadoonet','Cat API: show open and closed indices in _cat/indices\nWhen asking for `GET /_cat/indices?v`, you can now retrieve closed indices in addition to opened ones.\r\n\r\n```\r\nhealth status index              pri rep docs.count docs.deleted store.size pri.store.size\r\nyellow open   .marvel-2014.05.21   1   1       8792            0     21.7mb         21.7mb\r\n       close  test\r\nyellow open   .marvel-2014.05.22   1   1       3871            0     10.7mb         10.7mb\r\nred    open   .marvel-2014.05.27   1   1\r\n```\r\n\r\nCloses #7907.'
7927,'colings86','log4j rollingPolicy support\nI\'ve been trying to get elasticsearch to compress log files with gzip. This requires a rollingFileAppender using a TimeBasedRollingPolicy (http://stackoverflow.com/questions/3329385/compress-log4j-files). I\'ve tried to do this using the configuration below but without success, I think because log4j extras isn\'t included in elasticsearch (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/logging/log4j/LogConfigurator.java) .\r\n\r\nCould support for this be added at some point?\r\n\r\n```\r\nappender:\r\n  file:\r\n    type: rollingFile\r\n    file: ${path.logs}/elasticsearch.log.gz\r\n    rollingPolicy: TimeBasedRollingPolicy\r\n    rollingPolicy.FileNamePattern: ${path.logs}/elasticsearch%d{yyyy-MM-dd}.log.gz\r\n    layout:\r\n      type: pattern\r\n      conversionPattern: "%d{ISO8601}"\r\n```\r\nThanks'
7926,'clintongormley','ArrayIndexOutOfBoundsException\nI am using the latest version of elasticsearch and I got this error when I use scroll with large number size and scan as a search type\r\n\r\n{"error":"ArrayIndexOutOfBoundsException[-131072]","status":500}\r\n\r\nthous it perfectly works woth small sizes\r\n\r\nex. \r\n```bash\r\n[01:21:39] lnxg33k@ruined-sec ➜ ~: curl -XGET "http://localhost:9200/dns_logs/pico/_search?search_type=scan&scroll=1m" -d "{\r\n                                   "query": { "match_all": {}},\r\n                                   "size":  100000\r\n                                   }"\r\n{"_scroll_id":"c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7","took":132,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":52076688,"max_score":0.0,"hits":[]}}⏎                                                                                                                     [01:21:50] lnxg33k@ruined-sec ➜ ~: curl -XGET "http://localhost:9200/_search/scroll?scroll=1m&scroll_id=c2Nhbjs1OzUxOjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTM6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1Mjo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU0OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTU6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7" > xxx.json\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  262M  100  262M    0     0   129M      0  0:00:02  0:00:02 --:--:--  129M\r\n[01:22:03] lnxg33k@ruined-sec ➜ ~: du -sh xxx.json \r\n263M\txxx.json\r\n```\r\n\r\n```bash\r\n[01:22:07] lnxg33k@ruined-sec ➜ ~: curl -XGET "http://localhost:9200/dns_logs/pico/_search?search_type=scan&scroll=1m" -d "{\r\n                                   "query": { "match_all": {}},\r\n                                   "size":  1000000\r\n                                   }"\r\n{"_scroll_id":"c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7","took":128,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":52076688,"max_score":0.0,"hits":[]}}⏎                                                                                                                     [01:22:38] lnxg33k@ruined-sec ➜ ~: curl -XGET "http://localhost:9200/_search/scroll?scroll=1m&scroll_id=c2Nhbjs1OzU2OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NTc6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzs1ODo4Zko2ODZBVVRueVpsbE90WXF4MmpnOzU5OjhmSjY4NkFVVG55WmxsT3RZcXgyamc7NjA6OGZKNjg2QVVUbnlabGxPdFlxeDJqZzsxO3RvdGFsX2hpdHM6NTIwNzY2ODg7"\r\n{"error":"ArrayIndexOutOfBoundsException[null]","status":500}⏎                                                           \r\n```'
7925,'colings86','Docs: Added breaking changes docs for Indices APIs\nAdds the breaking changes defaults for the change of default indices options for the GET Aliases API'
7915,'imotov','Restore with rewriting could create alias with the same name as index\nIn `/_aliases` it looks like this:\r\n\r\n```json\r\n{\r\n  "statistics-20131006" : {\r\n    "aliases" : {\r\n      "statistics-20131006" : { }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI restored index `statistics-20131006-compacted` that had alias to `statistics-20131006` with removal of `-compacted` suffix. Now I cannot remove those aliases.\r\n\r\nProbably restore process should check for such things. @imotov.\r\n'
7914,'clintongormley','Disable dynamic mapping with template\nHi !\r\n\r\nIt seems that disable "dynamic" with a template doesn\'t work ? Here is my template which I can get with this query : `curl "http://localhost:9200/_template/default_mapping?pretty=1"`\r\n\r\n```js\r\n{\r\n  "default_mapping" : {\r\n    "order" : 0,\r\n    "template" : "company_*",\r\n    "settings" : {\r\n      // ...\r\n    },\r\n    "mappings" : {\r\n      "_default_" : {\r\n        "dynamic" : false,\r\n        "properties" : {\r\n          // ...\r\n        },\r\n        "dynamic_date_formats" : [ "yyyy-MM-dd", "dd-MM-yyyy", "dateOptionalTime", "yyyy/MM/dd HH:mm:ss Z", "yyyy/MM/dd Z", "date" ]\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa3d" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "656d61696c2d746872656164" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa3a" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa41" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa3c" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa3b" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa40" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      },\r\n      "5252ce4ce4cfcd16f55cfa3f" : {\r\n        "properties" : {\r\n          // ...\r\n        }\r\n      }\r\n    },\r\n    "aliases" : { }\r\n  }\r\n}\r\n```\r\n\r\nNormally, ES won\'t update the mapping when an unknown property is set. But in fact ... Here is the log of ES with this template : \r\n\r\n```\r\n[2014-09-29 18:17:23,757][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] creating index, cause [auto(index api)], shards [5]/[0], mappings [5252ce4ce4cfcd16f55cfa41, _default_, 5252ce4ce4cfcd16f55cfa3d, 5252ce4ce4cfcd16f55cfa3f, 5252ce4ce4cfcd16f55cfa40, 5252ce4ce4cfcd16f55cfa3a, 656d61696c2d746872656164, 5252ce4ce4cfcd16f55cfa3c, 5252ce4ce4cfcd16f55cfa3b]\r\n[2014-09-29 18:17:25,581][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] update_mapping [5252ce4ce4cfcd16f55cfa40] (dynamic)\r\n[2014-09-29 18:17:26,681][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] update_mapping [5252ce4ce4cfcd16f55cfa3f] (dynamic)\r\n[2014-09-29 18:17:27,421][INFO ][cluster.metadata         ] [Ogre] [company_54298612814192d3227e86b6] deleting index\r\n```\r\n\r\n\r\nIt\'s seems than disable dynamic mapping works, but why this two lines of log with "update_mapping (dynamic)" ?'
7913,'mikemccand','add more checks to build_release.py\nWhen I ran build_release.py to release 1.3.3 I noticed the git clone I\r\nwas working from had an extra "elasticsearch" subdirectory (probably I\r\ndid this earlier) and git status said "# Your branch is ahead of\r\n\'origin/1.3\' by 33 commits." which is spooky.\r\n\r\nI ran "git clean -f", and Simon suggested "git fetch origin" and sure\r\nenough that fixed "git status" to be "clean" again.\r\n\r\nI think the release script should check to make sure there are no\r\nuntracked files in the area, there are no uncommitted changes, you\r\nhave all changes from origin, and you don\'t have any unpushed changes.\r\n\r\n(Separately we should maybe upgrade the git on the build box ... it\'s\r\n1.7.9.5 now.)\r\n\r\nI tried to make these changes to build_release.py but likely messed it\r\nup (git is ... magical to me).\r\n\r\nIt seems like it would be best overall if the script just made a fresh\r\nclone of the branch for release?  Seems spooky to reuse a clone from\r\none release to the next...\r\n\r\nI also put in a few other cleanups that I noticed when building\r\n1.3.3.\r\n'
7911,'javanna',"[TEST] create client nodes using node.client: true\nCreate client nodes using `node.client: true` instead of `node.data: false` and `node.master: false`.\r\n\r\nWe should create client nodes in our test infra using the `node.client:true` settings as that is the one that users use, and the one that we use as well in `ClientNodePredicate` thus we end up not finding client nodes otherwise as they weren't created with the proper setting.\r\n\r\nUpdated also the `DataNodePredicate` so that `client: true` is enough, no need for `data: false` as well."
7907,'dadoonet',"Show open and closed indices in _cat/indices\nThere doesn't seem to be any other way to definitively and easily list the open/closed indexes in a cluster from what I could see, and it would be really helpful if this option was available in _cat.\r\n\r\nPlease :)"
7905,'mikemccand',"Internal: upgrade to Lucene 4.10.1 release\nLucene 4.10.1 just hit Maven Central ... I'll go upgrade to it.  Should be trivial because I just upgraded to the 4.10.1 snapshot a few days ago..."
7904,'rjernst','Internal: Optimize with `force=true` can sidestep max segments to merge at once used by delegate\nThe implementation of "forcing" optimize right now will cause the ElasticsearchMergePolicy to return one giant OneMerge if `force==true`.  However, this can cause IO issues.  The existing MP impls chain merging through "cascading", so that no OneMerge merges more than some X segments (e.g. X = 30 for TieredMP).  Forcing should do the same...'
7902,'colings86','Mapping: Posting a mapping with default analyzer fails\nCloses #2716\r\n\r\nWhen merging two mappings, default index analyzers are represented by either null or by a "default"-named index analyzer object. Fixed a spot where only the null representation was being considered as default.\r\n\r\nWrote a simple REST test to confirm the behavior is fixed. All tests pass.'
7897,'clintongormley','"filter" :  { ... },\nThe ending braces.'
7896,'GaelTadh','Stats : Add time in index throttle to stats.\nThis PR adds the time spent throttling indexing to a single thread to the stats API.\r\n\r\nCloses #7861'
7894,'clintongormley','Update id-field.asciidoc\nIt is strange to provide an example with `"store" : false` when talking about possibility of enabling the field to be stored.\r\nBroke the line in the mapping in two lines for better readability.\r\nMade a sentence above the mapping more verbose.'
7892,'colings86','Indices API: fixes GET Alias API backwards compatibility\nFor just the case when only the aliases are requested, the default indices options are to ignore missing indexes. When requesting any other feature or any combination of features, the default will be to error on missing indices.\r\n\r\nCloses #7793'
7890,'dadoonet',"Plugins: Don't overwrite plugin configuration when removing/upgrading plugins\nWhen removing and installing again the plugin all configuration files will be removed in `config/pluginname` dir.\r\nThis is bad as users may have set and added specific configuration files.\r\n\r\nWe don't have yet an upgrade command which could handle this nicely.\r\n\r\nSo we remove that code for now and we will implement an upgrade method in another PR.\r\n\r\nRelated to  #5064."
7888,'clintongormley','fix mismatched curly bracket\n'
7882,'mikemccand','Tests: let Lucene\'s MockDirectoryWrapper.close run CheckIndex when shard is closed\nSpinoff from #7730: today, we wrap with MockDirectoryWrapper the directories under DistributedDirectory, but this means we have to do "special things" to get Lucene\'s CheckIndex to run when a shard is closed.\r\n\r\nI think instead we should wrap above DistributorDirectory (somehow)?  Then MockDirectoryWrapper.close would check the index for us ...'
7880,'dadoonet','Add timezone setting for query_string\nThe timezone addition to DSL for range query/filter is great ! (https://github.com/elasticsearch/elasticsearch/pull/7113).  The Lucene standard query parser appears to support setTimeZone so maybe there is a way for us to expose that as an option when using query_string? \r\n'
7875,'imotov','simple_query_string parser may fail with NumberFormatException while parsing flags\nTo reproduce:\r\n\r\n```\r\ncurl -XDELETE \'http://localhost:9200/test/?pretty\'\r\n\r\ncurl -XPUT \'http://localhost:9200/test/?pretty\' -d \'{\r\n  "settings": {\r\n    "index.number_of_shards": 1\r\n  },\r\n  "mappings": {\r\n    "document": {\r\n      "_routing" : {\r\n        "required": true\r\n      },\r\n      "properties": {\r\n        "title": {\r\n          "type": "string"\r\n\t}\r\n      }\r\n    },\r\n    "entity": {\r\n      "_parent": {\r\n        "type": "document"\r\n      },\r\n      "_routing" : {\r\n        "required": true\r\n      },\r\n      "properties": {\r\n        "body": {\r\n          "type": "string"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n\r\ncurl -XPOST \'http://localhost:9200/_bulk?pretty\' --data-binary \'\r\n{"index": {"_index": "test", "_type": "document", "_id" : "1", "_routing": "1"}}\r\n{"title": "New document"}\r\n{"index": {"_index": "test", "_type": "entity", "_id" : "1", "_routing": "1", "_parent": "1"}}\r\n{"body": "document body"}\r\n\'\r\ncurl -XPOST \'http://localhost:9200/test/_refresh?pretty\'\r\ncurl -XGET \'http://localhost:9200/test/document/_search?pretty\' -d \'\r\n{\r\n  "query" : {\r\n    "bool": {\r\n      "minimum_should_match": 1,\r\n      "disable_coord": true,\r\n      "should": [\r\n        {\r\n          "simple_query_string": {\r\n            "query": "old document",\r\n            "default_operator": "and",\r\n            "fields": ["title^10"],\r\n            "flags": "NONE"\r\n          }\r\n        },\r\n        {\r\n          "has_child": {\r\n            "query": {\r\n              "bool": {\r\n                "minimum_should_match": 1,\r\n                "disable_coord": true,\r\n                "should": [\r\n                  {\r\n                    "simple_query_string" : {\r\n                      "query" : "\\"document body\\"~2",\r\n                      "default_operator" : "and",\r\n                      "flags": "PHRASE|SLOP",\r\n                      "fields": ["body^5"]\r\n                    }\r\n                  },\r\n                  {\r\n                    "simple_query_string" : {\r\n                      "query" : "document body",\r\n                      "default_operator" : "and",\r\n                      "fields": ["body"],\r\n                      "flags": "NONE"\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            },\r\n            "type": "entity",\r\n            "score_mode": "sum"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n\'\r\n```\r\n\r\nThe search fails with `NumberFormatException[For input string: \\"PHRASE|SLOP\\"];` exception.\r\n\r\nThe problem occurs because `simple_query_string` parser is using `XContentParser.hasTextCharacters()` method to check for the presence of text in the token, while this method should be only used to detect internal presentation of the string. \r\n\r\nThe issue was originally reported on the mailing list https://groups.google.com/forum/#!topic/elasticsearch-ru/SeiifNQW-qo'
7874,'colings86','null_value and dynamic_templates not working correctly\nIf you have a dynamic template that maps a field with a null_value, and you index 1 document with an explicit null value to the field, the null_value mapping does not apply correctly.\r\n\r\nSteps to reproduce:\r\n\r\n```\r\ncurl -XDELETE localhost:9200/test\r\n\r\ncurl -XPUT localhost:9200/test -d \'{\r\n  "mappings": {\r\n    "event": {\r\n      "dynamic_templates": [\r\n      {\r\n        "values_as_string": {\r\n          "match": "property",\r\n          "mapping": {\r\n            "type": "string",\r\n            "null_value": "_null_"\r\n          }\r\n        }\r\n      }\r\n      ]\r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XPUT localhost:9200/test/event/2 -d \'\r\n{\r\n  "property": null\r\n}\r\n\'\r\n\r\ncurl -XPOST localhost:9200/test/_refresh\r\n\r\n# returns no results, expected to return above document\r\ncurl -XPOST localhost:9200/test/_search -d \'\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "term": {\r\n          "property": "_null_"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n'
7873,'martijnvg','Resiliency: Perform write consistency check just before writing on the primary shard\nBefore this change the write consistency change was performed on the node that receives the write request and the node that holds the primary shard. This change removes the check on the node that receives the request, since it is redundant.\r\n\r\nAlso this change moves the write consistency check on the node that holds the primary shard to a later moment after forking of the thread to perform the actual write on the primary shard. '
7872,'clintongormley','Update regexp-syntax.asciidoc\nThe plus sign "+" can be used to repeat the preceding shortest pattern once or more time.\r\n\r\nat-least once therefor, For string "aaabbb":\r\n\r\naa+bbb+  #  no match'
7870,'javanna',"Internal: split internal fetch request used within scroll and search\nSimilar to #7856 but relates to the fetch shard level requests. We currently use the same internal request when we need to fetch within search and scroll. The two original requests though diverged after #6933 as `SearchRequest` implements `IndicesRequest` while `SearchScrollRequest` doesn't. That said, with #7319 we made `FetchSearchRequest` implement `IndicesRequest` by making it hold the original indices taken from the original request, which are null if the fetch was originated by a search scroll, and that is why original indices are optional there.\r\n\r\nThis commit introduces a separate fetch request and transport action for scroll, which doesn't hold original indices. The new action is only used against nodes that expose it, the previous action name will be used for nodes older than 1.4.0.Beta1.\r\n\r\nAs a result, in 1.4 we have a new `indices:data/read/search[phase/fetch/id/scroll]` action that is equivalent to the previous `indices:data/read/search[phase/fetch/id]` whose request implements now IndicesRequest and holds the original indices coming from the original request. The original indices in the latter request can only be null during a rolling upgrade (already existing version checks make sure that serialization is bw compatible), when some nodes are still < 1.4."
7867,'clintongormley','Update suggesters.asciidoc\nA request was malformed'
7861,'GaelTadh',"Add index throttling to node stats\nToday, when single thread index throttling kicks in, it's only logged. We don't capture it in our stats. \r\nWe should add the time a shard spent being index level throttled to our stats. \r\n"
7860,'javanna','[TEST] add regular scroll REST test\n'
7856,'javanna',"Internal: split internal free context request used after scroll and search\nWe currently use the same internal request when we need to free the search context after a search and a scroll. The two original requests though diverged after #6933 as `SearchRequest` implements `IndicesRequest` while `SearchScrollRequest` and `ClearScrollRequest` don't. That said, with #7319 we made `SearchFreeContextRequest` implement `IndicesRequest` by making it hold the original indices taken from the original request, which are null if the free context was originated by a scroll or by a clear scroll call, and that is why original indices are optional there.\r\n\r\nThis commit introduces a separate free context request and transport action for scroll, which doesn't hold original indices. The new action is only used against nodes that expose it, the previous action name will be used for nodes older than 1.4.0.Beta1.\r\n\r\nAs a result, in 1.4 we have a new `indices:data/read/search[free_context/scroll]` action that is equivalent to the previous `indices:data/read/search[free_context]` whose request implements now `IndicesRequest` and holds the original indices coming from the original request. The original indices in the latter request can only be null during a rolling upgrade (already existing version checks make sure that serialization is bw compatible), when some nodes are still < 1.4."
7855,'javanna','Internal: Clarify when a shard search request gets created to be only used locally\nIn some cases a shard search request gets created on a node to be only used there and never sent over the transport. This commit clarifies that and creates a new base class called `ShardSearchLocalRequest` that can and will be only used locally. `ShardSearchTransportRequest` on the other hand delegates to the local version but extends `TransportRequest` and is `Streamable`, which means that it is supposed to be sent over the transport.\r\n\r\nThis way we can make the `OriginalIndices` only required (and mandatory now) in the transport variant.\r\n\r\nTook the chance to remove an unused InternalScrollSearchRequest constructor and an empty else branch in `TransportSearchScrollQueryAndFetchAction`.'
7853,'colings86','Aggregations: Significant Terms Heuristics now registered correctly\nCloses #7840'
7849,'clintongormley','Add Release notes to every new release\nHey guys, just a heads up asking if you could consider adding the release notes altogether with every release on github?\r\nIt is certainly a good thing for those who are following :+1: \r\n\r\nI like how github does with [Atom](/github/atom/releases)'
7845,'jpountz','java api - multiple dimension arrays of shorts are casted to multiple dimension arrays of floats\nHi,\r\n\r\nI believe there is a typo in org.elasticsearch.common.xcontent.XContentBuilder.java\r\nhttps://github.com/elasticsearch/elasticsearch/blob/v1.3.2/src/main/java/org/elasticsearch/common/xcontent/XContentBuilder.java#L1224\r\n\r\nI think\r\nfor (float v : (short[]) value) {\r\nshould be\r\nfor (short v : (short[]) value) {\r\n\r\nFor example, right now, if you try to insert a 2D array of shorts you end up with a 2D array of floats.\r\n\r\nThanks!'
7844,'mikemccand',"Internal: Upgrade to Lucene 4.10.1 snapshot\nUpgrades to a snapshot release of 4.10.1, hopefully sidestepping the 1.8.0_20 JVM bug we've been hitting in our builds recently after we upgraded to 4.10.0.  This also gives us test coverage of the upcoming Lucene point release..."
7843,'clintongormley','GeoJSON is lon,lat not lat,lon\nAlthough emphasized in the text, the example was backwards.'
7842,'clintongormley','Improve sentence structure\n'
7841,'clintongormley','Update documentation related to fielddata eviction behavior\nAs noted in https://groups.google.com/forum/#!topic/elasticsearch/0sSYSFfmmXM the following page needs to be corrected: http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_limiting_memory_usage.html.  \r\n\r\nOn the second issue, potential deprecation of indices.fielddata.cache.expire, I think deprecating that setting would be the wrong thing to do.  '
7840,'markharwood','Aggregations: NPE in SignificanceHeuristicStreams.read while deserializing response\nThis is caused by the TransportClient failing to register a module that is now required to deserialize responses correctly.\r\nThe fix is to add this line to the constructor:\r\n\r\n        modules.add(new SignificantTermsHeuristicModule());\r\n\r\nThanks to Felipe Hummel for reporting the error and providing a failing test case here: https://groups.google.com/forum/#!topic/elasticsearch/R42Nyyfr73I'
7833,'s1monw',"Internal: Use internal time estimation for time limited search collector\nToday `TimeLimitingCollector` uses it's own thread to do time estimations for search request timeouts. This thread is not controllable via threadpools etc. we should use our own infrastructure to signal timeouts. Luckily we already have an estimating thread in the ThreadPool class we just need to expose it to the collector."
7827,'clintongormley','Remove comma in JSON\nThere was a comma too many in the documentation for the mapping char filter.'
7824,'dadoonet',"Admin: add total index memory in `_cat/indices`\nThis patch adds to `_cat/indices` information about memory usage per index by adding memory used by FieldData, IdCache, Percolate, Segments (memory, index writer, version map).\r\n\r\n```\r\n% curl 'localhost:9200/_cat/indices?v&h=i,tm'\r\ni     tm\r\nwiki  8.1gb\r\ntest  30.5kb\r\nuser  1.9mb\r\n```\r\n\r\nCloses #7008"
7818,'dadoonet','Document the Java BulkProcessor\nCloses #7638.'
7817,'clintongormley','search + highlight + sorting -> missing results\nHi!\r\n\r\nI\'m experiencing a weird issue with highlighting. \r\n\r\nWhen I execute search with highlighting on, I\'m getting different number of results based on which sort I choose.\r\n\r\nIf I run this command (no highlighting, size 500), I\'m always getting 500:\r\n```\r\ncurl -XGET \'http://localhost:9200/_all/_search?pretty\' -d \'{\r\n  "query": {     \r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "*",\r\n                "fields": [\r\n                  "text"\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "match_all": {}\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "size": 500,\r\n  "sort": [\r\n    {\r\n      "@timestamp": {\r\n        "order": "asc",\r\n        "ignore_unmapped": true\r\n      }\r\n    }\r\n  ]\r\n}\' | grep \'_source\' | wc -l\r\n```\r\n\r\nBut, if I run this command (highlighting, size 500), I\'m always getting <500 (always the same, 421 for my data):\r\n```\r\ncurl -XGET \'http://localhost:9200/_all/_search?pretty\' -d \'{\r\n  "query": {     \r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "*",\r\n                "fields": [\r\n                  "text"\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "match_all": {}\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "highlight": {\r\n    "fields": {\r\n      "text": {}\r\n    },\r\n    "fragment_size": 100,\r\n    "pre_tags": [\r\n      "@start-highlight@"\r\n    ],\r\n    "post_tags": [\r\n      "@end-highlight@"\r\n    ]\r\n  },\r\n  "size": 500,\r\n  "sort": [\r\n    {\r\n      "@timestamp": {\r\n        "order": "asc",\r\n        "ignore_unmapped": true\r\n      }\r\n    }\r\n  ]\r\n}\' | grep \'_source\' | wc -l\r\n```\r\nIf I choose some other sort (desc, or other column), I\'ll get different number. Of course, there are cases when I get 500.\r\n\r\nYou can see that those queries were generated by Kibana, but I\'m running those directly on ES - so it\'s not a problem in Kibana. I read documentation and couldn\'t conclude that this query is invalid - sorry if it is, and it\'s Kibana bug.'
7816,'brwe','[Corner case] Script working event after disabling dynamic scripting \nOn executing\r\n```\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "sum": {\r\n      "sum": {\r\n        "script": "doc.score"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI am seeing \r\n{\r\ntook: 13\r\ntimed_out: false\r\n_shards: {\r\ntotal: 1\r\nsuccessful: 1\r\nfailed: 0\r\n}\r\nhits: {\r\ntotal: 1946\r\nmax_score: 0\r\nhits: [ ]\r\n}\r\naggregations: {\r\nsum: {\r\nvalue: 1946\r\n}\r\n}\r\n}\r\n\r\nBut on executing \r\n\r\n```\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "sum": {\r\n      "sum": {\r\n        "script": "_score"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI am getting - SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[OtHPQX2iRj2yhtfeBMqJgA][restaurants][0]: SearchParseException[[restaurants][0]: from[-1],size[0]: Parse Failure [Failed to parse source [{"size":0,"aggs":{"sum":{"sum":{"script":"_score"}}}}]]]; nested: ScriptException[dynamic scripting for [mvel] disabled]; }]'
7813,'clintongormley','Resolves #7812 : adding elasticsearch-dsl to list of python clients\nadding [elasticsearch-dsl](https://github.com/elasticsearch/elasticsearch-dsl-py) to list of python clients.\r\n\r\nThis resolve Documentation Issue #7812'
7807,'markharwood','Docs : Filtering based on exact values not working\nIf I try to do that, as a doc, I have a parsing error.\r\n\r\n    {\r\n       "aggs" : {\r\n           "tags" : {\r\n                "terms" : {\r\n                    "field" : "make",\r\n                    "exclude" : ["mazda", "honda"]\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nErrors exemple : \r\n\r\n    Parse Failure [Unknown key for a START_ARRAY in [dedup]: [exclude].]];\r\n\r\nIf I want it to work I have to do use a regular expression :\r\n\r\n    {\r\n       "aggs" : {\r\n           "tags" : {\r\n                "terms" : {\r\n                    "field" : "tags",\r\n                    "exclude" : "mazda|honda"\r\n                }\r\n            }\r\n        }\r\n    }'
7802,'clintongormley',"Fix Idaho spelling\nDoesn't have two `d`s."
7797,'dakrone','NullPointerException in update api when try to add item to a non-existing list\nI want to add items as nested-type to a list in an existing document, but I get this error message:\r\n\r\nElasticsearchIllegalArgumentException[failed to execute script]; nested: GroovyScriptExecutionException[NullPointerException[Cannot execute null+null]];\r\n\r\nThe document exist but it does not contains a nested list/array. \r\nIs this a bug or how can I write a update script there create the list if it not exist?\r\n\r\n----\r\nCreate document:\r\n```JSON\r\nPUT test/mytype/myid\r\n{\r\n  "name": "TestName"\r\n}\r\n```\r\n\r\nUpdate there fail:\r\n```JSON\r\nPOST test/mytype/myid/_update?lang=groovy\r\n{\r\n  "script": "ctx._source.tags += newtag;",\r\n  "params": {\r\n    "newtag": [{\r\n      "value": 7,\r\n      "innerName": "John"\r\n    }]\r\n  }\r\n}\r\n```\r\n\r\nMapping:\r\n```JSON\r\nPOST test/_mapping/mytype\r\n{\r\n  "properties": {\r\n     "name": {\r\n        "type": "string"\r\n     },\r\n     "tags": {\r\n       "type": "nested",\r\n        "properties": {\r\n           "innerName": {\r\n              "type": "string"\r\n           },\r\n           "value": {\r\n              "type": "long"\r\n           }\r\n        }\r\n     }\r\n  }\r\n}\r\n```'
7795,'javanna','[TEST] move REST tests to their own test group\n'
7786,'colings86','[DOC] Add GET Alias API note to breaking changes\nNote explains that GET Alias API now supports IndicesOptions and will error if a index is missing'
7784,'javanna',"[TEST] Make it possible to customize whether the REST tests are run by d...\nWe currently run REST tests by default. There might be third parties that use our test infra and don't want to run REST tests by default, especially since they need some additional configuration for spec and tests that are not shipped with the tests jar file. In those cases we might want to customize the value for the property that controls whether REST tests are run by default or not.\r\n\r\nThis change makes it possible to change the default value from a static block like the following:\r\n\r\n```\r\nstatic {\r\n    ElasticsearchRestTests.REST_TESTS_ENABLED_BY_DEFAULT = false;\r\n}\r\n```"
7780,'dakrone','Internal: Refactor the Translog.read(Location) method\nIt was only used by `readSource`, it has been changed to return a\r\nTranslog.Operation, which can have .getSource() called on it to return\r\nthe source. `readSource` has been removed.\r\n\r\nThis also removes the checked IOException, any exception thrown is\r\nunexpected and should throw a runtime exception.\r\n\r\nMoves the ReleasableBytesStreamOutput allocation into the body of the\r\ntry-catch block so the lock can be released in the event of an exception\r\nduring allocation.'
7776,'clintongormley','Added example of sysctl.conf for file descriptors change\nI added an example of the sysctl.conf change required for file descriptors'
7774,'martijnvg','IllegalArgumentException when \'from\' or \'size\' is too large \nFor request:\r\n\r\n```javascript\r\n {"sort":{"_uid":"asc"},"from":2147483647,"size":10, "query":{"query_string":{"query":"*:*"}}}\r\n```\r\n\r\nFailed with a IllegalArgumentException: \r\n\r\n> IllegalArgumentException[numHits must be > 0; please use TotalHitCountCollector if you just need the total hit count]; }]'
7771,'pickypg','Adding "min" score mode to parent-child queries\nSupport for "max", "sum", and "avg" already existed.\r\n\r\nCloses #7603 '
7770,'dadoonet',"Ability to check version without starting server\nI want to be able to know what version I'm running without starting the server and `curl`ing "
7767,'javanna','Internal: make sure that internally generated percolate request re-uses the original headers and request context\nWhen percolating an existing document we internally recreate a new `PercolateRequest` that we go execute. We need to make sure that original headers and request context are preserved though when doing that.'
7766,'javanna','Internal: make sure that update internal requests share the same original headers and request context\nUpdate request internally executes index and delete operations. We need to make sure that those internal operations hold the same headers and context as the original update request. Achieved via copy constructors that accept the current request and the original request.'
7756,'suyograo','NoShardAvailableActionException - Nest the original exception\nIn `TransportShardSingleOperationAction.AsyncSingleAction.perform` we create a new `NoShardAvailableActionException` without nesting the original exception. \r\n\r\n``` java\r\nif (failure == null || isShardNotAvailableException(failure)) {\r\n    failure = new NoShardAvailableActionException(shardIt.shardId());\r\n```\r\n\r\nIt would be useful to know what exactly happened when looking at stack traces like this\r\n\r\n``` java\r\nCaused by: org.elasticsearch.action.NoShardAvailableActionException: [.scripts][4] null \r\nat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:144) \r\nat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:124) \r\nat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72) \r\nat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49) \r\nat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:65) \r\nat org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92) \r\nat org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:189) \r\nat org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:196) \r\nat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:91) \r\nat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:65) \r\nat org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:73) \r\nat org.elasticsearch.script.ScriptService.queryScriptIndex(ScriptService.java:385) \r\nat org.elasticsearch.script.ScriptService.queryScriptIndex(ScriptService.java:379) \r\nat org.elasticsearch.script.ScriptService.getScriptFromIndex(ScriptService.java:401) \r\nat org.elasticsearch.script.ScriptService.compile(ScriptService.java:309) \r\nat org.elasticsearch.script.ScriptService.executable(ScriptService.java:497) \r\nat org.elasticsearch.action.update.UpdateHelper.prepare(UpdateHelper.java:149)\r\n\r\n```\r\n'
7753,'dakrone',"Disk decider can allocate more data than the node can handle\nWe had a disk full event recently that exposed a potentially dangerous behavior of the disk-based shard allocation.\r\n\r\nIt appears that the disk-based allocation algorithm checks to see whether shards will fit on a node and disallows shards that would increase the usage past the high watermark. That's good. But in our case, the disks filled up anyway.\r\n\r\nWe had a cluster where every node's data partition was close to full. When a node (we'll call it node A) ran out of space, most of the shards allocated to it failed and were deleted. This took it very close to the low watermark, but not quite under. Later, an unknown event (possibly a merge, possibly a human doing something) freed more space and brought disk usage back under the low watermark. Elasticsearch allocated a few of the failed shards from before back to the same node. However, recovery of those shards failed due to disk full errors.\r\n\r\nAt roughly the same time, another node in the cluster (node B) ran out of disk and failed a bunch of shards.\r\n\r\nI believe the recovery failed because two events triggered allocation at roughly the same time. The first caused the disk-based allocator to allocate some shards to the node. While those shards were initializing, the second event caused another instance of the allocator to allocate even more shards to the same node.\r\n\r\nDoes the disk-based allocator consider the expected disk usage after current recoveries are finished, or does it ignore current recoveries?\r\n\r\nUnfortunately I don't have logs of allocation decisions, so I don't know exactly which shards were allocated where. I know that all the shards that failed recovery were originally allocated to node A. It's possible that none of the shards from node B were actually allocated to node A.\r\n\r\nRegardless of what actually happened, I'm hoping that someone can explain to me what the disk-based allocator would be expected to do in the above case where there are two allocation events in a short time."
7750,'colings86',"Aggregations: More consistent response format for scripted metrics aggregation\nChanges the name of the field in the scripted metrics aggregation from 'aggregation' to 'value' to be more in line with the other metrics aggregations like 'avg'"
7746,'bleskes','Docs: Document the most important changes to zen discovery.\n'
7740,'kimchy',"Internal: Make `TransportMasterNodeOperationAction#checkBlock` abstract\n`TransportMasterNodeOperationAction#checkBlock` should be implemented by any subclasses of ``TransportMasterNodeOperationAction` but it's returning `null` by default. We should make that abstract to force implementations for it."
7739,'clintongormley','bad text wrapping\nOn the page http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-synonym-tokenfilter.html\r\n\r\neven on a huge monitor the text is being wrapped the next way\r\n```\r\n# If expand==true, "ipod, i-pod, i pod" is equivalent to the explicit\r\nmapping:\r\nipod, i-pod, i pod => ipod, i-pod, i pod\r\n# If expand==false, "ipod, i-pod, i pod" is equivalent to the explicit\r\nmapping:\r\nipod, i-pod, i pod => ipod\r\n```\r\n\r\nSo one can think that "mapping:" is not in comment and is a part of syntax. But the lines are less than 80 chars, so perhaps the problem is in the page layout and there may be some other pages in the reference where the text is also being wrapped in an undesirable way.'
7737,'javanna','[TEST] Update REST client before each test in our REST tests\nIn #7723 we removed the `updateAddresses` method from `RestClient` under the assumption that the addresses never change during the suite execution, as REST tests rely on the global cluster. Due to #6734 we restart the global cluster though before each test if there was a failure in the suite. If that happens we do need to make sure that the REST client points to the proper nodes. What was missing before was the http call to verify the es version every time the addresses change, which we do now since we effectively recreate the REST client from scratch when needed (if the http addresses have changed).'
7736,'javanna','Internal: make sure that all delete mapping internal requests share the same original headers and context\nDelete mapping executes flush, delete by query and refresh operations internally. Those internal requests are now initialized by passing in the original delete mapping request so that its headers and request context are kept around.'
7734,'javanna',"Internal: add indices setter to IndicesRequest interface\nWe currently expose generic getters for `indices` and `indicesOptions` on the `IndicesRequest` interface. This commit adds a generic setter as well, which can be used to set the indices to a request. The setter impl throws `UnsupportedOperationException` if called on internal requests, since it makes sense to eventually replace the indices on external requests only. Also throws exception when the set operation doesn't make sense."
7730,'mikemccand','Test: always run CheckIndex after a test, and fail the test if it detects corruption\nSimple pull request; all tests passed at least once.\r\n\r\nThe boolean I added is a bit hackity but seems to work; if anyone knows a cleaner way let me know ... (exceptions thrown here are simply caught & logged above).\r\n\r\nTests that create known corrupted shards already seem to set the boolean setting (index.store.mock.check_index_on_close) to false ...'
7728,'colings86','Aggregations: Removes isSingleUserCriteria check\nThis change removes the backwards compatibility workaround that checks that a compoundOrder originated from a single user defined criteria for the purposes of serialising to older versioned nodes.'
7724,'mikemccand',"Tests: always run CheckIndex on all shards created by the test\nToday we randomly CheckIndex 10% of the time, in MockFSDirectoryService.\r\n\r\nAlso (I think?), if the CheckIndex fails we don't fail the test; maybe there are known tests that create corrupted indices, but I think we should fail by default and then white list such tests?  We need to know if anything unexpectedly created corrupted indices ..."
7714,'markharwood','Aggs: filtering values using array of values, including numeric values\nIn facets, we can filter a Terms Facet using an [array of values](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_excluding_terms), including numeric values:\r\n```json\r\n{\r\n    "query" : {\r\n        "match_all" : { }\r\n    },\r\n    "facets" : {\r\n        "tag" : {\r\n            "terms" : {\r\n                "field" : "tag_id",\r\n                "exclude" : [101, 202]\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nThis cannot be achieved with the regular expression support in the current exclude syntax as it only support string fields.  Some excellent work was done on #6782 to support string terms arrays but this complementary feature for numeric arrays is still missing.\r\n\r\nThere\'s a more detailed description in [this email thread](https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/elasticsearch/8g74ov0run0/yLfGwVF7WVgJ) including a [curl gist test case](https://gist.github.com/nezda/60932c73a8485e9d9a49).\r\n\r\ncc @micpalmia '
7709,'brwe','field name lookup: return List instead of Set for names matching a patte...\n...rn\r\n\r\n\r\nThe returned sets are only used for iterating. Therefore we might\r\nas well return a list since this guaratees order.\r\n\r\nThis is the same effect as in\r\nhttps://github.com/elasticsearch/elasticsearch/pull/7698\r\nThe test SimpleIndexQueryParserTests#testQueryStringFieldsMatch\r\nfailed on openjdk 1.7.0_65 with\r\n<jdk.map.althashing.threshold>0</jdk.map.althashing.threshold>\r\n\r\nUnsure if this is the best solution - we might as well just change the test SimpleIndexQueryParserTests#testQueryStringFieldsMatch '
7706,'bleskes','Discovery: make sure we maintain the latest PingResponse from each node\nWhen each node gets a unicast ping, in responds with a list of ping it has received in the last 3 seconds. With #7702, we run into an issue were earlier pings in this list were preferred to later pings - causing election to be done based on stale information.  \r\n\r\nIt is how ever still the case that ping list from *different* sources may case old pings to override new pings. We should introduce an increasing only ping id and use it to make sure older pings never replace new information.'
7701,'dadoonet','Update java source example\nFrom the version 1.0 FilterBuilders and QueryBuilders are not part from org.elasticsearch.index.query.xcontent package no more.'
7697,'martijnvg','Aggregations: Inconsistent sorting of top_hits fixed\nIn the reduce logic of the `top_hits` aggregation if the first shard result to process contained has no results then the merging of all the shard results can go wrong resulting in an incorrect sorted hits.\r\n\r\nThis bug can only manifest with a sort other than score.'
7692,'colings86','[DOCS] clarification of breaking changes to 1.4 due to GET index API\n'
7691,'javanna','More Like This API: remove unused search_query_hint parameter\n'
7689,'imotov','NullPointerException on ResourceWatcherService\nI made a mistake while i deployed a script. I created a file (instead of a directory) named "scripts" to save my script. And now, even if scripts is a directory (with the script inside), i got the following exception\r\n\r\n`[2014-09-11 13:55:15,117][WARN ][threadpool               ] [Barristan] failed to run org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor@5985e4fa\r\njava.lang.NullPointerException\r\n\tat org.elasticsearch.watcher.FileWatcher$FileObserver.updateChildren(FileWatcher.java:184)\r\n\tat org.elasticsearch.watcher.FileWatcher$FileObserver.checkAndNotify(FileWatcher.java:93)\r\n\tat org.elasticsearch.watcher.FileWatcher.doCheckAndNotify(FileWatcher.java:47)\r\n\tat org.elasticsearch.watcher.AbstractResourceWatcher.checkAndNotify(AbstractResourceWatcher.java:43)\r\n\tat org.elasticsearch.watcher.ResourceWatcherService$ResourceMonitor.run(ResourceWatcherService.java:102)\r\n\tat org.elasticsearch.threadpool.ThreadPool$LoggingRunnable.run(ThreadPool.java:440)\r\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\r\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)\r\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)`'
7686,'dakrone','ES not throwing parse exception `ids` query double-nested array\n[2014-09-11 11:47:51,946][DEBUG][index.search.slowlog.query] [n020] [index][3] took[3.1s], took_millis[3141], types[type], stats[], search_type[QUERY_THEN_FETCH], total_shards[5], source[{"size":12,"from":0,"sort":{"ats":"desc"},"query":{"filtered":{"query":{"query_string":{"query":"ten words string","fields":["title","tags"],"default_operator":"OR"}},"filter":{"bool":{"must":[{"range":{"ats":{"lte":1410428944}}},{"terms":{"aid":[27]}}],"must_not":[{"ids":{"values":[["ten-words-dash-separated-string"]]}}]}}}}}], extra_source[]'
7678,'clintongormley','Update thrift.asciidoc\nFix typos, add clarifications and link.'
7675,'javanna','Internal: refactor copy headers mechanism to not require a client factory\nWith #7594 we replaced the static `BaseRestHandler#addUsefulHeaders` by introducing the `RestClientFactory` that can be injected and used to register the relevant headers. To simplify things, we can now register relevant headers through the `RestController` and remove the `RestClientFactory` that was just introduced.'
7671,'clintongormley','Add that children aggregation is coming in 1.4.0\nI had a syntax error because I tried the `children` aggregation on my deployed instance of ElasticSearch 1.3.2, and I had to look in the source repository to understand where it was coming from.\r\n\r\nBetter to avoid that hassle for other people.'
7664,'brwe','Fixed explanation for GaussDecayFunction\nThe explanation now gives the correct value instead of the negative.'
7662,'colings86','Aggregations: Adds ability to sort on multiple criteria\nThe terms aggregation can now support sorting on multiple criteria by replacing the sort object with an array or sort object whose order signifies the priority of the sort. The existing syntax for sorting on a single criteria also still works.\r\n\r\nContributes to #6917\r\nReplaces #7588'
7658,'dakrone','Reduce permgen use from Groovy scripts\nGroovy scripts seem to be using more permgen than MVEL scripts do, especially when sent dynamically.\r\n\r\nIt would be nice if we reduce the amount of permgen used by these scripts, or make the permgen space recoverable in the event the script is not used.'
7654,'clintongormley','Clarify s3 snapshot compress behavior\nClarify s3 snapshot compress behavior only applies to metadata and no index files.'
7652,'pickypg','RestNodesAction - Should report File Descriptors\n`RestNodesAction` does not currently report anything related to the number of file descriptors even though it reports other similarly useful metrics, such as heap usage.\r\n\r\n- Adding `file_desc.current` - The current number of file descriptors in use.\r\n- Adding `file_desc.percent` - The percentage of the maximum number of file descriptors in use (`current / max * 100`).\r\n- Adding `file_desc.max` - The maximum number of file descriptors.\r\n\r\nIn addition, I feel that it would be useful to be able to see the `heap.current` and `ram.current` (as `ByteSizeValue`s rather than only being able to see the percentage).'
7649,'gmarz','ElasticSearch 1.3.2 Server Will not Run Under Windows 8.1\nI installed ElasticSearch 1.3.2 on my Windows 8.1 laptop (64 bit) and got the server to run under the Windows prompt.  When I attempt to perform basic Puts/Gets, I receive a 404 error that the server is not recognized (status 0).  I use localhost:9200 and followed the tutorial documentation with no success.  I also tried to run service install and service start and I obtain a failed error message.  I setup JAVA_HOME as required under the Java JRE1.8.20 directory.  Any suggestions on how to get the server to respond?  I am new to ElasticSearch ... thanks in advance ...'
7647,'GaelTadh','Breaking change not documented in release notes\nSomewhere between v1.2.2 and v1.3.1 (I suspect 1.3.0), the DSL for _search/template changed such that, for a template that is deployed to the scripts directory in the file system has to be specified by:\r\n\r\n{\r\n    "params": {...},\r\n    "template": {\r\n        "file": "simple"\r\n    }\r\n}\r\n\r\ninstead of \r\n\r\n{\r\n    "params": {...},\r\n    "template": "simple"\r\n}\'\r\n\r\nThis is a breaking change in either direction (neither form works for both versions).\r\n\r\nThis is not identified in the release notes on the downloads page.'
7646,'clintongormley','Update object-type.asciidoc\nMade the object definition more explicit. '
7638,'dadoonet','Document the Java BulkProcessor\n'
7625,'s1monw',"Internal: Searcher might be released twice in the case of a LONG GC\nIn the case of a long GC searchers might be released twice once by the reaper and once by the actual releasing thread. It's a cosmetic problem since we protect from double releasing but we should fix it.\r\n\r\nThis has been seen in the field:\r\n```\r\norg.elasticsearch.ElasticsearchIllegalStateException: Double release \r\nat org.elasticsearch.index.engine.internal.InternalEngine$EngineSearcher.close(InternalEngine.java:1512) \r\nat org.elasticsearch.common.lease.Releasables.close(Releasables.java:45) \r\nat org.elasticsearch.common.lease.Releasables.close(Releasables.java:60) \r\nat org.elasticsearch.common.lease.Releasables.close(Releasables.java:65) \r\nat org.elasticsearch.search.internal.DefaultSearchContext.doClose(DefaultSearchContext.java:212) \r\nat org.elasticsearch.search.internal.SearchContext.close(SearchContext.java:96) \r\nat org.elasticsearch.search.SearchService.freeContext(SearchService.java:560) \r\nat org.elasticsearch.search.SearchService.access$100(SearchService.java:97) \r\nat org.elasticsearch.search.SearchService$Reaper.run(SearchService.java:957) \r\n```"
7623,'s1monw','Indexed Scripts/Templates: Indexed Scripts used during reduce phase sometimes hang\nIndexed scripts might need to get fetched via a GET call which is very cheap since those shards are local since they expand `[0-all]` but sometimes in the case of a node client holding no data we need to do a get call on the first get. Yet this get call seems to be executed on the transport thread and might deadlock since it needs that thread to process the get response. See stacktrace below... The problem here is that some of the actions in `SearchServiceTransportAction` don\'t use the `search` threadpool but use `SAME` instead which can cause this issue. We should use `SEARCH` instead for the most of the operations except of free context I guess.\r\n\r\n```\r\n2> "elasticsearch[node_s2][local_transport][T#1]" ID=1421 WAITING on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72\r\n  2> \tat sun.misc.Unsafe.park(Native Method)\r\n  2> \t- waiting on org.elasticsearch.common.util.concurrent.BaseFuture$Sync@2b1fdd72\r\n  2> \tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)\r\n  2> \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)\r\n  2> \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)\r\n  2> \tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)\r\n  2> \tat org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:274)\r\n  2> \tat org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\r\n  2> \tat org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\r\n  2> \tat org.elasticsearch.script.ScriptService.getScriptFromIndex(ScriptService.java:377)\r\n  2> \tat org.elasticsearch.script.ScriptService.compile(ScriptService.java:295)\r\n  2> \tat org.elasticsearch.script.ScriptService.executable(ScriptService.java:457)\r\n  2> \tat org.elasticsearch.search.aggregations.metrics.scripted.InternalScriptedMetric.reduce(InternalScriptedMetric.java:99)\r\n  2> \tat org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)\r\n  2> \tat org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:374)\r\n  2> \tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchDfsQueryThenFetchAction.java:209)\r\n  2> \tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.finishHim(TransportSearchDfsQueryThenFetchAction.java:196)\r\n  2> \tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:172)\r\n  2> \tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.onResult(TransportSearchDfsQueryThenFetchAction.java:166)\r\n  2> \tat org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:440)\r\n  2> \tat org.elasticsearch.search.action.SearchServiceTransportAction$18.handleResponse(SearchServiceTransportAction.java:431)\r\n  2> \tat org.elasticsearch.transport.local.LocalTransport$3.run(LocalTransport.java:322)\r\n  2> \tat com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)\r\n  2> \tat org.elasticsearch.transport.local.LocalTransport.handleParsedResponse(LocalTransport.java:317)\r\n  2> \tat org.elasticsearch.test.transport.AssertingLocalTransport.handleParsedResponse(AssertingLocalTransport.java:59)\r\n  2> \tat org.elasticsearch.transport.local.LocalTransport.handleResponse(LocalTransport.java:313)\r\n  2> \tat org.elasticsearch.transport.local.LocalTransport.messageReceived(LocalTransport.java:238)\r\n  2> \tat org.elasticsearch.transport.local.LocalTransportChannel$1.run(LocalTransportChannel.java:78)\r\n  2> \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n  2> \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n  2> \tat java.lang.Thread.run(Thread.java:745)\r\n  2> \tLocked synchronizers:\r\n  2> \t- java.util.concurrent.ThreadPoolExecutor$Worker@2339bcc9\r\n  2> \r\n```'
7611,'dadoonet','Wrong class name\nWas QFilterBuilders, should be FilterBuilders.'
7597,'clintongormley','Sorting using SortOrder.ASC - issue with white space in sort-by field?\nWe\'re using 1.3.0 - sorry I can\'t test against 1.3.2 ATM.  We had a test break where we were expecting a set of results to be sorted in ascending order.  Comparing to what Collections.sort(<list>, String.CASE_INSENSITIVE_ORDER) returns, we see an issue when the field has whitespace in it.  It looks like the string "polar Bear" was sorted ignoring the first word "polar".  Is this a known issue or is the collation somehow different between Java 1.7 and Elasticsearch\'s sorting?\r\n\r\n...list was not sorted,\r\nwant: \r\n[0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, badger, Bear, Coyote, Crocodile, duck, Hamster, jaguar, Lynx, Monkey, Ocelot, Octopus, **polar Bear**, Sloth, stingray, tortoise]\r\n got:\r\n[0000panther, 01Skunk, 01squirrel, 22penguin, 33333Tiger, 99Wolf, Aardvark, Administrators, badger, **polar Bear**, Bear, Coyote, Crocodile, duck, Hamster, jaguar, Lynx, Monkey, Ocelot, Octopus, Sloth, stingray, tortoise]: \r\narrays first differed at element [9]; expected:<[]Bear> but was:<[polar ]Bear>\r\n\r\n'
7594,'javanna','Internal: Refactor copy headers mechanism in REST API\nThe functionality of copying headers in the REST layer (from REST requests to transport requests) remains the same. Made it a bit nicer by introducing a ClientFactory component that is a singleton and allows to register useful headers without requiring static methods.\r\n\r\nPlugins just have to inject the ClientFactory now, and call its `addUsefulHeaders` method that is not static anymore.\r\n\r\nRelates to #6513'
7592,'mikemccand','~28k threads on es java client\nHello,\r\n\r\nI run a java client (a real client not just a transport) to send bulk index request to ES.\r\n\r\nAt some point I have around 28k threads blocked with the following stacktraces:\r\n\r\n```\r\n"elasticsearch[Trump][generic][T#62615]" daemon prio=10 tid=0x00007f23f9db7800 nid=0x9b0c waiting on condition [0x00007f1e0f82a000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n    at sun.misc.Unsafe.park(Native Method)\r\n    - parking to wait for  <0x00000006cbd1e030> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n    at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n    at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n    at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n    at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n    at java.util.concurrent.CopyOnWriteArrayList.remove(CopyOnWriteArrayList.java:507)\r\n    at org.elasticsearch.cluster.service.InternalClusterService.remove(InternalClusterService.java:182)\r\n    at org.elasticsearch.action.support.master.TransportMasterNodeOperationAction$3.onTimeout(TransportMasterNodeOperationAction.java:179)\r\n    at org.elasticsearch.cluster.service.InternalClusterService$NotifyTimeout.run(InternalClusterService.java:492)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:744)\r\n```\r\n\r\nI\'ve added this stacktrace in #5152 to underline the consequence of unbounded thread pool.\r\nHowever the real issue might be a deadlock.\r\n\r\nThis only other reference to CopyOnWriteArrayList I found is add:\r\n```\r\n"elasticsearch[Trump][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007f2475927000 nid=0x8a7c waiting on condition [0x00007f242e6e5000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000006cbd1e030> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.CopyOnWriteArrayList.add(CopyOnWriteArrayList.java:416)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$1.run(InternalClusterService.java:217)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n```\r\n\r\nOf course the consequence of this huge number of threads is a slowdown and eventually a crash of the application.\r\n\r\nThis is ES 1.1.1 on java ```Java(TM) SE Runtime Environment (build 1.7.0_51-b13)```'
7590,'javanna','Internal: Deduplicate useful headers that get copied from REST to transport layer\nThe useful headers are now stored into a `Set` instead of an array so we can easily deduplicate them. A set is also returned instead of an array by the `usefulHeaders` static getter.'
7588,'colings86','Aggregations: Adds ability to sort on multiple criteria\nThe terms aggregation can now support sorting on multiple criteria by replacing the sort object with an array or sort object whose order signifies the priority of the sort. The existing syntax for sorting on a single criteria also still works.\r\n\r\nContributes to #6917'
7586,'s1monw','Checksum state files written to disk\nWe write a number of state files to disk, including persistent cluster settings, the state of indices and so forth. In order to detect disk corruption in these files we should also write the checksum of these files to disk. On read we can verify the integrity of these files if a checksum file is present.'
7579,'clintongormley','Not parsing an ISO date when field not inlcuded in _all\nGiven the following mapping:\r\n{\r\n "root": {\r\n  "properties": {\r\n   "prop1": {"type": "date"},\r\n   "prop2": {\r\n    "properties": {\r\n     "prop3": {"type": "date", "include_in_all": false}\r\n    }\r\n   }\r\n  }\r\n }\r\n}\r\nThe following object:\r\n{\r\n "prop1": "2014-09-03T16:48:35",\r\n "prop2": {\r\n  "prop3": "2014-09-02T16:48:35"\r\n }\r\n}\r\nThrows an error while trying to map prop2.prop3 as it tries to parse it as a long. But if I remove the "include_in_all": false part, it parses it perfectly.\r\n\r\nRegards,\r\nMarc'
7576,'bleskes','Resiliency: Fixed race condition in file list\nThe list of files in an active recovery is accessed by multiple threads.\r\nAccess was not synchronized, so incorrect file counts and sizes could be\r\nreported. This commit adds synchronization to the affected collections\r\nand primitives.'
7569,'javanna','Indexed Scripts/Templates: Make sure headers are handed over to internal requests and streamline versioning support\nThe get, put and delete indexed script apis map to get, index and delete api and internally create those corresponding requests. We need to make sure that the original headers are handed over to the new request by passing the original request in the constructor when creating the new one.\r\n\r\nAlso streamlined the support for version and version_type in the REST layer since the parameters were not consistently parsed and set to the internal java API requests. Unified the Rest actions code to make sure that the same parameters are supported in both scripts and templates actions.\r\n\r\nModified the REST delete template and delete script actions to make use of a client instead of using the `ScriptService` directly.\r\n\r\nRemoved injected client from transport actions as script service has already its own, no need to pass another one in.'
7568,'GaelTadh',"Rest api: get indexed script and get template don't return metadata fields\nI think get template and get indexed scripts api should be consistent with the get api and return the same metadata fields when possible. We should at least return `_lang`, `_id` and `_version`, maybe the index name as well."
7567,'GaelTadh','Java api: get indexed script support for preference, realtime and refresh is incomplete\nThe get indexed script supports `preference`, `realtime` and `refresh` parameters. Those parameters can only be set through java api though, never parsed on the REST layer.\r\n\r\nThe set preference is also never used internally as we force the preference to `_local` all the time. Also, I wonder if `realtime` and `refresh` make sense since we always refresh internally after each write (put/delete indexed script).\r\n\r\nMy vote is for removing all three parameters from the java API.'
7566,'mikemccand',"Mapping: Remove pulsing/bloom_pulsing postings format\nIn #7238 it looked like index corruption (checksum errors) but in fact it was simply that the user selected bloom_pulsing postings format, which we don't support yet still allow.\r\n\r\nWe recently removed documentation showing these postings format as a choice, but it's still really dangerous we allow this option at all since it creates unusable indices in ES when we migrate shards and try to check integrity.  Before 1.3, ES didn't check Lucene checksums, so these postings formats worked fine, but with 1.3 any index using pulsing will fail.\r\n\r\nThe pulsing optimization has already been folded into the default postings format for quite a while now.\r\n\r\nI think we should remove them; we are already removing pulsing from Lucene (https://issues.apache.org/jira/browse/LUCENE-5915)"
7565,'colings86','Aggregations: Fixes resize bug in Geo bounds Aggregator\nCloses #7556'
7560,'GaelTadh','Internal: get indexed script holds an always null fetch source context\n`GetIndexedScriptRequest` holds a fetch source context and supports serializing it over the transport, although the field has no setter nor getter.\r\n\r\nQuestion is: do we want to remove it or does it make sense to properly support fetch source context?'
7559,'GaelTadh',"Java api: get indexed script support for routing is incomplete\n`GetIndexedScriptRequest` supports setting a routing value, which never gets serialized over the transport though nor read when converting the request to the internal get one, also the correspoinding write operations `PutIndexedScriptRequest` and `DeleteIndexedScriptRequest` don't support it, thus it makes no sense to support it when reading.\r\n\r\nQuestion is: does it make sense to support routing here or shall we remove the support for it as it never worked?"
7556,'colings86','Aggregations: Geo bounds aggregation throwing ArrayIndexOutOfBoundsException on array resize\nThere is a bug in the collect method of org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator. When it resizes all the arrays at the top of the method it resizes posLefts twice instead of resizing posRights. This causes and ArrayIndexOutOfBoundsException.\r\n'
7554,'clintongormley','Different queryNorm parameters over several shards during one request\nGiven: 5 shards, multi_match query with fuzziness set to `auto`.\r\nMapping has several names for each language(i.e. name_ru, name_en, name_fr etc).\r\n```\r\nquery: { \r\n    multi_match: {\r\n          fields: ["name_*"],\r\n          query: "Марсель",\r\n          fuzziness: :auto,\r\n          operator: :and,\r\n          prefix_length: 3\r\n    } \r\n}, \r\nsort: \'_score\', \r\nsearch_type: \'dfs_query_then_fetch\'\r\n```\r\n\r\nQuery returns several results. Expected that the document with field containing value `Марсель` should get highest score. But highest score gets document with field containig value `Марсала`.\r\n\r\nDigging into explain result gives:\r\nfor `Марсель`:\r\n```\r\n "_shard" : 2,\r\n....(omitted)\r\n"details" : [ {\r\n            "value" : 6.5739636,\r\n            "description" : "weight(name_ru:марсел in 3782) [PerFieldSimilarity], result of:",\r\n            "details" : [ {\r\n              "value" : 6.5739636,\r\n              "description" : "score(doc=3782,freq=1.0 = termFreq=1.0\\n), product of:",\r\n              "details" : [ {\r\n                "value" : 0.6098557,\r\n                "description" : "queryWeight, product of:",\r\n                "details" : [ {\r\n                  "value" : 10.779539,\r\n                  "description" : "idf(docFreq=1, maxDocs=35337)"\r\n                }, {\r\n                  "value" : 0.056575306,\r\n                  "description" : "queryNorm"\r\n                } ]\r\n```\r\n\r\nfor `Марсала`:\r\n\r\n```\r\n "_shard" : 3,\r\n....(omitted)\r\n"details" : [ {\r\n            "value" : 7.622285,\r\n            "description" : "weight(name_ru:марса^0.6 in 3827) [PerFieldSimilarity], result of:",\r\n            "details" : [ {\r\n              "value" : 7.622285,\r\n              "description" : "score(doc=3827,freq=1.0 = termFreq=1.0\\n), product of:",\r\n              "details" : [ {\r\n                "value" : 0.70710677,\r\n                "description" : "queryWeight, product of:",\r\n                "details" : [ {\r\n                  "value" : 0.6,\r\n                  "description" : "boost"\r\n                }, {\r\n                  "value" : 10.779539,\r\n                  "description" : "idf(docFreq=1, maxDocs=35337)"\r\n                }, {\r\n                  "value" : 0.10932854,\r\n                  "description" : "queryNorm"\r\n                } ]\r\n              }, \r\n```\r\n\r\nAll other significant values are almost the same, but the difference within.\r\n\r\nI understand that every shard is an independent Lucene index, but is there any workaround to this issue except using single shard for that? '
7553,'javanna',"Java API: get indexed script shouldn't allow to set the index\n`GetIndexedScriptRequest` currently allows to set the index to the request, although ignored. Also the index gets serialized over the transport although not needed."
7548,'dadoonet','Docs: Is the IndexResponse.matches() method outdated?\nI am checking the Java APIs in http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/index_.html. The page includes the following code snippet:\r\n\r\nIndexResponse response = client.prepareIndex("twitter", "tweet", "1")\r\n        .setSource(json)\r\n        .execute()\r\n        .actionGet();\r\nList<String> matches = response.matches();\r\n\r\nHowever, when checking the source code, I could not find the matches() method in the IndexResponse class (even in its parent classes). I was wondering if the code snippet is outdated.\r\n\r\nThanks in advance,\r\nSeonah '
7546,'dadoonet','Fix incorrect eclipse m2e markdown syntax\n'
7544,'clintongormley','fixed typo in shard query cache reference docs\n'
7539,'clintongormley','Fixes node client section name on Java API Doc\nThe links to the "node client" section on the [client java-api doc](http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/master/client.html) don\'t work due to a minor typo in the references.'
7535,'clintongormley','Fixes a simple typo.\n'
7534,'colings86','Mappers: Better validation of mapping JSON\nThe parsers for the mappers now remove each setting as they parse it and an error will be thrown if any settings are left after parsing is complete\r\n\r\nCloses #7205 '
7528,'markharwood','Add exact-match (ie non-regex) terms to agg Include/Exclude clauses\nThe include/exclude clause used in aggregations like _terms_ agg uses regex syntax to identify terms.\r\nWhile this offers a lot of flexibility for matching, the list of terms supplied by a user is sometimes a fixed set of raw values and it is:\r\na) inconvenient for the user to have to escape these strings into "legal" regex patterns and\r\nb) inefficient to parse and interpret these as regex patterns when a simple hashset would suffice\r\n\r\nThe proposed change is the addition of a "values" array to both _include_ and _exclude_  clauses:\r\n\r\n    "terms" : {\r\n        "field" : "domains.raw", \r\n        "include" : {\r\n            "values":  [ "http://www.foo.com", "http://www.bar.com"]\r\n        }\r\n    }\r\n\r\nThis can be used in conjunction or instead of the existing "pattern" clause in a search. If an include or exclude statement contains a mix of regex ("pattern") and exact ("values") clauses then this would be a logical OR - a match on the regex clause OR the exact value clause would constitute a match.\r\n\r\n@clintongormley you may have some input on this?\r\n\r\n\r\n\r\n'
7525,'clintongormley','Update plugins.asciidoc\nadd search by sql'
7522,'javanna',"Test: Unify the randomization logic for number of shards and replicas\nWe currently have two ways to randomize the number of shards and replicas: random index template, that stays the same for all indices created under the same scope, and the overridable `indexSettings` method, called by `createIndex` and `prepareCreate` which uses a different value for each new index.\r\n\r\nNow that the `randomIndexTemplate` method is not static anymore, we can easily apply the same logic in both cases. Especially for number of replicas, we used to have slightly different behaviours, where more than one replicas were only rarely used through random index template, which gets now applied to the `indexSettings` method too (might speed up the tests a bit).\r\n\r\nSide note: `randomIndexTemplate` had its own logic which didn't depend on `numberOfReplicas` or `maximumNumberOfReplicas`, which was causing bw comp tests failures since in some cases too many copies of the data are requested, which cannot be allocated to older nodes, and the write consistency quorum cannot be met, thus indexing times out."
7516,'clintongormley','Update cardinality-aggregation.asciidoc\n'
7510,'colings86','Geo: [TEST] Adds tests for GeoUtils\nAlso added unit tests for GeoUtils'
7507,'areek','Java API: Improved Suggest Client API\n- Added `SuggestBuilders` (analogous to `QueryBuilders`)\r\n - supporting `term`, `phrase`, `completion` and `fuzzyCompletion` suggestion builders\r\n- Added `suggest(SuggestionBuilder)` to `SuggestRequest`\r\n   - previously only `suggest(BytesReference)` was supported\r\n- Use new `SuggestBuilders` methods in tests, instead of directly instantiating specific suggestion builder.\r\n\r\ncloses #7435'
7506,'brwe','Formula typo in documentation of the DECAY_FUNCTION\nIn this documentation page:\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html\r\n\r\nIn the "Decay Function" section, where the "gauss" function is detailed, it says:\r\n\r\n`sigma^2 = scale^2 / log(decay)`\r\n\r\nThis is incorrect. It\'s actually:\r\n\r\n`sigma^2 = -scale^2 / ( 2 * log(decay) )`\r\n\r\n(minus sign and factor 2 missing)\r\n\r\n'
7505,'martijnvg','Getting incorrect value count using reverse nested aggregation when using more than 1 nested level\n<p>\r\nUsing the following hierarchical data structure\r\n\r\n</p>\r\n\r\n<ul>\r\n   <li>author</li>\r\n   <li><ul>\r\n\t\t\t<li>book</li>\r\n\t\t\t\t<li>\r\n                    <ul>\r\n\t\t\t\t\t    <li>review</li>\r\n\t\t\t\t\t</ul>\r\n       </li>\r\n       </ul>\r\n    </li>\r\n</ul>\r\n<p>\t\r\nI am trying to find number of books by genre given book.publisher and book.review.rating,\r\nbut getting incorrect value count aggregate result.\r\n</p>\r\n<p>\r\n\r\nThis is working correctly if I use only 2 levels (book and review), but when I add author level also\r\nthen it is failing.\r\n</p>\r\n\r\nMapping, data and query used below:\r\n<pre>\r\n<code>\r\n\r\ncurl -XDELETE localhost:9200/authors\r\n\r\ncurl -XPUT  localhost:9200/authors\r\n\r\ncurl -XPUT  localhost:9200/authors/author/_mapping\r\n\'{\r\n    "author": {\r\n      "properties": {\r\n        "author_id": {\r\n          "type": "long"\r\n        },\r\n        "name": {\r\n          "type": "string"\r\n        },\r\n        "book": {\r\n          "type": "nested",\r\n          "properties": {\r\n            "book_id": {\r\n              "type": "long"\r\n            },\r\n            "name": {\r\n              "type": "string"\r\n            },\r\n            "genre": {\r\n              "type": "string"\r\n            },\r\n            "publisher": {\r\n              "type": "string"\r\n            },\r\n            "review": {\r\n              "type": "nested",\r\n              "properties": {\r\n                "rating": {\r\n                  "type": "string"\r\n                },\r\n                "posted_by": {\r\n                  "type": "string"\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\'\r\n  \r\n curl -XPUT localhost:9200/authors/author/0\r\n \'{\r\n  "author_id": "1",\r\n  "name": "a1",\r\n  "book": [\r\n    {\r\n      "book_id": "11",\r\n      "name": "a1-b1",\r\n      "genre": "g1",\r\n      "publisher": "p1",\r\n      "review": [\r\n        {\r\n          "rating": "1s",\r\n          "posted_by": "a"\r\n        },\r\n        {\r\n          "rating": "2s",\r\n          "posted_by": "b"\r\n        },\r\n        {\r\n          "rating": "1s",\r\n          "posted_by": "a"\r\n        }\r\n      ]\r\n    },\r\n    {\r\n      "book_id": "12",\r\n      "name": "a1-b2",\r\n      "genre": "g1",\r\n      "publisher": "p1",\r\n      "review": [\r\n        {\r\n          "rating": "1s",\r\n          "posted_by": "a"\r\n        },\r\n        {\r\n          "rating": "2s",\r\n          "posted_by": "b"\r\n        },\r\n        {\r\n          "rating": "1s",\r\n          "posted_by": "a"\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\'\r\n\r\n\r\nThe book count (book_count) from the following query should be 2 but instead it is 1. \r\nThe output at filter by rating is correct, but the value count isn\'t.\r\n\r\n\r\ncurl -XPOST localhost:9200/authors/_search\r\n\'{\r\n  "size": 0,\r\n  "aggs": {\r\n    "nested_book": {\r\n      "nested": {\r\n        "path": "book"\r\n      },\r\n      "aggregations": {\r\n        "group_by_genre": {\r\n          "terms": {\r\n            "field": "genre"\r\n          },\r\n          "aggregations": {\r\n            "filter_by_publisher": {\r\n              "filter": {\r\n                "bool": {\r\n                  "must": {\r\n                    "term": {\r\n                      "book.publisher": "p1"\r\n                    }\r\n                  }\r\n                }\r\n              },\r\n              "aggregations": {\r\n                "nested_review": {\r\n                  "nested": {\r\n                    "path": "book.review"\r\n                  },\r\n                  "aggregations": {\r\n                    "filter_by_rating": {\r\n                      "filter": {\r\n                        "bool": {\r\n                          "must": {\r\n                            "term": {\r\n                              "book.review.rating": "1s"\r\n                            }\r\n                          }\r\n                        }\r\n                      },\r\n                      "aggregations": {\r\n                        "reverse_to_book": {\r\n                          "reverse_nested": {\r\n                            "path": "book"\r\n                          },\r\n                          "aggregations": {\r\n                            "book_count": {\r\n                              "value_count": {\r\n                                "field": "book_id"\r\n                              }\r\n                            }\r\n                          }\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n</code>\r\n</pre>\r\n'
7504,'clintongormley','Still swapping with mlockall enabled\nRunning an ES 1.0.1 instance with 32GB on a 256GB machine. Am locking memory and\r\n```\r\ncurl -s 0:9200/_nodes/_local/process?pretty\r\n```\r\nsays:\r\n```\r\n      "process" : {\r\n        "refresh_interval" : 1000,\r\n        "id" : 34381,\r\n        "max_file_descriptors" : 500000,\r\n        "mlockall" : true <---\r\n      }\r\n```\r\nSo its really locked. Still\r\n```\r\n$ fgrep Swap /proc/34381/status\r\nVmSwap:\t  394800 kB\r\n```\r\n\r\nIts still swapping.\r\n\r\nThis makes me doubt the \'mockall\' paragraph on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-configuration.html saying:\r\n```\r\nThe third option on Linux/Unix systems only, is to use mlockall to try to lock the process\r\naddress space into RAM, preventing any Elasticsearch memory from being swapped out. \r\n```'
7498,'clintongormley','Update update.asciidoc\n'
7495,'dakrone','Validation of number_of_shards and number_of_replicas request to reject illegal number\nIf create an index the following settings, elasticsearch success to create an index.\r\nHowever, when we create a document, we have error.\r\nThere are servral errors.\r\n\r\n### set 0 to number_of_shards\r\nsetting and create a document\r\n```\r\ncurl -XPUT "http://localhost:9200/hoge" -d\'\r\n{\r\n  "settings": {\r\n    "number_of_shards": 0\r\n  }\r\n}\'\r\n\r\ncurl -XPUT "http://localhost:9200/hoge/fuga/1" -d\'{  "title": "fuga"}\'\r\n```\r\nerorr\r\n```\r\n{\r\n   "error": "ArithmeticException[/ by zero]",\r\n   "status": 500\r\n}\r\n```\r\n\r\n### set -2 to number_of_shards\r\nsetting and create adocument\r\n```\r\ncurl -XPUT "http://localhost:9200/hoge" -d\'\r\n{\r\n  "settings": {\r\n    "number_of_shards": -2\r\n  }\r\n}\'\r\n\r\ncurl -XPUT "http://localhost:9200/hoge/fuga/1" -d\'{  "title": "fuga"}\'\r\n```\r\nerror\r\n```\r\n{\r\n   "error": "IndexShardMissingException[[hoge][0] missing]",\r\n   "status": 404\r\n}\r\n```\r\n\r\n### set -2 to number_of_replicas\r\nsetting and create a document\r\n```\r\ncurl -XPUT "http://localhost:9200/hoge" -d\'\r\n{\r\n  "settings": {\r\n    "number_of_shards": 2,\r\n    "number_of_replicas": -2\r\n  }\r\n}\'\r\n\r\ncurl -XPUT "http://localhost:9200/hoge/fuga/1" -d\'{  "title": "fuga"}\'\r\n```\r\nerror\r\n```\r\n{\r\n   "error": "UnavailableShardsException[[hoge][0] [0] shardIt, [0] active : Timeout waiting for [1m], request: index {[hoge][fuga][1], source[{\\n  \\"title\\": \\"fuga\\"\\n}\\n]}]",\r\n   "status": 503\r\n}\r\n```\r\n\r\nElasticsearch should return error message and should not create an index.\r\n'
7490,'dadoonet','After Upgrade 1.3.1 to 1.3.2 then Can\'t find master node(1.3.1) on AWS EC2 with multicast\nHi \r\nWhen I upgraded Elasticsearch cluster 1.3.1 to 1.3.2, and restarting node by node, \r\nBut when i restarted first node, it could\'t find existing master node, and became a master node by self. So I restart that node several times, but it still couldn\'t find existing master node which is previous version of ES(1.3.1).\r\nSo It parted from the cluster and make new cluster with same name.\r\nAfter all other nodes restarted. all nodes joined new version of ES cluster. \r\n\r\nIt means, when I upgrade Elasticsearch cluster which using multicast on AWS EC2, I should stop the service  and restart all nodes. Is it right?\r\n\r\nMy Env\r\n * AWS EC2 \r\n * 6 data nodes (master is 3 node only)\r\n * multicast\r\n\r\nContent of `/etc/elasticsearch/elasticsearch.yml`:\r\n\r\n```\r\ncluster.name: MyCluster\r\nnode.name: "Node1"\r\nnode.master: true\r\nnode.data: true\r\nnode.zone: eu-west-1a \r\ncluster.routing.allocation.awareness.force.zone.values: eu-west-1a,eu-west-1b,eu-                                                                                                                                                                                               west-1c\r\ncluster.routing.allocation.awareness.attributes: zone\r\n# Node1 - zone:eu-west-1a - master&data <= first restart for upgrade node\r\n# Node2 - zone:eu-west-1a - data only\r\n# Node3 - zone:eu-west-1b - master&data <= current master node 1.3.1\r\n# Node4 - zone:eu-west-1b - data only\r\n# Node5 - zone:eu-west-1c - master&data\r\n# Node6 - zone:eu-west-1c - data only\r\n\r\ndiscovery.zen.ping.multicast.enabled: true\r\ncloud:\r\n    aws:\r\n        access_key: Key\r\n        secret_key: Key\r\n        region: eu-west-1\r\ndiscovery:\r\n    type: ec2\r\n```\r\n\r\nThanks.\r\n\r\nDaniel'
7487,'colings86','StackOverflowError running query script and agg script\nThis query was generated by a benchmarking framework that creates random combinations of clauses and this combo of scripted agg and script_score that references "_score" causes a StackOverflowError\r\n\r\n\tcurl -XPUT "http://localhost:9200/test?pretty=true" -d\'\r\n\t{\r\n\t  "mappings": {\r\n\t\t"car": {\r\n\t\t  "properties": {\r\n\t\t\t"make": {\r\n\t\t\t  "type": "string",\r\n\t\t\t  "index": "not_analyzed"\r\n\t\t\t},\r\n\t\t\t"model": {\r\n\t\t\t  "type": "string",\r\n\t\t\t  "index": "not_analyzed"\r\n\t\t\t},\r\n\t\t\t"mileage": {\r\n\t\t\t  "type": "integer"\r\n\t\t\t}\r\n\t\t  }\r\n\t\t}\r\n\t  }\r\n\t}\'\r\n\tcurl -XPOST "http://localhost:9200/test/car/1?pretty=true" -d\'\r\n\t{\r\n\t  "make": "bmw",\r\n\t  "model": "m3",\r\n\t  "mileage": 30000\r\n\t}\'\t\r\n\tcurl -XPOST "http://localhost:9200/test/car/_search?pretty" -d\'\r\n\t{\r\n\t   "query": {\r\n\t\t  "function_score": {\r\n\t\t\t "query": {\r\n\t\t\t\t"term": {\r\n\t\t\t\t   "model": "m3"\r\n\t\t\t\t}\r\n\t\t\t },\r\n\t\t\t "script_score": {\r\n\t\t\t\t"script": "_score * doc[\\"mileage\\"].value "\r\n\t\t\t },\r\n\t\t\t "boost_mode": "replace"\r\n\t\t  }\r\n\t   },\r\n\t   "aggs": {\r\n\t\t  "makes": {\r\n\t\t\t "terms": {\r\n\t\t\t\t"script": "doc[\\"make\\"].value"\r\n\t\t\t }\r\n\t\t  }\r\n\t   }\r\n\t}\'\t'
7486,'jpountz','Mappings: Make lookup structures immutable.\nThis commit makes the lookup structures that are used for mappings immutable.\r\nWhen changes are required, a new instance is created while the current instance\r\nis left unmodified. This is done efficiently thanks to a hash table\r\nimplementation based on a array hash trie, see\r\norg.elasticsearch.common.collect.CopyOnWriteHashMap.\r\n\r\nManyMappingsBenchmark returns indexing times that are similar to the ones that\r\ncan be observed in current master.\r\n\r\nUltimately, I would like to see if we can make mappings completely immutable as\r\nwell and updated atomically. This is not trivial however, eg. because of dynamic\r\nmappings. So here is a first baby step that should help move towards that\r\ndirection.'
7478,'martijnvg',"Internal: Stuck on java.util.HashMap.get?\nWe seem to have a problem with stuck threads in an Elasticsearch cluster. It appears at random, but once a thread is stuck it seems to keep being stuck until elasticsearch on that node is restarted. The theads get stuck in a busy loop and the stack trace of one is:\r\n```\r\nThread 3744: (state = IN_JAVA)\r\n - java.util.HashMap.getEntry(java.lang.Object) @bci=72, line=446 (Compiled frame; information may be imprecise)\r\n - java.util.HashMap.get(java.lang.Object) @bci=11, line=405 (Compiled frame)\r\n - org.elasticsearch.search.scan.ScanContext$ScanFilter.getDocIdSet(org.apache.lucene.index.AtomicReaderContext, org.apache.lucene.util.Bits) @bci=8, line=156 (Compiled frame)\r\n - org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(org.apache.lucene.index.AtomicReaderContext, org.apache.lucene.util.Bits) @bci=6, line=45 (Compiled frame)\r\n - org.apache.lucene.search.FilteredQuery$1.scorer(org.apache.lucene.index.AtomicReaderContext, boolean, boolean, org.apache.lucene.util.Bits) @bci=34, line=130 (Compiled frame)\r\n - org.apache.lucene.search.IndexSearcher.search(java.util.List, org.apache.lucene.search.Weight, org.apache.lucene.search.Collector) @bci=68, line=618 (Compiled frame)\r\n - org.elasticsearch.search.internal.ContextIndexSearcher.search(java.util.List, org.apache.lucene.search.Weight, org.apache.lucene.search.Collector) @bci=225, line=173 (Compiled frame)\r\n - org.apache.lucene.search.IndexSearcher.search(org.apache.lucene.search.Query, org.apache.lucene.search.Collector) @bci=11, line=309 (Interpreted frame)\r\n - org.elasticsearch.search.scan.ScanContext.execute(org.elasticsearch.search.internal.SearchContext) @bci=54, line=52 (Interpreted frame)\r\n - org.elasticsearch.search.query.QueryPhase.execute(org.elasticsearch.search.internal.SearchContext) @bci=174, line=119 (Compiled frame)\r\n - org.elasticsearch.search.SearchService.executeScan(org.elasticsearch.search.internal.InternalScrollSearchRequest) @bci=49, line=233 (Interpreted frame)\r\n - org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(org.elasticsearch.search.internal.InternalScrollSearchRequest, org.elasticsearch.transport.TransportChannel) @bci=8, line=791 (Interpreted frame)\r\n - org.elasticsearch.search.action.SearchServiceTransportAction$SearchScanScrollTransportHandler.messageReceived(org.elasticsearch.transport.TransportRequest, org.elasticsearch.transport.TransportChannel) @bci=6, line=780 (Interpreted frame)\r\n - org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run() @bci=12, line=270 (Compiled frame)\r\n - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1145 (Compiled frame)\r\n - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615 (Interpreted frame)\r\n - java.lang.Thread.run() @bci=11, line=724 (Interpreted frame)\r\n```\r\n\r\nIt looks very much as the known problem of using the non-synchronized HashMap class in a threaded environment, see (http://stackoverflow.com/questions/17070184/hashmap-stuck-on-get). Unfortunately I'm not familiar enough with the es code to know if this can be the issue.\r\n\r\nThe solution mentioned at the link is to use ConcurrentHashMap instead."
7476,'colings86','[TEST] removed AwaitsFix, added checks to make sure indexed scripts are ...\n...put correctly'
7474,'jpountz',"Internal: Removing useless methods and method parameters from ObjectMapper.java and TypeParsers.java\nWhile working on #7271 I found some methods that were not being used (I searched for the names too, to see if I could find any reflective calls) and some method parameters too.\r\n\r\nTo make it easier to merge #7271 I am submitting this as a side Pull request.\r\n\r\nI've ran all tests and they OK!\r\n\r\nThanks"
7469,'clintongormley','Getting different results while using bool query vs bool query with function score query\nI am trying to add a custom boost to the different should clauses in the bool query, but I am getting different number of results when I use the bool query with 2 should clauses containing 2 simple query string query vs a bool query with 2 should clauses with 2 function score query encapsulating the same simple query string queries. \r\nThe following query returns me 2 results for my data set:\r\n{\r\n  "query" : {\r\n    "filtered" : {\r\n          "query" : {\r\n            "bool" : {\r\n              "should" : [ {\r\n                    "simple_query_string" : {\r\n                      "query" : "128",\r\n                      "fields" : [ "content.name_enu.simple" ]\r\n                    }\r\n                  }, {\r\n                    "simple_query_string" : {\r\n                      "query" : "128",\r\n                      "fields" : [ "content.name_enu.simple_with_numeric" ]\r\n                    }\r\n                  } ]\r\n            }\r\n      },\r\n      "filter" : {\r\n        "bool" : {\r\n          "must" : [ {\r\n            "term" : {\r\n              "securityInfo.securityType" : "open"\r\n            }\r\n          }, {\r\n            "bool" : {\r\n              "must" : [ {\r\n                "term" : {\r\n                  "sourceId.sourceSystem" : "jmeter_007971_numeric"\r\n                }\r\n              }, {\r\n                "term" : {\r\n                  "sourceId.type" : "file"\r\n                }\r\n              } ]\r\n            }\r\n          } ],\r\n          "_cache" : true\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "fields" : [ "elementId", "sourceId.id", "sourceId.type", "sourceId.sourceSystem", "sourceVersion", "content.name_enu" ]\r\n}\r\n\r\nWhere as if I use the following query I get 5 results, same simple query strings but with function scores:\r\n{\r\n  "query" : {\r\n    "filtered" : {\r\n          "query" : {\r\n            "bool" : {\r\n              "should" : [ {\r\n                "function_score" : {\r\n                  "query" : {\r\n                    "simple_query_string" : {\r\n                      "query" : "128",\r\n                      "fields" : [ "content.name_enu.simple" ]\r\n                    }\r\n                  },\r\n                  "boost_factor" : 1.5\r\n                }\r\n              }, {\r\n                "function_score" : {\r\n                  "query" : {\r\n                    "simple_query_string" : {\r\n                      "query" : "128",\r\n                      "fields" : [ "content.name_enu.simple_with_numeric" ]\r\n                    }\r\n                  },\r\n                  "boost_factor" : 2.5\r\n                }\r\n              } ]\r\n            }\r\n      },\r\n      "filter" : {\r\n        "bool" : {\r\n          "must" : [ {\r\n            "term" : {\r\n              "securityInfo.securityType" : "open"\r\n            }\r\n          }, {\r\n            "bool" : {\r\n              "must" : [ {\r\n                "term" : {\r\n                  "sourceId.sourceSystem" : "jmeter_007971_numeric"\r\n                }\r\n              }, {\r\n                "term" : {\r\n                  "sourceId.type" : "file"\r\n                }\r\n              } ]\r\n            }\r\n          } ],\r\n          "_cache" : true\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "fields" : [ "elementId", "sourceId.id", "sourceId.type", "sourceId.sourceSystem", "sourceVersion", "content.name_enu" ]\r\n}\r\n\r\nFrom my understanding of how the should clause works I was expecting both the queries to return 5 results but I am not able to understand why the 1st query returns me 2 results for my data set. The "content.name_enu.simple" uses a simple analyzer, whereas simple_with_numeric uses whitespace tokenizer and lowercase filter'
7466,'rjernst','Add LZF safe encoder in LZFCompressor\nSelecting the safe encoder fixes a 64bit JVM crash on big-endian architectures with\r\nLZF UnsafeChunkEncoderBE.\r\n\r\nExample of such a big-endian architecture is Solaris SPARC 64bit (another one is POWER).\r\n\r\nWithout safe encoder, LZF uses the unsafe encoder, and crashes when\r\nfor example this command is executed\r\n\r\n        PUT /_template/logstash\r\n        {\r\n                "template" : "logstash-*",\r\n                "settings" : {\r\n                        "index.refresh_interval" : "5s"\r\n                },\r\n                "mappings" : {\r\n                        "_default_" : {\r\n                                "_all" : { "enabled" : true },\r\n                                "dynamic_templates" : [ {\r\n                                        "string_fields" : {\r\n                                                "match" : "*",\r\n                                                "match_mapping_type" : "string",\r\n                                                "mapping" : {\r\n                                                        "type" : "string",\r\n                                                        "index" : "analyzed",\r\n                                                        "omit_norms" : true,\r\n                                                        "fields" : {\r\n                                                                "raw" : {\r\n                                                                        "type": "string",\r\n                                                                        "index" : "not_analyzed",\r\n                                                                        "ignore_above" : 256\r\n                                                                }\r\n                                                        }\r\n                                                }\r\n                                        }\r\n                                } ],\r\n                                "properties" : {\r\n                                        "@version": { "type": "string", "index": "not_analyzed" },\r\n                                        "geoip"  : {\r\n                                                "type" : "object",\r\n                                                "dynamic": true,\r\n                                                "path": "full",\r\n                                                "properties" : {\r\n                                                        "location" : { "type" : "geo_point" }\r\n                                                }\r\n                                        }\r\n                                }\r\n                        }\r\n                }\r\n        }\r\n\r\nA crash file is available at https://gist.github.com/jprante/79f4b4c0b9fd83eb1c9b'
7459,'javanna','Java API: Add mapping(Object... source) simplified setter to PutIndexTemplateRequest\nAs we did in https://github.com/elasticsearch/elasticsearch/commit/8919e7e602b9ec6d705fbc8c1e6186a5f92c03e8 for `CreateIndexRequest` and `PutMappingRequest`, add simplified `mapping(Object... source)` to PutIndexTemplateRequest and corresponding builder.'
7457,'javanna','LogConfigurator resolveConfig  also reads .rpmnew or .bak files\nCurrently the LogConfigurator will attempt to read any file that starts with "logging." in the env.configFile() path.\r\n\r\nA common practise is to suffix files with .bak or in case of package managers they often add a suffix if the file has been modified upstream.\r\n\r\nElasticsearch will read those values anyway and that might lead to very confusing logging behaviour.\r\n\r\nI am not sure how to best approach this, either have a whitelist or a backlist of suffixes that are allowed/disallowed ?'
7455,'s1monw','Internal: Wait until engine has started up when acquiring searcher\nThis exception happened after a node restart and a delete-by-query hits the node immediately. Happened on 1.3.1\r\n\r\n```\r\n[2014-08-25 16:34:50,926][ERROR][index.engine.internal    ] [node_name] [2013_09][3] failed to acquire searcher, source delete_by_query\r\njava.lang.NullPointerException\r\n    at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:694)\r\n    at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:653)\r\n    at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:139)\r\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:242)\r\n    at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:221)\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:744)\r\n[2014-08-25 16:34:50,927][WARN ][index.engine.internal    ] [node_name] [2013_09][3] failed engine [deleteByQuery/shard failed on replica]\r\n[2014-08-25 16:35:13,744][WARN ][cluster.action.shard     ] [node_name] [2013_09][3] sending failed shard for [2013_09][3], node[0Y7oOI64Qea6GCaSh3OtLw], [R], s[INITIALIZING], indexUUID [_na_], reason [engine failure, message [deleteByQuery/shard failed on replica][EngineException[[2013_09][3] failed to acquire searcher, source delete_by_query]; nested: NullPointerException; ]]\r\n```'
7454,'javanna','Index templates: Made template filtering generic and extensible\nAdded the ability to register index template filters that are being applied when a new index is created, in order to decide whether a matching index template should be applied or not. The default filter that checks whether the template pattern matches the index name always runs first, additional filters can also be registered so that templates can be filtered out based on custom logic.\r\n\r\nTook the chance to add the handy `source(Object... source)` method to `PutIndexTemplateRequest` and corresponding builder.\r\n\r\nCloses #7459'
7444,'clintongormley','Aggregations: Inconsitent behavior when using field + scripts simultaneously.\nDocumentation states that when field and script settings are both present in an agg, then the script should only be able to work on the _value and should not be able to access anything at the doc[] level. However, this behavior is not consistent. Below is a script with comments to reproduce the inconsistency.\r\n\r\nWe should likely add a validation check wherein if you use field + script together, and your script contains a doc[] expression, that should fail with a clear error that it is not allowed.\r\n\r\nRepro case:\r\n\r\n```\r\ncurl -XDELETE localhost:9200/foo\r\n\r\ncurl -XPUT localhost:9200/foo -d \'\r\n{\r\n  "settings": {\r\n    "index.number_of_shards": 1,\r\n    "index.number_of_replicas": 0\r\n  }\r\n}\'\r\n\r\ncurl -XPOST localhost:9200/foo/doc/1 -d \'{\r\n  "s": 1\r\n}\'\r\n\r\ncurl -XPOST localhost:9200/foo/doc/2 -d \'{\r\n  "s": 2\r\n}\'\r\n\r\ncurl -XPOST localhost:9200/foo/doc/3 -d \'{\r\n  "s": 3\r\n}\'\r\n\r\ncurl -XPOST localhost:9200/foo/_refresh\r\n\r\n#fails as expected in ES 1.2.3\r\n#succeeds in ES 1.3.2\r\ncurl -XPOST "localhost:9200/foo/_search" -d \'\r\n{\r\n  "aggs": {\r\n    "2": {\r\n      "sum": {\r\n        "field": "s",\r\n        "script": "doc[\\"s\\"].value"\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n#succeeds unexpectedly in ES 1.2.3\r\n#succeeds in ES 1.3.2\r\ncurl "localhost:9200/foo/_search?search_type=count&pretty" -d \'\r\n{\r\n  "aggs": {\r\n    "1": {\r\n      "sum": {\r\n        "script": "doc[\\"s\\"].value"\r\n      }\r\n    },\r\n    "2": {\r\n      "sum": {\r\n        "field": "s",\r\n        "script": "doc[\\"s\\"].value"\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```'
7443,'jpountz',"Fielddata: Remove soft/resident caches.\nThese caches have no advantage compared to the default node cache. Additionally,\r\nthe soft cache makes use of soft references which make fielddata loading quite\r\nunpredictable in addition to pushing more pressure on the garbage collector.\r\n\r\nThe `none` cache is still there because of tests. There is no other good\r\nreason to use it.\r\n\r\nLongFieldDataBenchmark has been removed because the refactoring exposed a\r\ncompilation error in this class, which seems to not having been working for a\r\nlong time. In addition it's not as much useful now that we are progressively\r\nmoving more fields to doc values."
7437,'bleskes',"discovery.id.seed doesn't look like its working\nTried using discovery.id.seed and it didn't work as expected."
7435,'areek',"Java API improvement: Factory methods for SuggestionBuilders\nIntroduce a static factory class SuggestionBuilders (like QueryBuilders, FilterBuilders), containing static method to create SuggestionBuilder instances.\r\n\r\nSuggestBuilder class:\r\n- has termSuggestion and phraseSuggestion factory methods \r\n- hasn't  completionSuggestion nor fuzzyCompletionSuggestion\r\n"
7434,'s1monw','Snapshot/Restore: Add BWC layer to .si / segments_N hashing to identify segments accurately\nThis is a followup from #7351 where we added extra validation to `.si` and `segments_n` files. We now use their content as a has when we diff the snapshot against the store and wise versa. In order to not double the size of the snapshot when the hash is not present we should recalculate that "hash" from the blob store instead.'
7433,'markharwood','Suggesters: infinite loop in GeolocationContextMapping\nThe parsing logic has the following loop that never terminates:\r\n\r\n```\r\nwhile (token != Token.END_ARRAY) {\r\n  result.add(GeoUtils.parseGeoPoint(parser).geohash());\r\n}\r\n```\r\n\r\nIt misses a `token = parser.nextToken();` between iterations. '
7431,'jpountz','date_histogram facet float possible overflow\nI am using ELK stack to visualising our monitoring data, yesterday i came across a weird problem: ElasticSearch date_histogram facet returned floating results that look like an overflow ("min" : 4.604480259023595E18).\r\nOur dataflow is : collectd (cpu/memory) -> riemann -> logstash -> elasticsearch <- kibana.\r\nAt first the values were correct, after a few days the values became huge (see attached snapshot of kibana graph)\r\n![image](https://cloud.githubusercontent.com/assets/7490448/4027631/1ce315ba-2c32-11e4-9448-6b14fe147811.png)\r\nthe problem solved at midnight when new index created by logstash.\r\n\r\nfiltered query + Result:\r\nquery:\r\n```json\r\nurl -XGET \'http://localhost:9200/logstash-2014.08.24/_search?pretty\' -d \'{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "subservice.raw:\\"processes-cpu_percent/gauge-collectd\\" AND (plugin_instance:\\"cpu_percent\\")"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "range": {\r\n                "@timestamp": {\r\n                  "from": 1408884312966,\r\n                  "to": 1408884612966\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "range": {\r\n                "@timestamp": {\r\n                  "from": 1408884311948,\r\n                  "to": 1408884327941\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "fquery": {\r\n                "query": {\r\n                  "query_string": {\r\n                    "query": "subservice:(\\"processes-cpu_percent/gauge-collectd\\")"\r\n                  }\r\n                },\r\n                "_cache": false\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "size": 500,\r\n  "sort": [\r\n    {\r\n      "metric": {\r\n        "order": "desc",\r\n        "ignore_unmapped": false\r\n      }\r\n    },\r\n    {\r\n      "@timestamp": {\r\n        "order": "desc",\r\n        "ignore_unmapped": false\r\n      }\r\n    }\r\n  ]\r\n}\'\r\n```\r\nresult:\r\n```json\r\n{\r\n  "took" : 47,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : null,\r\n    "hits" : [ {\r\n      "_index" : "logstash-2014.08.24",\r\n      "_type" : "gauge",\r\n      "_id" : "SlzG8bGJQziU0LMoN7nrbQ",\r\n      "_score" : null,\r\n      "_source":{"host":"host1","service":"instance-2014-08-24T1106/processes-cpu_percent/gauge-collectd","state":null,"description":null,"metric":0.7,"tags":["collectd"],"time":"2014-08-24T12:45:25.000Z","ttl":20.0,"type":"gauge","source":"host1","ds_type":"gauge","plugin_instance":"cpu_percent","ds_name":"value","type_instance":"collectd","plugin":"processes","ds_index":"0","@version":"1","@timestamp":"2014-08-24T12:45:15.079Z"},\r\n      "sort" : [ 4604480259023595110, 1408884325088 ]\r\n\r\n    }, {\r\n\r\n      "_index" : "logstash-2014.08.24",\r\n      "_type" : "gauge",\r\n      "_id" : "8hxToMjpQ5WQIw15DQqIGA",\r\n      "_score" : null,\r\n      "_source":{"host":"host1","service":"instance-2014-08-24T1106/processes-cpu_percent/gauge-collectd","state":null,"description":null,"metric":0.5,"tags":["collectd"],"time":"2014-08-24T12:45:15.000Z","ttl":20.0,"type":"gauge","source":"host1","ds_type":"gauge","plugin_instance":"cpu_percent","ds_name":"value","type_instance":"collectd","plugin":"processes","ds_index":"0","@version":"1","@timestamp":"2014-08-24T12:45:15.079Z"},\r\n      "sort" : [ 4602678819172646912, 1408884315079 ]\r\n    } ]\r\n  }\r\n}\r\n```\r\ndate histogram Facet + Results:\r\nquery:\r\n```json\r\ncurl -XGET \'http://localhost:9200/logstash-2014.08.24/_search?pretty\' -d \'{\r\n  "facets": {\r\n    "0": {\r\n      "date_histogram": {\r\n        "key_field": "@timestamp",\r\n        "value_field": "metric",\r\n        "interval": "1s"\r\n      },\r\n      "global": true,\r\n      "facet_filter": {\r\n        "fquery": {\r\n          "query": {\r\n            "filtered": {\r\n              "query": {\r\n                "query_string": {\r\n                  "query": "subservice.raw:\\"processes-cpu_percent/gauge-collectd\\" AND (plugin_instance:cpu_percent) AND *"\r\n                }\r\n              },\r\n              "filter": {\r\n                "bool": {\r\n                  "must": [\r\n                    {\r\n                      "range": {\r\n                        "@timestamp": {\r\n                          "from": 1408884199622,\r\n                          "to": 1408884499623\r\n                        }\r\n                      }\r\n                    },\r\n                    {\r\n                      "range": {\r\n                        "@timestamp": {\r\n                          "from": 1408884311948,\r\n                          "to": 1408884327941\r\n                        }\r\n                      }\r\n                    },\r\n                    {\r\n                      "fquery": {\r\n                        "query": {\r\n                          "query_string": {\r\n                            "query": "subservice:(\\"processes-cpu_percent/gauge-collectd\\")"\r\n                          }\r\n                        },\r\n                        "_cache": true\r\n                      }\r\n                    }\r\n                  ]\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "size": 0\r\n}\' \r\n```\r\n\r\nresult:\r\n```java\r\n{\r\n  "took" : 24,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1197141,\r\n    "max_score" : 0.0,\r\n    "hits" : [ ]\r\n  },\r\n  "facets" : {\r\n    "0" : {\r\n      "_type" : "date_histogram",\r\n      "entries" : [ {\r\n        "time" : 1408884315000,\r\n        "count" : 1,\r\n        "min" : 4.6026788191726469E18,\r\n        "max" : 4.6026788191726469E18,\r\n        "total" : 4.6026788191726469E18,\r\n        "total_count" : 1,\r\n        "mean" : 4.6026788191726469E18\r\n      }, {\r\n        "time" : 1408884325000,\r\n        "count" : 1,\r\n        "min" : 4.604480259023595E18,\r\n        "max" : 4.604480259023595E18,\r\n        "total" : 4.604480259023595E18,\r\n        "total_count" : 1,\r\n        "mean" : 4.604480259023595E18\r\n      } ]\r\n    }\r\n  }\r\n}\r\n```'
7430,'s1monw',"Internal: Indexes unuseable after upgrade from 0.2 to 1.3 and cluster restart\nWe recently tried to upgrade a ES cluster from 0.2 to 1.3. The actual upgrade worked out fine, but once we restarted the whole cluster, we saw those warnings for all shards (constantly repeating):\r\n\r\n```\r\nIndexShardGatewayRecoveryException[[maki-log-2014-08-21][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: [write.lock, _checksums-1408609875350, _10f.nrm, segments.gen, _17x.nrm, _10f.tis, _17z.si, _17y.fdt, _17y.tis, _17x.fdx, _10f.frq, _17x.fdt, _17y.tii, _17x.prx, _17y.nrm, _10f.fdx, _10f.fnm, _17x.tis, _10f.tii, _10f.fdt, _17z.cfe, _17x.frq, _17x.tii, segments_j, _10f.prx, _17y.fnm, _17y.fdx, _17y.prx, _17y.frq, _17z.cfs, _17x.fnm]]; nested: FileNotFoundException[No such file [_10f.si]]; ]]\r\n[2014-08-21 13:44:15,826][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][3] received shard failed for [maki-log-2014-08-21][3], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\r\n[2014-08-21 13:44:15,841][WARN ][cluster.action.shard     ] [Ghost Dancer] [maki-log-2014-08-21][4] received shard failed for [maki-log-2014-08-21][4], node[QsfMdS40Qve8PukS4er9oA], [P], s[INITIALIZING], indexUUID [_na_], reason [master [Ghost Dancer][QsfMdS40Qve8PukS4er9oA][gboanea-ThinkPad-W520][inet[/10.200.54.63:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\r\n[2014-08-21 13:44:15,844][WARN ][indices.cluster          ] [Ghost Dancer] [maki-log-2014-08-21][1] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] failed to fetch index version after copying it over\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:152)\r\n\tat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [maki-log-2014-08-21][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: [_17o.nrm, _zv.fdx, write.lock, _zv.fdt, segments.gen, _17o.fdx, _17o.tis, _17o.fdt, _zv.fnm, _17p.cfe, _zv.prx, _zv.frq, _17n.prx, _17n.frq, _17o.tii, _17n.nrm, _17n.tii, _17o.frq, _checksums-1408609865048, _17p.cfs, segments_j, _17o.fnm, _17o.prx, _17n.tis, _17n.fdx, _17n.fdt, _zv.tis, _zv.nrm, _17n.fnm, _zv.tii, _17p.si]\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:131)\r\n\t... 4 more\r\nCaused by: java.io.FileNotFoundException: No such file [_zv.si]\r\n\tat org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:173)\r\n\tat org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)\r\n\tat org.elasticsearch.index.store.DistributorDirectory.openInput(DistributorDirectory.java:130)\r\n\tat org.apache.lucene.store.FilterDirectory.openInput(FilterDirectory.java:80)\r\n\tat org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:375)\r\n\tat org.apache.lucene.codecs.lucene3x.Lucene3xSegmentInfoReader.read(Lucene3xSegmentInfoReader.java:103)\r\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:361)\r\n\tat org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:457)\r\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:907)\r\n\tat org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:753)\r\n\tat org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:453)\r\n\tat org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:98)\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)\r\n\t... 4 more..\r\n```\r\nWhen we shutdown the cluster a couple minutes after bringing it up, with the new version, we saw this behavior just for the newest index. After about an hour the behavior would be the same for other indexes after a cluster restart.\r\n\r\nWe found out that the indexes are updated and on shutdown nearly all segment info (*.si) files are deleted (those which have a corresponding marker _upgraded.si). Those si files surviving seemed to be not upgraded (at least they don't have those marker files). And there content is like this or this:\r\n\r\n```\r\n?�l\x17\x13Lucene3xSegmentInfo3.6.21�\r\nos.version\x1d2.6.39-300.17.2.el6uek.x86_64\x02osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40\x06sourceflushos.archamd64\r\n                                                                                                                                  java.versio1.7.0_51\r\n         java.vendor\x12Oracle Corporation\r\n_175.fdt\x10_175_upgraded.s_175.fdx_175.s_175.fn_175.ti_175.ti_175.nr_175.fr_175.prx%\r\n```\r\n\r\n```\r\n1408712122907SegmentInfo\x01\x034.9�\x01 timestamp\r\nos.version\x103.2.0-67-generic\x02osLinuxlucene.version+4.9.0 1604085 - rmuir - 2014-06-20 06:22:23\x06sourceflushos.archamd64\r\n                                                                                                                     java.versio1.7.0_65\r\n                                                                                                                                        java.vendor\x12Oracle Corporation_18n.cf_18n.cfs_18n.si�(����\\%\r\n```\r\n\r\nWhile those updated contain afterwards this kind of information:\r\n\r\n```\r\n?�l\x17\x13Lucene3xSegmentInfo3.6.2�2�\t\r\n                                        mergeFactor\x0210\r\nos.version\x1d2.6.39-300.17.2.el6uek.x86_64\x02osLinuxlucene.version+3.6.2 1423725 - rmuir - 2012-12-18 19:45:40\x06sourcemergeos.archamd64\x13mergeMaxNumSegments\x02-1\r\n             java.versio1.7.0_51\r\n                                java.vendor\x12Oracle Corporation\r\n_1mx.ti_1mx.fr_1mx.pr_1mx.fd_1mx.nr_1mx.fdt_1mx.si\x10_1mx_upgraded.s_1mx.fn_1mx.tis%  \r\n```\r\n\r\nWe could force the same behavior triggering an optimize for a given index. By restarting one node at a time and waiting till it fully integrated into the cluster we were able to restore the deleted si files through other nodes (including the _upgraded.si marker files). Afterwards the si files where safe and didn't got deleted.\r\n\r\nTo me it looks like either ES or Lucene is memorizing to delete the _upgraded.si files on VM shutdown but by accident deletes the actual si files as well."
7429,'jpountz','Aggregations: index-out-of-bounds exception when upgrading cardinality agg to hyperloglog\nReported on the mailing-list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/elasticsearch/Hs9jKg8NLlY/ijSPw2bKL0QJ'
7427,'imotov','5 out of 10 shards are failed in snapshot\nI tried snaptshot api of elasticsearch, its saying "state" : "SUCCESS" but 5 out of 10 shards are done and 5 are failed. Why is that so?\r\n\r\n"reason" : "RepositoryMissingException[] missing]" Checked and repository is exists.\r\n\r\nUsing elasticsearch 1.3.0 on centos 6.3.'
7425,'jpountz',"Aggregations: Encapsulate AggregationBuilder name and make getter public\nAs mentioned in https://groups.google.com/forum/?#!msg/elasticsearch/fe9S2mECMEI/57xZK7i6V7MJ, I'd like the name of an AggregationBuilder to be public. I'm attempting to build a wrapper that will provide some type safety around the type of aggregation in the query and the result and not having access to the name is making the API much clunkier than it would otherwise be."
7423,'dakrone','Circuit breakers should be dynamically updatable\nAccording to the docs: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-fielddata.html#circuit-breaker all circuit breakers should be dynamically updatable, but this isn\'t the case:\r\n\r\n    PUT /_cluster/settings\r\n    {\r\n        "transient" : {\r\n            "indices.fielddata.breaker.limit" : "20%" \r\n        }\r\n    }\r\n\r\nResults in:\r\n\r\n    [WARN ][action.admin.cluster.settings] [Ebon Seeker] ignoring transient setting [indices.fielddata.breaker.limit], not dynamically updateable'
7419,'clintongormley',"Update regexp-syntax.asciidoc\nstring is ’aaabbb'   , then the regexp  aa+bbb+   match the string ."
7413,'javanna','Introduced IndexTemplateFilter\nAdded the ability to register filters on the index templates that are being applied when a new index is created. These filters will be able to filter out templates that should not be applied to newly created index.'
7409,'s1monw',"Test: Use a default host name if localAddress is not available\nI'm trying to do integration testing on an application backed by elasticsearch, and running in to trouble.\r\n\r\nAt this point, the specific test method that is being called is essentially empty; as a stub, it is\r\n\r\n    @ClusterScope(scope=Scope.SUITE, numNodes=1)\r\n    public class EsMappingTest extends ElasticsearchIntegrationTest {\r\n        @Test\r\n        public void testMappingForIndexContainer() {}\r\n    }\r\n\r\nI receive the attached error when attempting to run that test; the strange thing is, it is not reproducible on other machines that we've tried, and continues to happen even when we use the JAR files compiled on those other machines.  All other (non-elasticsearch) tests work perfectly well.\r\n\r\nHere's the error:\r\n\r\nJUnit version 4.10\r\nE14/08/22 09:32:52 ERROR elasticsearch.test: FAILURE  : com.lumiata.lumigraph.datastore.internal.elasticsearch.EsMappingTest\r\nREPRODUCE WITH  : mvn test -Dtests.seed=42012D4D1D344754 -Dtests.class=com.lumiata.lumigraph.datastore.internal.elasticsearch.EsMappingTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles\r\nThrowable:\r\njava.lang.NullPointerException\r\n    __randomizedtesting.SeedInfo.seed([42012D4D1D344754]:0)\r\n    org.elasticsearch.test.TestCluster.clusterName(TestCluster.java:308)\r\n    org.elasticsearch.test.ElasticsearchIntegrationTest.beforeClass(ElasticsearchIntegrationTest.java:178)\r\n    [...sun.*, com.carrotsearch.randomizedtesting.*, java.lang.reflect.*]\r\n    org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n    org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\r\n    [...com.carrotsearch.randomizedtesting.*]\r\n    org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\r\n    org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n    org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\r\n    org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\r\n    [...com.carrotsearch.randomizedtesting.*]\r\n    java.lang.Thread.run(Thread.java:745)\r\n"
7406,'javanna',"Internal: made original indices optional for broadcast delete and delete by query shard requests\nShard requests like broadcast delete and delete by query, that needs to be executed on primary and all replicas, get read and written out to the transport on the same node. That means that if we add some field version checks are not enough to maintain bw comp since a newer node that holds the primary might receive the request from an older node, that didn't provide the field. Yet, when writing the request out again to a newer node that holds the replica, we do try and serialize the field although it's missing. The newer fields just needs to be set to optional in these cases, in addition to the version checks.\r\n\r\nRe-enabled testDeleteByQuery and testDeleteRoutingRequired bw comp tests since this was the cause of their failures."
7402,'martijnvg','Parent/Child: Add support for the field data loading option to the `_parent` field.\nPR for #7394'
7400,'jpountz','DocumentMapper never calls RootMapper.parse\nThis is not an issue in practice since all our root mappers do actual stuff in preParse and postParse, but this might be surprising to someone writing an external root mapper.'
7398,'jpountz','Java API: allow nullable queryBuilder in FilteredQueryBuilder to match rest api\nfix for #7365'
7396,'s1monw',"Test: Use a dedicated port range per test JVM\nFor reliability and debug purposes each test JVM should use it's own\r\nTCP port range if executed in parallel. This also moves away from the\r\ndefault port range to prevent conflicts with running ES instance on the local\r\nmachine."
7394,'martijnvg','Add option to eagerly build global ordinals for parent-child ID cache\nWith a large number of children, building the global ordinals cache can represent a big performance hit on the first query after a refresh.\r\n\r\nAdding eager global ordinals will improve that greatly.'
7392,'mikemccand','Internal: Removed ConcurrentHashMapV8\nBasically reverting  #6400 as requested by @mikemccand\r\n\r\nCloses #7296'
7386,'s1monw','Internal: Upgrade caused shard data to stay on nodes\nUpgrade caused shard data to stay on nodes even after it isn\'t useful any more.\r\n\r\nThis comes from https://groups.google.com/forum/#!topic/elasticsearch/Mn1N0xmjsL8\r\n\r\n\r\nWhat I did:\r\nStarted upgrading from Elasticsearch 1.2.1 to Elasticsearch 1.3.2.  For each of the 6 nodes I updated:\r\n* Set allocation to primaries only\r\n* Sync new plugins into place\r\n* Update deb package\r\n* Restart Elasticsearch\r\n* Wait for Elasticsearch to respond on the local host\r\n* Set allocation to all\r\n* Wait for Elasticsearch to report GREEN\r\n* Sleep for half an hour so the cluster can rebalance itself a bit\r\n\r\nWhat happened:\r\nThe new version of Elasticsearch came up but didn\'t remove all the shard data it can\'t use.  This picture from Whatson shows the problem pretty well:\r\nhttps://wikitech.wikimedia.org/wiki/File:Whatson_out_of_disk.png\r\nThe nodes on the left were upgraded and blue means disk usage by Elasticsearch and brown is "other" disk usage.\r\n\r\nWhen I dig around on the filesystem all the space usage is in the shard storage directory (/var/lib/elasticsearch/production-search-eqiad/nodes/0/indices) but when I compare the list of open files to the list of files on the file system [with this](https://gist.github.com/nik9000/d2dba49c156a5259a7d6) I see that whole directories are just sitting around, unused.  Hitting the ```/_cat/shards/<directory_name>``` corroborates that the shard in the directory isn\'t on the node.  Oddly, if we keep poking around we find open files in directories representing shards that we don\'t expect to be on the node either....\r\n\r\nWhat we\'re doing now:\r\nWe\'re going to try restarting the upgrade and blasting the data directory on the node as we upgrade it.\r\n\r\nReproduction steps:\r\nNo idea.  And I\'m a bit afraid to keep pushing things on our cluster with it in the state that it is in.'
7384,'clintongormley',"Update scripting.asciidoc\nI was confused for a while why I couldn't access an object-valued field in a script using doc[...] until I found the explanation in the Script Fields documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-script-fields.html). I basically just copied the explanation from that page and added it here."
7378,'javanna','Internal: move index templates api back to indices category and make put template and create index implement IndicesRequest\n'
7377,'brwe','Mapping: Report conflict when merging `_all` field mapping and throw exception when doc_values specified\n- _all field was never merged when mapping was updated and no conflict reported\r\n- _all accepted doc_values format although it is always tokenized. this setting was ignored before.\r\n\r\nrelates to #777\r\n\r\nNote that the exception when `doc_values` was configured is new. This might cause problems for users that configured it and upgrade since the old mapping cannot be parsed anymore after this change.'
7373,'colings86','NPE with geo_bounds agg\nI\'ve received a report of an NPE which shows up when using a second geo_bounds agg. I\'ve been unable to reproduce it, but the query looks like this:\r\n\r\n\r\n    {\r\n      "size": 0,\r\n      "sort": [\r\n        {\r\n          "@timestamp": {\r\n            "order": "asc"\r\n          }\r\n        }\r\n      ],\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "range": {\r\n              "@timestamp": {\r\n                "from": "now-1h/h",\r\n                "to": "now+1h/h"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      "aggs": {\r\n        "one_min_interval": {\r\n          "date_histogram": {\r\n            "field": "@timestamp",\r\n            "interval": "1m"\r\n          },\r\n          "aggs": {\r\n            "customer": {\r\n              "terms": {\r\n                "field": "soc_customer",\r\n                "size": 10,\r\n                "collect_mode": "breadth_first"\r\n              },\r\n              "aggs": {\r\n                "destinationCountry": {\r\n                  "terms": {\r\n                    "field": "country_dst.raw",\r\n                    "size": 50,\r\n                    "collect_mode": "breadth_first"\r\n                  },\r\n                  "aggs": {\r\n                    "geo_dst_bounds": {\r\n                      "geo_bounds": {\r\n                        "field": "geo_dst"\r\n                      }\r\n                    },\r\n                    "sourceCountry": {\r\n                      "terms": {\r\n                        "field": "country_src.raw",\r\n                        "size": 200,\r\n                        "collect_mode": "breadth_first"\r\n                      },\r\n                      "aggs": {\r\n                        "geo_src_bounds": {\r\n                          "geo_bounds": {\r\n                            "field": "geo_src"\r\n                          }\r\n                        },\r\n                        "threatCategory": {\r\n                          "terms": {\r\n                            "field": "threat_category",\r\n                            "size": 100\r\n                          }\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n\r\nAnd the stacktrace as follows:\r\n\r\n    [org.elasticsearch.action.search.SearchRequest@174ed99a] lastShard [true]\r\n    java.lang.ArrayIndexOutOfBoundsException: 8\r\n    \tat org.elasticsearch.common.util.BigArrays$DoubleArrayWrapper.get(BigArrays.java:264)\r\n    \tat org.elasticsearch.search.aggregations.metrics.geobounds.GeoBoundsAggregator.collect(GeoBoundsAggregator.java:135)\r\n    \tat org.elasticsearch.search.aggregations.BucketCollector$2.collect(BucketCollector.java:81)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.collect(DeferringBucketCollector.java:94)\r\n    \tat org.elasticsearch.search.aggregations.FilteringBucketCollector.collect(FilteringBucketCollector.java:63)\r\n    \tat org.elasticsearch.search.aggregations.RecordingPerReaderBucketCollector$PerSegmentCollects.replay(RecordingPerReaderBucketCollector.java:108)\r\n    \tat org.elasticsearch.search.aggregations.RecordingPerReaderBucketCollector.replayCollection(RecordingPerReaderBucketCollector.java:153)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.prepareSelectedBuckets(DeferringBucketCollector.java:110)\r\n    \tat org.elasticsearch.search.aggregations.Aggregator.runDeferredCollections(Aggregator.java:263)\r\n    \tat org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:158)\r\n    \tat org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)\r\n    \tat org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)\r\n    \tat org.elasticsearch.search.aggregations.BucketCollector$2.gatherAnalysis(BucketCollector.java:102)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.gatherAnalysis(DeferringBucketCollector.java:104)\r\n    \tat org.elasticsearch.search.aggregations.FilteringBucketCollector.gatherAnalysis(FilteringBucketCollector.java:81)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.gatherAnalysis(DeferringBucketCollector.java:125)\r\n    \tat org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)\r\n    \tat org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:162)\r\n    \tat org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)\r\n    \tat org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector$1.gatherAnalysis(DeferringBucketCollector.java:104)\r\n    \tat org.elasticsearch.search.aggregations.FilteringBucketCollector.gatherAnalysis(FilteringBucketCollector.java:81)\r\n    \tat org.elasticsearch.search.aggregations.bucket.DeferringBucketCollector.gatherAnalysis(DeferringBucketCollector.java:125)\r\n    \tat org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)\r\n    \tat org.elasticsearch.search.aggregations.bucket.terms.GlobalOrdinalsStringTermsAggregator.buildAggregation(GlobalOrdinalsStringTermsAggregator.java:162)\r\n    \tat org.elasticsearch.search.aggregations.Aggregator.gatherAnalysis(Aggregator.java:362)\r\n    \tat org.elasticsearch.search.aggregations.AggregatorFactories$1.gatherAnalysis(AggregatorFactories.java:143)\r\n    \tat org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:122)\r\n    \tat org.elasticsearch.search.aggregations.bucket.histogram.HistogramAggregator.buildAggregation(HistogramAggregator.java:116)\r\n    \tat org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:133)\r\n    \tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:171)\r\n    \tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:261)\r\n    \tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n    \tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n    \tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n    \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    \tat java.lang.Thread.run(Thread.java:745)\r\n'
7371,'javanna','Internal: Make sure that multi_percolate request hands over its context and headers to its corresponding shard requests\n'
7370,'brwe','Test: Write heapdump per default and document -Dtests.heap.size and -Dtests.jvm.argline\n'
7369,'markharwood','Suggesters: Bugs with encoding multiple levels of geo precision\n1) One issue reported by a user is due to the truncation of the geohash string. Added YAML test for this scenario\r\n2) Another suspect piece of code was the “toAutomaton” method that only merged the first of possibly many precisions into the result.\r\n\r\nCloses #7368'
7368,'markharwood','Suggester:  No results returned for certain geo precisions\nAn encoding issue in GeolocationContextMapping means that certain precision levels are being skipped and consequently cannot be queried.\r\nPartial YAML test here: \r\n\r\n\tsetup:\r\n\t  - do:\r\n\t\t  indices.create:\r\n\t\t\t  index: test\r\n\t\t\t  body:\r\n\t\t\t\tmappings:\r\n\t\t\t\t  test:\r\n\t\t\t\t\t"properties":\r\n\t\t\t\t\t  "suggest_geo_multi_level":\r\n\t\t\t\t\t\t "type" : "completion"\r\n\t\t\t\t\t\t "context":\r\n\t\t\t\t\t\t\t"location":\r\n\t\t\t\t\t\t\t\t"type" : "geo"\r\n\t\t\t\t\t\t\t\t"precision" : [1,2,3,4,5,6,7,8,9,10,11,12]\r\n\t  - do:\r\n\t\t  index:\r\n\t\t\tindex: test\r\n\t\t\ttype:  test\r\n\t\t\tid:    1\r\n\t\t\tbody:\r\n\t\t\t  suggest_geo_multi_level:\r\n\t\t\t\tinput: "Hotel Marriot in Amsterdam"\r\n\t\t\t\tcontext:\r\n\t\t\t\t  location:\r\n\t\t\t\t\tlat : 52.22\r\n\t\t\t\t\tlon : 4.53\r\n\r\nThis call works:\r\n\r\n\t  - do:\r\n\t\t  suggest:\r\n\t\t\tindex: test\r\n\t\t\tbody:\r\n\t\t\t  result:\r\n\t\t\t\ttext: "hote"\r\n\t\t\t\tcompletion:\r\n\t\t\t\t  field: suggest_geo_multi_level\r\n\t\t\t\t  context:\r\n\t\t\t\t\tlocation:\r\n\t\t\t\t\t  lat : 52.22\r\n\t\t\t\t\t  lon : 4.53\r\n\t\t\t\t\t  precision : 3                  \r\n\t  - length: { result: 1  }\r\n\r\nbut a precision length of 4 does not. In fact precisions 1,2,3 and 12 work and all others fail.\r\nSo there are gaps in the encoding of the data.\r\nThe reason is that the encoding logic is given precisions in this order:  [12, 3, 10, 6, 2, 1, 7, 11, 9, 5, 4, 8]\r\nand the encoding logic mistakenly truncates the "geohash" string while in this loop:\r\n\r\n        for (String geohash : geohashes) {\r\n            for (int p : mapping.precision) {\r\n                int precision = Math.min(p, geohash.length());\r\n                geohash = geohash.substring(0, precision);\r\n                if(mapping.neighbors) {\r\n                    GeoHashUtils.addNeighbors(geohash, precision, locations);\r\n                }\r\n                locations.add(geohash);\r\n            }\r\n        }\r\n\r\nThe required fix is to not change the "geohash" string value in the inner loop which ensures all precisions are then encoded correctly.'
7362,'martijnvg','Parent/Child: If _parent field points to a non existing parent type, then skip the has_parent query/filter\nPR for #7349'
7360,'rmuir','Resiliency: Verify checksums on merge\nThis just exposes the current lucene option `checkIntegrityAtMerge` from LiveIndexWriterConfig. When enabled, all parts of the index are verified before merging.'
7359,'clintongormley',"Index API docs: incomplete refresh description\nIt seems that refresh parameter can cause refresh of relevant shard only (as for get and delete operations). But documentation doesn't contain this information. "
7353,'areek','NRTSuggester: Support near real-time deleted document filtering for suggestions\n**NOTE:** This is a PR against `feature/nrt_suggester` and not the `master`!\r\n\r\n**Idea:**\r\n  - encode lucene docids to corresponding FST outputs\r\n  - make sure no surface form is lost even though there may be exact duplicates\r\n  - use encoded docids to perform near real-time deleted doc filtering\r\n\r\n**Implementation & Considerations:**\r\n  - Currently the FST BytesRef Output is in the format:\r\n     `surface_form` + `PAYLOAD_SEP` + `payload` + `PAYLOAD_SEP` + `docID`\r\n  - Duplicate surface forms are stored uniquely with dedup bytes. (uses the `END_BYTE` before the dedup bytes, maybe we can support `exact_first` in the future)\r\n  - `maxAnalyzedPathsForOneInput` is now dynamically set by taking into account the # of surfaces per analyzed form at build time (to deepen the `TopNSearcher` queue for deduplication of same output form)\r\n  - `TopNSearcher` queue size is dynamically changed using a heuristic based on the cardinality of deleted docs\r\n  - Minor refactoring\r\n  - Adds Suggester benchmarks\r\n\r\n**Benchmarks:**\r\n```\r\n-- Build time\r\n  AnalyzingSuggester input: 3472, time[ms]: 255 [+- 9.08]\r\n  XAnalyzingSuggester input: 3472, time[ms]: 412 [+- 11.78]\r\n  XNRTSuggester   input: 3472, time[ms]: 405 [+- 9.85]\r\n\r\n-- RAM consumption\r\n  AnalyzingSuggester size[B]:      807,840\r\n  XAnalyzingSuggester size[B]:      758,480\r\n  XNRTSuggester   size[B]:      770,448\r\n\r\n-- Lookup performance (prefixes: 100-200, num: 7)\r\n  AnalyzingSuggester queries: 3472, time[ms]: 125 [+- 3.41], ~kQPS: 28\r\n  XAnalyzingSuggester queries: 3472, time[ms]: 165 [+- 5.77], ~kQPS: 21\r\n  XNRTSuggester   queries: 3472, time[ms]: 170 [+- 3.50], ~kQPS: 20\r\n\r\n-- Lookup performance (prefixes: 6-9, num: 7)\r\n  AnalyzingSuggester queries: 3472, time[ms]: 109 [+- 3.93], ~kQPS: 32\r\n  XAnalyzingSuggester queries: 3472, time[ms]: 94 [+- 4.75], ~kQPS: 37\r\n  XNRTSuggester   queries: 3472, time[ms]: 107 [+- 5.55], ~kQPS: 32\r\n\r\n-- Lookup performance (prefixes: 2-4, num: 7)\r\n  AnalyzingSuggester queries: 3472, time[ms]: 178 [+- 6.05], ~kQPS: 20\r\n  XAnalyzingSuggester queries: 3472, time[ms]: 151 [+- 4.59], ~kQPS: 23\r\n  XNRTSuggester   queries: 3472, time[ms]: 173 [+- 7.49], ~kQPS: 20\r\n\r\n-- NRT Lookup performance with deleted doc filtering\r\n  -- prefixes: 100-200, num: 7\r\n   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 165 [+- 7.11], ~kQPS: 21\r\n   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 165 [+- 5.80], ~kQPS: 21\r\n   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 166 [+- 7.01], ~kQPS: 21\r\n   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 167 [+- 5.39], ~kQPS: 21\r\n   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 167 [+- 5.59], ~kQPS: 21\r\n  -- prefixes: 6-9, num: 7\r\n   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 105 [+- 3.79], ~kQPS: 33\r\n   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 114 [+- 5.90], ~kQPS: 31\r\n   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 135 [+- 14.80], ~kQPS: 26\r\n   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 137 [+- 4.74], ~kQPS: 25\r\n   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 166 [+- 5.32], ~kQPS: 21\r\n  -- prefixes: 2-4, num: 7\r\n   [ 0% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 170 [+- 4.01], ~kQPS: 20\r\n   [20% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 192 [+- 5.98], ~kQPS: 18\r\n   [40% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 219 [+- 6.19], ~kQPS: 16\r\n   [60% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 296 [+- 12.09], ~kQPS: 12\r\n   [80% deleted docs] XNRTSuggester   queries: 3472, time[ms]: 559 [+- 11.32], ~kQPS: 6\r\n```\r\ncloses #7133'
7349,'martijnvg','[ES 1.3.2] NullPointerException while parsing hasParent query/filter\nHi, \r\nI met a strange problem with the latest version of Elasticsearch (1.3.2) - strage as always when NPE occurs :-)\r\nNoticed that Elasticsearch 0.90 did not have such an issue.\r\n\r\nHaving single incorrect type which references to inexisting parent results in NPE while executing hasParent query/filter on another - correct - type.\r\n\r\nTo reproduce the issue please refer to description below.\r\n\r\nCreate index:\r\n```\r\nPOST /test\r\n```\r\n\r\nCorrect mapping:\r\n```\r\nPUT /test/children/_mapping\r\n{\r\n\t"children": {\r\n\t\t"_parent": {\r\n\t\t\t"type": "parents"\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nMapping for type with missing parent type:\r\n```\r\nPUT /test/children2/_mapping\r\n{\r\n\t"children2": {\r\n\t\t"_parent": {\r\n\t\t\t"type": "parents2"\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nAdd something to parents (corrent one) to create mapping:\r\n```\r\nPOST /test/parents\r\n{\r\n\t"someField" : "someValue"\r\n}\r\n```\r\n```\r\nPOST /test/children/_search\r\n{\r\n    "filter": {\r\n    \t\t"has_parent": {\r\n\t\t\t   "type": "parents",\r\n\t\t\t   "query": {\r\n    \t\t    "query_string": {\r\n    \t\t       "query": "*"\r\n    \t\t    }   \r\n\t\t\t   }\r\n\t\t\t}\r\n\t}\r\n}\r\n```\r\nAbove query is gonna fail with NullPointerException without possiblity to catch a real problem (debug helps here :)),\r\n```\r\norg.elasticsearch.search.SearchParseException: [test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n    "filter": {\r\n                "has_parent": {\r\n                           "type": "parents",\r\n                           "query": {\r\n                    "query_string": {\r\n                       "query": "*"\r\n                    }\r\n                           }\r\n                        }\r\n        }\r\n}\r\n]]\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:664)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:515)\r\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:487)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:256)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:158)\r\n        at org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:290)\r\n        at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:271)\r\n        at org.elasticsearch.index.query.IndexQueryParserService.parseInnerFilter(IndexQueryParserService.java:282)\r\n        at org.elasticsearch.search.query.PostFilterParseElement.parse(PostFilterParseElement.java:33)\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:648)\r\n        ... 9 more\r\n```\r\n\r\nBtw. Query on incorrect type (children2) fails fine since it throws:\r\n```\r\n[test] [has_parent] filter configured \'parent_type\' [parents2] is not a valid type];\r\n```'
7348,'clintongormley','Search result changed since 1.24 (current 1.3.2)\nHeya,\r\n\r\nin ES 1.2.4 i do something like this:\r\n```bash\r\ncurl -XPUT \'http://localhost:9200/twitter/user/kimchy\' -d \'{ "name_test" : "Shay Banon" }\'\r\ncurl -XPUT \'http://localhost:9200/twitter/user/foo\' -d \'{ "name_test" : "" }\'\r\ncurl -XPOST \'http://localhost:9200/_search\' -d \'{"query":{"filtered":{"filter":{"and":{"filters":[{"missing":{"field": "name_test"}}]}}}}}\'\r\n```\r\n\r\nFilter in "nice view"\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "and": {\r\n          "filters": [\r\n            {\r\n              "missing": {\r\n                "field": "name_test"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis one gives 1 hit.\r\n\r\nIn Version 1.3.2 i do the same stuff, but got 0 hits.\r\n\r\nAm I missing something? (I read the changelog, but didnt find something that could possible do this...) \r\n\r\nThat happens, as i may suggest, when the field name got an underscore.\r\nTested it with "name" then it will also give one hit.\r\n\r\nThanks for watchin.\r\n\r\nDominik'
7347,'brwe','Query DSL: Empty bool {} should return match_all\nThis also fixes has_parent filters with a nested empty bool filter\r\n(see test SimpleChildQuerySearchTests#test6722, the test should actually expect\r\neither 0 results when searching for has_parent "test" or one result when\r\nsearch for has_parent "foo")\r\n\r\ncloses #7240'
7344,'imotov',"random FileNotFoundExceptions when performing a snapshot\ni had a 1.2.2 cluster of 6 nodes with an nfs folder on one of the servers shared between all of them\r\nwhen trying to do a snapshot i'm getting exceptions like\r\n```\r\nCreateSnapshotResponse[snapshotInfo=SnapshotInfo[name=2014-08-11-16-31-04,state=PARTIAL,reason=<null>,indices=Object[][{my_idx3,my_idx2}],\r\nstartTime=1407774870154,endTime=1407775114709,totalShards=17,successfulShards=14,\r\nshardFailures=Object[][{\r\n[my_idx2][4] failed, reason [IndexShardSnapshotFailedException[[my_idx2][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx2/4/__0 (No such file or directory)]; ],\r\n[my_idx2][3] failed, reason [IndexShardSnapshotFailedException[[my_idx2][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx2/3/__0 (No such file or directory)]; ],\r\n[my_idx3][0] failed, reason [IndexShardSnapshotFailedException[[my_idx3][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/0/__0 (No such file or directory)]; ]}]],\r\nheaders=<null>,remoteAddress=inet[masternode/redacted:9300]]\r\n```\r\nthis only happens for some shards, others are snapshotted fine and i was able to restore them succesfully\r\nafter upgrade to 1.3.1 one of the indexes did not exhibit the problem anymore but the other one continued\r\nsince the upgrade i've created a few more indexes and now i'm getting same errors for them:\r\n```\r\nCreateSnapshotResponse[snapshotInfo=SnapshotInfo[name=2014-08-19-00-57-20,state=PARTIAL,reason=<null>,indices=Object[][{my_idx3,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7,my_idx_redacted_7}],startTime=1408410040861,endTime=1408410112552,totalShards=96,successfulShards=64,shardFailures=Object[][\r\n{[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][1] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][1] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/1/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][7] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/7/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][1] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][1] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/1/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][7] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/7/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__2 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][10] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][10] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/10/__2 (No such file or directory)]; ],\r\n[my_idx_redacted_7][9] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][9] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/9/__0 (No such file or directory)]; ]\r\n,[my_idx_redacted_7][3] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/3/__1 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][4] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/4/__2 (No such file or directory)]; ],\r\n[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][3] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][3] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/3/__1 (No such file or directory)]; ],\r\n[my_idx_redacted_7][4] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][4] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/4/__3 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__2 (No such file or directory)]; ],\r\n[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__1 (No such file or directory)]; ],\r\n[my_idx_redacted_7][10] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][10] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/10/__0 (No such file or directory)]; ],\r\n[my_idx3][7] failed, reason [IndexShardSnapshotFailedException[[my_idx3][7] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/7/__0 (No such file or directory)]; ],\r\n[my_idx3][0] failed, reason [IndexShardSnapshotFailedException[[my_idx3][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx3/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][2] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][2] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/2/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][0] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][0] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/0/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][6] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][6] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/6/__0 (No such file or directory)]; ],\r\n[my_idx_redacted_7][8] failed, reason [IndexShardSnapshotFailedException[[my_idx_redacted_7][8] Failed to perform snapshot (index files)]; nested: FileNotFoundException[/home/shared_dir/indices/my_idx_redacted_7/8/__0 (No such file or directory)]; ]}]],\r\nheaders=<null>,remoteAddress=inet[redacted/redacted:9300]]\r\n```"
7342,'spinscale','HttpComponents Client 4.3.5 GA Released\nhttp://mail-archives.apache.org/mod_mbox/www-announce/201408.mbox/%3C1407690982.9788.2.camel%40ubuntu%3E\r\n\r\nhttp://mail-archives.apache.org/mod_mbox/www-announce/201408.mbox/CVE-2014-3577'
7340,'colings86','Geo: Fixes BoundingBox across complete longitudinal range\nAdds a special case to the GeoBoundingBoxFilterParser so that the left of the box is not normalised int he case where left = -180 and right = 180.  Before this change the left would be normalised to 180 in this case and the filter would only match points with a longitude of 180 (or -180).\r\n\r\nCloses #5218'
7338,'colings86','Geo: Fix circle radius calculation\nThis change fixes the creation circle shapes o it calculates it correctly instead of essentially using the diameter as the radius.  The radius has to be converted into degrees but calculating the ratio of the desired radius to the circumference of the earth and then multiplying it by 360 (number of degrees around the earths circumference).  This issue here was that it was only multiplied by 180 making the result out by a factor of 2.  Also made the test for circles actually check to make sure it has the correct centre and radius.\r\n\r\nCloses #7301'
7333,'javanna','Internal: Get request while percolating existing documents to keep around headers and context of the original percolate request\n'
7331,'javanna','Internal: Auto create index to keep around headers and context of the request that caused it\n'
7319,'javanna',"Internal: Make sure that all shard level requests hold the original indices\nA request that relates to indices (`IndicesRequest` or `CompositeIndicesRequest`) might be converted to some other internal request(s) (e.g. shard level request) that get distributed over the cluster. Those requests contain the concrete index they refer to, but it is not known which indices (or aliases or expressions) the original request related to.\r\n\r\nThis commit makes sure that the original indices are available as part of the shard level requests and makes them implement `IndicesRequest` as well.\r\n\r\nAlso every internal request should be created passing in the original request, so that the original headers, together with the eventual original indices and options, get copied to it. Corrected some places where this information was lost.\r\n\r\nNOTE: As for the bulk api and other multi items api (e.g. multi_get), their shard level requests won't keep around the whole set of original indices, but only the ones that related to the bulk items sent to each shard, the important bit is that we keep the original names though, not only the concrete ones."
7318,'mikemccand','Internal: Management thread pool should reject requests when there are too many\nToday, the management thread pool (used by stats and cats) is bounded to 5, but it still accepts further requests, and then waits indefinitely for a thread to free up.\r\n\r\nThis is dangerous because node stats can be a somewhat costly operation (in proportion to number of shards on the node)....\r\n\r\nAnd it confounds debugging, because it can cause loooong hangs in e.g. node stats requests via browser/curl, and it also is not graceful for recovering from "too many management requests" overload.\r\n\r\nIf we instead rejected the request it would make it clearer which clients are causing too much load.\r\n'
7315,'imotov',"Recovery files left behind when replica building fails\nWe have a situation where several indices need replicas either relocated or rebuilt (we're not sure exactly which of the two caused this situation, but I think it was the initial replica build, rather than relocation).\r\n\r\nIn one situation, the nodes which we were trying to send the shards to went over their high disk threshold, and the recovery was aborted.\r\nIn another, we tickled the recently found bug on recovery and compression.\r\n\r\nIn both cases (afaict), the shard directory on disk was littered with files named `recovery.*`. Sometimes terabytes of files.\r\nEven when the replica build cancelled, moved on to another host, etc, those files aren't being cleaned up."
7313,'colings86','Aggregations: Fixes pre and post offset serialisation for histogram aggs\nChanges the serialisation of pre and post offset to use Long instead of VLong so that negative values are supported.  This actually only showed up in the case where minDocCount=0 as the rounding is only serialised in this case.\r\n\r\nCloses #7312'
7312,'colings86','Aggregations: DateHistogram with negative \'pre_offset\' or \'post_offset\' value ends with "Message not fully read (response) for"\nHey guys,\r\n\r\nWe\'re using version 1.3.2 and Oracle JRE: 1.7.0_67. The cluster has 3 nodes (2 data nodes and 1 used only to route searches).\r\n\r\nThere is a separate (different named) cluster of 1 node using marvel to collect statistics from the first one. No other nodes in the network, and no custom/manually made java clients around.\r\n\r\nAs the title says, when doing a DateHistogramAggregation using a negative value for either **pre_offset** or **post_offset** yields the message:\r\n\r\n" [transport.netty          ]  Message not fully read (response) for [69208] handler org.elasticsearch.search.action.SearchServiceTransportAction$6@f58070b, error [false], resetting" in the logs.\r\n\r\nNote that every other query works flawlessly. This is only reproduceable when using the arguments *pre_offset* and/or *post_offset* in a DateHistogram with a negative value and with more than 1 node in the cluster.\r\n\r\nFacts so far:\r\n * Gist with the output of [_nodes?jvm=true&pretty](https://gist.github.com/marcelog/010c0bdb1c6bf9b664f1).\r\n * Gist with [sample query](https://gist.github.com/marcelog/d96f5ad06944da1231d7)\r\n * **Same query**, but **without negative values** in *pre_offset* and *post_offset* **works** (it doesn\'t return the results we expect, of course, but no errors/warnings are shown in the logs)\r\n * This [previous issue](https://github.com/elasticsearch/elasticsearch/issues/5178) doesn\'t seem to be related, there are no other nodes in the network and there are no "custom" java clients around either. \r\n * The sample query **works when there is only one node in the cluster** and we start to get these error messages when adding more nodes (2 will suffice to reproduce the issue).\r\n\r\nAny ideas? In the meantime, we solved the issue by using *pre_zone* and *post_zone* instead of *pre_offset* and *post_offset*. Negative values are ok, and everything is running smoothly with those (we tried index, search, and snapshot operations).  No error messages in the logs.\r\n\r\nThanks in advance,\r\n'
7307,'colings86','Mapping: Fixes using nested doc array with strict mapping\nCloses #7304'
7306,'mikemccand',"Internal: DistributorDirectory should not invoke distributor when reading an existing file\nI noticed some hot threads doing this while computing node stats:\r\n\r\n```\r\njava.io.UnixFileSystem.getSpace(Native Method)\r\n       java.io.File.getUsableSpace(File.java:1862)\r\n       org.elasticsearch.index.store.distributor.AbstractDistributor.getUsableSpace(AbstractDistributor.java:60)\r\n       org.elasticsearch.index.store.distributor.LeastUsedDistributor.doAny(LeastUsedDistributor.java:45)\r\n       org.elasticsearch.index.store.distributor.AbstractDistributor.any(AbstractDistributor.java:52)\r\n       org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:176)\r\n       org.elasticsearch.index.store.DistributorDirectory.getDirectory(DistributorDirectory.java:144)\r\n       org.elasticsearch.index.store.DistributorDirectory.fileLength(DistributorDirectory.java:113)\r\n       org.apache.lucene.store.FilterDirectory.fileLength(FilterDirectory.java:63)\r\n       org.elasticsearch.common.lucene.Directories.estimateSize(Directories.java:43)\r\n       org.elasticsearch.index.store.Store.stats(Store.java:174)\r\n       org.elasticsearch.index.shard.service.InternalIndexShard.storeStats(InternalIndexShard.java:524)\r\n       org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:130)\r\n       org.elasticsearch.action.admin.indices.stats.ShardStats.<init>(ShardStats.java:49)\r\n       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:195)\r\n       org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)\r\n       org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:338)\r\n       org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:324)\r\n       org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:275)\r\n       java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n       java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n       java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\nWhich is odd because why would we invoke the least_used distributor when checking fileLength (or opening for read, in other cases) an already-existing file?  Seems like we should only check this when writing a new file.\r\n\r\nLooking at line 176 of 1.x of DistributorDirectory.java, it looks like we do this to simplify concurrency (so we can use CHM.putIfAbsent), but I think we should fix this code to only invoke the distributor when it's writing a new file?"
7304,'colings86','Mapping: First index of nested value as an array fails when dynamic is strict\nThis works on 1.3.1, fails in master:\r\n\r\n    DELETE /myapp\r\n    PUT /myapp\r\n    {\r\n       "mappings" : {\r\n          "multiuser" : {\r\n             "properties" : {\r\n                "timestamp" : {\r\n                   "type" : "date"\r\n                },\r\n                "entry" : {\r\n                   "properties" : {\r\n                      "last" : {\r\n                         "type" : "string"\r\n                      },\r\n                      "first" : {\r\n                         "type" : "string"\r\n                      }\r\n                   },\r\n                   "dynamic" : "strict",\r\n                   "type" : "nested"\r\n                }\r\n             },\r\n             "_timestamp" : {\r\n                "path" : "timestamp",\r\n                "enabled" : 1\r\n             },\r\n             "numeric_detection" : 1,\r\n             "dynamic" : "strict"\r\n          }\r\n       },\r\n       "settings" : {}\r\n    }\r\n    \r\n    POST /myapp/multiuser?op_type=create\r\n    {\r\n       "timestamp" : 1408198082386,\r\n       "entry" : [\r\n          {\r\n             "first" : "john",\r\n             "last" : "smith"\r\n          }\r\n       ]\r\n    }\r\n\r\nThis throws: StrictDynamicMappingException[mapping set to strict, dynamic introduction of [entry] within [multiuser] is not allowed]\r\n\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:604)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:489)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:533)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:482)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:384)\r\n\tat org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:193)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:431)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)'
7303,'uboness','Internal: Refactored TransportMessage context\nRemoved CHM in favour of an OpenHashMap and synchronized accessor/mutator methods. Also, the context is now lazily inititialied (just like we do with the headers)'
7301,'colings86','Geo: Geo-shape circles using `radius` as diameter\n\r\n    PUT /attractions\r\n    {\r\n      "mappings": {\r\n        "landmark": {\r\n          "properties": {\r\n            "name": {\r\n              "type": "string"\r\n            },\r\n            "location": {\r\n              "type": "geo_shape"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\n    PUT /attractions/landmark/dam_square\r\n    {\r\n        "name" : "Dam Square, Amsterdam",\r\n        "location" : {\r\n            "type" : "polygon", \r\n            "coordinates" : [[ \r\n              [ 4.89218, 52.37356 ], \r\n              [ 4.89205, 52.37276 ], \r\n              [ 4.89301, 52.37274 ], \r\n              [ 4.89392, 52.37250 ], \r\n              [ 4.89431, 52.37287 ], \r\n              [ 4.89331, 52.37346 ], \r\n              [ 4.89305, 52.37326 ], \r\n              [ 4.89218, 52.37356 ]\r\n            ]]\r\n        }\r\n    }\r\n    \r\nThis point is less than 700m from the above shape, but the search only matches if you set the radius to 1.4km, ie twice the distance:\r\n\r\n    GET /attractions/landmark/_search\r\n    {\r\n      "query": {\r\n        "geo_shape": {\r\n          "location": {\r\n            "shape": {\r\n              "type": "circle",\r\n              "coordinates": [\r\n                4.89994,\r\n                52.37815\r\n              ],\r\n              "radius": "1.4km"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\nI\'ve tried the same thing at much bigger distances and it exhibits the same problem. The radius needs to be double the distance in order to overlap, which makes me think that it is being used as a diameter instead.\r\n'
7298,'spinscale','Support querying more indices than 4096 bytes-worth\nBackground: Most logstash users use the default "logstash-YYYY.MM.dd" index naming scheme. Some use hourly. The main way to access Elasticsearch in this use case is often Kibana.\r\n\r\nIn Elasticsearch 1.3.1 (and older, probably), aborts a query if the request line itself is longer than 4096 bytes, which limites the default logstash use case to about 195 days of querying (each day is 20 bytes of index name, plus comma, 4096 / 21 = 195.04;ignoring remainder of request line)\r\nFor hourly partitions, this limits you to 170 indices (7 days of data).\r\n\r\nIt would be lovely if users could still use these partitioning schemes and query more than 195 days (or 7 days, for hourly indexes) of data in a single query.\r\n\r\nRelated: https://github.com/elasticsearch/kibana/issues/1406'
7296,'mikemccand',"Core: remove ConcurrentHashMapV8\nIn #6400 we pulled in a snapshot of CHM V8 to reduce RAM overhead, notable for the live version map.\r\n\r\nBut I think this is actually quite risky going forward .. the implementation in the OpenJDK has already changed quite a bit since Doug Lea's version, and it's risky if we don't pull in bug fixes.\r\n\r\nI think we should just revert back to the JVM's implementation?  Users can upgrade to Java 8 to reduce RAM usage ... I think this means we need to conditionalize the RAM usage logic in LiveVersionMap."
7294,'javanna','Java API: Some PercolateRequest "setters" allow for method chaining, some don\'t\nCloses #7294'
7293,'brwe','_ttl: enabled: false has no effect\n(Updated - my first examples were wrong)\r\n\r\nIn the following example the document will be deleted:\r\n\r\n```\r\nDELETE testidx\r\nPUT testidx\r\nPUT testidx/doc/_mapping\r\n{\r\n  "_ttl":{\r\n    "enabled": "true"\r\n  }\r\n}\r\n\r\nGET testidx/_mapping\r\n\r\n\r\nPUT testidx/doc/_mapping\r\n{\r\n  "_ttl":{\r\n    "enabled": "false"\r\n  }\r\n}\r\n# _ttl is enabled anyway\r\nGET testidx/_mapping\r\nPOST testidx/doc/1\r\n{\r\n  "text":"foo",\r\n  "_ttl": "10ms"\r\n}\r\n\r\n#document will be deleted after a while\r\nGET testidx/doc/1\r\n```'
7292,'clintongormley','Docs: Fix ambigous explanation of the "fields" parameter in `query_strin...\n...g` query'
7290,'colings86',"REST API: Allows all options for expand_wildcards parameter\nThis change means that the default settings for expand_wildcards are only applied if the expand_wildcards parameter is not specified rather than being set upfront. It also adds the none and all options to the parameter to allow the user to specify no expansion and expansion to all indexes (equivalent to 'open,closed')\r\n\r\nCloses #7258"
7286,'clintongormley','Update snapshots.asciidoc\nAdded a link so you directly get to the page that tells how to open/close an index.\r\nLink should go here https://github.com/elasticsearch/elasticsearch/blob/1.x/docs/reference/indices/open-close.asciidoc'
7285,'javanna',"Internal: Made it possible to disable the main transport handler in TransportShardSingleOperationAction\n`TransportShardSingleOperationAction` is currently subclassed by different transport actions. Some of them are internal only, meaning that their execution will take place only in the same node where their parent execution took place. That means that their main transport handler doesn't need to be registered, the only transport handler that's needed is the shard level one. \r\n\r\nAdded abstract method `isSubAction` to the parent class that tells whether the action is a main one or a subaction, used to decide whether we need to register the main transport handler."
7283,'clintongormley','Update prefix-query.asciidoc\nTypo (missing word)'
7282,'dadoonet','Internal: Parsing command line args multiple times throws `AlreadySelectedException`\nThis issue has been fixed in commons-cli:1.3 project which sadly has not been released yet.\r\nSee https://issues.apache.org/jira/browse/CLI-183\r\n\r\nThis patch builds another list of options with no selected groups by default.\r\n\r\nWhen commons-cli:1.3 will be released, we need to remove this patch.'
7281,'clintongormley','Adds missing explanation\nThe documentation is confusing as it is written.'
7277,'clintongormley',"Remove the 'Factor' paragraph to reflect #6490\nThe current implementation of 'date_histogram' does not understand\r\nthe `factor` parameter. Since the docs shouldn't raise false hopes,\r\nI removed the section.\r\n\r\nSN: I wonder, wether the parameter was intentionally left out\r\nin anticipation of #6599 ?"
7274,'clintongormley','documentation missing filter be more explicit\nHey,\r\n\r\nit would be awesome if the documentation about the missing filter would be more accurate/specific/explicit according to the null_value that the field it self needs the null_value defined to to be mapped.\r\n\r\nIt took my quite some time to figure it out.'
7273,'polyfractal','NPE in  case of null_value creation with value as null\nHey,\r\n\r\nas we tried to understand the missing filter\r\nwe came across a null pointer exception in creating a default null_value in the mapping.\r\n\r\nThe NPE is thrown if we try to set the "null_value" to null.\r\n\r\n    PUT /foo\r\n    {\r\n      "mappings": {\r\n        "bar": {\r\n          "properties": {\r\n            "exception": {\r\n              "null_value": null,\r\n              "type": "integer"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }'
7272,'colings86','Geo: Improved error handling in geo_distance\ngeo_distance filter now throws a parse exception if no distance parameter is supplied\r\n\r\nClose #7260'
7271,'jpountz',"REST API: Added support for empty field arrays in mappings\nFix for #6133, added the ability to send empty arrays as part of an index mapping json. For single and multi field properties objects\r\n\r\nSorry for the import changes, IntelliJ doesn't like eclipse imports and fights it like the devil.\r\n\r\nIf the test are in a non standard format please advise. I just find them easier to read like this.\r\n\r\n"
7270,'clintongormley','(pre_zone_adjust_large_interval = true) AND (min_doc_count = 0) -> OutOfMemory\nUsing a dateHistogram query with `pre_zone` adjustment together with `pre_zone_adjust_large_interval = true` and `min_doc_count = 0` leads to an infinite loop and consequently OutOfMemory error.\r\n\r\nWe have observed this in v1.0.2 and verified that the same issue is still in v1.3.1.\r\n\r\nTo verify this, we used the `DateHistogramTests` test case `singleValue_WithPreZone_WithAadjustLargeInterval` and added the line `.minDocCount(0L)` to the `dateHistogram` builder.\r\n\r\nThe issue seems to be in `InternalHistogram.reduce(ReduceContext)` in the `if (minDocCount == 0) {` branch. For the previous test case, the line `while (key != nextBucket.key) {` loops endlessly because the zone adjustments seem not to be taken into account.\r\n\r\nWe have seen similar things with post_zone (unfortunately, I could not reproduce it with a test case).'
7267,'dakrone',"Docs: Fixed a typo\nI read that so many times, I just couldn't stand the typo anymore :)\r\nThanks!"
7263,'clintongormley','Remove phrase duplication in api-conventions.asciidoc\n'
7262,'clintongormley','Fix typo in phrase-suggest.asciidoc\n'
7260,'colings86','Geo: Geo-distance without distance throws an NPE\n    GET /attractions/restaurant/_search\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "geo_distance": {\r\n            "location": {\r\n              "lat": 40.715,\r\n              "lon": -73.998\r\n            }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nNPE:\r\n\r\n    Caused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.unit.DistanceUnit$Distance.parseDistance(DistanceUnit.java:319)\r\n\tat org.elasticsearch.common.unit.DistanceUnit$Distance.access$000(DistanceUnit.java:245)\r\n\tat org.elasticsearch.common.unit.DistanceUnit.parse(DistanceUnit.java:162)\r\n\tat org.elasticsearch.index.query.GeoDistanceFilterParser.parse(GeoDistanceFilterParser.java:147)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:290)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:271)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:234)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.innerParse(IndexQueryParserService.java:342)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:263)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:669)\r\n\t... 9 more'
7258,'colings86','REST API: Cannot expand_wildcards for only closed indices\nThe expand_wildcards option supports \'open\', \'closed\', and \'open,closed\'.  If you specify \'closed\' and the defaultSettings are \'open\', both open and closed indices will match. This is because the defaults are pre-selected so even if only \'closed\' is provided in the request, open will still be set as well. Need to change it so it only sets the defaults if the "expand_wildcards" parameter is not provided\r\n\r\nsee: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/support/IndicesOptions.java#L155-168\r\n\r\n'
7257,'brwe','Internal: Fix explanation streaming\nComplex explanations were always read as Explanations. Depending\r\non if the response was streamed or not the explanation was\r\ntherefore generated by a ComplexExplanation or by a regular\r\nExplanation.'
7255,'javanna','Internal: Adjusted BroadcastShardOperationResponse subclasses visibility\nAdjusted `BroadcastShardOperationResponse` subclasses visibility to package private when possible.\r\nAlso replaced `int`,`String` pair with `ShardId` that holds the same info and serializes it the same way.\r\nReplaced shardId and index getters in `BroadcastOperationRequest` with a single `ShardId` getter that allows us to reuse the same object in responses.'
7254,'dadoonet','Internal: VerboseProgress(PrintWriter) does not set the writer\n'
7253,'dadoonet','Test: move plugin dir to plugins dir\nWe should be consistent in our naming for classes and resources.'
7247,'colings86','Geo: fixes computation of geohash neighbours\nThe geohash grid it 8 cells wide and 4 cells tall. GeoHashUtils.neighbor(String,int,int.int) set the limit of the number of cells in y to < 3 rather than <= 3 resulting in it either not finding all neighbours or incorrectly searching for a neighbour in a different parent cell.\r\n\r\nCloses #7226'
7241,'dakrone','REST API: A content decompressor that throws a human readable message when\ncompression is disabled and the user sends compressed content.\r\n\r\nThis replaces https://github.com/elasticsearch/elasticsearch/pull/1678'
7240,'brwe','Query DSL: Empty bool {} should return match_all\nThis is somewhat related to the following request:\r\nhttps://github.com/elasticsearch/elasticsearch/issues/6722\r\n\r\n6722 actually causes a NPE when the clauses within the bool are null:\r\n\r\n```\r\n"bool" : {\r\n    "must": [],\r\n    "must_not": [],\r\n    "should": []\r\n  }\r\n```\r\n\r\nFor this ticket, there are use cases when Kibana is generating requests like the following:\r\n\r\n```\r\n "facet_filter": {\r\n                "fquery": {\r\n                    "query": {\r\n                        "filtered": {\r\n                            "query": {\r\n                                "bool": {\r\n                                }\r\n                            },\r\n                            "filter": {\r\n                                "fquery": {\r\n                                    "query": {\r\n                                        "query_string": {\r\n                                            "query": "_type:apache"\r\n                                        }\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n```\r\n\r\nThe above query ignores the facet_filter\'s filter clause when it should really be returning a match_all plus the filter applied.\r\n\r\nWhen a query with just a bool {} is run on its own, the empty bool clause in this case does not throw a NPE and is treated as a valid query, except that it returns no documents (when it should really be returning a match_all):\r\n\r\n```\r\n   "query": {\r\n         "bool": {\r\n         }\r\n    }\r\n```'
7238,'mikemccand','Index Corruption\nWe have two nodes running in our elasticsearch set-up with the cluster regularly going into yellow state. As far as we have been able to determine, this is usually caused by a shard of one particular index regularly becoming unassigned. Restarting the elasticsearch instance helps for a short while (green state), but it usually turns yellow again in half an hour or so.\r\n\r\nLog File Excerpt:\r\n=============\r\n[2014-08-12 13:16:15,183][INFO ][node                     ] [Natalie Portman] started\r\n[2014-08-12 13:16:18,742][INFO ][gateway                  ] [Natalie Portman] recovered [19] indices into cluster_state\r\n[2014-08-12 13:16:20,982][INFO ][cluster.service          ] [Natalie Portman] added {[Jennifer Lawrence][cTovvVXzSgmKBSgi9AoYaA][ip-XXXXX][inet[/XXXX]],}, reason: zen-disco-receive(join from node[[Jennifer Lawrence][cTovvVXzSgmKBSgi9AoYaA][XXXXX][inet[/XXXXX]]])\r\n[2014-08-12 14:13:50,511][WARN ][index.merge.scheduler    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed to merge\r\njava.lang.ArrayIndexOutOfBoundsException: 219\r\n\tat org.apache.lucene.util.FixedBitSet.set(FixedBitSet.java:256)\r\n\tat org.apache.lucene.codecs.PostingsConsumer.merge(PostingsConsumer.java:87)\r\n\tat org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:110)\r\n\tat org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)\r\n\tat org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:399)\r\n\tat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:112)\r\n\tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4163)\r\n\tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3759)\r\n\tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\r\n\tat org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:106)\r\n\tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\r\n[2014-08-12 14:13:50,515][WARN ][index.engine.internal    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed engine [merge exception]\r\n[2014-08-12 14:13:50,784][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] sending failed shard for [fuelup-accounts-revisions][2], node[wqRW6lbSQ6yk1B3bxbCF6w], [R], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]\r\n[2014-08-12 14:13:50,785][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[wqRW6lbSQ6yk1B3bxbCF6w], [R], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]\r\n[2014-08-12 14:13:50,888][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[cTovvVXzSgmKBSgi9AoYaA], [P], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [engine failure, message [merge exception][MergeException[java.lang.ArrayIndexOutOfBoundsException: 219]; nested: ArrayIndexOutOfBoundsException[219]; ]]\r\n[2014-08-12 14:13:50,890][WARN ][cluster.action.shard     ] [Natalie Portman] [fuelup-accounts-revisions][2] received shard failed for [fuelup-accounts-revisions][2], node[cTovvVXzSgmKBSgi9AoYaA], [P], s[STARTED], indexUUID [PMsFEnTWQ1e_uqGFx8yUXw], reason [master [Natalie Portman][wqRW6lbSQ6yk1B3bxbCF6w][ip-XXXXX][inet[/XXXXX]] marked shard as started, but shard has not been created, mark shard as failed]\r\n[2014-08-12 14:13:52,371][WARN ][index.merge.scheduler    ] [Natalie Portman] [fuelup-accounts-revisions][2] failed to merge\r\n\r\n--\r\n\r\nWe have verified that the machines have both enough storage and memory, so we are wondering what is going wrong and how we could further debug this. Suggestions are appreciated.'
7235,'javanna','Internal:  Adjusted visibility for BroadcastShardOperationRequest subclasses and their constructors\nAdjusted visibility to package private for BroadcastShardOperationRequest subclasses and their constructors\r\n\r\nAlso replaced the String,int pair for index and shard_id with `ShardId` object that holds the same info and serializes it the same way too.'
7234,'colings86','Indices API: Added GET Index API\nReturns information about settings, aliases, warmers, and mappings. Basically returns the IndexMetadata.\r\n\r\nCloses #4069'
7232,'dakrone','Resiliency: Add translog checksums\nSwitches TranslogStreams to check a header in the file to determine the\r\ntranslog format, delegating to the version-specific stream.\r\n\r\nVersion 1 of the translog format writes a `0xffff_ffff_0000_0001` header\r\nat the beginning of the file and appends a checksum for each translog\r\noperation written.\r\n\r\nAlso refactors much of the translog operations, such as merging\r\n.hasNext() and .next() in FsChannelSnapshot\r\n\r\nRelates to #6554'
7231,'mikemccand',"Java API: CreateIndexRequestBuilder.addMapping should throw exc if that type already has a mapping\nI think it's trappy today, that .addMapping makes it seem like you can merge in mappings for different fields in the same type, whereas what it actually does is silently overwrite any previous .addMapping for that type.\r\n\r\n"
7228,'clintongormley','Clarify if non-bitset filters can be added to bool filters (without the need for or/and, etc..)\nTraditionally, we have recommended to wrap mixed bitset and non-bitset filters in and/or/not, and place the bitset filters first, eg.\r\n\r\n```\r\n{\r\n  "and" : [\r\n    {\r\n      "bool" : {\r\n        "must" : [\r\n          { "term" : {} },\r\n          { "range" : {} },\r\n          { "term" : {} }\r\n        ]\r\n      }\r\n    },\r\n    { "custom_script" : {} },\r\n    { "geo_distance" : {} }\r\n  ]\r\n}\r\n```\r\nRecent conversations with dev revealed that the bool filter has been improved to be aware of non-bitset filters and automatically moves them to the end, so that they are executed after any bitset filters (and this change may have already be in 0.90.x).   @clintongormley also ran a benchmark and just using bool for both non-bitset and bitset seems to perform fine.  Would be great to see a doc or blog update to confirm our current recommendation, eg. if there is still a need to use and/or/not to separate out bitset and non-bitset filters, if they can now be combined within a bool, will the end user have to order them manually by "cost", or if it is automatic, etc.. And starting in what version the change was made, etc..'
7226,'colings86','Geo: Geohash_cell produces bad neighbors\n    GET /_validate/query?explain\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "geohash_cell": {\r\n              "location": {\r\n                "lat": 51.521568,\r\n                "lon": -0.141257\r\n              },\r\n              "precision": "100km",\r\n              "neighbors": true\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nReturns geohashes:\r\n\r\n*  `ebzs` - see http://geohash.2ch.to/ebzs\r\n* `ebzu` - see http://geohash.2ch.to/ebzu\r\n* `gcpt` - see http://geohash.2ch.to/gcpt\r\n* `gcpv` - see http://geohash.2ch.to/gcpv\r\n* `s0bh` - see http://geohash.2ch.to/s0bh\r\n* `u10j` - see http://geohash.2ch.to/u10j\r\n\r\nOnly `gcpt`, `gpcv`, and `u10j` are in the right place. The others are in the Gulf of Guinea.\r\n\r\n\r\n\r\n'
7224,'clintongormley','Update frontends.asciidoc\nSimple search client for elastic search.'
7223,'javanna',"Internal: changed every single index operation to not replace the index within the original request\nAn anti-pattern that we have in our code, noticeable if you use java API, is that we modify incoming requests by replacing the index or alias with the concrete index. This way not only the original user request has changed, but all following communications that use that request will lose the information on whether the original request was performed against an alias or an index, and its name.\r\n\r\nRefactored the following base classes: `TransportShardReplicationOperationAction`, `TransportShardSingleOperationAction`, `TransportSingleCustomOperationAction`, `TransportInstanceSingleOperationAction` and all subclasses by introducing an InternalRequest object that holds the original request plus additional info (e.g. the concrete index). This internal request doesn't get sent over the transport but rebuilt on each node on demand (not different to what currently happens anyway, as concrete index gets re-set on each node). When the request becomes a shard level request, instead of using the only int shardId we serialize the `ShardId` that contains both concrete index name (which might then differ from the original one within the request) and shard id.\r\n\r\nUsing this pattern we can move get, multi_get, explain, analyze, term_vector, multi_term_vector, index, delete, update, bulk to not replace the index name with the concrete one within the request. The index name within the original request will stay the same.\r\n\r\nMade it also clearer within the different transport actions when the index needs to be resolved (user facing requests) and when that's not needed (e.g. shard level request), by exposing `resolveIndex` method. Moved check block methods to parent classes as their content was always the same on every subclass.\r\n\r\nImproved existing tests by randomly introducing the use of an alias, and verifying that the responses always contain the concrete index name and not the original one, as that's the expected behaviour.\r\n\r\nAdded backwards compatibility tests to make sure that the change is applied in a backwards compatible manner."
7221,'javanna','Internal: Adjusted TermVectorRequest serialization to not serialize and de-serialize the index twice\n'
7218,'colings86','Internal: Store index creation time in index metadata\nThis change stores the index creation time in the index metadata when an index is created.  The creation time cannot be changed but can be set as part of the create index request to allow for correct creation times for historical data.\r\n\r\nCloses #7119'
7216,'clintongormley','Increase shard count in marvel index mapping\nCurrently the mapping has;\r\n````\r\n         "index.number_of_replicas": "1",\r\n         "index.number_of_shards": "1",\r\n````\r\n\r\nCan we up shards to 5, that\'s inline with the default in Elasticsearch\'s yml config for shards.'
7215,'martijnvg','Endless mapping re-sync problem\n1. We are running ES 1.3.1, but we had spotted this same issue on the previous versions as well\r\n2. It can be solved temporarily by closing/opening index, but it will get back to such a state later\r\n\r\nCorresponding mapping and index settings: https://gist.github.com/AVVS/bef59f42760256e2b5e8\r\n\r\nBasically it looks like this and can last forever:\r\n\r\n```\r\n[2014-08-09 02:24:10,836][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:11,069][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:11,303][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:11,613][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:11,854][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:12,272][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:12,514][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:12,755][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:12,997][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:13,382][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:13,632][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:13,874][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:14,108][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:14,350][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:14,601][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:14,852][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:15,100][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:15,418][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:15,761][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:16,036][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:16,292][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:16,552][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:16,802][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:17,044][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:17,287][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:17,518][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:17,753][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:18,096][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:18,346][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:18,597][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:18,847][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:19,096][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:19,415][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:19,666][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:19,917][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:20,151][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:20,585][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:20,859][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:21,076][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:21,335][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:21,585][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:21,818][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:22,059][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n[2014-08-09 02:24:22,294][WARN ][cluster.metadata         ] [ubuntu74] [profiles-2014-08-01] re-syncing mappings with cluster state for types [[profiles_v1]]\r\n```'
7214,'javanna','Internal: refactored TransportSingleCustomOperationAction, subclasses and requests\n`TransportSingleCustomOperationAction` is subclassed by two similar, yet different transport actions: `TransportAnalyzeAction` and `TransportGetFieldMappingsIndexAction`. Made their difference and similarities more explicit by sharing common code and moving specific code to subclasses:\r\n- moved index field to the parent `SingleCustomOperationAction` class\r\n- moved the common check blocks code to the parent transport action class\r\n- moved the main transport handler to the `TransportAnalyzeAction` subclass as it is only used to receive external requests through clients. In the case of the `TransportGetFieldMappingsIndexAction` instead, the action is internal and executed only locally as part of the user facing `TransportGetFieldMappingsAction`. The corresponding request gets sent over the transport though as part of the related shard request\r\n- removed the get field mappings index action from the action names mapping as it is not a transport handler anymore. It was before although never used.'
7212,'jpountz',"Java API: XContentBuilder.map(Map) method modified to use a wildcard for value's type.\nThe XContentBuilder.map(Map<String, Object>) method could be modified to use a wildcard for value's type.\r\n\r\nThis could make the API slightly more convenient. \r\nFor exemple, when I got a org.elasticsearch.common.settings.Settings and call getAsMap(), it returns a Map<String, String> that can not be passed to map(Map<String, Object>).\r\n\r\nUsers of the current API are forced to pass a Map<String, Object>, but any reference with such type can be passed to Map<String, ?>, so they will require no changes.\r\nConsumers of a Map<String, ?> get Object as type for the entry.getValue(), as in XContentBuilder.writeMap(), so it will require no changes here.\r\n\r\nThis could be backported easily to other branches."
7211,'javanna',"Internal: Removed needless serialization code from TransportIndexReplicationAction and corresponding request object\nTransportIndexReplicationAction is always executed locally, as an internal action that is part of either delete by query or delete (when routing is required but not specified). Only the corresponding shard level requests get sent over the transport, hence no transport endpoint is needed for the index version, nor the index request itself is supposed to be sent over the transport.\r\n\r\nMade all index requests fields final and removed `validate` methods from index requests as we can rely on validation performed by the callers on main transport actions. No need to validate it again as the request doesn't even get sent over the transport.\r\n\r\nMoved classes from `org.elasticsearch.action.delete.index` to `org.elasticsearch.action.delete` and adjusted visibility so that internal requests are not public anymore.\r\n\r\nAlso removed serialization code from IndexDeleteResponse as it never gets sent over transport either."
7206,'areek','Suggester: add suggestRequest to Requests and fix broken javadocs in client\n'
7205,'colings86','Validation of mappings request to reject unsupported fields\nAt the moment the mappings request is not validated to ensure that all the specified fields are valid for the particular mapping type.  There are three types of field in a mapping field definition:\r\n* type field - single field which indicates which field mapper to use\r\n* common fields - fields which all types use (such as stored or indexed fields)\r\n* type specific fields - fields which are unique to the particular field mapper\r\n\r\nThese types are parsed separately and the parser for each does not know (and should not know) what the valid fields for the other parsers are.  The solution is therefore for the parsers to remove the fields it processes and then a check to be made at the end to see if there are any fields which have not parsed  If there are any fields not parsed, these should be returned in an thrown exception.\r\n\r\nThis functionality has already been implemented for the root mappings in #6093'
7204,'dadoonet','Range filter with no value for from and to should be replaced by a match_all filter\nA range filter without any from/to value generates a range filter:`[* TO *]`.\r\nIt would be better to generate in that case a `match_all` filter.\r\n\r\n'
7203,'clintongormley','Inconsistent ceiling round up in range search for "lt" (less than)\nWIth range searches (default "mapping.date.round_ceil": true),  we have \r\n* gt, gte, from --- round off to floor\r\n* lte, to --- round off to ceiling\r\n\r\nHowever "lt" behaves inconsistently that it rounds off to floor. This is confusing in search behavior. It should instead also round off to ceiling by default. \r\n\r\nTo replicate, just create an index with two documents, with past midnight and next midnight. Searching with "lt" matches none, "lte" matches both.\r\n```\r\nPUT dateround\r\n{\r\n  "settings": {\r\n    "number_of_shards": 1\r\n  }\r\n}\r\n\r\nPUT dateround/document/1\r\n{\r\n  "testdate" : "2014-08-08T00:00:00"\r\n}\r\n\r\nPUT dateround/document/2\r\n{\r\n  "testdate" : "2014-08-09T00:00:00"\r\n}\r\n\r\nGET dateround/document/_search\r\n{\r\n  "explain": false, \r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "range": {\r\n          "testdate": {\r\n            "lt" : "now/d"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```'
7202,'colings86','[DOCS] Added section describing how to return only agg results\nCloses #5875'
7201,'javanna','Java API: Add index, type and id to ExplainResponse\nIndex, type and id were returned as part of the REST explain api response, but not through java api. That info was read out of the request, relying on the fact that the index would get overridden with the concrete one within that same request instance.'
7195,'clintongormley','Fix minor typo in readme\n'
7193,'clintongormley','Updated the community clients for Kafka Consumer in Integration Page\nI got an email from Clinton Gormley to update the details on the "Kafka Consumer for ElasticSearch" in Community support clients integration page and I did it.\r\n\r\nDetails of the "Kafka Consumer for ElasticSearch" can be found here: https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer'
7192,'clintongormley','Update repositories.asciidoc\nAdded apt-get update to the commands for clarity.'
7190,'colings86','Geo: Better error for invalid multipolygon\nCloses #7126'
7189,'dadoonet','Search: add `format` support for date range filter and queries\nWhen the date format is defined in mapping, you can not use another format when querying using range date query or filter.\r\n\r\nFor example, this won\'t work:\r\n\r\n```\r\nDELETE /test \r\n\r\nPUT /test/t/1\r\n{\r\n  "date": "2014-01-01"\r\n}\r\n\r\nGET /test/_search\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "range": {\r\n          "date": {\r\n            "from": "01/01/2014"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIt causes:\r\n\r\n```\r\nCaused by: org.elasticsearch.ElasticsearchParseException: failed to parse date field [01/01/2014], tried both date format [dateOptionalTime], and timestamp number\r\n```\r\n\r\nIt could be nice if we can support at query time another date format just like we support `analyzer` at search time on String fields.\r\n\r\nSomething like:\r\n\r\n```\r\nGET /test/_search\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "range": {\r\n          "date": {\r\n            "from": "01/01/2014",\r\n            "format": "dd/MM/yyyy"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nSame for queries.\r\n\r\nFor now, we can still support it by setting in mapping different expected formats:\r\n\r\n```\r\nDELETE /test \r\n\r\nPUT /test\r\n{\r\n  "mappings": {\r\n    "t": {\r\n      "properties": {\r\n        "date": {\r\n          "type": "date",\r\n          "format": "yyyy-MM-dd||dd/MM/yyyy"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /test/t/1\r\n{\r\n  "date": "2014-01-01"\r\n}\r\n\r\nGET /test/_search\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "range": {\r\n          "date": {\r\n            "from": "01/01/2014"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n'
7188,'colings86','Geo: Fix geo_shapes which intersect dateline\nIf a geo_shape had edges which either ran vertically along the dateline or touched the date line but did not cross it they would fail to parse.  This is because the code which splits a polygon along the dateline did not take into account the case where the polygon touched but did not cross the dateline.  This PR fixes those issues and provides tests for them.\r\n\r\nClose #7016'
7186,'javanna',"Java API: Remove operationThreaded setter from ExplainRequestBuilder\nIt's already available in base class `SingleShardOperationRequestBuilder` and it doesn't follow the setter convention that we adopted for request builders.\r\nFixed also javadocs warning caused byt missing descriptions for tag."
7175,'colings86','Mapping: Fix dynamic mapping of geo_point fields\nIf a dynamic mapping for a geo_point field is defined and the first document specifies the value of the field as a geo_point array, the dynamic mapping throws an error as the array is broken into individual number before consulting the dynamic mapping configuration.  This change adds a check of the dynamic mapping before the array is split into individual numbers.\r\n\r\nCloses #6939'
7173,'Mpdreamz','Percolator: Added missing percolate API parameters to the rest spec\n(percolate_routing, percolate_preference) to the REST API Spec\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/issues/3380\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-percolate.html#_percolating_an_existing_document'
7170,'areek',"Completion & Context Suggester: Support near real-time deleted document filtering for suggestions\nIdea:\r\n  - encode lucene docids to corresponding FST outputs\r\n  - make sure no surface form is lost even though there may be exact duplicates\r\n  - use encoded docids to perform real-time deleted doc filtering\r\n\r\nImplementation & Considerations:\r\n  - Currently the FST BytesRef Output is in the format:\r\n     `surface_form` + `PAYLOAD_SEP` + `payload` + `PAYLOAD_SEP` + `docID`\r\n  - Duplicate surface forms are stored uniquely with dedup bytes. (uses the `END_BYTE` before the dedup bytes, maybe we can support `exact_first` in the future)\r\n  - `maxAnalyzedPathsForOneInput` is now dynamically set by taking into account the # of surfaces per analyzed form (to deepen the `TopNSearcher` queue)\r\n  - This implementation modifies the `XAnalyzingSuggester` further, I think it is a good idea to make lucene's `AnalyzingSuggester` more pluggable instead in the future.\r\n  - Minor refactoring\r\n\r\ncloses #7133"
7168,'imotov','Benchmarks: Re-factored benchmark infra\nMajor re-factoring to use a dual-channel strategy for executing\r\nbenchmarks. Uses cluster metadata for managing lifecycle events, but\r\ntransport channel to send benchmark definitions and results between\r\nmaster and executor nodes.'
7164,'martijnvg','Aggregations: Properly support top_hits aggregation in a nested and reverse_nested aggregations.\nAt the moment the `top_hits` aggregation fails if it is being put in a nested or reverse_nested aggregations. This PR add proper support for nested inner objects being emitted as hits in the `top_hits` aggregator. All the known fetch phase features are supported.\r\n\r\nThis PR adds fetch infrastructure to get #3022 in as well.'
7162,'imotov',"Issues with index routing allocation in 1.3.1\nI noticed an issue after upgrading from 0.90.9 to 1.3.1 that the index.routing.allocation.include._ip setting per index is not being properly handled.  I have used this setting in the past to move indices to an archiving server for long term storage (outside of the cluster).\r\n\r\nIn  the past the past I have been able to do the following:\r\n\r\nPre-move settings:\r\n```perl\r\n'index' => {\r\n\t'routing' => {\r\n\t\t'allocation' => {\r\n\t\t\t'include' => {\r\n\t\t\t\t'tag' => 'storage',\r\n\t\t\t},\r\n\t\t\t'total_shards_per_node' => '2'\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n\r\nMove settings:\r\n```perl\r\n'index' => {\r\n\t'routing' => {\r\n\t\t'allocation' => {\r\n\t\t\t'include' => {\r\n\t\t\t\t'tag' => '',\r\n\t\t\t\t'_ip' => '192.168.0.51',\r\n\t\t\t},\r\n\t\t\t'total_shards_per_node' => '999'\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n\r\nThis would force one copy of each shard to migrate to the archiving server from the storage cluster.\r\n\r\nNow in 1.3.1 the shards do not move at all with these settings, instead I have to set the following:\r\n```perl\r\n'index' => {\r\n\t'routing' => {\r\n\t\t'allocation' => {\r\n\t\t\t'include' => {\r\n\t\t\t\t'tag' => 'storage,backup',\r\n\t\t\t\t'_ip' => '192.168.0.51',\r\n\t\t\t},\r\n\t\t\t'total_shards_per_node' => '999'\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nI did a quick test and it does look like if I do not provide the _ip setting, but only use the tags setting the migration works as intended.  This leads me to believe that the _ip field is being completely ignored.\r\n\r\nIn my use case I can't rely on the backup tag since I will have more than one backup server (using that tag), but need all of the data to go to a single server (ie need _ip, _host, etc).\r\n\r\nUnfortunately, I was running on an old cluster for awhile so I am not sure  exactly where this broke at. But would have been somewhere between 0.90.9 and 1.3.1."
7160,'colings86','Aggregations: key_as_string only shown when format specified in terms agg\nThe key_as_string field is now not shown in the terms aggregation for long and double fields unless the format parameter is specified\r\n\r\nCloses #7125'
7159,'imotov','Resiliency: Automatic verification of all files that are being snapshotted with Snapshot/Restore\nAdds automatic verification of all files that are being snapshotted. Closes #5593'
7158,'rmuir','Update forbidden-apis to 1.6.1\nForbidden APIs was released in version 1.6.1 (1.6 had broken Maven decriptor). The new version allows to skip checks with:\r\n`mvn -Dforbiddenapis.skip=true test`'
7154,'dadoonet','Plugins: `bin` and `config` only plugins do not install correctly\nWhen installing a bin only plugin, it is identified as a site plugin.\r\n\r\nA current workaround would be to create in the zip file another empty dir. So if you have:\r\n\r\n* `bin/myfile.sh`\r\n* `empty/empty.txt`\r\n\r\nthe `bin` content will be extracted as expected.\r\n\r\nCloses #7152.'
7152,'dadoonet',"plugins: `bin` and `config` only plugins do not install correctly\nWhen installing a bin plugin, it is identified as a site plugin. \r\nI've already been chatting with @dadoonet about this issue. "
7151,'spinscale',"Disable CORS by default\nElasticsearch currently [defaults CORS to being enabled](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-http.html) for all domains.\r\n\r\n![cors-true](https://cloud.githubusercontent.com/assets/4592/3799663/d59db5ea-1bef-11e4-99d1-6d9cec88d2fb.png)\r\n\r\nThis is a **security issue** for any developer running Elasticsearch locally at `localhost:9200` -- any website they visit can fire off arbitrary HTTP requests to their local machine. This **should be disabled by default**.\r\n\r\nI became aware of this issue during the hubbub over the dynamic scripting vulnerability in 1.1's defaults (now changed in 1.2, and scripting sandboxed in 1.3). CORS magnified the effect of that bug for developers browsing the web, but even with dynamic scripting disabled, websites can still perform arbitrary Elasticsearch actions to a local instance of ES.\r\n\r\nWhen I mention  to developers that `localhost:9200` is accessible via any website they visit, they are very surprised, as was I. Yes, maybe it's something I and everyone should have understood going in, but that's not happening. And while it may not be unique to Elasticsearch, this isn't a problem with most databases and database-like systems developers are used to running locally.\r\n\r\nI've seen people [recommend using Elasticsearch in a VM](https://www.found.no/foundation/elasticsearch-security/#staying-safe-while-developing-with-elasticsearch) during development, but this is overhead caused by a choice Elasticsearch makes. Elasticsearch's current default CORS setting add convenience for some, at the expense of security for many developers.\r\n\r\nAny plugins or support systems that depend on enabling CORS for Elasticsearch can provide instruction to enable CORS (along with a warning of serious side effects) as part of their installation. \r\n\r\n**Elasticsearch should provide a safe experience by default.** The dynamic scripting issue, which took some time to be seen as a security issue, is now [CVE-2014-3120](http://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2014-3120). Elasticsearch should get in front of this one."
7145,'javanna','MultiGet: Fail when using no routing on an alias to an index that requires routing\nThe multi_get api and multi_term_vector apis should always return an error when trying to get a document without routing if the routing is set to required. Yet, when using an alias a failure is not returned.\r\n\r\nThe problem is that the` routingRequired` check needs to be done passing in the resolved concrete index.'
7144,'markharwood','Update API: Add support for scripted upserts.\nIn the case of inserts the UpdateHelper class will now allow the script used to apply updates to run on the upsert doc provided by clients. This allows the logic for managing the internal state of the data item to be managed by the script and is not reliant on clients performing the initialisation of data structures managed by the script.\r\nAssociated issue: https://github.com/elasticsearch/elasticsearch/issues/7143'
7143,'markharwood','Update API - allow scripted upserts\nIn some scenarios it is useful if the update script used to add new content to an existing document is also capable of being called when the document does not already exist (aka Upsert requests).\r\n\r\nNow that we have stored scripts they can be non-trivial chunks of code and may have a lot of business logic to manage the state of an item. Object orientation teaches us the value of encapsulation and this enhancement allows the scripts to encapsulate the logic used in the construction of new documents as well as the current logic for modifying existing documents.\r\n\r\nThe suggested change is that a new `scripted_upsert` parameter can be passed to upsert requests to indicate if the script should be called to perform inserts. The default value is "false" to reflect the current behaviour. If set to "true" and an insert is being formed the following steps occur:\r\n1) The example `upsert` document passed by the client (which now could be {} ) is presented in the script context as the initial state of the document held in `ctx._source`.\r\n2) The `ctx.op` field in the script context is set to `create` to indicate that this is an insert as opposed to update operation\r\n3) The script (or stored script) used to perform updates is called with the `params` values passed from the client (in the same way updates are invoked). It mutates the map of values held in `ctx._source` appropriately.\r\n4) The script can set `ctx.op` to `none` if it wants to ignore the insert, otherwise the document held in `ctx._source` after the script completes is stored\r\n'
7142,'mikemccand','Core: create operation on replica should not throw DocumentAlreadyExistsException\nFor a create operation, today we (by design) throw DAEE if the given _uid is already present in the index, even if its version is "old".\r\n\r\nBut when there are concurrent operations in flight against the same _uid this can be wrong, and was causing [rare] failures in SimpleVersioningTests.testRandomIDsAndVersions... the specific case was a given _uid was already in the index, and then a delete op (with higher version), and a create op (with higher version still) were issued concurrently.\r\n\r\nOn primary, the delete came first, succeeded, and then the create came second, and it succeeded.  But on the replica, the create came first, and it threw DAEE (which we intentionally suppress & don\'t throw back to user), then the delete came, and it succeeded, leaving primary & replica silently out-of-sync.\r\n\r\nTo fix this, for a create op, if we are a replica, we should not throw DAEE: it means the primary has already decided the document should be indexed.  Instead, we should fallback to the same logic index operation does, making sure the version is newer, and we must use IW.updateDocument/s in this case since the doc is in the index with an older version.  We can still use the auto-gen\'d ID optimization...\r\n'
7141,'spinscale','Mapping: Improve IP address validation\nUntil now, IP addresses were only checked for four dots, which\r\nallowed invalid values like 127.0.0.111111\r\n\r\nThis adds an additional check for validation.\r\n\r\n**Note**: This does have a performance impact in the log file indexing case as it adds an additional parsing step. Maybe this was the reason, why it had not been implemented in the first case? We could potentially just reuse the code from guavas `InetAddresses.textToNumericFormatV4()` which is unfortunately private\r\n\r\nCloses #7131'
7140,'dakrone','Check for existence of field mapping in script\nCurrently, if you try to load an unmapped field in a script, it throws an exception, eg:\r\n\r\n    "script": "if (doc[\'foo\'].value == null ) ..."\r\n\r\nIt would be nice to have a way to check if the field exists in the current mapping without throwing an exception because eg you may be using the same script on multiple types which have different mappings.'
7137,'brwe','Function Score: Add optional weight parameter per function\nWeights can be defined per function like this:\r\n\r\n```\r\n"function_score": {\r\n    "functions": [\r\n        {\r\n            "filter": {},\r\n            "FUNCTION": {},\r\n            "weight": number\r\n        }\r\n        ...\r\n```\r\nIf `weight` is given without `FUNCTION` then `weight` behaves like `boost_factor`.\r\nThis commit deprecates `boost_factor`.\r\n\r\nThe following is valid:\r\n\r\n```\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "weight": 2\r\n    }\r\n  }\r\n}\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "functions": [\r\n        {\r\n          "weight": 2\r\n        },\r\n        ...\r\n      ]\r\n    }\r\n  }\r\n}\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "functions": [\r\n        {\r\n          "FUNCTION": {},\r\n          "weight": 2\r\n        },\r\n        ...\r\n      ]\r\n    }\r\n  }\r\n}\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "functions": [\r\n        {\r\n          "filter": {},\r\n          "weight": 2\r\n        },\r\n        ...\r\n      ]\r\n    }\r\n  }\r\n}\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "functions": [\r\n        {\r\n          "filter": {},\r\n          "FUNCTION": {},\r\n          "weight": 2\r\n        },\r\n        ...\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe following is not valid:\r\n\r\n```\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "weight": 2,\r\n      "FUNCTION(including boost_factor)": 2\r\n    }\r\n  }\r\n}\r\n\r\nPOST testidx/_search\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "functions": [\r\n        {\r\n          "weight": 2,\r\n          "boost_factor": 2\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n````\r\n\r\ncloses #6955'
7134,'clintongormley','Updated typo in getting-started.asciidoc\nA very small typo in the description.'
7133,'areek',"NRTSuggester: Support near real-time deleted document filtering for suggestions\nCurrently the Completion & Context Suggesters suggests from documents that were deleted but not still merged away by Lucene. Ideally the suggesters should never suggest from deleted documents.\r\n\r\nThe solution is to encode lucene docids in the generated FST used by suggesters, and filter out suggestions from deleted documents at query time. This approach would also allow us to make use of the lucene docids to make the suggesters more flexible in terms of returning fields not specified at 'index' time in the future. This approach also implies changing how the FSTs are built currently (i.e. dedup-ing same surface forms might not make sense anymore, as each entry will be tied to a lucene doc).\r\n\r\n**NOTE:** this is an issue for `feature/nrt_suggester`"
7132,'spinscale','Packaging: Add default oracle jdk 7 (x64) path to JDK_DIRS\nOn Debian amd64, oracle jdk .deb packages made using make-jpkg (from\r\njava-package) default to /usr/lib/jvm/jdk-7-oracle-x64.'
7131,'spinscale','Mapping API: Improve IP address validation\nWhen indexing IPs, the IP parser is too lenient.  For example:\r\n\r\n```bash\r\nPUT /ipaddr/\r\n{\r\n  "mappings" : {\r\n    "temp" : {\r\n      "properties" : {\r\n        "addr" : {\r\n          "type" : "ip"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPOST ipaddr/temp\r\n{\r\n  "addr" : "127.0.011.1111111"\r\n}\r\n```\r\n\r\nThis address is considered "valid", since the parser only checks for 4 dots.  If there are four dots, the, string is split and each numeric is shifted to obtain the resulting Long. ([source](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/ip/IpFieldMapper.java#L82-L87))\r\n\r\nThis IP is therefore "converted" into an entirely different IP:\r\n\r\n```bash\r\nGET ipaddr/temp/_search?search_type=count\r\n{\r\n  "aggs": {\r\n    "ips": {\r\n      "terms": {\r\n        "field": "addr"\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n...\r\n   "aggregations": {\r\n      "ips": {\r\n         "buckets": [\r\n            {\r\n               "key": 2131820359,\r\n               "key_as_string": "127.16.255.71",\r\n               "doc_count": 1\r\n            }\r\n         ]\r\n      }\r\n   }\r\n}\r\n```\r\n'
7126,'colings86','Geo: geo_shape MultiPolygon parsing problem\nTrying to index this shape results in an IndexOutOfBoundsException\r\n\r\n```\r\nDELETE /countries\r\nPUT /countries\r\nPUT /countries/location/_mapping\r\n{\r\n  "location" : {\r\n    "properties" : {\r\n      "location" : {\r\n        "type" : "geo_shape"\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n\r\nPUT countries/location/somewhere-in-sweden\r\n{ "location" : { \r\n  "type": "MultiPolygon",\r\n  "coordinates": [\r\n    [\r\n      [\r\n        [22.183173, 65.723741],\r\n        [21.213517, 65.026005],\r\n        [21.369631, 64.413588],        \r\n        [22.183173, 65.723741]\r\n      ],\r\n      [\r\n        [17.061767, 57.385783],\r\n        [17.210083, 57.326521],\r\n        [16.430053, 56.179196],        \r\n        [17.061767, 57.385783]\r\n      ]\r\n    ]\r\n  ]\r\n} } \r\n\r\n```\r\n\r\nThe shape looks valid here https://gist.github.com/spinscale/0c014b3a0f15f90b5c4c'
7125,'colings86','Aggregations: Terms Aggregation should only show key_as_string when format is specified\nRelates to #6655'
7124,'alexksikes','Term Vectors: Return found: false for docs requested between index and refresh\nCloses #7121'
7123,'colings86','Geo: Adds support for GeoJSON GeometryCollection\nCloses #2796'
7122,'dadoonet','Query DSL: Cache range filter on date field by default\nA range filter on a date field with a numeric `from`/`to` value is **not** cached by default:\r\n\r\n    DELETE /test\r\n\r\n    PUT /test/t/1\r\n    {\r\n      "date": "2014-01-01"\r\n    }\r\n\r\n    GET /_validate/query?explain\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "range": {\r\n              "date": {\r\n                "from": 0\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nReturns:\r\n\r\n    "explanation": "ConstantScore(no_cache(date:[0 TO *]))"\r\n\r\nThis patch fixes as well not caching `from`/`to` when using `now` value not rounded.\r\nPreviously, a query like:\r\n\r\n    GET /_validate/query?explain\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "range": {\r\n              "date": {\r\n                "from": "now"\r\n                "to": "now/d+1"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nwas cached.\r\n\r\nAlso, this patch does not cache anymore `now` even if the user asked for caching it.\r\nAs it won\'t be cached at all by definition.\r\n\r\nAdded as well tests for all possible combinations.\r\n\r\nCloses #7114.'
7121,'alexksikes','`_termvector` returns `JsonGenerationException` if called between index and refresh\nTo reproduce:\r\n\r\n```\r\n\r\nDELETE testidx\r\n\r\nPUT testidx\r\n{\r\n  "settings": {\r\n    "index.translog.disable_flush": true,\r\n    "index.number_of_shards": 1,\r\n    "refresh_interval": "1h"\r\n  },\r\n  "mappings": {\r\n    "doc": {\r\n      "properties": {\r\n        "text": {\r\n          "type": "string",\r\n          "term_vector": "with_positions_offsets"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nPOST testidx/doc/1\r\n{\r\n  "text": "foo bar"\r\n}\r\n\r\nGET testidx/doc/1/_termvector\r\n\r\n```\r\n\r\nresults in \r\n\r\n```\r\n{\r\n   "error": "JsonGenerationException[Current context not an object but ROOT]",\r\n   "status": 500\r\n}\r\n```\r\n\r\nA more meaningful error message maybe?'
7119,'colings86','Core: Formalize index creation time as part of index metadata\nWe should store a timestamp for when the index was created. This would allow us to do things like index-level TTL or scheduled operations in the future.'
7117,'clintongormley',"Local shard initialisation after node restart with disabled allocation doesn't appear to work as expected\nWhen doing a rolling restart for an upgrade or other reason, per [the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html) , allocation is disabled and once the node(s) are back in the cluster allocation is enabled.\r\n\r\nHowever I've noticed and I'm also seeing more and more people comment on the same thing on IRC; shards are not being locally initialised as expected but are being reallocated to other nodes and then initialised.\r\nThis obviously slows recovery rather dramatically.\r\n\r\nGiven I don't have anything solid to back this up beyond anecdata, I am hoping I can kick off a discussion around what sort of data could be collected to try and confirm/deny this."
7115,'kimchy',"On Solaris 10 (Illumos), setting TCP_NODELAY on a closed socket causes elasticsearch to be unresponsive\nWe're on ElasticSearch 1.1.1 running on Illumos (Solaris 10 derivative on Joyent).\r\n\r\nWe ran into an issue today where elasticsearch became completely unresponsive after the following exception:\r\n\r\n```\r\n[2014-07-31 12:30:18,081][WARN ][monitor.jvm              ] [HOSTNAME] [gc][young][3604571][140866] duration [1.9s], collections [1]/[2.2s], total [\r\n1.9s]/[1.2h], memory [22.7gb]->[21.4gb]/[29.1gb], all_pools {[young] [1.3gb]->[29.2mb]/[1.4gb]}{[survivor] [70mb]->[55.3mb]/[191.3mb]}{[old] [21.3gb]->[21.3\r\ngb]/[27.4gb]}\r\n[2014-07-31 12:30:27,075][WARN ][monitor.jvm              ] [HOSTNAME] [gc][young][3604579][140869] duration [1.2s], collections [1]/[1.9s], total [\r\n1.2s]/[1.2h], memory [22.3gb]->[21.2gb]/[29.1gb], all_pools {[young] [1.1gb]->[29.8mb]/[1.4gb]}{[survivor] [52.9mb]->[46.8mb]/[191.3mb]}{[old] [21.2gb]->[21\r\n.2gb]/[27.4gb]}\r\n[2014-07-31 12:30:35,954][WARN ][http.netty               ] [HOSTNAME] Caught exception while handling client http traffic, closing connection [id:\r\n0x810b66dd, /IPSOURCE:48650 => /IPDEST:9200]\r\norg.elasticsearch.common.netty.channel.ChannelException: java.net.SocketException: Invalid argument\r\n        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setTcpNoDelay(DefaultSocketChannelConfig.java:178)\r\n        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setOption(DefaultSocketChannelConfig.java:54)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.DefaultNioSocketChannelConfig.setOption(DefaultNioSocketChannelConfig.java:70)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelConfig.setOptions(DefaultChannelConfig.java:36)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.DefaultNioSocketChannelConfig.setOptions(DefaultNioSocketChannelConfig.java:54)\r\n        at org.elasticsearch.common.netty.bootstrap.ServerBootstrap$Binder.childChannelOpen(ServerBootstrap.java:399)\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:77)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n        at org.elasticsearch.common.netty.channel.Channels.fireChildChannelStateChanged(Channels.java:541)\r\n        at org.elasticsearch.common.netty.channel.Channels.fireChannelOpen(Channels.java:167)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioAcceptedSocketChannel.<init>(NioAcceptedSocketChannel.java:42)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.registerAcceptedChannel(NioServerBoss.java:137)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.process(NioServerBoss.java:104)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)\r\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.net.SocketException: Invalid argument\r\n        at sun.nio.ch.Net.setIntOption0(Native Method)\r\n        at sun.nio.ch.Net.setSocketOption(Net.java:373)\r\n        at sun.nio.ch.SocketChannelImpl.setOption(SocketChannelImpl.java:189)\r\n        at sun.nio.ch.SocketAdaptor.setBooleanOption(SocketAdaptor.java:295)\r\n        at sun.nio.ch.SocketAdaptor.setTcpNoDelay(SocketAdaptor.java:330)\r\n        at org.elasticsearch.common.netty.channel.socket.DefaultSocketChannelConfig.setTcpNoDelay(DefaultSocketChannelConfig.java:176)\r\n        ... 20 more\r\n```\r\n\r\nOn solaris, setsocketopt has different behavior that on other platforms. It will return EINVAL causing java to raise an InvalidArgument exception when the socket has been closed. Apparently this happens when the client closes the connection before the server has finished it's accept. Elasticsearch appears to have been doing a garbage collection around that time.\r\n\r\nHere's a couple references to this bug occurring in other projects:\r\n\r\nhttp://bugs.java.com/view_bug.do?bug_id=6378870\r\nhttps://java.net/jira/browse/GLASSFISH-5342\r\nhttps://jira.atlassian.com/browse/STASH-3624\r\n\r\nIt also appears that in Netty 4.0+ this might have been fixed by: https://github.com/netty/netty/commit/39357f3835f971e6cc1a0e41a805fa1293e7005e#diff-dbfa6a222217d4fc2c12d20ee3496eb3R50\r\n\r\nUnfortunately, this is a bit difficult to reproduce and it only happens rarely. I'd imagine it can by reproduced by running elasticsearch on Solaris 10, finding a way to stall the server long enough for the client to close the connection before the server has set the socket options. Elasticsearch search should then stall and stop responding to any requests (as is the behavior that we saw).\r\n\r\nThanks,\r\nPaul\r\n"
7114,'dadoonet','Query DSL: Cache range filter on date field by default\nA range filter on a date field with a numeric `from`/`to` value is **not** cached by default:\r\n\r\n    DELETE /test \r\n    \r\n    PUT /test/t/1\r\n    {\r\n      "date": "2014-01-01"\r\n    }\r\n    \r\n    GET /_validate/query?explain\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "range": {\r\n              "date": {\r\n                "from": 0\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nReturns:\r\n\r\n    "explanation": "ConstantScore(no_cache(date:[0 TO *]))"'
7113,'dadoonet','Query DSL: Add time zone setting for relative date math in range filter/query\nFilters and Queries now supports `time_zone` parameter which defines which time zone should be applied to the query or filter to convert it to UTC time based value.\r\n\r\nWhen applied on `date` fields the `range` filter and queries accept also a `time_zone` parameter.\r\n\r\nThe `time_zone` parameter will be applied to your input lower and upper bounds and will move them to UTC time based date:\r\n\r\n```js\r\n{\r\n    "constant_score": {\r\n        "filter": {\r\n            "range" : {\r\n                "born" : {\r\n                    "gte": "2012-01-01",\r\n                    "lte": "now",\r\n                    "time_zone": "+1:00"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n{\r\n    "range" : {\r\n        "born" : {\r\n            "gte": "2012-01-01",\r\n            "lte": "now",\r\n            "time_zone": "+1:00"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIn the above examples, `gte` will be actually moved to `2011-12-31T23:00:00` UTC date.\r\n\r\nNOTE: if you give a date with a timezone explicitly defined and use the `time_zone` parameter, `time_zone` will be\r\nignored. For example, setting `from` to `2012-01-01T00:00:00+01:00` with `"time_zone":"+10:00"` will still use `+01:00` time zone.\r\n\r\nCloses #3729.'
7108,'dadoonet','Remove `numeric_range` filter\nAs done with #4034, `numeric_range` filter has been deprecated since 1.0.0.\r\n\r\nRemoved in 2.0.0'
7106,'imotov','deleting a snapshot throws FileNotFoundException\nes version: 1.2.1\r\n\r\nwhen deleting one snapshot via [elasticsearch-py](https://github.com/elasticsearch/elasticsearch-py):\r\n`es.snapshot.delete(repository=args.repository, snapshot=snapshot["snapshot"])`\r\n\r\nit throws: \r\n`elasticsearch.exceptions.NotFoundError: TransportError(404, u\'SnapshotMissingException[[es_backup_fast:2014-07-24_11:00:02] is missing]; nested: FileNotFoundException[/mnt/es_backup/fast_snapshot/snapshot-2014-07-24_11:00:02 (No such file or directory)]; \')`\r\n\r\ni controlled the snapshot was there before via:\r\n`es.snapshot.get(repository=args.repository, snapshot=\'_all\')` and it was definitely there ...\r\n\r\nAnyhow the snapshot is deleted properly. Any idea why a delete operation would throw an exception not finding what it deleted? I\'m using [elasticsearch-py](https://github.com/elasticsearch/elasticsearch-py) Version 1.1.1\r\n\r\nI\'m building a es cluster object via:\r\n`es = Elasticsearch(\r\n    args.Hosts,\r\n    sniff_on_start=True,\r\n    sniff_on_connection_fail=True,\r\n    sniffer_timeout=60\r\n)\r\n`\r\nwhere args.Hosts are two es cluster nodes from one cluster.\r\n\r\nthanks in advance'
7105,'javanna',"Internal: Better categorization for transport actions\nOur transport relies on action names that tell what we need to do with each message received and sent on any node, together with the content of the request itself.\r\nThe action names could use a better categorization and more consistent naming though, the following are the categories introduced with this commit:\r\n\r\n- `indices`: for all the apis that execute against indices\r\n  - `admin`: for the apis that allow to perform administration tasks against indices\r\n  - `data`: for the apis that are about data\r\n    - `read`: apis that read data\r\n    - `write`: apis that write data\r\n    - `benchmark`: apis that run benchmarks\r\n\r\n- `cluster`: for all the cluster apis\r\n  - `admin`: for the cluster apis that allow to perform administration tasks\r\n  - `monitor`: for the cluster apis that allow to monitor the system\r\n\r\n- `internal`: for all the internal actions that are used from node to node but not directly exposed to users\r\n\r\nThe change is applied in a backwards compatible manner: we keep the mapping old-to-new action name around, and when receiving a message, depending on the version of the node we receive it from, we use the received action name or we convert it to the previous version (old to new if version < 1.4). When sending a message, depending on the version of the node we talk to, we use the updated action or we convert it to the previous version (new to old if version < 1.4).\r\nFor the cases where we don't know the version of the node we talk to, namely unicast ping, transport client nodes info and transport client sniff mode (which calls cluster state), we just use a lower bound for the version, thus we will always use the old action name, which can be understood by both old nodes and new nodes.\r\n\r\nAdded test that enforces known updated categories for transport action names and test that verifies all action names have a pre 1.4 version for bw compatibility\r\n\r\nAdded backwards compatibility tests for unicast and transport client in sniff mode, the one for the ordinary transport client (that call nodes info) is implicit as it's used all the time in our bw comp tests.\r\nAdded also backwards comp test that sends an empty message to any of the registered transport handler exposed by older nodes and verifies that what gets back is not `ActionNotFoundTransportException`, which would mean that there is a problem in the actions mappings.\r\n\r\nAdded `TestCluster#getClusterName` abstract method and allow to retrieve externalTransportAddress and internalCluster from `CompositeTestCluster`."
7103,'jpountz','Add "size" parameter to historgram aggregation (limit number of returned results)\nIt would be very helpful to be able to limit the number of returned results for histogram and date histogram aggregations.\r\n\r\nExample use case: collecting requests logs in Elasticsearch. You want to see the maximum req/sec rate over the last hour (or whatever time period). Using aggregations it\'s easy to use date histogram with a second interval, sort by doc count and get the top second with most requests. But as the time period you query over is bigger, the response size becomes huge (and if on top of that you have a parent bucket - like histogram per request type - it\'s even worse). It\'s very wasteful to get the entire histogram results which consumes a lot of network and high latency when all you need are the top couple of buckets. '
7101,'clintongormley','dataOptionalTime field value corrupted after migrated from ES1.1.1 to 1.2.2\nWe recently migrated our ES 1.1.1 cluster to ES 1.2.2, after that we found that one field \'displayPublishedDateTime\', which is defined as dataOptionalTime type in the mapping, seems corrupted and not able to order by it in the query.  \r\nIn the query, when order by desc on it, its sort value is \'\\\\\\b\', and order by asc, its sort value is shown as \'\' \\u0001\\u0000\\u0000\\u0000(aC3@X", see the following query and results. In other queries, which order by \'displayPublishedDateTime\', the order actually didn\'t take effect. It looks like that Elasticsearch was unable to get the correct sort value for this field. \r\n\r\nQuery\r\n\r\n    GET /doc-v2/_search\r\n    {\r\n      "query": {\r\n          "match": {\r\n             "_id": "AAUYRY"\r\n          }\r\n      },\r\n      "_source":[\r\n        "_id",\r\n        "_document.displayPublishedDateTime",\r\n        ],\r\n      "sort": [\r\n        {\r\n          "_document.displayPublishedDateTime": {\r\n            "order": "desc"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n\r\nResults for \'desc\'\r\n\r\n    {\r\n       "took": 46,\r\n       "timed_out": false,\r\n       "_shards": {\r\n          "total": 25,\r\n          "successful": 25,\r\n          "failed": 0\r\n       },\r\n       "hits": {\r\n          "total": 1,\r\n          "max_score": null,\r\n          "hits": [\r\n             {\r\n                "_index": "doc-v2",\r\n                "_type": "article",\r\n                "_id": "AAUYRY",\r\n                "_score": null,\r\n                "_source": {\r\n                   "_document": {\r\n                      "displayPublishedDateTime": "2014-05-20T06:58:47Z"\r\n                   }\r\n                },\r\n                "sort": [\r\n                   "\\\\\\b"\r\n                ]\r\n             }\r\n          ]\r\n       }\r\n    }\r\n\r\nResults for \'asc\'\r\n\r\n    {\r\n       "took": 78,\r\n       "timed_out": false,\r\n       "_shards": {\r\n          "total": 25,\r\n          "successful": 25,\r\n          "failed": 0\r\n       },\r\n       "hits": {\r\n          "total": 1,\r\n          "max_score": null,\r\n          "hits": [\r\n             {\r\n                "_index": "doc-v2",\r\n                "_type": "article",\r\n                "_id": "AAUYRY",\r\n                "_score": null,\r\n                "_source": {\r\n                   "_document": {\r\n                      "displayPublishedDateTime": "2014-05-20T06:58:47Z"\r\n                   }\r\n                },\r\n                "sort": [\r\n                   " \\u0001\\u0000\\u0000\\u0000(aC3@X"\r\n                ]\r\n             }\r\n          ]\r\n       }\r\n    }'
7099,'imotov','Snapshotting NPE\nA snapshot run failed with `IndexShardSnapshotFailedException` caused by an NPE for a lot of shards:\r\n\r\n```\r\n[2014-07-30 18:11:56,063][WARN ][snapshots                ] [node.name] [[index.name][274]] [node.name:index.name] failed to create snapshot\r\norg.elasticsearch.index.snapshots.IndexShardSnapshotFailedException: [index.name][274] null\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:141)\r\n        at org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.snapshot(IndexShardSnapshotAndRestoreService.java:86)\r\n        at org.elasticsearch.snapshots.SnapshotsService$6.run(SnapshotsService.java:829)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$SnapshotContext.snapshot(BlobStoreIndexShardRepository.java:456)\r\n        at org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.snapshot(BlobStoreIndexShardRepository.java:131)\r\n        ... 5 more\r\n```\r\n\r\nThis is an index with 500 shards, snapshotting to an NFS mount.'
7096,'imotov','Validate repository settings\nAs discussed in https://github.com/elasticsearch/elasticsearch-cloud-azure/issues/21, we need to let plugins validate repository settings.\r\n\r\nSo `BlobStoreRepository` could have a new protected method which could be overloaded by plugins or repository implementations, such as:\r\n\r\n```java\r\nvoid validateRepositorySettings() throws RepositoryException;\r\n```\r\n'
7092,'dakrone','Bug: cluster.routing.allocation.allow_rebalance seems to be ignored\nIn v1.1.1, when trying to update the `cluster.routing.allocation.allow_rebalance` setting, nothing seems to happen: neither in the elasticsearch log, nor on the request feedback:\r\n``` json\r\n⚡ curl -XPUT 0:9200/_cluster/settings\\?pretty -d \'\r\n{\r\n  "persistent": {\r\n    "cluster.routing.allocation.allow_rebalance": "indices_all_active",\r\n    "cluster.routing.allocation.enable": "none",\r\n    "cluster.routing.allocation.cluster_concurrent_rebalance": 2\r\n  }\r\n}\r\n\'\r\n{\r\n  "acknowledged" : true,\r\n  "persistent" : {\r\n    "cluster" : {\r\n      "routing" : {\r\n        "allocation" : {\r\n          "enable" : "none",\r\n          "cluster_concurrent_rebalance" : "2"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "transient" : { }\r\n}\r\n```\r\nIn the server log:\r\n```\r\n[2014-07-30 14:32:16,797][INFO ][cluster.routing.allocation.decider] [node] updating [cluster.routing.allocation.cluster_concurrent_rebalance] from [4], to [2]\r\n[2014-07-30 14:32:16,797][INFO ][cluster.routing.allocation.decider] [node] updating [cluster.routing.allocation.enable] from [PRIMARIES] to [NONE]\r\n\r\n```'
7089,'dakrone','add groovy.util.GroovyCollections to the default receiver_whitelist\nIt provides helper utilities, see: http://groovy.codehaus.org/api/groovy/util/GroovyCollections.html'
7088,'dakrone','Odd script config behavior\nJust upgraded from `1.2.1` with the groovy plugin to `1.3.1` (plugin removed) and I\'m seeing some weird behaviour when trying to use my installed scripts, I\'ve written a [test script here](https://gist.github.com/mal/68bd3479e30258d833e2).\r\n\r\n**Settings**\r\n```\r\nscript.disable_dynamic: true\r\n```\r\n\r\n**Query**\r\n```json\r\n{\r\n  "script_fields": {\r\n    "minimum": {\r\n      "lang": "groovy",\r\n      "script": "lowest",\r\n      "params": {\r\n        "field": "vals"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\n**Script**\r\n\r\n```groovy\r\nGroovyCollections.min(doc[field].values as Integer[])\r\n```\r\n\r\n**Results**\r\n\r\n```\r\nScriptException[dynamic scripting for [groovy] disabled]\r\n```\r\n\r\nWhich, while true, shouldn\'t apply since I\'m trying to run a script stored on disk ...'
7075,'colings86','Aggregations: Scriptable Metrics Aggregation\nA metrics aggregation which runs specified scripts at the init, collect, combine, and reduce phases\r\n\r\nCloses #5923'
7071,'GaelTadh','Fix typo in scripting.asciidoc\nReplace the mvel by groovy in the forgotten place.\r\nI add the previous change in this one.\r\nSorry for the spam!'
7070,'GaelTadh','Fix typo in scripting.asciidoc\n'
7069,'spinscale','"Uncomment if you want to disable JSONP" in yml file\nhttp://www.elasticsearch.org/blog/elasticsearch-1-3-0-released/\r\n\r\nThe following is new in 1.3.0 yml file:\r\n\r\n```\r\n# Uncomment if you want to disable JSONP as a valid return transport on the\r\n# http server. With this enabled, it may pose a security risk, so disabling\r\n# it unless you need it is recommended.\r\n#\r\n#http.jsonp.enable: false\r\n```\r\n\r\nCurrently, the first sentence in the comment suggests that JSONP is still enabled by default.  Given that JSONP is disabled by default in 1.3.0, do we really mean "Uncomment and set to true if you want to *enable* JSONP" .... ?  '
7067,'javanna','Transport client: Don\'t add listed nodes to connected nodes list in sniff mode\nThis commit effectively reverts e1aa91d , as it is not needed anymore to add the original listed nodes. The cluster state local call made will in fact always return at least the local node (see #6811).\r\n\r\nThere were a couple of downsides caused by putting the original listed nodes among the connected nodes:\r\n1) in the following retries, they weren\'t seen as listed nodes anymore, thus the light connect wasn\'t used\r\n2) among the connected nodes some were "bad" duplicates as they are already there and don\'t contain all needed info for each node. This was causing serialization problems for instance given that the node version was missing on the `DiscoveryNode` object (or `minCompatibilityVersion` after #6894).\r\n\r\n(As a side note, the fact that nodes were appearing twice in the list was hiding #6829 in sniff mode, as more nodes than expected were in the list and then retried)\r\n\r\nNext step is to enable transport client `sniff` mode in our tests, already in the work on a public branch: https://github.com/elasticsearch/elasticsearch/tree/enhancement/test-enable-transport-client-sniff .'
7064,'spinscale','RPM misnamed\nThe RPM should be named \\<name\\>-\\<version\\>-\\<release\\>.noarch.rpm instead of \\<name\\>-\\<version\\>.noarch.rpm\r\n\r\nThis is the standard naming format used by Fedora and RedHat'
7058,'colings86','Aggregations: Stops direct subclassing of InternalNumericMetricsAggregation\n...ion\r\n\r\nMust subclass either InternalNumericMetricsAggregation.SingleValue or InternalNumericMetricsAggregation.MultiValue'
7056,'dadoonet','Plugins: Lucene version checker should use `Lucene.parseVersionLenient`\nWith commit 07c632a2d4dbefe44e8f25dc4ded6cf143d60e41, we now have a new Lucene.parseVersionLenient(String, Version) method which tries to find an existing Lucene version based on the two first digits X.Y of X.Y.Z String.'
7051,'colings86','Aggregations: fixed value count so it can be used in terms order\nCloses #7050'
7050,'colings86','Aggregations: Value Count Agg cannot be used for sort order\nIf a value count agg is used for the sort column of a terms aggregation an exception is thrown stating:\r\n\r\n``` \r\nReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ElasticsearchIllegalArgumentException[Invalid order path [grades_count]. Missing value key in [grades_count] which refers to a multi-value metric aggregation]; \r\n```\r\n\r\nMarvel commands to reproduce error: https://gist.github.com/colings86/4dfcb7de6c474ae69c32\r\n\r\nSetting order to:\r\n\r\n```\r\n"order": {\r\n    "grades_count": "desc"\r\n}\r\n```\r\n\r\nproduces the following error:\r\n\r\n```\r\n{\r\n   "error": "ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: ClassCastException[org.elasticsearch.search.aggregations.metrics.valuecount.InternalValueCount cannot be cast to org.elasticsearch.search.aggregations.metrics.InternalNumericMetricsAggregation$MultiValue]; ",\r\n   "status": 503\r\n}\r\n```\r\n'
7049,'clintongormley','Update span-not-query.asciidoc\nThe example given does not clearly explain what the query does. span_not query is unituitive by nature therefore a more clearer example as given above will be better.'
7042,'martijnvg',"[Discovery] add cluster name and cluster state version to fault detection pinging\nAdd the cluster name and cluster state version to NodesFaultDetection pings. This allows resolving the case where a old master node become unresponsive and later wakes up and pings all the nodes in the cluster, allowing the newly elected master to decide whether it should step down or ask the old master to rejoin.\r\n\r\nAdd the cluster name to MasterFaultDetection ping request, to verify that the master we're pinging is of the same cluster. This is useful where multiple test clusters run on the same network and adds another protection layer.\r\n\r\nTo simulate situations where this is helpful, a new test is added utilising a new  ClusterSchemeDisruption to long GCs on a cluster's master node.\r\n\r\nNote: this PR is against the improve zen brunch.\r\nNote2: DiscoveryWithNetworkFailuresTests class is renamed to DiscoveryWithServiceDisruptions and marks it as Slow"
7037,'martijnvg','Internal: Introduced the notion of a FixedBitSetFilter that guarantees to produce a FixedBitSet\nNested and parent/child rely on the fact that type filters produce a FixedBitSet, the RandomAccessFilter does this. By moving away from filter cache this filters will also never be evicted because of LRU reasons, which is what is desired for nested and parent/child.\r\n\r\nAlso if nested and parent/child is configured the type filters are eagerly loaded by default.\r\n\r\nPR for #7031'
7036,'dadoonet','Mapping: Add new `default` option for timestamp field\nIndex process fails when having `_timestamp` enabled and `path` option is set.\r\nIt fails with a `TimestampParsingException[failed to parse timestamp [null]]` message.\r\n\r\nReproduction:\r\n\r\n```\r\nDELETE test\r\nPUT  test\r\n{\r\n    "mappings": {\r\n        "test": {\r\n            "_timestamp" : {\r\n                "enabled" : "yes",\r\n                "path" : "post_date"\r\n            }\r\n        }\r\n    }\r\n}\r\nPUT test/test/1\r\n{\r\n  "foo": "bar"\r\n}\r\n```\r\n\r\nYou can now define a default value for when timestamp is not provided\r\nwithin the index request or in the `_source` document.\r\n\r\nBy default, the default value is `now` which means the date the document was processed by the indexing chain.\r\n\r\nYou can disable that default value by setting `default` to `null`. It means that `timestamp` is mandatory:\r\n\r\n```\r\n{\r\n    "tweet" : {\r\n        "_timestamp" : {\r\n            "enabled" : true,\r\n            "default" : null\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIf you don\'t provide any timestamp value, indexation will fail.\r\n\r\nYou can also set the default value to any date respecting timestamp format:\r\n\r\n```\r\n{\r\n    "tweet" : {\r\n        "_timestamp" : {\r\n            "enabled" : true,\r\n            "format" : "YYYY-MM-dd",\r\n            "default" : "1970-01-01"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIf you don\'t provide any timestamp value, indexation will fail.\r\n\r\nCloses #4718.'
7033,'clintongormley','Update search-template.asciidoc\nhello,\r\n\r\njust need to remove extra commas in template query ;-)\r\n\r\ngreat job with the 1.3 version!\r\n\r\nregards,\r\nchris.'
7031,'martijnvg','Nested and parent child filters should live outside of filter cache\nQuery and filters that rely on filters to be cached and have random access (FixedBitSet) to run fast should be kept around **outside** of the filter cache. The filter cache may evict these filters and nested and parent/child need then to be cached in order to be fast.\r\n\r\nThere should be separate service that keeps just FixedBitSet instance around. \r\n'
7028,'dadoonet','Version 1.3.0 error indexing document with icu_analyzer\nWhen I index a document I get this error\r\n\r\n```\r\njava.lang.NoClassDefFoundError: org/apache/lucene/util/AttributeSource$AttributeFactory\r\n        at org.apache.lucene.analysis.icu.segmentation.ICUTokenizer.<init>(ICUTokenizer.java:84)\r\n        at org.apache.lucene.analysis.icu.segmentation.ICUTokenizer.<init>(ICUTokenizer.java:71)\r\n        at org.elasticsearch.indices.analysis.IcuIndicesAnalysis$1.create(IcuIndicesAnalysis.java:59)\r\n        at org.elasticsearch.index.analysis.CustomAnalyzer.createComponents(CustomAnalyzer.java:83)\r\n        at org.apache.lucene.analysis.AnalyzerWrapper.createComponents(AnalyzerWrapper.java:102)\r\n        at org.apache.lucene.analysis.AnalyzerWrapper.createComponents(AnalyzerWrapper.java:102)\r\n        at org.apache.lucene.analysis.Analyzer.tokenStream(Analyzer.java:180)\r\n        at org.apache.lucene.document.Field.tokenStream(Field.java:554)\r\n        at org.apache.lucene.index.DefaultIndexingChain$PerField.invert(DefaultIndexingChain.java:597)\r\n        at org.apache.lucene.index.DefaultIndexingChain.processField(DefaultIndexingChain.java:342)\r\n        at org.apache.lucene.index.DefaultIndexingChain.processDocument(DefaultIndexingChain.java:301)\r\n        at org.apache.lucene.index.DocumentsWriterPerThread.updateDocuments(DocumentsWriterPerThread.java:258)\r\n        at org.apache.lucene.index.DocumentsWriter.updateDocuments(DocumentsWriter.java:412)\r\n        at org.apache.lucene.index.IndexWriter.updateDocuments(IndexWriter.java:1321)\r\n        at org.apache.lucene.index.IndexWriter.addDocuments(IndexWriter.java:1282)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.innerIndex(InternalEngine.java:555)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.index(InternalEngine.java:486)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:409)\r\n        at org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:195)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:527)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\nthis is the system out of the version\r\n\r\n```\r\n{\r\n  "status" : 200,\r\n  "name" : "Boom Boy",\r\n  "version" : {\r\n    "number" : "1.3.0",\r\n    "build_hash" : "1265b1454eee7725a6918f57415c480028700fb4",\r\n    "build_timestamp" : "2014-07-23T13:46:36Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.9"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n```'
7027,'clintongormley','Update scripting.asciidoc\na small change to correct the type of the script in an example. Groovy is used and mvel is stated.'
7026,'areek','Search: Add Exists API to check if any matching documents exist for a given query\nImplements a new Exists API allowing users to do fast exists check on any matched documents for a given query.\r\n\r\nThis API should be faster then using the Count API as it will:\r\n - early terminate the search execution once any document is found to exist\r\n - return the response as soon as the first shard reports matched documents\r\n\r\ncloses #6995'
7023,'javanna','Internal: Fixed filters execution order and fix potential concurrency issue in filter chains\nFix filters ordering which seems to be the opposite to what it should be (#7019). Added missing tests for rest filters.\r\n\r\nSolved concurrency issue in both rest filter chain and transport action filter chain (#7021).\r\n\r\nCloses #7019\r\nCloses #7021'
7021,'javanna','Internal: concurrency issue in rest and action filter chains\nA test failure (`TransportActionFilterChainTests#testTooManyContinueProcessing`) revealed a concurrency issue in both rest filter chain and action filter chain. We use a `volatile` int to keep track of the current position in the chain, which might get incremented from concurrent threads. `volatile` is not enough if the filters call `continueProcessing` concurrently.'
7019,'javanna',"Rest filters execution order doesn't reflect javadocs\n`RestFilter`s allow to configure their execution order through the `order` method. Their javadocs say:\r\n> Execution is done from lowest value to highest.\r\n\r\nThe filters are ordered and executed the opposite way though, from highest to lowest."
7016,'colings86','Geo: Fix comparison of doubles in ShapeBuilder.intersections()\nCode in question: https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/geo/builders/ShapeBuilder.java#L297-299\r\n\r\nThis code seems to compare the current point with the one after next and continue if their comparison value is equal.  The reason for doing this is not clear and there are no tests which rely on this code.\r\n\r\nThe compare method is not limited to return -1, 0 and 1 so the chance of this condition being met seems slim, further adding to the confusion as to what its purpose is.'
7010,'dakrone',"Look into adding a service that cleans Guava caches occasionally\nWhen an item has been expired from a Guava cache, the cache does not actually clean up the entry until periodic maintenance is performed. In the event that an entry is expired but the cache is no longer read or written to, it's possible that the entry can stick around longer than intended.\r\n\r\nWe should add a scheduled action to manually call `Cache.cleanUp()` every N minutes, where N could default to 30 minutes."
7008,'dadoonet','Total index memory in _cat/indices\nCurrently memory stats are available separately (which is good), but it not in accumulated metric like total memory used by index. Some `awk` could fix that, but having built-in ability would be great.\r\n\r\n```\r\n# curl -s http://es:9200/_cat/indices?h=index,totalMemory\r\nstatistics-20140620 45.2mb\r\n```\r\n\r\nAlso, bloom filter memory usage is not available in `_cat/indices`. Not sure if something else is missing. Is there a hope to see full memory usage per index?'
7001,'clintongormley','Add Drupal Search API Elasticsearch module\nThe module at drupal.org/project/elasticsearch has been abandoned. The Search API Elasticsearch module allows Drupal to use Elasticsearch as a backend for Search API.'
6995,'areek','Exists API: Allow user to find if any document exists for a given query\nThe idea behind the new API is to enable fast exists functionality for documents, given a query. It would be very similar to the existing count API.\r\n\r\nThis API will be faster then using the Count API (setting terminate_after=1) in the general case, as it would return the response after the first shard reports existing documents, instead of blocking for responses from all the shards. In the worst case, if no documents actually exists, it would be as performant as using the Count API with terminate_after=1.\r\n\r\n(Check corresponding PR for req/response)\r\n\r\nrelated to #6876 '
6994,'martijnvg','Within a sub-aggregation of a reverse_nested aggregation, cannot filter on nested fields\nI\'m unsure if this is a bug or something that wasn\'t intended to work. In the example below, I\'m trying to find the \'foo\' object that has the nested \'bar\' object named \'bar0\'. I then want to know how many nested \'baz\' objects the \'foo\' contains. After using reverse_nested I can use the first-level fields of \'foo0\', but not the nested fields.\r\n\r\nUsing my actual data this sometimes did work, which is why I thought there might be a bug here. So for example two root objects would contain a certain nested object (\'bar\'), but only one was returning a proper count of another nested object (baz). With my example I wasn\'t able to recreate this.\r\n\r\n    DELETE /_all\r\n    \r\n    PUT /foos\r\n    \r\n    PUT /foos/foo/_mapping\r\n    {\r\n      "foo": {\r\n        "properties": {\r\n          "bar": {\r\n            "type": "nested",\r\n            "properties": {\r\n              "name": {\r\n                "type": "string"\r\n              }\r\n            }\r\n          },\r\n          "baz": {\r\n            "type": "nested",\r\n            "properties": {\r\n              "name": {\r\n                "type": "string"\r\n              }\r\n            }\r\n          },\r\n          "name": {\r\n            "type": "string"\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\n    PUT /foos/foo/0\r\n    {\r\n      "bar": [\r\n        {\r\n          "name": "bar0"\r\n        },\r\n        {\r\n          "name": "bar1"\r\n        }\r\n      ],\r\n      "baz": [\r\n        {\r\n          "name": "baz0"\r\n        },\r\n        {\r\n          "name": "baz1"\r\n        }\r\n      ],\r\n      "name": "foo0"\r\n    }\r\n    PUT /foos/foo/1\r\n    {\r\n      "bar": [\r\n        {\r\n          "name": "bar2"\r\n        },\r\n        {\r\n          "name": "bar3"\r\n        }\r\n      ],\r\n      "baz": [\r\n        {\r\n          "name": "baz2"\r\n        },\r\n        {\r\n          "name": "baz3"\r\n        }\r\n      ],\r\n      "name": "foo1"\r\n    }\r\n    \r\n    #this should give a count of 2 under baz, but it\'s 0 instead\r\n    \r\n    POST /foos/foo/_search?pretty\r\n    {\r\n      "size": 0,\r\n      "aggs": {\r\n        "bar": {\r\n          "nested": {\r\n            "path": "bar"\r\n          },\r\n          "aggs": {\r\n            "bar_filter": {\r\n              "filter": {\r\n                "bool": {\r\n                  "must": {\r\n                    "term": {\r\n                      "bar.name": "bar0"\r\n                    }\r\n                  }\r\n                }\r\n              },\r\n              "aggs": {\r\n                "foo": {\r\n                  "reverse_nested": {},\r\n                  "aggs": {\r\n                    "name": {\r\n                      "terms": {\r\n                        "field": "name"\r\n                      },\r\n                      "aggs": {\r\n                        "baz": {\r\n                          "nested": {\r\n                            "path": "baz"\r\n                          }\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \r\n    # this works and give the right number of baz\r\n    POST /foos/foo/_search?pretty\r\n    {\r\n      "size": 0,\r\n      "aggs": {\r\n        "foo_filter": {\r\n          "filter": {\r\n            "bool": {\r\n              "must": {\r\n                "term": {\r\n                  "name": "foo0"\r\n                }\r\n              }\r\n            }\r\n          },\r\n          "aggs": {\r\n            "baz": {\r\n              "nested": {\r\n                "path": "baz"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }'
6990,'javanna','Internal: streamline use of IndexClosedException introduced with #6475\nSingle index operations to use the newly added IndexClosedException introduced with #6475. This way we can also fail faster when we are trying to execute operations on closed indices and their use is not allowed (depending on indices options). Indices blocks are still checked but we can throw error earlier on while resolving indices (MetaData#concreteIndices).\r\n\r\nEffectively this change also affects what we return when using one of the following apis: analyze, bulk, index, update, delete, explain, get, multi_get, mlt, term vector, multi_term vector. We now return: \r\n```\r\n{"error":"IndexClosedException[[test] closed]","status":403}\r\n``` \r\ninstead of \r\n```\r\n{"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}\r\n```\r\n\r\nCloses #6988'
6988,'javanna',"Internal: streamline use of IndexClosedException\nWith #6475 we introduced the use of a new `IndexClosedException` that gets thrown whenever we refer to closed indices within apis that don't allow them (behaviour can also be changed through indices options in some cases). We should streamline the use of this exception and use it within single index apis like index api, which currently return an index block during the execution while we could throw the newly added `IndexClosedException` instead."
6985,'colings86',"Aggregations: Better JSON output scoping\nBefore this change each aggregation had to output an object field with its name and write its JSON inside that object.  This allowed for badly behaved aggregations which could write JSON content in the root of the 'aggs' object.  this change move the writing of the aggregation name to a level above the aggregation itself, ensuring that aggregations can only write within there own scope in the JSON output.\r\n\r\nCloses #7004"
6982,'colings86','Aggregations: Fix JSON output of geo bounds aggregation\nThe geo bounds aggregation was not outputting the aggregation name and in the case where there was no data would output malformed JSON (extra } )'
6980,'colings86','Aggregations: Added pre and post offset to histogram aggregation\nAdded preOffset and postOffset parameters to the API for the histogram aggregation which work in the same way as in the date histogram\r\n\r\nCloses #6605'
6976,'colings86','Geo: Fixes parse error with complex shapes\nThe bug reproduces when the point under test for the placement of the hole of the polygon has an x coordinate which only intersects with the ends of edges in the main polygon. The previous code threw out these cases as not relevant but an intersect at 1.0 of the distance from the start to the end of an edge is just as valid as an intersect at any other point along the edge.  The fix corrects this and adds a test.\r\n\r\nCloses #5773'
6974,'colings86','Aggregations: Added Filters aggregation\nA multi-bucket aggregation where multiple filters can be defined (each filter defines a bucket). The buckets will collect all the documents that match their associated filter.\r\n\r\nThis aggregation can be very useful when one wants to compare analytics between different criterias. It can also be accomplished using multiple definitions of the single filter aggregation, but here, the user will only need to define the sub-aggregations only once.\r\n\r\n(this is a continuation of Pull Request #6119)\r\n\r\nCloses #6118,#6119'
6965,'jpountz','[date_histogram aggregation] OutOfMemoryError when using "time_zone" attribute\nElasticsearch version : 1.2.2\r\nOperation : date_histogram aggregation\r\n\r\nIn date_histogram aggregation [(documentation)](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#search-aggregations-bucket-datehistogram-aggregation), when I use the "time_zone" attribute, I have an OutOfMemoryError (see response below).\r\n\r\n```\r\n{\r\n   "error": "ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: OutOfMemoryError[Java heap space]; ",\r\n   "status": 503\r\n}\r\n```\r\n\r\nThe complete stacktrace is: \r\n\r\n```\r\n[2014-07-22 19:01:15,800][DEBUG][action.search.type       ] [Briquette] failed to reduce search\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] \r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:140)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$1.onResult(TransportSearchQueryThenFetchAction.java:112)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$1.onResult(TransportSearchQueryThenFetchAction.java:106)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:526)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.OutOfMemoryError: Java heap space\r\n\tat org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.createBucket(InternalDateHistogram.java:146)\r\n\tat org.elasticsearch.search.aggregations.bucket.histogram.InternalDateHistogram.createBucket(InternalDateHistogram.java:36)\r\n\tat org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:324)\r\n\tat org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:140)\r\n\tat org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:545)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:151)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:138)\r\n\t... 6 more\r\n```\r\n\r\nQuery detail : \r\n\r\n\r\n```\r\nPOST index/type/_search\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      }\r\n    }\r\n  },\r\n  "aggregations": {\r\n    "tendance": {\r\n      "date_histogram": {\r\n        "field": "date_creation",\r\n        "interval": "month",\r\n        "min_doc_count": 0,\r\n        "time_zone": 1,\r\n        "extended_bounds": {\r\n          "min": "2013-01-01T00:00:00.000+01:00",\r\n          "max": "2013-01-31T23:59:59.999+01:00"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWith ```"time_zone": "Europe/Paris"```, I have the same error.\r\nWith ```"time_zone": 0```, everything is good and the query tooks 19ms.\r\n\r\nThe document mapping is:\r\n\r\n```\r\n"mappings":{  \r\n      "marche_prive":{  \r\n         "properties":{  \r\n            "date_creation":{  \r\n               "include_in_all":false,\r\n               "store":true,\r\n               "format":"dateOptionalTime",\r\n               "type":"date"\r\n            }\r\n         }\r\n      }\r\n   }\r\n```\r\nI haven\'t any ideas of what happens! '
6961,'clintongormley','Add new entry for the Elasticsearch SIREn plugin\nThe link to a new plugin for Elasticsearch.'
6960,'colings86','Aggregations: Change to default shard_size in terms aggregation\nThe default shard size in the terms aggregation now uses BucketUtils.suggestShardSideQueueSize() to set the shard size if the user does not specify it as a parameter.\r\n\r\nCloses #6857'
6958,'brwe',"Unable to configure eager fielddata loading for _timestamp field via PUT mapping API for type with existing mapping\nWhen creating a new index, if I have eager fielddata loading configured for the `_timestamp` field, that sticks.\r\n\r\nHowever when updating this setting via the PUT mapping API, although the request is acknowledged when I check the mapping via GET, fielddata configuration for that field is blank.\r\n\r\nI have been able to successfully configure it for other fields via the PUT mapping API, though. Seems like this problem may be specific to the special underscore-prefixed fields, though I've only really tried `_timestamp`."
6957,'dakrone',"Docs: Reflect that 'field_value_factor' is only in 1.2.x\nWhile the blogpost http://www.elasticsearch.org/blog/2014-04-02-this-week-in-elasticsearch/ states, that feature #5519 was added to 1.x, the release notes for, e.g. v1.1.2, however tell otherwise.\r\nOnly the release notes for 1.2.0 list #5519 as a new feature.\r\n\r\nSince the 1.x docs deprecate/discourage from using `_boost`, and seemingly give a migration example at\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html#function-score-instead-of-boost\r\nusers of 1.1.x should be warned."
6955,'brwe','Make function scores functions tunable\nWith the `function_score` query, each function can return a range of values: the decay functions return a value between 0 and 1, and `field_value_factor` function can return any value but with (eg) logarithmic functions a typical score is between 0 and 3, the `random_score` function currently returns a very large number (but see #6907), and the `script_score` can return whatever value you calculate.\r\n\r\nIt isn\'t easy to tune the contribution of each function.  If you have two decay clauses: one for location and one for price, you can\'t easily say that "location is more important than price".\r\n\r\nWhat about making each function accept the `boost_factor` parameter which is multiplied with the output of the function?'
6939,'colings86','Mapping: Geopoint with array broken (dynamic mapping): geo_point expected\nI use Elasticsearch with Logstash and dynamic mapping.\r\nI my logs, I have geopoints with the array syntax.\r\nIt was OK with ES 1.0.X, but it\'s broken with ES 1.2.2.\r\n\r\nTemplate:\r\n```\r\ncurl -XPUT localhost:9200/_template/logstash -d \'{\r\n    "template": "logstash_*",\r\n    "settings" : {\r\n        "refresh_interval": "30s"\r\n    },\r\n    "mappings": {\r\n        "logs": {\r\n            "_all" : {\r\n                "enabled": false\r\n            },\r\n            "dynamic_templates": [\r\n                {\r\n                    "location": {\r\n                        "match": "location*",\r\n                        "mapping": {\r\n                            "type": "geo_point"\r\n                        }\r\n                    }\r\n                },\r\n                {\r\n                    "generic": {\r\n                        "match": "*",\r\n                        "match_mapping_type": "string",\r\n                        "mapping": {\r\n                            "type": "string",\r\n                            "index": "not_analyzed"\r\n                        }\r\n                    }\r\n                }\r\n            ],\r\n            "dynamic_date_formats": [\r\n                "dateOptionalTime",\r\n                "yyyy-MM-dd",\r\n                "yyyy-MM-dd HH:mm:ss"\r\n            ]\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nDelete index:\r\n```\r\ncurl -XDELETE localhost:9200/logstash_test\r\n```\r\n\r\nTry to add a doc/log:\r\n```\r\ncurl -XPOST localhost:9200/logstash_test/logs -d \'{\r\n  "location_array": [\r\n    2.3069244,\r\n    48.8881598\r\n  ]\r\n}\'\r\n```\r\n\r\nIt fails with this message:\r\n```\r\n[2014-07-21 11:31:49,168][INFO ][cluster.metadata         ] [fr-dev-01] [logstash_test] creating index, cause [auto(index api)], shards [5]/[1], mappings [logs]\r\n[2014-07-21 11:31:49,465][DEBUG][action.index             ] [fr-dev-01] [logstash_test][1], node[zzMEQe9JSS6qEgw0oRgdVA], [P], s[STARTED]: Failed to execute [index {[logstash_test][logs][PhqP9fpgQkS4tHfOs105GQ], source[{"location_array":[2.3069244,48.8881598]}]}]\r\norg.elasticsearch.index.mapper.MapperParsingException: failed to parse\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:536)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:373)\r\n\tat org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:534)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:433)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: org.elasticsearch.ElasticsearchParseException: geo_point expected\r\n\tat org.elasticsearch.common.geo.GeoUtils.parseGeoPoint(GeoUtils.java:421)\r\n\tat org.elasticsearch.index.mapper.geo.GeoPointFieldMapper.parse(GeoPointFieldMapper.java:530)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parseDynamicValue(ObjectMapper.java:819)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:639)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.serializeArray(ObjectMapper.java:625)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:482)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)\r\n\t... 8 more\r\n```\r\n\r\nIf I insert a doc with a geopoint as an object, it works, and the mapping is created dynamically.\r\nAfter that, I can insert a doc with a geopoint as an array without error.'
6937,'martijnvg','Core: Add extra write consistency validation\nAdd additional optional validation on top of the write consistency check. After the cluster state write validation check we have today, an optional extra write consistency validation may happens that actually goes the nodes containing the shard copies and verifies if that nodes are in a started state.\r\n\r\nNote: this PR is based on improve_zen branch'
6933,'javanna','Internal: expose the indices names every action relates to if applicable\nIf a request relates to indices, expose which ones it relates to in a generic manner.'
6928,'martijnvg','Settings: Add a `index.query.parse.allow_unmapped_fields` setting that fails if queries refer to unmapped fields.\nThe percolator and filter parsing for aliases should forcefully enforce strict query parsing.\r\n\r\nStrict parsing for percolator and filter alias parsing is only enforced on indices created after to upgrade to `1.4.0`\r\n\r\nPR for #6664'
6927,'areek','PhraseSuggester: Collate option should allow returning phrases with no matching docs\nCurrently `collate` option in `PhraseSuggester` only filters out phrases that did not match any documents. It would be useful if it also allowed returning all the generated phrases with  some indicator to whether there were any doc matches for the entries.\r\n\r\nRelated #3482'
6925,'mikemccand',"Settings: Allow `index.merge.scheduler.max_thread_count` to be dynamically changed\nLucene allows the max_thread_count to be updated, but this wasn't\r\nfully exposed in Elasticsearch.\r\n\r\nCloses #6882"
6920,'dadoonet','Qualify termQuery method under the Boolean Query section\nAs static keyword was not used when importing QueryBuilders, the termQuery method needs properly qualified when being passed to the must/mustNot methods in the Boolean Query section of the documentation'
6910,'electrical',"Update as-a-service.asciidoc\nThere is a step missing where the auto-start on boot isn't enabled for RPM based installs."
6907,'rjernst','Random score order changes on doc updates\nNot sure how the random score is calculated, but it seems like the docs positions are changing on updates.\r\nOur docs are continuously getting updates in some cron-jobs or by user actions, thus the pagination is not really possible on random scored lists.\r\n\r\nWhat exactly is elasticsearch using to generate random score besides the provided seed?\r\nWhy not just taking the doc UID & the seed to calculate the score?\r\n\r\nBtw. there was a post a while ago on your forums:\r\nhttps://groups.google.com/forum/#!topic/elasticsearch/QOP3kSK5qR0'
6903,'s1monw','Java API Mapping Annotations\nIt would be very helpful to have a set of Java annotations used to define mappings.\r\n\r\nRight now the only option is to create a json using json builder, but that is cumbersome job especially since there are no good code examples.\r\n\r\nThe idea with annotation is to have all required annotations to define ES type mappings. These annotations would be defined on a POJO beans. \r\n\r\nBeside these annotation it would be good to have a classpath scanner that would scan for those annotated classes and create defined mappings. This scanner can also have an option (like many ORM tools have) to either CREATE (create mappings if they do not exist) or REINDEX (to re-index documents if mapping merge is not possible).\r\n\r\nPlease let me know if you are interested in this feature, since I am interested in contributed a code for it.\r\n'
6900,'jpountz','ArrayIndexOutOfBoundsException when using 2-level terms aggregation\nHi, I\'m using ES 1.0.0.\r\nAnd when using 2-level terms aggregation. In my response I get some failures consistently:\r\n         {\r\n            "index": "blah",\r\n            "shard": 8,\r\n            "status": 500,\r\n            "reason": "ArrayIndexOutOfBoundsException[0]"\r\n         },\r\nI\'m wondering what could be the reason for these errors? and does it imply that I may not be getting all the values?'
6894,'s1monw',"Unknown node version should be a lower bound\nToday when we start a `TransportClient` we use the given transport addresses and create a `DiscoveryNode` from it without knowing the actual nodes version. We just use the `Version.CURRENT` which is an upper bound. Yet, the other node might be a version less than the currently running and serialisation of the nodes info might break. We should rather use a lower bound here which is the version of the first release with the same major version as `Version.CURRENT` since this is what we officially support. \r\n\r\nWe changed the format of the `NodesInfo` serialisation today and BWC tests broken on that. Yet we found a away to work around changing it but in the future we should be able to change transport protocol even if it's `NodesInfo`\r\n\r\nNote: this is not a problem until today but in the future this might prevent us from enhancing the protocol here."
6893,'colings86','Aggregations: histo "interval" should allow coercion from string\nThis fails with `Unexpected token VALUE_STRING in aggregation [histo]` because "50" is passed as a string:\r\n\r\n\r\n    curl -XGET \'http://localhost:9200/_search?pretty=1\' -d \'\r\n    {\r\n       "aggs" : {\r\n          "histo" : {\r\n             "histogram" : {\r\n                "interval" : "50",\r\n                "field" : "number"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\nPerl won\'t guarantee that a number is a number and not a string.\r\n\r\n'
6885,'areek','Search: Add an option to early terminate document collection when searching/counting\nThe idea is to add an option which will let the user control the number of matched documents collected per shard before scoring (if applicable). This will be helpful for exists-type functionality or when not all matched document has to be scored.\r\n\r\nCloses #6876 '
6882,'mikemccand',"Log when dynamically changing merge thread pool size\nI tried to dynamically set ```index.merge.scheduler.max_thread_count``` and it looked as though nothing happened.  No log or anything.  It looks like that is supposed to be possible in ConcurrentMergeSchedulerProvider.java:161 but it wasn't working."
6880,'GaelTadh','[FIX] normalize serialization of ScriptType in UpdateRequest\n'
6876,'areek','Add option to Count API to terminate early if a specified count is reached\nIt would be nice if the Count API had an option to terminate once a specified count has been reached. This would be a nice enhancement if the purpose of the count request is just to figure out if matching documents exists given a query (as is the case for collate option in PhraseSuggester #3482)'
6867,'dadoonet','Mapping: Add multi_field support for Mapper externalValue (plugins)\nIn context of mapper attachment and other mapper plugins, when dealing with multi fields, sub fields never get the `externalValue` although it was set.\r\n\r\nRelated to https://github.com/elasticsearch/elasticsearch-mapper-attachments/issues/57\r\n\r\nCloses #5402.'
6860,'javanna','Internal: Make transport action name available in TransportAction base class\nEach transport action is associated with at least an action name, which is the action name that gets serialized together with the request and identifies what to do with the request itself. Also, the action name is the name of the registered transport handler that handles incoming request for the transport action.\r\n\r\nThis PR makes the action name available in a generic manner in the TransportAction base class, so that it can be used when needed by subclasses, or in the base class for instance for action filtering.'
6857,'colings86','Aggregations: Better shard_size default for terms aggregation\nFor the Terms Aggregation the shardSize currently defaults to size.  This is not a particularly good default value and also is not inline with the defaults set by the significant terms and geohash grid aggregaitons.  We should change the defaults for the terms aggregation to use BucketUtils.suggestShardSideQueueSize() to be consistent and provide a more useful default'
6854,'areek','Expose IndexWriter and VersionMap RAM usage\nImplements exposing the IndexWriter and VersionMap RAM usage added in #6443 to the _cat endpoint. \r\n\r\nCloses #6483'
6852,'rjernst','AbstractDoubleSearchScript params does not support any kind of Object\nHello,\r\n\r\nI\'m trying to pass an Object inside the parameters Map in a native script. When I\'m accesing it inside the script I have an exception "java.lang.String can not be casted to yourObjectClassType" even the Object implements Serializable.\r\n\r\n```java\r\nQueryBuilder query = QueryBuilders.customScoreQuery( QueryBuilders.matchAllQuery() )\r\n\t.lang("native")\r\n\t.script("Test")\r\n\t.param("testobject", new TestObject() );\r\n```\r\n\r\n```java\r\npackage com.imagenii.elasticsearch.scripts;\r\n\r\nimport  com.imagenii.elasticsearch.scripts.TestObject;\r\nimport java.util.Map;\r\n\r\nimport org.elasticsearch.common.Nullable;\r\nimport org.elasticsearch.script.AbstractDoubleSearchScript;\r\n\r\npublic class Test extends AbstractDoubleSearchScript \r\n{\r\n\tpublic Test(@Nullable Map<String,Object> params) \r\n\t{\r\n\t\ttry\r\n\t\t{\r\n\t\t\tTestObject testobject= (TestObject)params.get("testobject");\r\n\r\n\t\t}catch(Exception e)\r\n\t\t{\r\n\t\t\tSystem.out.println(e.getMessage());\r\n\t\t}\r\n\t}\r\n\r\n\t@Override\r\n    public double runAsDouble() \r\n    {\r\n\t\treturn 0.0;\r\n    }\r\n}\r\n```\r\n\r\n```java\r\npackage com.imagenii.elasticsearch.scripts;\r\n\r\nimport java.io.Serializable;\r\n\r\npublic class TestObject implements Serializable\r\n{\r\n\tprivate static final long serialVersionUID = 1L;\r\n\t\r\n\tpublic String say = "Hello";\r\n}\r\n\r\n```'
6851,'GaelTadh','Scripting: Docs for the indexed templates/script features\nPull Request #5921 adds new functionality for storing and retrieving indexed scripts and templates from a .scripts index. We need to update the documentation to reflect this new feature.\r\n'
6848,'clintongormley','Controlled repeat filter\nAdds filter that repeats every token it recieves a configured number of times.\r\nThat number of times is the integer value of the first token it receives.\r\n\r\nCloses #6847'
6830,'colings86','Aggregations: Fixed Histogram key_as_string bug\nThe key as string field in the response for the histogram aggregation will now only show if format is specified on the request.\r\n\r\nCloses #6655'
6829,'javanna','Transport Client: fixed the node retry mechanism which could fail without trying all the connected nodes\nThe RetryListener was notified twice for each single failure, which caused some additional retries, but more importantly was making the client reach the maximum number of retries (number of connected nodes) too quickly, meanwhile ongoing retries which could succeed are not completed yet.\r\n\r\nThe `TransportService` already notifies the listener of any exception received from a separate thread through the request holder, no need to notify the retry listener again in any other place (either catch or `onFailure` method itself).'
6827,'clintongormley',"script_field leaks value on failure\n**Seen on v1.2.1**\r\n\r\nI'm new to MVEL and below is the script I was using to find the lowest value in an array like structure:\r\n\r\n```mvel\r\nset = doc[field].values;\r\nlowest = set[0];\r\n\r\nfor (candidate : set) {\r\n    lowest = min(lowest, candidate);\r\n}\r\n\r\nreturn lowest;\r\n```\r\n\r\nIt worked perfectly (or so I thought) always returning the minimum, or a null (if the array was empty or the field was missing).\r\n\r\nHowever I started to notice something a bit strange. While the nulls occurred as expected at the start of the result set, as soon as a real result was found, any subsequent results that should have contained null, now held the last good result, and weirder still, as a string (all good results were ints).\r\n\r\nOnce I realised what was happening I added a guard clause to my script solving the issue for me.\r\n\r\n```mvel\r\nif (set.size() < 1) {\r\n  return null;\r\n}\r\n```\r\n\r\nMy assumption is that the out of bounds array access failed silently and somehow the last known good result got leaked into the current hit. In the event that failing silently is intentional behaviour, the value certainly shouldn't leak."
6824,'clintongormley','Changing ES_MAX_MEM default from \'1gb\' to \'1g\'\nIf you set ES_HEAP_SIZE to \'1gb\' as suggested, Java will yield an "Invalid initial heap size".'
6823,'clintongormley',"Command to add apt-key does not work under sudo\nThis command, as given on the instructions for using the Elasticsearch repositories (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html), only works as root.\r\n\r\n```\r\nwget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -\r\n```\r\n\r\nA user using non-passwordless sudo must modify the flags to wget so as to properly respond to the sudo password prompt, as well as use sudo for the apt-key portion of the command, ala:\r\n\r\n```\r\nwget -qO - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add -\r\n```\r\n\r\nSince it's generally regarded as bad practice to run as root, I'd argue that the command included in the instructions should be the version that works with sudo. Or at the very least, there should be a note that the command needs to be run as root. The error message encountered when running as a non-root user is difficult to read and unhelpful at best.\r\n\r\n```\r\n$ wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -\r\n--2014-07-10 20:30:29--  http://packages.elasticsearch.org/GPG-KEY-elasticsearch\r\nResolving packages.elasticsearch.org (packages.elasticsearch.org)... 54.231.2.113\r\nConnecting to packages.elasticsearch.org (packages.elasticsearch.org)|54.231.2.113|:80... ERROR: This command can only be used by root.\r\nconnected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 1768 (1.7K) [binary/octet-stream]\r\nSaving to: ‘STDOUT’\r\n\r\n 0% [                                                                                                                                            ] 0           --.-K/s   in 0s\r\n\r\n\r\nCannot write to ‘-’ (Broken pipe).\r\n```\r\n\r\nHappy to submit a pull request if it makes sense.\r\n\r\n"
6822,'jpountz','Update API: Detect noop updates\nSome source systems (like mine) don\'t have an easy way to check if an update is actually a noop.  You could implement this check using update scripts but its a bit jangly to do generically.  It\'d be way more convenient if you could have Elasticsearch do the detection for you.  Something like:\r\n```js\r\ncurl -XPOST \'localhost:9200/test/type1/1/_update\' -d \'{\r\n    "doc" : {\r\n        "name" : "new_name"\r\n    },\r\n    "doc_as_upsert" : true,\r\n    "detect_noop": true\r\n}\'\r\n```\r\nDetecting the noop rather then just performing the update could prevent extra writes and deleted documents.\r\n\r\nSecondarily, it might be cool to be able to specify tolerances for numeric fields so small changes could be ignored but that might be out of scope for any initial implementation.'
6817,'dadoonet',"bin/plugin removes itself\nIf you call `bin/plugin --remove es-plugin` the plugin got removed but the file `bin/plugin` itself was also deleted.\r\n\r\nWe now don't allow the following plugin names:\r\n\r\n* elasticsearch\r\n* elasticsearch-service-x86.exe\r\n* plugin\r\n* elasticsearch-service-mgr.exe\r\n* elasticsearch.bat\r\n* plugin.bat\r\n* elasticsearch-service-x64.exe\r\n* elasticsearch.in.sh\r\n* service.bat\r\n\r\nCloses #6745"
6814,'colings86','Aggregations: Fixed DateHistogramBuilder to accept preOffset and postOff...\n...set as Strings\r\n\r\nThis is what DateHistogramParser expects so will enable the builder to build valid requests using these variables.\r\nAlso added tests for preOffset and postOffset since these tests did not exist\r\n\r\nCloses #5586'
6808,'s1monw','Resiliency: Recovering replicas might get stuck in initializing state \nIf a primary fails while a replica starts recovery but has not yet initialized the recovery process the replica will retry until the primary is allocated again on the node. This never happens and the replica gets stuck in INITIALIZING state and will never cleaned up.'
6807,'clintongormley','Update plugins.asciidoc to fix typo\nChanged the name of the European Environment Agency (from European Environmental Agency)'
6806,'martijnvg','Percolate performance difference in 1.0.0 and 1.2.2\nHi,\r\n\r\nI was doing some test with percolate and I noticed huge difference in performance of percolate query between 1.0.0 and 1.2.2. In 1.0.0 indexing of percolate queries takes much longer than in 1.2.2 (around 10 times), but percolate query is much faster. With 400k queries I get tens of ms in 1.0.0 to percolate document and couple of seconds in 1.2.2. I performed all tests with default settings always on clean ES instance. Percolate document looks like:\r\n\r\n    {\r\n    "query": {\r\n       "filtered": {\r\n          "filter": {   \r\n             "and": [      \r\n                {\r\n                   "geo_distance": {\r\n                      "distance": "94km",\r\n                      "location": {\r\n                         "lat": "97",\r\n                         "lon": "-76"\r\n                      }       \r\n                   }       \r\n                }       \r\n             ]       \r\n          },\r\n          "query": {\r\n             "match": {\r\n                "_all": "note father surprise"\r\n             }       \r\n          }       \r\n       }       \r\n    },\r\n    "type": "offer"\r\n    }\r\n   \r\nand percolate query:\r\n\r\n    GET /test-index/offer/_percolate\r\n    {\r\n        "doc": {\r\n            "name": "note",\r\n            "location": [-7,-80]\r\n        }\r\n    }\r\n\r\nHas something changes with default setting of ES between these versions regarding percolate? I couldn\'t find anything in release notes.'
6804,'jpountz','Add an option to create "other" bucket for Terms aggregation\nWhen using "terms" aggregation, it\'s often useful to get top X terms (achieved by using `size` parameter), but as well get a separate bucket for all other terms together (possibly constrained by minimum doc count). \r\n\r\nThe query syntax might be:\r\n```\r\n{\r\n    "aggs" : {\r\n        "tags" : {\r\n            "terms" : { \r\n                "field" : "tag",\r\n                "size": 3,\r\n                "min_doc_count": 10,\r\n                "other": "_other_terms"\r\n             }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAnd the response might look like:\r\n```\r\n{\r\n    ...\r\n    "aggregations" : {\r\n        "tags" : {\r\n            "buckets" : [\r\n                {\r\n                    "key" : "soccer",\r\n                    "doc_count" : 500\r\n                },\r\n                {\r\n                    "key" : "hockey",\r\n                    "doc_count" : 400\r\n                },\r\n                {\r\n                    "key" : "basketball",\r\n                    "doc_count" : 300\r\n                },\r\n                {\r\n                    "key" : "_other_terms",\r\n                    "doc_count" : 150\r\n                },\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThe `_other_terms` bucket will be based on all tags with doc_count > 10 per tag, excluding already listed (top 3).\r\n\r\nRelated to #5324'
6795,'kimchy','Security: Disable JSONP by default\nBy default, disable the option to use JSONP in our REST layer'
6792,'areek',"Remove Lucene's deprecated PatternAnalyzer\nInstead of using the PatternAnalyzer, the functionality was replicated by using Lucene's StopFilter, PatterTokenizer and LowerCaseFilter\r\n\r\nCloses #6717"
6790,'GaelTadh','Bulk API: Do not fail whole request on closed index\nThe bulk API request was marked as completely failed,\r\nin case a request with a closed index was referred in\r\nany of the requests inside of a bulk one.\r\n\r\nImplementation Note: Currently the implementation is a bit more verbose in order to prevent an `instanceof` check and another cast - if that is fast enough, we could execute that logic only once at the beginning of the loop (thinking this might be a bit overoptimization here).\r\n\r\nCloses #6410'
6789,'kimchy',"Internal: Upgrade to Jackson 2.4.1.1\nI ran into this one https://github.com/FasterXML/jackson-core/issues/145, which doesn't seem to happen in 2.3.3. Lets see how the investigation proceeds here, but potentially we need to revert the upgrade before releasing 1.3."
6782,'markharwood','Aggs: filtering values using array of values\nIn facets, we can filter a Terms Facet using an [array of values](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_excluding_terms):\r\n\r\n```json\r\n{\r\n    "query" : {\r\n        "match_all" : { }\r\n    },\r\n    "facets" : {\r\n        "tag" : {\r\n            "terms" : {\r\n                "field" : "tag",\r\n                "exclude" : ["term1", "term2"]\r\n            }\r\n        }\r\n    }\r\n}\r\n``` \r\n\r\nIn aggs, we can\'t use the same syntax anymore as [IncludeExclude](https://github.com/elasticsearch/elasticsearch/blob/1.2/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/support/IncludeExclude.java#L141-155) does not support arrays.\r\n\r\nSame apply for include.\r\n\r\nWe can probably use `|` character to separate terms but it could be handy to be able to specify directly an array of terms.\r\n\r\ncc @jpountz '
6781,'bleskes','Internal: Fixes Logger class for BackgroundIndexer\nFixes https://github.com/elasticsearch/elasticsearch/issues/6780'
6778,'colings86','Aggregations: Added an option to show the upper bound of the error for the terms aggregation\n...he terms aggregation.\r\n\r\nThis is only applicable when the order is set to _count.  The upper bound of the error in the doc count is calculated by summing the doc count of the last term on each shard which did not return the term.  The implementation calculates the error by summing the doc count for the last term on each shard for which the term IS returned and then subtracts this value from the sum of the doc counts for the last term from ALL shards.\r\n\r\nCloses #6696'
6777,'javanna',"Internal: Don't replace indices within ActionRequest and check blocks against concrete indices\nIf the request is being executed on the master node, there is no need to replace its content with the concrete indices obtained by calling MetaData#concreteIndices. Leave the request as it is and use the concrete indices to perform the requested operation (commit #1).\r\n\r\nConcrete indices is now called multiple times when needed instead of changing what's inside the incoming request with the concrete indices. Ideally we want to keep the original aliases or indices or wildcard expressions in the request (commit #2).\r\n\r\nAlso made sure that the check blocks is done against the concrete indices, which wasn't the case for delete index, delete mapping, open index, close index, types exists and indices exists (commit #2).\r\n\r\nNote: I think replacing the indices within the request is a bad pattern and I would love to find a way to make the indices final somehow, so that we don't make the same mistake again in the future. On the other hand, requests are exposed through java api and serialized over the wire, which both make it hard to make the indices field final and to remove the setter, which needs to be exposed to the request builders. I thought about making the setter package private, but that wouldn't help as the transport action is usually in the same package and the same could happen again. Ideas are welcome here!"
6774,'clintongormley',"Added a little note to scroll docs\nIt wasn't so clear the first time around. Thought it would help a little. Likewise, this section might also need the same edits: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-search-type.html#scan."
6773,'areek','Phrase Suggester: Add option to filter out phrase suggestions\n\r\nThe newly added filter option will let the user provide a template query which will be executed for every\r\nphrase suggestions generated to ensure that the suggestion matches at least one document for the query.\r\nThe filter query is only executed on the local node for now. When the new filter option is used, the\r\nsize of the suggestion is restricted to 20.\r\n\r\nCloses #3482'
6764,'clintongormley','Update filtered-query.asciidoc\n'
6758,'s1monw','Internal: Separate parsing implementation from setter in SearchParseElement\nThis, in order to allow reuse of parsing logic by plugins or other internal parts.\r\n\r\nCloses #3602'
6754,'clintongormley','New entry/front-end\nAdding, Calaca, a simple search client for Elasticsearch.'
6752,'dadoonet','Windows: Modify command window title (windows)\nWhen launching multiple nodes in MS Windows env, batch command window does not come by default with any title.\r\n\r\nThis patch add `Elasticsearch VERSION` as a title, where VERSION is the current elasticsearch version.\r\n\r\nCloses #6336'
6750,'clintongormley','[DOCS] YAML mappings use colons, not equals sign\nYAML  mappings use a colon and space (`: `) to mark each key: value pair, not the equals sign (`=`)\r\n\r\nAlso,\r\n* `elasticsearch.yml` is a file, not a directory\r\n* modify the AsciiDoc language attribute'
6746,'clintongormley','Adding Tiki Wiki CMS Groupware\n'
6745,'dadoonet','Plugins: bin/plugin removes itself\nI was experimenting a bit with plugins and created a test plugin and called it "es-plugin". Maybe this was not the best choice for a name but i did it.\r\n\r\nIf you call ``bin/plugin --remove es-plugin`` the plugin got removed but the file ``bin/plugin`` itself was also deleted.'
6741,'bleskes','Internal: Add a best effort waiting for ongoing recoveries to cancel on close\nCurrently one can close the engine while there are still on going recoveries. This is not a problem because the engine is close in tandem with the shard it belongs to, which in turn cancels the recoveries. This does cause some issues in our tests as we check the no resources were left behind after an index was deleted, which trips if the recoveries are not canceled.'
6739,'dakrone',"Circuit Breaker: Add HierarchyCircuitBreakerService\nAdds a breaker for request BigArrays, which are used for parent/child\r\nqueries as well as some aggregations. Certain operations like Netty HTTP\r\nresponses and transport responses increment the breaker, but will not\r\ntrip.\r\n\r\nThis also changes the output of the nodes' stats endpoint to show the\r\nparent breaker as well as the fielddata and request breakers.\r\n\r\nThere are a number of new settings for breakers now:\r\n\r\n`indices.breaker.total.limit`: starting limit for all memory-use breaker,\r\ndefaults to 70%\r\n\r\n`indices.breaker.fielddata.limit`: starting limit for fielddata breaker,\r\ndefaults to 60%\r\n`indices.breaker.fielddata.overhead`: overhead for fielddata breaker\r\nestimations, defaults to 1.03\r\n\r\n(the fielddata breaker settings also use the backwards-compatible\r\nsetting `indices.fielddata.breaker.limit` and\r\n`indices.fielddata.breaker.overhead`)\r\n\r\n`indices.breaker.request.limit`: starting limit for request breaker,\r\ndefaults to 40%\r\n`indices.breaker.request.overhead`: request breaker estimation overhead,\r\ndefaults to 1.0\r\n\r\nThe breaker service infrastructure is now generic and opens the path to\r\nadding additional circuit breakers in the future.\r\n\r\nFixes #6129"
6738,'clintongormley','Fixed query in the example for the histogram aggs page\nIn the query example added missing "filter" key around the actual filter. Closes #6737'
6737,'jpountz','Wrong query example for histogram aggregations doc\nOn the *histogram aggregations* documentation page example has an incorrect query. The filter in a filtered query should be under "filter" key inside "filtered", but it appears under top "filtered" key.\r\nHow it\'s now:\r\n`\r\n"filtered" : { "range" : { "price" : { "to" : "500" } } }\r\n`\r\n\r\nHow it should be:\r\n`\r\n"filtered" : { "filter": { "range" : { "price" : { "to" : "500" } } } }\r\n`'
6735,'dadoonet',"Removing plugin does not fail when plugin dir is read only\nIf you try to remove a plugin in read only dir, you get a successful result:\r\n\r\n```\r\n$ bin/plugin --remove marvel\r\n-> Removing marvel\r\nRemoved marvel\r\n```\r\n\r\nBut actually the plugin has not been removed.\r\n\r\nWhen installing, if fails properly:\r\n\r\n```\r\n$ bin/plugin -i elasticsearch/marvel/latest\r\n-> Installing elasticsearch/marvel/latest...\r\n\r\nFailed to install elasticsearch/marvel/latest, reason: plugin directory /usr/local/elasticsearch/plugins is read only\r\n```\r\n\r\nThis change throw an exception when we don't succeed removing the plugin.\r\n\r\nCloses #6546."
6724,'s1monw','DocumentMissingException is also thrown on update retries\n- Closes #6355.\r\n- Contains the bugfix and an integration test demonstrating the bug and validating the fix.\r\n- The fix in TransportUpdateAction is a simple try-catch block calling the appropriate failure handler. The fix prevents DocumentMissingExceptions from ending up uncaught and thus makes sure that an HTTP response is actually sent to the client instead of not sending any at all.'
6722,'s1monw','Search: Replace empty bool queries with match_all to prevent NullPointerExceptions\nI have setup a parent/child relationship where parent is optional and i handle the routing in that case. And this worked fine in 1.0.1 but when i bumped to 1.2.1 to use aggregations it stopped working.\r\n\r\nThe exception i get\r\n```java\r\n[2014-07-04 07:59:30,333][INFO ][node                     ] [Tantra] version[1.2.1], pid[23438], build[6c95b75/2014-06-03T15:02:52Z]\r\n[2014-07-04 07:59:30,334][INFO ][node                     ] [Tantra] initializing ...\r\n[2014-07-04 07:59:30,339][INFO ][plugins                  ] [Tantra] loaded [], sites []\r\n[2014-07-04 07:59:32,581][INFO ][script                   ] [Tantra] compiling script file [/etc/elasticsearch/scripts/activity-period.mvel]\r\n[2014-07-04 07:59:33,016][INFO ][node                     ] [Tantra] initialized\r\n[2014-07-04 07:59:33,016][INFO ][node                     ] [Tantra] starting ...\r\n[2014-07-04 07:59:33,092][INFO ][transport                ] [Tantra] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.0.2.15:9300]}\r\n[2014-07-04 07:59:36,133][INFO ][cluster.service          ] [Tantra] new_master [Tantra][jUaCLA6GTyadduwbg_tTwg][packer-virtualbox][inet[/10.0.2.15:9300]], reason: zen-disco-join (elected_as_master)\r\n[2014-07-04 07:59:36,171][INFO ][discovery                ] [Tantra] elasticsearch/jUaCLA6GTyadduwbg_tTwg\r\n[2014-07-04 07:59:36,262][INFO ][http                     ] [Tantra] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.0.2.15:9200]}\r\n[2014-07-04 07:59:37,094][INFO ][gateway                  ] [Tantra] recovered [33] indices into cluster_state\r\n[2014-07-04 07:59:37,116][INFO ][node                     ] [Tantra] started\r\n[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][2], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [application][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "has_parent": {\r\n                "type": "clients",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                },\r\n                "filter": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "terms": {\r\n                          "interests": [\r\n                            "hello"\r\n                          ]\r\n                        }\r\n                      }\r\n                    ],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "geo_distance": {\r\n                "distance": "20km",\r\n                "coordinate": "59.85499699999999,17.6490213"\r\n              }\r\n            }\r\n          ],\r\n          "must_not": [],\r\n          "should": []\r\n        }\r\n      },\r\n      "query": []\r\n    }\r\n  },\r\n  "size": 25\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:511)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)\r\n\tat org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)\r\n\tat org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)\r\n\tat org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)\r\n\t... 9 more\r\n[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][1], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [application][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "has_parent": {\r\n                "type": "clients",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                },\r\n                "filter": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "terms": {\r\n                          "interests": [\r\n                            "hello"\r\n                          ]\r\n                        }\r\n                      }\r\n                    ],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "geo_distance": {\r\n                "distance": "20km",\r\n                "coordinate": "59.85499699999999,17.6490213"\r\n              }\r\n            }\r\n          ],\r\n          "must_not": [],\r\n          "should": []\r\n        }\r\n      },\r\n      "query": []\r\n    }\r\n  },\r\n  "size": 25\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:511)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)\r\n\tat org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)\r\n\tat org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)\r\n\tat org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)\r\n\t... 9 more\r\n[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][0], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [application][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "has_parent": {\r\n                "type": "clients",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                },\r\n                "filter": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "terms": {\r\n                          "interests": [\r\n                            "hello"\r\n                          ]\r\n                        }\r\n                      }\r\n                    ],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "geo_distance": {\r\n                "distance": "20km",\r\n                "coordinate": "59.85499699999999,17.6490213"\r\n              }\r\n            }\r\n          ],\r\n          "must_not": [],\r\n          "should": []\r\n        }\r\n      },\r\n      "query": []\r\n    }\r\n  },\r\n  "size": 25\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:511)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)\r\n\tat org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)\r\n\tat org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)\r\n\tat org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)\r\n\t... 9 more\r\n[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][4], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [application][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "has_parent": {\r\n                "type": "clients",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                },\r\n                "filter": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "terms": {\r\n                          "interests": [\r\n                            "hello"\r\n                          ]\r\n                        }\r\n                      }\r\n                    ],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "geo_distance": {\r\n                "distance": "20km",\r\n                "coordinate": "59.85499699999999,17.6490213"\r\n              }\r\n            }\r\n          ],\r\n          "must_not": [],\r\n          "should": []\r\n        }\r\n      },\r\n      "query": []\r\n    }\r\n  },\r\n  "size": 25\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:511)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)\r\n\tat org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)\r\n\tat org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)\r\n\tat org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)\r\n\t... 9 more\r\n[2014-07-04 07:59:45,985][DEBUG][action.search.type       ] [Tantra] [application][3], node[jUaCLA6GTyadduwbg_tTwg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@6fa23b68]\r\norg.elasticsearch.search.SearchParseException: [application][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "has_parent": {\r\n                "type": "clients",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                },\r\n                "filter": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "terms": {\r\n                          "interests": [\r\n                            "hello"\r\n                          ]\r\n                        }\r\n                      }\r\n                    ],\r\n                    "must_not": [],\r\n                    "should": []\r\n                  }\r\n                }\r\n              }\r\n            },\r\n            {\r\n              "geo_distance": {\r\n                "distance": "20km",\r\n                "coordinate": "59.85499699999999,17.6490213"\r\n              }\r\n            }\r\n          ],\r\n          "must_not": [],\r\n          "should": []\r\n        }\r\n      },\r\n      "query": []\r\n    }\r\n  },\r\n  "size": 25\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:649)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:511)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:483)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:206)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$5.call(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:517)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.xcontent.XContentHelper.createParser(XContentHelper.java:46)\r\n\tat org.elasticsearch.index.query.support.XContentStructure.asQuery(XContentStructure.java:88)\r\n\tat org.elasticsearch.index.query.support.XContentStructure$InnerQuery.asQuery(XContentStructure.java:154)\r\n\tat org.elasticsearch.index.query.HasParentFilterParser.parse(HasParentFilterParser.java:115)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.BoolFilterParser.parse(BoolFilterParser.java:92)\r\n\tat org.elasticsearch.index.query.QueryParseContext.executeFilterParser(QueryParseContext.java:283)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:264)\r\n\tat org.elasticsearch.index.query.FilteredQueryParser.parse(FilteredQueryParser.java:74)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:227)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:334)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:633)\r\n\t... 9 more\r\n[2014-07-04 07:59:45,999][DEBUG][action.search.type       ] [Tantra] All shards failed for phase: [query]\r\n```\r\nMapping\r\n```json\r\n{\r\n    "application": {\r\n        "mappings": {\r\n            "clients": {\r\n                "properties": {\r\n                    "createdAt": {\r\n                        "format": "date_time_no_millis",\r\n                        "type": "date"\r\n                    },\r\n                    "email": {\r\n                        "type": "string"\r\n                    },\r\n                    "facebookId": {\r\n                        "type": "long"\r\n                    },\r\n                    "favorites": {\r\n                        "type": "integer"\r\n                    },\r\n                    "id": {\r\n                        "type": "integer"\r\n                    },\r\n                    "interests": {\r\n                        "type": "string"\r\n                    },\r\n                    "name": {\r\n                        "type": "string"\r\n                    },\r\n                    "surname": {\r\n                        "type": "string"\r\n                    },\r\n                    "updatedAt": {\r\n                        "format": "date_time_no_millis",\r\n                        "type": "date"\r\n                    }\r\n                }\r\n            },\r\n            "devices": {\r\n                "_parent": {\r\n                    "type": "clients"\r\n                },\r\n                "_routing": {\r\n                    "required": true\r\n                },\r\n                "properties": {\r\n                    "active": {\r\n                        "type": "boolean"\r\n                    },\r\n                    "appName": {\r\n                        "type": "string"\r\n                    },\r\n                    "appVersion": {\r\n                        "type": "string"\r\n                    },\r\n                    "carrier": {\r\n                        "type": "string"\r\n                    },\r\n                    "coordinate": {\r\n                        "type": "geo_point"\r\n                    },\r\n                    "country": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n                    "createdAt": {\r\n                        "format": "date_time_no_millis",\r\n                        "type": "date"\r\n                    },\r\n                    "deviceId": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n                    "deviceModel": {\r\n                        "type": "string"\r\n                    },\r\n                    "deviceName": {\r\n                        "type": "string"\r\n                    },\r\n                    "id": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n                    "inactivatedAt": {\r\n                        "format": "date_time_no_millis",\r\n                        "type": "date"\r\n                    },\r\n                    "mobileCountryCode": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n                    "os": {\r\n                        "type": "string"\r\n                    },\r\n                    "osVersion": {\r\n                        "type": "string"\r\n                    },\r\n                    "pushNotificationId": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n                    "updatedAt": {\r\n                        "format": "date_time_no_millis",\r\n                        "type": "date"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```'
6718,'clintongormley','Docs: fixed a typo in the docs\n'
6717,'areek','Analysis: PatternAnalyzer should use PatternTokenFilter instead\nThe PatternAnalyzer in Lucene is deprecated and it should be implemented using the PatternTokenizer, LowerCaseFilter and StopTokenFilter instead.\r\n\r\nWould fix the bug in #895'
6716,'clintongormley','grammatical error\n'
6715,'s1monw','Resiliency: If the node initialisation fails, make sure the node environment is closed correctly\n...ed correctly (and thus all locks being properly released)'
6712,'clintongormley',"[DOC] Update plugins.asciidoc\nI'd like to add our plugins to plugins page:\r\n\r\n* Elasticsearch River Web Plugin\r\n* Elasticsearch Taste Plugin"
6710,'costin','Hive Elasticsearch integration issue.\nI am integrating Hive and elasticsearch as per:\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/hadoop/current/hive.html#_writing_data_to_elasticsearch_2\r\n\r\nand \r\n\r\nhttps://github.com/hortonworks/hadoop-tutorials/blob/master/Community/T07_Elasticsearch_Hadoop_Integration.md\r\n\r\nI have started a EMR cluster on aws also started the elasticsearch cluster on 2 ec2 classic instances.\r\nI am stuck up while trying to insert data from the hive table to hive-elasticsearch tbale it throws me the following error log:\r\n\r\nDiagnostic Messages for this Task:\r\nError: java.lang.RuntimeException: Error in configuring object\r\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\r\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\r\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:427)\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\r\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:415)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\r\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\r\n        ... 9 more\r\nCaused by: java.lang.RuntimeException: Error in configuring object\r\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)\r\n        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)\r\n        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)\r\n        at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)\r\n        ... 14 more\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:606)\r\n        at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)\r\n        ... 17 more\r\nCaused by: java.lang.RuntimeException: Map operator initialization failed\r\n        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:126)\r\n        ... 22 more\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException\r\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:402)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:451)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:407)\r\n        at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:62)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:451)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:407)\r\n        at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:186)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)\r\n        at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:543)\r\n        at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:375)\r\n        at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:105)\r\n        ... 22 more\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:339)\r\n        ... 34 more\r\n\r\nI have already tried many things like adding the jar files like : \r\nadd jar hive/lib/hive-hbase-handler-0.11.0.jar;\r\nadd jar hive/lib/hbase-0.94.18.jar;\r\nadd jar hive/lib/zookeeper-3.4.3.jar;\r\nadd jar hive/lib/guava-15.0.jar;\r\nadd jar hive/lib/protobuf-java-2.5.0.jar;\r\nadd jar hive/lib/hive-serde-0.11.0.jar;\r\nadd jar hive/lib/hive-contrib-0.11.0.jar\r\n\r\nit still does not work. \r\n\r\nI have downloaded the version of elasticsearch-hadoop jar file from the elasticsearch.org.\r\n\r\nCould anyone help me track the issue. \r\n\r\nThanks in advance.\r\nRegards,\r\nSurender.'
6708,'colings86',"Aggregations: Aggregation names can now include dot\nAggregation name are now able to use any character except '[', ']', and '>'. Dot syntax may still be used to reference values (e.g. in the order field) but may only reference the value directly beneath the last aggregation in the list. more complex structures will need to be accessed using the aggname[value] syntax\r\n\r\nCloses #6702"
6703,'s1monw','[TEST] Expose `tests.filter` for elasticsearch tests.\n`-Dtests.filter` allows to pass filter expressions to the elasticsearch\r\ntests. This allows to filter test annotaged with TestGroup annotations\r\nlike @Slow, @Nightly, @Backwards, @Integration with a boolean expresssion like:\r\n\r\n * to run only backwards tests run:\r\n     `mvn -Dtests.bwc.version=X.Y.Z -Dtests.filter="@backwards"`\r\n * to run all integration tests but skip slow tests run:\r\n     `mvn -Dtests.filter="@integration and not @slow"\r\n * to take defaults into account ie run all test as well as backwards:\r\n     `mvn -Dtests.filter="default and @backwards"\r\n\r\nThis feature is a more powerful alternative to flags like\r\n`-Dtests.nighly=true|false` etc.'
6702,'colings86',"Aggregations: Extend allowed characters in aggregation name\nCurrently aggregations names must match the pattern `[a-zA-Z0-9\\\\-_]+`. This is pretty restrictive and prevents, for example, a common naming pattern of using dots in names. We should extend this pattern to be less restrictive.\r\n\r\nCurrently an aggregation can refer to values within child aggregations using two methods:\r\n1. agg_name['val_name']\r\n2. agg_name.val_name\r\n\r\nAllowing dots in the aggregation name will mean that we drop support for option 2 in favour of 1."
6701,'jpountz','"copy_to" behaviour in nested fields doesn\'t match the mapping\nI have a nested field, containing some fields with the \'copy_to\' attribute. I index a document. Then, I look at the mappings - these show the \'copy_to\' field as appearing in the root object, not the nested objects.\r\n\r\n(See https://gist.github.com/tstibbs/dc36483f96f8d1471385 for the necessary commands to reproduce.)\r\n\r\nHowever, the fields appear to be copied into the \'copy_to\' field in the nested object.\r\n\r\nIf the \'copied\' field is created in the root object, the following query should return my document (but it doesn\'t):\r\n\r\n    {\r\n      "query": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "match" : {\r\n                "copied" : "rover"\r\n              }\r\n            },\r\n            {\r\n              "nested": {\r\n                "path": "children",\r\n                "score_mode": "avg",\r\n                "query": {\r\n                  "match": {\r\n                    "colour": "red"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n\r\nIf the \'copied\' field is created in each child, then following query should match (it does):\r\n\r\n    {\r\n      "query": {\r\n        "nested": {\r\n          "path": "children",\r\n          "score_mode": "avg",\r\n          "query": {\r\n            "bool": {\r\n              "must": [\r\n                {\r\n                  "match": {\r\n                    "copied": "rover"\r\n                  }\r\n                },\r\n                {\r\n                  "match": {\r\n                    "colour": "red"\r\n                  }\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nTo me, the current behaviour makes sense - the fields are copied to a field within the item itself. However, I think the issue is that the mapping doesn\'t match the behaviour, and the documentation doesn\'t specify which is correct.\r\n\r\nSo, I can\'t currently use this feature, because I don\'t know which issue is the bug!'
6696,'colings86',"Aggregations: Return an upper bound of the maximum error for terms\nThe fact that terms aggregations don't give accurate counts is a bit [deceptive](https://github.com/elasticsearch/elasticsearch/issues/1305). Without changing the way they are implemented, maybe we should make terms aggregations return an upper bound of the maximum error on the document count as part of the response? I think this would help make clear that there are potential accuracy issues, as well as make this inaccuracy easier to manage since there is a known upper bound on the error?"
6694,'javanna','Internal: Check for index blocks against concrete indices on master operations\nSome master operations require to check for index blocks, done by overriding `TransportMasterNodeOperationAction#checkBlock`. We need to make sure though that the check is done against concrete indices, otherwise we might not get blocks back altough there are.\r\n\r\nFor instance you can currently delete a read-only index using an alias that points to it, or using the `_all` alias:\r\n\r\n```\r\ncurl -XPUT localhost:9200/foo\r\n\r\ncurl -XPUT \'localhost:9200/foo/_settings\' -d \'\r\n{\r\n    "index" : {\r\n        "blocks.read_only" : true\r\n    } }\r\n\'\r\n\r\ncurl -XDELETE localhost:9200/foo\r\n#{"error":"ClusterBlockException[blocked by: [FORBIDDEN/5/index read-only (api)];]","status":403}\r\n\r\ncurl -XDELETE localhost:9200/_all\r\n#{"acknowledged":true}\r\n```\r\n\r\nThe problem seems to affect the following apis:\r\n- delete index\r\n- delete mapping\r\n- types exists\r\n- indices exists\r\n- open index\r\n- close index'
6693,'rmuir','Analysis: Add additional Analyzers, Tokenizers, and TokenFilters from Lucene\nAdd `irish` analyzer\r\nAdd `sorani` analyzer (Kurdish)\r\n\r\nAdd `classic` tokenizer: specific to english text and tries to recognize hostnames, companies, acronyms, etc.\r\nAdd `thai` tokenizer: segments thai text into words.\r\n\r\nAdd `classic` tokenfilter: cleans up acronyms and possessives from classic tokenizer\r\nAdd `apostrophe` tokenfilter: removes text after apostrophe and the apostrophe itself\r\nAdd `german_normalization` tokenfilter: umlaut/sharp S normalization\r\nAdd `hindi_normalization` tokenfilter: accounts for hindi spelling differences\r\nAdd `indic_normalization` tokenfilter: accounts for different unicode representations in Indian languages\r\nAdd `sorani_normalization` tokenfilter: normalizes kurdish text\r\nAdd `scandinavian_normalization` tokenfilter: normalizes Norwegian, Danish, Swedish text\r\nAdd `scandinavian_folding` tokenfilter: much more aggressive form of `scandinavian_normalization`\r\nAdd additional languages to stemmer tokenfilter: `galician`, `minimal_galician`, `irish`, `sorani`, `light_nynorsk`, `minimal_nynorsk`\r\n\r\nAdd support access to default Thai stopword set "_thai_"\r\n\r\nFix some bugs and broken links in documentation.\r\n\r\nCloses #5935'
6692,'martijnvg','Resiliency: Before deleting shard verify that another node holds an active shard instance\nBefore removing shard physically from disk verify that another node in the cluster actually holds an active shard instance.'
6690,'imotov','Snapshot/Restore: Add ability to restore indices without their aliases\nCloses #6457'
6683,'clintongormley','Update plugins.asciidoc\nUpdated the river plugins with the EEA RDF River Plugin. It allows harvesting metadata from SPARQL endpoints or plain RDF files into ElasticSearch.'
6681,'s1monw','Randomly tokenizing field data on dot\nI have indexed a few hundred documents into Elasticsearch. There is one field named "awsService" which contains values such as "ec2.amazonaws.com" or "rds.amazonaws.com" or "iam.amazonaws.com" etc. When I run aggregations on this field, it splits "ec2.amazonaws.com" to "ec2" and "amazonaws.com" and produces two separate aggregation records, but all the other values like "rds.amazonaws.com" or "iam.amazonaws.com" are intact and produces only one aggregation record.\r\n\r\nThis surely seems to be a bug in aggregation component of Elasticsearch. Please let me know if any more information is required.'
6680,'martijnvg',"[TEST] Also wait for fields to have been applied in the mapping in cluster state during teh waitForConcreteMappingsOnAll call\nThe concrete DocMapper on the master will be updated before the mapping in the cluster state. The DocMapper is updated during the cluster update task. This can lead to occasional assertion failures on the mapping response, because that is based on the mapping the cluster state, which may not yet have been updated. (time window between the DocMapping is updated, but the mapping in the cluster state isn't)"
6676,'brwe','GET: Add parameter to GET for checking if generated fields can be retrieved\nI index a text field with type `token_count`. When indexing, this creates an additional field that holds the number of tokens in the text field.\r\nWhen a document is retrieved from the transaction log (because no flush happened yet),  and I want to get the `token_count` of my text field, I would assume that the `token_count` field is simply not retrieved, because it does not exist yet. Instead I get a `NumberFormatException`.\r\n\r\nHere are the steps to reproduce:\r\n\r\n```\r\nDELETE testidx\r\n\r\nPUT testidx\r\n{\r\n  "settings": {\r\n    "index.translog.disable_flush": true,\r\n    "index.number_of_shards": 1,\r\n    "refresh_interval": "1h"\r\n  },\r\n  "mappings": {\r\n    "doc": {\r\n      "properties": {\r\n        "text": {\r\n          "fields": {\r\n            "word_count": {\r\n              "type": "token_count",\r\n              "store": "yes",\r\n              "analyzer": "standard"\r\n            }\r\n          },\r\n          "type": "string"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT testidx/doc/1\r\n{\r\n  "text": "some text"\r\n}\r\n\r\n#ok, get document from translog\r\nGET testidx/doc/1?realtime=true\r\n#ok, get document from index but it is not there yet\r\nGET testidx/doc/1?realtime=false\r\n# try to get the document from translog but also field text.word_count which is not there yet: NumberFormatException\r\nGET testidx/doc/1?fields=text.word_count&realtime=true\r\n\r\n```'
6673,'clintongormley','How to use my-similarity in version 1.0.1?\nWrite a new similarity plugin, and it worked if i add "index.similarity.default.type: org.elasticsearch.plugin.similarity.MySimilarityProvider" into elasticsearch.xml.\r\nBut it doesn\'t work well when i only put mapping in create index code. I guess something is error in search code. Do i need add any code in search process?'
6672,'jpountz',"Templates: GET templates doesn't honor the `flat_settings` parameter.\nClose #6671"
6671,'jpountz','Index Templates API: GET templates doesn\'t honor the `flat_settings` parameter.\nIt appears there is no way to get nested settings from template GET.\r\n\r\n```\r\n$ curl localhost:9200\r\n{\r\n  "status" : 200,\r\n  "name" : "Ezekiel",\r\n  "version" : {\r\n    "number" : "1.2.1",\r\n    "build_hash" : "6c95b759f9e7ef0f8e17f77d850da43ce8a4b364",\r\n    "build_timestamp" : "2014-06-03T15:02:52Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.8"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n\r\n$ curl -XPUT localhost:9200/_template/test1 -d \'{\r\n    "template": "test1-*",\r\n    "settings": {\r\n        "index": {\r\n            "mapper": {\r\n                "dynamic": false\r\n            }\r\n        }\r\n    }\r\n}\'\r\n{"acknowledged":true}\r\n\r\n$ curl -XGET \'localhost:9200/_template/test1?pretty\'\r\n{\r\n  "test1" : {\r\n    "order" : 0,\r\n    "template" : "test1-*",\r\n    "settings" : {\r\n      "index.mapper.dynamic" : "false"\r\n    },\r\n    "mappings" : { },\r\n    "aliases" : { }\r\n  }\r\n}\r\n\r\ncurl -XGET \'localhost:9200/_template/test1?pretty&flat_settings=false\'\r\n{\r\n  "test1" : {\r\n    "order" : 0,\r\n    "template" : "test1-*",\r\n    "settings" : {\r\n      "index.mapper.dynamic" : "false"\r\n    },\r\n    "mappings" : { },\r\n    "aliases" : { }\r\n  }\r\n}\r\n```'
6670,'dadoonet','Docs: Document special case for type parameter requirement for _mget\nThe current documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-multi-get.html#docs-multi-get) indicates that type is optional in the _mget request.\r\n\r\nPer https://github.com/elasticsearch/elasticsearch/issues/6633, in the case of multiple types sharing the same ID, in order for _mget to fetch back documents sharing the same ID across types, a _type+_id parameter have to be specified (i.e. cannot simply specify _id to fetch documents across types sharing the same id).\r\n\r\n```\r\nGET /sameid/_mget/\r\n{\r\n  "docs" : [\r\n        {\r\n            "_type":"typeA",\r\n            "_id" : "1"\r\n        },\r\n        {\r\n            "_type":"typeB",\r\n            "_id" : "1"\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nWould be nice to add this as a special case in the documentation for _mget.'
6669,'kimchy','Mapping: Better logic on sending mapping update new type introduction\nwhen an indexing request introduces a new mapping, today we rely on the parsing logic to mark it as modified on the "first" parsing phase. This can cause sending of mapping updates to master even when the mapping has been introduced in the create index/put mapping case, and can cause sending mapping updates without needing to.\r\n This bubbled up in the disabled field data format test, where we explicitly define mappings to not have the update mapping behaviour happening, yet it still happens because of the current logic, and because in our test we delay the introduction of any mapping updates randomly, it can get in and override updated ones.'
6667,'bleskes','Internal: IndexingMemoryController should only update buffer settings of fully recovered shards\nAt the moment the IndexingMemoryController can try to update the index buffer memory of shards at any give moment. This update involves a flush, which may cause a FlushNotAllowedEngineException to be thrown in a concurrently finalizing recovery.\r\n\r\nCloses #6642'
6665,'bleskes','\'IllegalArgumentException: No type mapped for [105]\' on get document\nHi!\r\n\r\nI have updated elasticsearch from 1.1.0 to 1.2.1 and faced with a strange issue. I\'m not sure, but it can be related to the thread #6441  \r\n\r\nPreconditions:\r\n1. Elasticsearch 1.2.1 is newly installed. All settings have default values.\r\n2. Index is newly created.\r\n3. Index mapping created via API:\r\n```json\r\n{\r\n   "elasticbug": {\r\n      "mappings": {\r\n         "filesetdata": {\r\n            "properties": {\r\n               "fileContent": {\r\n                  "type": "string",\r\n                  "include_in_all": false\r\n               },\r\n               "format": {\r\n                  "type": "integer",\r\n                  "include_in_all": false\r\n               },\r\n               "metadata": {\r\n                  "properties": {\r\n                     "Fil_1": {\r\n                        "properties": {\r\n                           "OriginalFileID_2": {\r\n                              "type": "string",\r\n                              "index": "not_analyzed"\r\n                           },\r\n                           "OriginalFileName_3": {\r\n                              "type": "string",\r\n                              "index": "not_analyzed"\r\n                           }\r\n                        }\r\n                     }\r\n                  }\r\n               },\r\n               "state": {\r\n                  "type": "byte",\r\n                  "include_in_all": false\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\'fileContent\' field is used for storing text extracted from .pdf files via Apache Tika.\r\n4. All other index settings have default values.\r\n\r\nSteps:\r\nIndex this huge document:\r\n*PUT elasticbug/filesetdata/54c87535-1baf-46d7-a6f9-2b0c914c7479 [huge doc](https://onedrive.live.com/redir?resid=50939A116F9A78E1!126&authkey=!AJA5NQ6t__mQZqw&ithint=file%2c)*\r\n**Right after that** try to GET this document:\r\n*GET elasticbug/filesetdata/54c87535-1baf-46d7-a6f9-2b0c914c7479*\r\n\r\n[Expected]\r\n```\r\nDocument was successfully obtained\r\n```\r\n[Actual]\r\n\r\n```\r\n[2014-07-01 16:01:45,009][DEBUG][action.get] [Shard] [elasticbug][3]: failed to execute [[elasticbug][filesetdata][54c87535-1baf-46d7-a6f9-2b0c914c7479]: routing [null]]\r\njava.lang.IllegalArgumentException: No type mapped for [105]\r\n\tat org.elasticsearch.index.translog.Translog$Operation$Type.fromId(Translog.java:223)\r\n\tat org.elasticsearch.index.translog.TranslogStreams.readSource(TranslogStreams.java:59)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.get(InternalEngine.java:340)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.get(InternalIndexShard.java:469)\r\n\tat org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:195)\r\n\tat org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:106)\r\n\tat org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:109)\r\n\tat org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:43)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction$1.run(TransportShardSingleOperationAction.java:163)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n```\r\n\r\n**NOTE:** \r\n1. After a while (about 30m) GET works fine and obtains document without any problems. As far as i understand it depends on *index.translog.flush_threshold_period* setting.\r\n2. I also noticed that *refresh* parameter set to true in index command (*?refesh=true*) fixes this issue. Interesting that force *_refresh* via api DOES NOT help.\r\n3. As was mentioned in #6441, to work around the problem you can trigger a *_flush*\r\nbefore the get.\r\n4. This issue was not reproduced in 1.1.0.\r\n\r\nThanks!'
6664,'martijnvg','Strict query parsing on percolators and alias filters\nToday, if we create a filtered alias or a percolator query which references a field that is not yet mapped, the field is interpreted as a string.\r\n\r\nThis is a problem when (eg) the user then indexes a document where the field is really numeric or a date.  Any existing filters or percolator queries will not work correctly until the cached filter/query is regenerated (eg after a restart).\r\n\r\nInstead, we should support a "strict" query parsing mode which throws an error if an unmapped field is referenced.'
6661,'clintongormley','Fix typo in timestamp-field.asciidoc\n'
6658,'bleskes','Test: Randomize translog implementation\nWe have two types of transaction log implementation - a simple and a buffered one. We should randomly choose one of the two during testing.'
6657,'alexksikes',"More Like This Query: Ensure selection of best terms is indeed O(n)\nPreviously the size of the priority queue was wrongly set to the total number\r\nof terms. Instead, it should be set to 'maxQueryTerms'. This makes the\r\nselection of best terms O(n), instead of O(n*log(n)).\r\n\r\nJira patch: https://issues.apache.org/jira/browse/LUCENE-5795"
6655,'colings86','Aggregations: Histogram Aggregation key Bug\nHi,\r\n\r\nI\'m working with histogram aggregation but there is something strange with keys.\r\nFor instance (cf : http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html):\r\n\r\nIf I use this request :\r\n```json\r\n{\r\n    "aggs" : {\r\n        "prices" : {\r\n            "histogram" : {\r\n                "field" : "price",\r\n                "interval" : 50\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nI obtain something like this :\r\n```json\r\n{\r\n    "aggregations": {\r\n        "prices" : {\r\n            "buckets": [\r\n                {\r\n                    "key_as_string" : "0",\r\n                    "key": 0,\r\n                    "doc_count": 2\r\n                },\r\n                {\r\n                    "key_as_string" : "50",\r\n                    "key": 50,\r\n                    "doc_count": 4\r\n                },\r\n                {\r\n                    "key_as_string" : "150",\r\n                    "key": 150,\r\n                    "doc_count": 3\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nInstead of :\r\n```json\r\n{\r\n    "aggregations": {\r\n        "prices" : {\r\n            "buckets": [\r\n                {\r\n                    "key": 0,\r\n                    "doc_count": 2\r\n                },\r\n                {\r\n                    "key": 50,\r\n                    "doc_count": 4\r\n                },\r\n                {\r\n                    "key": 150,\r\n                    "doc_count": 3\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nYou could say, it\'s not important but it generates json ~1/3 bigger...\r\nIs there a mean to disable this ???\r\n\r\nMoreover, in Elasticsearch Java API, it could be fine to have a method to request the response as a hash instead keyed by the buckets keys (cf :http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-histogram-aggregation.html#_response_format)\r\n\r\nThanks!!!'
6651,'spinscale','Missing coma in the example.\n'
6649,'spinscale','JAVA API: Fix source excludes setting if no includes were provided\nDue to a bogus if-check in SearchSourceBuilder.fetchSource(String include, String exclude)\r\nthe excludes only got set when the includes were not null. Fixed this and added some\r\nbasic tests.\r\n\r\nCloses #6632'
6648,'kimchy',"Mappings: Update mapping on master in async manner\nToday, when a new mapping is introduced, the mapping is rebuilt (refreshSource) on the thread that performs the indexing request. This can become heavier and heavier if new mappings keeps on being introduced, we can move this process to another thread that will be responsible to refresh the source and then send the update mapping to the master (note, this doesn't change the semantics of new mapping introduction, since they are async anyhow).\r\nWhen doing so, the thread can also try and batch as much updates as possible, this is handy especially when multiple shards for the same index exists on the same node. An internal setting that can control the time to wait for batches is also added (defaults to 0).\r\n\r\nTesting wise, a new support method on ElasticsearchIntegrationTest#waitForConcreteMappingsOnAll to allow to wait for the concrete manifestation of mappings on all relevant nodes is added. Some tests mistakenly rely on the fact that there are no more pending tasks to mean mappings have been updated, so if we see, timing related, failures down later (all tests pass), then those will need to be fixed to wither awaitBusy on the master for the new mapping, or in the rare case, wait for the concrete mapping on all the nodes using the new method.\r\n\r\nNote, this change also removes `action.wait_on_mapping_change`, this is an internal setting, and is not recommended to set it. It was used using the old test infrastructure to validate if the problem was due to mapping propagation, but we have a much better infra for this now."
6646,'spinscale','Bulk API: Fix return of wrong request type on failed updates\nIn case an update request failed (for example when updating with a\r\nwrongly formatted date), the returned index operation type was index\r\ninstead of update.\r\n\r\nCloses #6630'
6645,'martijnvg','Resiliency: Cancel recovery if shard on the target node closes during recovery operation\nOn the target side if the shard closes while recovery is in progress (e.g. files being transferred), the recovery operation should be cancelled.'
6642,'bleskes','Failed to start shard when restarting elasticsearch\nThis just happened as I was restarting elasticsearch (1.1.1) on one node. As it came up again it failed to start one of the shards with the following exception:\r\n[2014-06-29 04:38:35,401][INFO ][node                     ] [es-6636e.recfut.net] started\r\n[2014-06-29 04:38:56,268][WARN ][indices.cluster          ] [es-6636e.recfut.net] [reference_2014-04-10_1][2] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [reference_2014-04-10_1][2] failed recovery\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:256)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:724)\r\nCaused by: org.elasticsearch.index.engine.FlushNotAllowedEngineException: [reference_2014-04-10_1][2] already flushing...\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.flush(InternalEngine.java:756)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryFinalization(InternalIndexShard.java:716)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:250)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:197)\r\n        ... 3 more\r\n[2014-06-29 04:38:56,824][WARN ][cluster.action.shard     ] [es-6636e.recfut.net] [reference_2014-04-10_1][2] sending failed shard for [reference_2014-04-10_1][2], node[IdoOCT9rTwWUZMCwf95hcg], [P], s[INITIALIZING], indexUUID [va11FwbtRTmsGsKW97LYkA], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[reference_2014-04-10_1][2] failed recovery]; nested: FlushNotAllowedEngineException[[reference_2014-04-10_1][2] already flushing...]; ]]\r\n\r\nAfter being anxious for a while I did another restart, and this time all shards started. There were no writes happening to the cluster during this. But it was very worrying.'
6640,'martijnvg','Some percolators are used only after restart\nscenario:\r\n1. put some stuff in the index\r\n2. add a few percolators\r\n3. percolate an existing document\r\n4. some of the percolators would not appear in the results until cluster is restarted\r\nonly some percolators behave this way, eg this works fine\r\n```\r\n"query": {\r\n                  "constant_score": {\r\n                     "filter": {\r\n                        "and": {\r\n                           "filters": [\r\n                              {\r\n                                 "and": {\r\n                                    "filters": [\r\n                                       {\r\n                                          "term": {\r\n                                             "customer_id": "customer_id"\r\n                                          }\r\n                                       },\r\n                                       {\r\n                                          "range": {\r\n                                             "stats.logins.count": {\r\n                                                "from": -1,\r\n                                                "to": null,\r\n                                                "include_lower": false,\r\n                                                "include_upper": true\r\n                                             }\r\n                                          }\r\n                                       }\r\n                                    ]\r\n                                 }\r\n                              },\r\n                              {\r\n                                 "term": {\r\n                                    "app_id": "53ade461bda195157e8dd2fd"\r\n                                 }\r\n                              }\r\n                           ]\r\n                        }\r\n                     }\r\n                  }\r\n               }\r\n```\r\nbut this consistently fails\r\n```\r\n"query": {\r\n                  "constant_score": {\r\n                     "filter": {\r\n                        "and": {\r\n                           "filters": [\r\n                              {\r\n                                 "and": {\r\n                                    "filters": [\r\n                                       {\r\n                                          "term": {\r\n                                             "customer_id": "customer_id"\r\n                                          }\r\n                                       },\r\n                                       {\r\n                                          "term": {\r\n                                             "stats.logins.count": 1\r\n                                          }\r\n                                       }\r\n                                    ]\r\n                                 }\r\n                              },\r\n                              {\r\n                                 "term": {\r\n                                    "app_id": "53ade461bda195157e8dd2fd"\r\n                                 }\r\n                              }\r\n                           ]\r\n                        }\r\n                     }\r\n                  }\r\n               }\r\n```\r\nno amount of flushing or refreshing or waiting will fix that behavior\r\nI use es 1.2.1 but can reproduce the same behavior with 1.1.2 on windows'
6637,'rmuir','Internal: Disable explicit GC by default\n'
6636,'s1monw','Internal: Make a hybrid directory default using `mmapfs` / `niofs`\n`mmapfs` is really good for random access but can have sideeffects if\r\nmemory maps are large depending on the operating system etc. A hybrid\r\nsolution where only selected files are actually memory mapped but others\r\nmostly consumed sequentially brings the best of both worlds and\r\nminimizes the memory map impact.\r\nThis commit mmaps only the `dvd` and `tim` file for fast random access\r\non docvalues and term dictionaries.'
6635,'kimchy','Test: Randomize netty worker and connection parameters\nTry and push our system to a state where there is only a single worker, trying to expose potential deadlocks when we by mistake execute blocking operations on the worker thread'
6632,'spinscale','Java API: Fix source excludes setting if no includes were provided\nLine 524 uses "include == null" to check if the exclude string is null.\r\n\r\nI don\'t know why this had to be a one-liner, but I don\'t like using ternary operators, particularly twice (!) on the same line. Makes it harder to read and can lead to bugs like this.\r\n\r\nAffects v1.2.1\r\n\r\nCheers!'
6630,'spinscale','Bulk API: Fix return of wrong request type on failed updates\nOpening an issue as suggested by @dadoonet after the following discussion on the mailing list: https://groups.google.com/forum/#!topic/elasticsearch/HAA4Y8Qziqg\r\n\r\nWhen a bulk update action fails, an "index" entry can be returned in the response.\r\n\r\nExample bulk commands list with an update command failing because the second date can\'t be parsed:\r\n```json\r\n{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } \r\n{ "title" : "Great Title of doc 1" } \r\n{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "2" } } \r\n{ "title" : "Great Title of doc 2" } \r\n{ "update" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } \r\n{ "doc" : { "date" : "2014-04-30T23:59:57" }} \r\n{ "update" : { "_index" : "test", "_type" : "type1", "_id" : "2" } } \r\n{ "doc" : { "date" : "2014-04-31T00:00:01" }} \r\n{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "1" } } \r\n{ "delete" : { "_index" : "test", "_type" : "type1", "_id" : "2" } }\r\n```\r\n\r\nHere is the actual response (elasticsearch v1.1):\r\n```json\r\n{\r\n  "took" : 4, \r\n  "errors" : true, \r\n  "items" : [ { \r\n    "index" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 8, \r\n      "status" : 201 \r\n    } \r\n  }, { \r\n    "index" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "_version" : 5, \r\n      "status" : 201 \r\n    } \r\n  }, { \r\n    "update" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 9, \r\n      "status" : 200 \r\n    } \r\n  }, { \r\n    "index" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "status" : 400, \r\n      "error" : "MapperParsingException[failed to parse [date]]; nested: MapperParsingException[failed to parse date field [2014-04-31T00:00:01], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalFieldValueException[Cannot parse \\"2014-04-31T00:00:01\\": Value 31 for dayOfMonth must be in the range [1,30]]; " \r\n    } \r\n  }, { \r\n    "delete" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 10, \r\n      "status" : 200, \r\n      "found" : true \r\n    } \r\n  }, { \r\n    "delete" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "_version" : 6, \r\n      "status" : 200, \r\n      "found" : true \r\n    } \r\n  } ] \r\n}\r\n```\r\nThe problem here concerns the bulk update response for the forth item. It\'s key is `index` while it should be `update`, as in the following hand edited response:\r\n```json\r\n{\r\n  "took" : 4, \r\n  "errors" : true, \r\n  "items" : [ { \r\n    "index" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 8, \r\n      "status" : 201 \r\n    } \r\n  }, { \r\n    "index" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "_version" : 5, \r\n      "status" : 201 \r\n    } \r\n  }, { \r\n    "update" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 9, \r\n      "status" : 200 \r\n    } \r\n  }, { \r\n    "update" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "status" : 400, \r\n      "error" : "MapperParsingException[failed to parse [date]]; nested: MapperParsingException[failed to parse date field [2014-04-31T00:00:01], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalFieldValueException[Cannot parse \\"2014-04-31T00:00:01\\": Value 31 for dayOfMonth must be in the range [1,30]]; " \r\n    } \r\n  }, { \r\n    "delete" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "1", \r\n      "_version" : 10, \r\n      "status" : 200, \r\n      "found" : true \r\n    } \r\n  }, { \r\n    "delete" : { \r\n      "_index" : "test", \r\n      "_type" : "type1", \r\n      "_id" : "2", \r\n      "_version" : 6, \r\n      "status" : 200, \r\n      "found" : true \r\n    } \r\n  } ] \r\n}\r\n```\r\nI didn\'t do a lot of code reading but a possible starting point for investigations is there: https://github.com/elasticsearch/elasticsearch/blob/1.1/src/main/java/org/elasticsearch/action/bulk/TransportShardBulkAction.java#L317'
6628,'dakrone',"Scripting: Wrap groovy script exceptions in a serializable Exception object\nFixes #6598\r\n\r\nIt prevents ES from trying to serialize the default Groovy exceptions, which want to carry over a lot of state that doesn't  serialize properly."
6627,'spinscale','REST API: Replace error code 503 with 429 when appropriate\nRight now Elasticsearch returns 503 on a couple of occasions, where the full cluster is actually not completely out of service, but a single node just has a problem, so that trying another node makes sense. On the other hand there are error messages, where telling the client to wait a bit makes more sense in order to reduce load (when the threadpool queue is full for example).\r\n\r\nThere is a HTTP status code for that, called 429 (Too many requests).\r\nIt can be found in RFC 6585 (Additional HTTP status codes), see https://tools.ietf.org/html/rfc6585'
6626,'mikemccand',"Test: Test that dynamically changing throttling settings works\nNot sure there is really an issue here, but maybe ...\r\n\r\nThe code goes through a number of layers when the application tries to either turn throttling on/off or change the rate, and it's not clear if dynamic updating is really working?"
6623,'rmuir','Internal: Upgrade to Lucene 4.9\n'
6618,'jpountz','Ability to predefine number of buckets\nIt would be great to have the ability to predefine the number of buckets on a date histogram (and possibly other types) and have the buckets be evenly divided across time based on the result set.\r\n\r\nCurrently you need to understand how your query results span time to know what interval makes sense to set on the aggregation (which is hard to know before the query is executed), because if your data spans years an interval of a minute may be too fine, and vice versa.\r\n\r\nWith this feature a sample query might look like...\r\n"aggregations" : {\r\n    "birth_date_time" : {\r\n      "date_histogram" : {\r\n        "field" : "birth_date_time",\r\n        "bucketCount" : 100,\r\n        "format" : "yyyy/MM/dd HH:mm:ss"\r\n      }\r\n    }\r\n  }\r\n\r\nAnd thus if your data spans 50 years each bucket would represent everyone who was born in approximately a 6 month window.\r\n\r\nNote that based on implementation there may be fewer than 100 buckets in the response if some buckets have 0 results, but there should be no more than 100 buckets.\r\n\r\nThis feature would be very beneficial when plotting to a histogram on a UI, since it will give the granularity that makes sense to display.'
6617,'dakrone','Docs: Added field data circuit breaker settings\nAdded field data circuit breaker settings to list of update-able properties by the cluster update api.'
6615,'clintongormley','DOC unescape regexes in Pattern Tokenizer docs\nCurrently regexes in Pattern Tokenizer docs are escaped (it seems according to Java rules). I think it is better not to escape them because JSON escaping should be automatic in client libraries, and string escaping depends on a client language used. The default pattern is `\\W+`, not `\\\\W+`.'
6614,'clintongormley','size:1 on a query applied to _all indexes only returns from the newest index, size:2 returns from all. \nOur system is searching for the oldest record that hits a criteria.  We attempted to accomplish this with a size limited ascending range query with bool filters.  \r\n\r\nOn ES .90 and 1.1.2 the following queries were run:\r\n\r\n curl -XGET http://localhost:9200/_all/meta/_search -d  \'{ "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ], query : {filtered : { filter : { bool : { must : [ { term : {"Written" : true }}  , { term :  { "LatestUpdate" : true } } , { range : { "TimeUpdated" : { gte : "2013/05/29 20:51:00" } } } ] } } } , _cache : false , from : 0,size : 1 , "fields" : [ "Session", "TimeUpdated", "TimeStart" ] } }\'\r\n\r\n{"took":616,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":42959661,"max_score":null,"hits":[{"_index":"network_2014_06_25","_type":"meta","_id":"f3bb5501-ce17-4f13-a262-48b89e175201_2","_score":null,"fields":{"TimeStart":["2014/06/25 03:03:49"],"TimeUpdated":["2014/06/25 03:25:49"],"Session":["f3bb5501-ce17-4f13-a262-48b89e175201"]},"sort":[1403666749000]}]}}\r\n\r\nThe indices are built by date, this one "network_2014_06_25" is the newest.  However adding one to the size gives a radically different result:\r\n\r\n curl -XGET http://localhost:9200/_all/meta/_search -d  \'{ "sort" : [ { "TimeUpdated" : { "order": "asc", "ignore_unmapped" : true } } ], query : {filtered : { filter : { bool : { t : [ { term : {"Written" : true }}  , { term :  { "LatestUpdate" : true } } , { range : { "TimeUpdated" : { gte : "2013/05/29 20:51:00" } } } ] } } } , _cache : false , from : 0,size : 2 , "fields" : [ "Session", "TimeUpdated", "TimeStart" ] } }\'\r\n\r\n{"took":770,"timed_out":false,"_shards":{"total":65,"successful":65,"failed":0},"hits":{"total":42973532,"max_score":null,"hits":[{"_index":"network_2014_05_25","_type":"meta","_id":"b97c12c0-b863-49d2-9355-483aee55de16_1","_score":null,"fields":{"TimeStart":["2014/05/25 01:13:05"],"TimeUpdated":["2014/05/25 01:14:04"],"Session":["b97c12c0-b863-49d2-9355-483aee55de16"]},"sort":[1400980444000]},{"_index":"network_2014_06_25","_type":"meta","_id":"bdc9d4b3-4e60-4667-b66a-db831cf2f59b_2","_score":null,"fields":{"TimeStart":["2014/06/25 03:03:49"],"TimeUpdated":["2014/06/25 03:25:49"],"Session":["bdc9d4b3-4e60-4667-b66a-db831cf2f59b"]},"sort":[1403666749000]}]}}\r\n\r\nFurther increasing the size keeps hitting the correct "b97c12c0-b863-49d2-9355-483aee55de16_1" record rather than the  certainly not the oldest "bdc9d4b3-4e60-4667-b66a-db831cf2f59b_2" record. '
6613,'clintongormley','Examples in language analyzers reference don\'t work\nI\'m trying to create an index with an analyzer as shown here (using Python wrapper): http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html#english-analyzer\r\n\r\nIt doesn\'t work as-is, raising the following exception:\r\n\r\n```\r\n[Lady Lark] [my-index] failed to create\r\norg.elasticsearch.indices.IndexCreationException: [my-index] failed to create index\r\n\tat org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:300)\r\n\tat org.elasticsearch.cluster.metadata.MetaDataCreateIndexService$2.execute(MetaDataCreateIndexService.java:343)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.ElasticsearchIllegalArgumentException: keyword filter requires either `keywords` or `keywords_path` to be configured\r\n```\r\n\r\nWithout "english_keywords" or with non-empty keywords it works.'
6612,'drewr','Transport client should not send requests to dedicated masters\nWhen we sample discoverable nodes, we should avoid dedicated masters since their whole purpose is to have very predictable load.'
6608,'dakrone','Docs: Fixed JSON in fielddata docs\n'
6605,'colings86',"Aggregations: Histogram Aggregation bounds offsets\nHi,\r\n\r\nI'm working with elasticsearch and 1st, it's an amazing tool, 2nd I would suggest something about histogram aggregation with extended bounds.\r\nExtended bounds is a good idea but it could be fine if we can force the bounds with, say, a boolean or another method...\r\n\r\nFor instance:\r\nI have these data : [100,101,102,103,104]\r\nI'd like to create an histogram with an interval of 3.\r\n\r\nElasticsearch will produce : \r\n[ [99,doc_count:2] , [102,doc_count:3] ]\r\nThe problem is first tick always start from 0 but if I want to start my bucket from 100, I can't...\r\n\r\nThen I can't obtain : \r\n[ [100,doc_count:3] , [103,doc_count:2] ]\r\n\r\n(The only solution is to perform a script on each data which will shift my data in the right bucket... then : creepy performances)"
6599,'jpountz','Mappings: Add transform to document before index.\nCloses #6566'
6598,'dakrone',"Scripting: Wrap groovy script exceptions in a serializable Exception object\nI'm playing around with groovy and I think exceptions aren't serializing properly:\r\n```\r\n[2014-06-23 18:19:54,091][INFO ][org.elasticsearch.index.mapper] Action Failed\r\norg.elasticsearch.transport.RemoteTransportException: [node_0][inet[/192.168.0.101:9300]][index]\r\nCaused by: org.elasticsearch.transport.RemoteTransportException: Failed to deserialize exception response from stream\r\nCaused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)\r\n\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.io.InvalidClassException: failed to read class descriptor\r\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1603)\r\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)\r\n\tat java.io.ObjectInputStream.readClass(ObjectInputStream.java:1483)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1333)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\r\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)\r\n\tat java.lang.Throwable.readObject(Throwable.java:914)\r\n\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\r\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)\r\n\tat java.lang.Throwable.readObject(Throwable.java:914)\r\n\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)\r\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:500)\r\n\tat java.lang.Throwable.readObject(Throwable.java:914)\r\n\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:167)\r\n\t... 23 more\r\nCaused by: java.lang.ClassNotFoundException: Script4\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\r\n\tat org.elasticsearch.common.io.ThrowableObjectInputStream.loadClass(ThrowableObjectInputStream.java:93)\r\n\tat org.elasticsearch.common.io.ThrowableObjectInputStream.readClassDescriptor(ThrowableObjectInputStream.java:67)\r\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1601)\r\n\t... 62 more\r\n```\r\n\r\nI'm in the middle of work on #6566 so I don't have easy reproduction steps, but I'll see if I can make some soon."
6586,'s1monw','Java API: Add a blocking variant of close() method to BulkProcessor\nBlocks until all bulk requests have completed.  \r\n\r\nUpdated based on feedback\r\nUpdated formatting\r\n\r\nCloses #4158 \r\nCloses #6314 '
6581,'bleskes',"Boostrap: Log startup exception to console if needed and to file as error\nWe ran into an issue with one of our data nodes which would quit immediately upon starting the service. Looking at every log there was nothing informative. We turned the log up to DEBUG and an exception came out:\r\n\r\n```\r\n[2014-06-20 14:48:00,502][DEBUG][bootstrap                ] Exception\r\norg.elasticsearch.ElasticsearchIllegalStateException: Failed to obtain node lock, is the following location writable?: [/usr/local/var/data/elasticsearch/contacts-search-new]\r\n    at org.elasticsearch.env.NodeEnvironment.<init>(NodeEnvironment.java:114)\r\n    at org.elasticsearch.node.internal.InternalNode.<init>(InternalNode.java:150)\r\n    at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:159)\r\n    at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:68)\r\n    at org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:201)\r\n    at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:32)\r\n```\r\n\r\nIt's helpful to have all exceptions printing to `ERROR` level logging or at least `WARN`. But in the case where `Bootstrap.java` is going to quit the JVM, the reason for doing so should be at `ERROR` level so it's clear to the developer what is going wrong."
6572,'martijnvg','percolator inconsistency/ not hitting on dynamic fields\nWe are using percolator and dynamic fields.\r\n\r\nSteps to reproduce:\r\n- create a mapping with an object\r\n- create a percolator for a subfield of that mapping and type\r\n- percolate a doc which has that dynamic field\r\n- no hits\r\n\r\nSteps to reproduce a hit:\r\n- create a mapping with an object\r\n- index a doc which has that dynamic field added to the idnex\r\n- create a percolator for a subfield of that mapping and type\r\n- percolate a doc which has that dynamic field\r\n- hit\r\n\r\nThis is very odd, as we use percolators to generate data into the doc, before indexing it.\r\nSo we have to index the doc, then percolate with it and then change the document.\r\n\r\nGist for ES0.90 (tested on 0.90.13):\r\nhttps://gist.github.com/julianhille/5203065480dc62bb6752\r\n\r\nGist for ES1.x (tested on 1.2.1):\r\nhttps://gist.github.com/julianhille/bd23e8e43e28a193401e'
6568,'clintongormley','Update nested-filter.asciidoc\nFix whitespace to standardize on spaces for indents on both code examples for readability.'
6567,'alexksikes','Term Vectors: Computes term vectors on the fly if not stored in index\n...index.\r\n\r\nAdds an option called `generate` to the term vectors and multi-termvectors API,\r\nwhich generates the term vectors for some chosen fields, if they have not been\r\nexplicitely stored in the index.\r\n\r\nRelates to #5184'
6563,'javanna','Java API: Client intermediate interfaces removal follow-up\nAfter #6517 we ended up registering all of the actions (included admin ones) to the NodeClient. \r\nMade sure that only the proper type of `Action` instances are registered to each client type.\r\nAlso fixed some compiler warnings: unused members, imports and non matching generic types.'
6559,'javanna','Internal: Refactored AckedClusterStateUpdateTask & co. to remove code repetitions in subclasses\nMade `AckedClusterStateUpdateTask` an abstract class instead of an interface, which contains the common methods.\r\nAlso introduced the `AckedRequest` interface to mark both `AcknowledgedRequest` & `ClusterStateUpdateRequest` so that the different ways of updating the cluster state (with or without a `MetaData*Service`) can share the same code.\r\nRemoved `ClusterStateUpdateListener` as we can just use its base class `ActionListener` instead.'
6556,'jpountz','Multi-word match query is slow when there are many words\nAccording to the documentation, a multi-word match query is converted into a boolean query that consists of one term query for each clause. When the the multi-match query contains many clauses (1000+), the query performance is significantly slower (5x-10x) than querying Lucene directly with a BooleanQuery object. Is it possible to add an option to search with a Lucene BooleanQuery instead of converting into many term queries?  '
6554,'dakrone','Translog entry checksums\nIt\'s possible that a transaction log can be corrupted on disk, causing it to throw exceptions like:\r\n\r\n```\r\nException in thread "main" org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [108]\r\n    at org.elasticsearch.index.VersionType.fromValue(VersionType.java:307)\r\n    at org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:506)\r\n    at org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:52)\r\n    at org.writequit.Main.main(Main.java:22)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:606)\r\n    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)\r\n```\r\n\r\nIt would be nice if we had checksums for each entry in the translog, to determine corruption in a more-friendly manner than throwing an exception with "No version type match".'
6551,'javanna',"Java API: BulkRequest#add(Iterable) to support UpdateRequests\nThe BulkRequest#add(Iterable) method doesn't support updates, probably some leftover."
6549,'clintongormley',"Fix 'top-tags' and 'terms' ordering in example\naggregation_name is switched with aggregation_type."
6546,'dadoonet','Plugins: Removing plugin does not fail when plugin dir is read only\nIf you try to remove a plugin in read only dir, you get a successful result:\r\n\r\n```\r\n$ bin/plugin --remove marvel\r\n-> Removing marvel \r\nRemoved marvel\r\n```\r\n\r\nBut actually the plugin has not been removed.\r\n\r\nWhen installing, if fails properly:\r\n\r\n```\r\n$ bin/plugin -i elasticsearch/marvel/latest\r\n-> Installing elasticsearch/marvel/latest...\r\n\r\nFailed to install elasticsearch/marvel/latest, reason: plugin directory /usr/local/elasticsearch/plugins is read only\r\n```\r\n'
6541,'clintongormley','New entry/front-end\nAdding, Calaca, a simple search client for ElasticSearch.'
6538,'jpountz','java.lang.AssertionError when scan & scroll\nI am getting the following error when performing a scroll request.\r\n\r\nI am running this in a test (ElasticSearchIntegrationTest) - I dont know if that is important or not. Oddly, I have never got this error when running normally.\r\n\r\n````\r\njava.lang.AssertionError\r\n\tat org.elasticsearch.common.util.BigArrays$LongArrayWrapper.get(BigArrays.java:200)\r\n\tat org.elasticsearch.test.cache.recycler.MockBigArrays$LongArrayWrapper.get(MockBigArrays.java:374)\r\n\tat org.elasticsearch.common.util.BytesRefHash.get(BytesRefHash.java:66)\r\n\tat org.elasticsearch.common.util.BytesRefHash.set(BytesRefHash.java:101)\r\n\tat org.elasticsearch.common.util.BytesRefHash.add(BytesRefHash.java:145)\r\n\tat org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator$WithOrdinals.collect(StringTermsAggregator.java:299)\r\n\tat org.elasticsearch.search.aggregations.AggregationPhase$AggregationsCollector.collect(AggregationPhase.java:164)\r\n\tat org.elasticsearch.common.lucene.MultiCollector.collect(MultiCollector.java:60)\r\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:65)\r\n\tat org.apache.lucene.search.AssertingScorer.score(AssertingScorer.java:136)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:282)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:270)\r\n\tat org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction.executeQueryPhase(TransportSearchScrollQueryThenFetchAction.java:200)\r\n\tat org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction.access$600(TransportSearchScrollQueryThenFetchAction.java:75)\r\n\tat org.elasticsearch.action.search.type.TransportSearchScrollQueryThenFetchAction$AsyncAction$2.run(TransportSearchScrollQueryThenFetchAction.java:184)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\n\r\n````\r\nLooks like a bug to me. I will try write a simple test that reproduces it.'
6531,'GaelTadh',"Mapping: Fix delete mapping race condition.\nIf multiple clients attempted to delete a mapping, or a single client attempted to delete a mapping as an index is being\r\ncreated a NPE could be observed in the MetaDataMappingService. This fix makes sure we don't try to access a null indexMetaData object.\r\nAlso add a test to spawn multiple create and delete threads to provoke this race condition.\r\n\r\nCloses #5997"
6518,'rmuir','Aggregations: GlobalOrdinalsStringTermsAggregator is inefficient for high-cardinality fields\nAfter doing some profiling and seeing surprising results, and looking at the code, and I can easily be reading it wrong...\r\n\r\nbuildAggregation() has the following loop:\r\n    \r\n    for (long globalTermOrd = Ordinals.MIN_ORDINAL; globalTermOrd < globalOrdinals.getMaxOrd(); ++globalTermOrd) {\r\n                    ...\r\n                    copy(globalValues.getValueByOrd(globalTermOrd), spare.termBytes);  \r\n    }\r\n\r\nThis is very costly for high-cardinality fields, because it means we lookup ord->term for every single one. Instead, can we use a PriorityQueue of "OrdAndSortValue", find the top-N, and then only at the end, lookup ord->term for the top-N? E.g. in solr faceting this OrdAndSortValue is just a long (32 bits ord and 32 bits count) but the representation is less important here.'
6513,'javanna','REST API: Copy the headers from REST requests to the corresponding TransportRequest(s)\nIntroduced the use of the `FilterClient` in all of the REST actions, which delegates all of the operations to the internal `Client`, but makes sure that the headers are properly copied if needed from REST requests to `TransportRequest`(s) when it comes to executing them.  Added new abstract handleRequest method to `BaseRestHandler` with additional `Client` argument and made private the client instance member (was protected before) to force the use of the client received as argument.  The list of headers to be copied over is by default empty but can be extended via plugins.\r\n\r\nReplaces #6464 as it provides a better and safer way to copy headers from REST layer to transport layer.'
6510,'s1monw','Test: Fix Test - Cluster naming\nToday we have `ImmutableCluster`, `ExternalTestCluster` and `TestCluster` which is not quite what they represent. IMO the basic class should be `TestCluster` and the we should rename `TestCluster` to `InternalTestCluster` that way the naming is consistent with where the nodes run (within or outside of the executing JVM.'
6508,'clintongormley',"Added tables for directory locations for deb+rpm and zip+tar.gz installs\nI raised an issue for this but Clint mentioned it's not required, please let me know though if you want me to sign the contributing agreement.\r\n\r\nPer the title, I've written down the specific directories being used by various install methods, hopefully it'll be of use to newer users. I wasn't able to find and indication of a work directory for the zip/tar.gz install though, not sure if that is by design."
6506,'imotov','Snapshot/Restore: NPE in ES when shutdown happens in the middle of snapshotting\nthe process has to be killed forecefully to come out of this state.. here is the stack trace - \r\n\r\n[2014-06-16 07:41:36,568][WARN ][snapshots                ] [Quentin Quire] Fail\r\ned to update snapshot state\r\njava.lang.NullPointerException\r\n        at org.elasticsearch.snapshots.SnapshotsService.processIndexShardSnapsho\r\nts(SnapshotsService.java:644)\r\n        at org.elasticsearch.snapshots.SnapshotsService.clusterChanged(Snapshots\r\nService.java:508)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:430)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n        at java.lang.Thread.run(Thread.java:636)\r\n'
6503,'clintongormley','Created and updated a section "Misc ElasticSearch Clients"(see bottom)\nKafka Standalone Consumer will read the messages from Kafka, processes and index them in ElasticSearch.\r\n\r\nMore details of this project can be found here: https://github.com/reachkrishnaraj/kafka-elasticsearch-standalone-consumer/wiki'
6495,'javanna','Java API: Make sure afterBulk is always called in BulkProcessor\nJava API: Make sure afterBulk is always called in BulkProcessor.\r\n\r\nAlso strenghtened BulkProcessorTests by adding randomizations to existing tests and new tests for concurrent request.\r\n\r\nCloses #5038'
6490,'jpountz','Factor parameter in Date Histogram aggregation not accepted\nAccording to documentation (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_factor_2) date histogram aggregation should accept *factor* parameter. But using it results in \r\n```\r\nParse Failure [Unknown key for a VALUE_NUMBER in [agg]: [factor].]\r\n```\r\nQuery example:\r\n```javascript\r\n"aggs" : { \r\n    "registration": {\r\n        "date_histogram": {\r\n            "field": "registration_date",\r\n            "interval": "day", \r\n            "format": "dd-MM-yyyy",\r\n            "min_doc_count": 0,\r\n            "extended_bounds": {\r\n                "min": "now/d-30d",\r\n                "max": "now/d"\r\n            },\r\n            "factor": 1000  \r\n        }\r\n    }\r\n}\r\n```\r\nWithout *factor*, query works just fine.\r\n\r\nUsing version 1.1.2, but looking to source of master (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java#L124), *factor* is not used there neither. '
6487,'clintongormley','Added missing comma in suggester example\n'
6486,'spinscale','Internal: Allow to serialize negative thread pool sizes\nAs a SizeValue is used for serializing the thread pool size, a negative number\r\nresulted in throwing an exception when deserializing (using -ea an assertionerror\r\nwas thrown).\r\n\r\nThis fixes a check for changing the serialization logic, so that negative numbers are read correctly.\r\n\r\nCloses #6325\r\nCloses #5357'
6483,'areek',"Admin: Expose IndexWriter and versionMap RAM usage in stats\n#6443 added RAM accounting to InternalEngine's versionMap.  In this issue we should make this available; Boaz suggested ShardStats and it's family and the indices cat api."
6479,'clintongormley',"Typo in histogram-facet.asciidoc\nSpotted a typo, which I've fixed."
6477,'colings86','Aggregations: Delegation of nextReader calls\nCurrently aggregations subscribe to setNextReader calls in AggregationContext.  When a new reader is used all reader aware objects are notified of the new reader.  With deferred aggregations children aggregations of a breath-first aggregation do not require to be notified of reader changes until the replay stage. Also, at the replay stage only the child aggregations of the breath-first aggregation need to be notified, we should not be notifying all the other aggregations of the new reader.\r\n\r\nTo solve this the calls for setNextReader are now handled by each aggregation so it can notify its child aggregations and any other ReaderContextAware objects (e.g. ValueSources) of the new reader at the relevant time.\r\n\r\nThe same idea has also been applied to the setScorer and setTopReader calls for Aggregators and ValueSources.'
6471,'martijnvg','The `ignore_unavailable=true` parameter seems to be ignored in REST\nWhen trying to search against a closed index via curl with the `ignore_unavailable` parameter, an error response is returned:\r\n\r\n```bash\r\ncurl -X DELETE \'http://localhost:9200/index_1?pretty\'\r\n\r\n# 2014-06-11T21:20:10+02:00 [200] (0.107s)\r\n#\r\n# {\r\n#   "acknowledged":true\r\n# }\r\n\r\ncurl -X DELETE \'http://localhost:9200/index_2?pretty\'\r\n\r\n# 2014-06-11T21:20:12+02:00 [200] (0.022s)\r\n#\r\n# {\r\n#   "acknowledged":true\r\n# }\r\n\r\ncurl -X PUT \'http://localhost:9200/index_1?pretty\'\r\n\r\n# 2014-06-11T21:20:25+02:00 [200] (0.218s)\r\n#\r\n# {\r\n#   "acknowledged":true\r\n# }\r\n\r\ncurl -X PUT \'http://localhost:9200/index_2?pretty\'\r\n\r\n# 2014-06-11T21:20:29+02:00 [200] (0.112s)\r\n#\r\n# {\r\n#   "acknowledged":true\r\n# }\r\n\r\ncurl -X POST \'http://localhost:9200/index_2/_close?pretty\'\r\n\r\n# 2014-06-11T21:20:37+02:00 [200] (0.024s)\r\n#\r\n# {\r\n#   "acknowledged":true\r\n# }\r\n\r\ncurl -X GET \'http://localhost:9200/index_1,index_2/_search?pretty&ignore_unavailable=true\'\r\n\r\n# 2014-06-11T21:20:57+02:00 [403] (0.005s)\r\n#\r\n# {"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}\r\n```\r\n'
6465,'Mpdreamz','[Aggregations] Added _meta construct to inject metadata\nThis PR adds the ability to associate a bit of state with each individual aggregation.\r\n\r\nUse cases:\r\n- Typed clients can inject some state so that the deserialization\r\n  process has enough information to deserialize the aggregations into\r\nthe correct type. Facets return `_type` to signal what kind of facet the\r\nresponse corresponds with.\r\n- UI state, since aggregations can be nested at arbitrary depths\r\n  injecting pieces of UI state can greatly simplify programming with\r\naggregations since there are no two tree data-structures to walk anymore only the aggregation data.\r\n\r\nthank you @martijnvg for the help reading the `byte[]` representation of _meta properly\r\n\r\n- this PR also fixes the response format of the new `geo_bounds` aggregation to wrap the result in a `"aggregationname" : {}`\r\n\r\n[Here\'s a gist of a sense session](https://gist.github.com/Mpdreamz/785a3347d4415e1fe6bb#file-aggs-metada) you can use to review this branch\r\n\r\n\r\nExample request:\r\n\r\n```json\r\n{\r\n"aggs": {\r\n    "name": {\r\n      "terms": {\r\n        "field": "title"\r\n      },\r\n      "meta": 2,\r\n      "aggs": {\r\n        "viewport" : {\r\n          "meta" : {\r\n               "complex" : "object"\r\n          },\r\n          "geo_bounds" : {\r\n            "field" : "location"\r\n          }\r\n        },\r\n        "empty_agg" : {\r\n          "meta" : "should still be returned",\r\n          "sum": {\r\n            "field": "i_do_not_exist"\r\n          }\r\n        },\r\n```\r\n\r\n\r\nExample response:\r\n\r\n```json\r\n{\r\n   "aggregations": {\r\n      "name": {\r\n         "meta": 2,\r\n         "buckets": [\r\n            {\r\n               "key": "banner",\r\n               "doc_count": 1,\r\n               "viewport": {\r\n                  "meta": {\r\n                    "complex": "object"\r\n                  },\r\n                  "bounds": {\r\n                     "top_left": {\r\n                        "lat": 28,\r\n                        "lon": 21\r\n                     },\r\n                     "bottom_right": {\r\n                        "lat": 28,\r\n                        "lon": 21\r\n                     }\r\n                  }\r\n               },\r\n               "nested_terms": {\r\n                  "meta": {\r\n                     "complex": "object",\r\n                     "nuber": 1\r\n                  },\r\n                  "buckets": [\r\n                     {\r\n                        "key": "a",\r\n                        "doc_count": 1,\r\n                        "counter": {\r\n                           "meta": 4.1,\r\n                           "value": 40\r\n                        }\r\n                     },\r\n```\r\n\r\n'
6464,'javanna',"Transport: copied over REST headers from RestRequest to TransportRequest\nMade sure that the REST headers are copied over from each `RestRequest` to the related `TransportRequest`(s).\r\n\r\nIntroduced new base class (`BaseActionRequestRestHandler`) for all the REST actions that map to a single `TransportRequest` (most of them). Split the existing `handleRequest` method in two phases: 1) request creation 2) request execution. Between the two we can automatically copy the headers to the transport request in the base class so we don't have to do it manually all the time.\r\n\r\nAlso added a utility method to copy the headers in `BaseRestHandler`, to be used by all the actions that don't extend the newly introduced `BaseActionRequestRestHandler` (e.g. cat api as each `RestRequest` maps to multiple `TransportRequest`s).\r\n\r\nThe following changes have been made as well:\r\n 1) split benchmark actions into different RestAction classes (submit, abort, status)\r\n 2) modified a couple of actions that were doing some validation during transport request creation to throw `ActionRequestValidationException` instead of directly sending the error through the channel (`RestGetSourceAction` and `RestIndexAction`)"
6463,'jpountz','Memory pre-allocation with size attribute\nWhen I do a filter or query, and set the size valeu to a huge value (like 10000000), the memory consuption is increased a lot even though the final result be 10 documents.\r\n\r\nIf I do the same thing with a huge amount of users the memory is increased so fast that the nodes would become bottlenecks (sometimes a node shutdowns itself).\r\n\r\nI solved that changing the size attribute to a low value, but I think it would be better if the ES deal with that in a better way, avoiding bad programmers to shutdown the server.\r\n\r\nRegards.'
6457,'imotov','Snapshot/Restore: it should be possible to restore an index without restoring its aliases\n'
6454,'javanna','Fixed is/if typo in Api Conventions doc.\n'
6447,'javanna','Update nested-aggregation.asciidoc\nJust a typo in the json request, coma missing.'
6446,'javanna',"Highlighting: Make the HighlightQuery class public\nWe're using the highlighter directly in our code, and the introduction of the package-scoped HighlightQuery in 4aa59aff0058b05e822592431eda4a469a6b9eef has made this impossible without building our own little fork."
6445,'javanna','Build: Generate source jars for tests\nCloses #6125'
6442,'colings86',"Null geopoint should not throw an exception\nRefers to: https://github.com/elasticsearch/elasticsearch/issues/5390 and https://gist.github.com/hkorte/9936192\r\n\r\nWhile upgrading to 1.2.1 we have run into this issue.\r\n\r\nWe use model objects that we serialize into JSON. One of the fields is a geopoint. Sometimes we don't have the lat/lon yet, so we were leaving the field null until we do have the data, then we update the document. \r\n\r\nI can understand that a location that is missing lat or missing lon would be invalid; this warrants an exception. But null or empty is not invalid, nor is it reason to throw an exception. \r\n\r\nSince, according to the Gist above there's is no longer a way to leave the geopoint null you are forcing me to write a custom JSON serializer to exclude the field when the geo data is not yet available. This is considerably more work/maintenance for myself and everyone else who encounters this. I don't see a good reason for this.\r\n\r\nI suggest that Elasticsearch should handle null and empty gracefully by ignoring/skipping the field instead of throwing an exception. Null and empty aren't invalid values. They simply mean there's no value for this field at this time.\r\n\r\n-----------------------------------------------------\r\n\r\nUpdate: Later on after writing this I realized I can use `@JsonInclude(JsonInclude.Include.NON_NULL)` to exclude null fields from my JSON. However, I still think my above points are valid. \r\n\r\nThanks, Matt\r\n\r\n\r\n\r\n"
6441,'bleskes','ElasticsearchIllegalArgumentException No version type match\nI\'m using es 1.2.1 in with settings like this\r\n```\r\n      NodeBuilder.nodeBuilder().local(true).data(true).settings(\r\nImmutableSettings.settingsBuilder().\r\n        put("path.data", dataPath).\r\n        put("node.http.enabled", false).\r\n        put("http.enabled", false).\r\n        put("index.number_of_shards", 1).\r\n        put("index.number_of_replicas", 1).\r\n        put("discovery.zen.ping.unicast.hosts", "localhost").\r\n        put("discovery.zen.ping.multicast.enabled", false).\r\n        build())\r\n```\r\nwhen i start two jvms and start inserting stuff in one of them after restart i get \r\n```\r\n[info] 2014-06-09 16:56:50,257 - o.e.node - [Taskmaster] version[1.2.1], pid[7220], build[6c95b75/2014-06-03T15:02:52Z]\r\n[info] 2014-06-09 16:56:50,258 - o.e.node - [Taskmaster] initializing ...\r\n[info] 2014-06-09 16:56:50,262 - o.e.plugins - [Taskmaster] loaded [], sites []\r\n[info] 2014-06-09 16:56:52,392 - o.e.node - [Taskmaster] initialized\r\n[info] 2014-06-09 16:56:52,392 - o.e.node - [Taskmaster] starting ...\r\n[info] 2014-06-09 16:56:54,643 - o.e.transport - [Taskmaster] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[apc/192.168.9.233:9300]}\r\n[info] 2014-06-09 16:56:57,681 - o.e.c.service - [Taskmaster] new_master [Taskmaster][4PROogcjRSSUjP-He9HNGA][apc][inet[apc/192.168.9.233:9300]]{http.enabled=false, local=false}, reason: zen-disco-join (elected_as_master)\r\n[info] 2014-06-09 16:56:57,689 - o.e.discovery - [Taskmaster] elasticsearch/4PROogcjRSSUjP-He9HNGA\r\n[info] 2014-06-09 16:56:58,544 - o.e.gateway - [Taskmaster] recovered [2] indices into cluster_state\r\n[info] 2014-06-09 16:56:58,545 - o.e.node - [Taskmaster] started\r\n[warn] 2014-06-09 16:56:59,204 - o.e.i.cluster - [Taskmaster] [users_idx3][0] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [users_idx3][0] failed to recover shard\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:241) ~[elasticsearch-1.2.1.jar:na]\r\n\tat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132) ~[elasticsearch-1.2.1.jar:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]\r\n\tat java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]\r\nCaused by: org.elasticsearch.ElasticsearchIllegalArgumentException: No version type match [49]\r\n\tat org.elasticsearch.index.VersionType.fromValue(VersionType.java:307) ~[elasticsearch-1.2.1.jar:na]\r\n\tat org.elasticsearch.index.translog.Translog$Index.readFrom(Translog.java:506) ~[elasticsearch-1.2.1.jar:na]\r\n\tat org.elasticsearch.index.translog.TranslogStreams.readTranslogOperation(TranslogStreams.java:52) ~[elasticsearch-1.2.1.jar:na]\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:218) ~[elasticsearch-1.2.1.jar:na]\r\n\t... 4 common frames omitted\r\n\r\n```\r\ni have to delete the index after this\r\nthis is on windows'
6435,'jpountz','Intermittent DateRange aggregation bug.\nAm seeing a intermittent bug whereby we see incorrect and internally inconsistent aggregation results (ie where the subaggregations have more items that the parent aggregation).\r\n\r\n```javascript\r\n{\r\n    "sales_quotas": {\r\n        "doc_count": 5,\r\n        "shipmentDate": {\r\n            "buckets": [\r\n                {\r\n                    "key": "Overdue",\r\n                    "to": 1.3989024E12,\r\n                    "to_as_string": "2014-05-01",\r\n                    "doc_count": 0,\r\n                    "commodity": {\r\n                        "buckets": [\r\n                            {\r\n                                "key": "类ベ\U0007e957蕏ɛ",\r\n                                "doc_count": 5,\r\n                                "quantityNormalised": {\r\n                                    "value": 5000.0\r\n                                },\r\n                                "unallocatedQuantityNormalised": {\r\n                                    "value": 5000.0\r\n                                }\r\n                            }\r\n                        ]\r\n                    },\r\n                    "nothingAllocated": {\r\n                        "doc_count": 5,\r\n                        "ME": {\r\n                            "doc_count": 3\r\n                        },\r\n                        "NOT_ME": {\r\n                            "doc_count": 2\r\n                        }\r\n                    },\r\n                    "notFullyAllocated": {\r\n                        "doc_count": 0,\r\n                        "ME": {\r\n                            "doc_count": 0\r\n                        },\r\n                        "NOT_ME": {\r\n                            "doc_count": 0\r\n                        }\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nI have written a test-case which (if you run it enough times) will reproduce it.\r\n\r\nHere is the test class:\r\n\r\n*AggregationBugs.java*\r\n```java\r\npackage elastic.bugs;\r\n\r\nimport java.io.IOException;\r\nimport java.util.Collection;\r\nimport java.util.Map;\r\n\r\nimport org.elasticsearch.action.search.SearchRequestBuilder;\r\nimport org.elasticsearch.action.search.SearchResponse;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.search.aggregations.Aggregations;\r\nimport org.elasticsearch.search.aggregations.bucket.SingleBucketAggregation;\r\nimport org.elasticsearch.search.aggregations.bucket.filter.InternalFilter;\r\nimport org.elasticsearch.search.aggregations.bucket.range.date.InternalDateRange;\r\nimport org.elasticsearch.search.aggregations.bucket.terms.StringTerms;\r\nimport org.elasticsearch.search.aggregations.bucket.terms.Terms;\r\nimport org.elasticsearch.search.aggregations.metrics.sum.InternalSum;\r\nimport org.elasticsearch.test.ElasticsearchIntegrationTest;\r\nimport org.junit.Test;\r\n\r\nimport com.fasterxml.jackson.core.JsonGenerator;\r\nimport com.fasterxml.jackson.databind.DeserializationFeature;\r\nimport com.fasterxml.jackson.databind.ObjectMapper;\r\nimport com.fasterxml.jackson.databind.SerializationFeature;\r\nimport com.fasterxml.jackson.datatype.joda.JodaModule;\r\n\r\nimport static org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder;\r\nimport static org.elasticsearch.test.ElasticsearchIntegrationTest.Scope.SUITE;\r\n\r\n@ElasticsearchIntegrationTest.ClusterScope(scope = SUITE, numNodes = 1)\r\npublic class AggregationBugs extends ElasticsearchIntegrationTest\r\n{\r\n    private static final String INDEX = "bugindex";\r\n    private final ObjectMapper json = new ObjectMapper();\r\n\r\n    public AggregationBugs()\r\n    {\r\n        json.configure(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS, false);\r\n        json.configure(SerializationFeature.WRITE_BIGDECIMAL_AS_PLAIN, true);\r\n        json.configure(DeserializationFeature.USE_BIG_DECIMAL_FOR_FLOATS, true);\r\n        json.configure(JsonGenerator.Feature.ESCAPE_NON_ASCII, true);\r\n        json.registerModule(new JodaModule());\r\n    }\r\n\r\n    @Override\r\n    protected Settings nodeSettings(int nodeOrdinal)\r\n    {\r\n        return settingsBuilder()\r\n            .put("path.data", "target/elastic-test-data")\r\n            .build();\r\n    }\r\n\r\n    private void index(String id, String type, String content) throws IOException\r\n    {\r\n        index(INDEX, type, id, json.readValue(content, Map.class));\r\n    }\r\n\r\n    @Test\r\n    public void subAggregationBug() throws IOException\r\n    {\r\n        admin()\r\n            .indices()\r\n            .prepareCreate(INDEX)\r\n            .execute()\r\n            .actionGet();\r\n\r\n        String propertiesSource = "{\\"properties\\":{\\"contractualLocationCity\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quantityUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"contractualLocationCountry\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"isAllocationNotInvoiced\\":{\\"type\\":\\"boolean\\"},\\"isFullyAllocated\\":{\\"type\\":\\"boolean\\"},\\"allocatedQuantity\\":{\\"type\\":\\"double\\"},\\"contractualLocation\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quantityNormalisedUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"isInvoicedFinal\\":{\\"type\\":\\"boolean\\"},\\"isZeroAllocated\\":{\\"type\\":\\"boolean\\"},\\"quotaId\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"long\\"},\\"counterParty\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"allocationStatus\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quantity\\":{\\"type\\":\\"double\\"},\\"allocatedQuantityUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"tradeRef\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"unallocatedQuantity\\":{\\"type\\":\\"double\\"},\\"contract\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"contractualLocationCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"shape\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quotaStatus\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"invoiceStatus\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"unallocatedQuantityNormalisedUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"isActive\\":{\\"type\\":\\"boolean\\"},\\"shipmentDate\\":{\\"format\\":\\"date\\",\\"type\\":\\"date\\"},\\"completedDate\\":{\\"format\\":\\"date\\",\\"type\\":\\"date\\"},\\"groupCompany\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"invoiceQuantity\\":{\\"type\\":\\"double\\"},\\"resolvedIds\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"expectedSalesMonth\\":{\\"format\\":\\"date\\",\\"type\\":\\"date\\"},\\"expectedSalesLocationCity\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"isCancelled\\":{\\"type\\":\\"boolean\\"},\\"neptuneQuotaId\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"uninvoicedQuantityNormalised\\":{\\"type\\":\\"double\\"},\\"contractDutyPaid\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"unallocatedQuantityNormalised\\":{\\"type\\":\\"double\\"},\\"contractIncotermsCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"groupCompanyCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"grade\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"invoiceQuantityUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"unallocatedQuantityUnitOfMeasure\\":{\\"index\\":\\"no\\",\\"type\\":\\"string\\"},\\"counterPartyCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"isPurchase\\":{\\"type\\":\\"boolean\\"},\\"isCompleted\\":{\\"type\\":\\"boolean\\"},\\"trafficOperatorFullName\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"groupCompanyId\\":{\\"include_in_all\\":false,\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"expectedSalesLocation\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"intentIncotermsCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"trafficOperatorSid\\":{\\"include_in_all\\":false,\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quantityNormalised\\":{\\"type\\":\\"double\\"},\\"commodity\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"contractIncoterms\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"expectedSalesLocationCode\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"brand\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quotaType\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"expectedSalesLocationCountry\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"},\\"quotaRef\\":{\\"index\\":\\"not_analyzed\\",\\"boost\\":2.0,\\"type\\":\\"string\\"},\\"comments\\":{\\"type\\":\\"string\\"},\\"intentIncoterms\\":{\\"index\\":\\"not_analyzed\\",\\"type\\":\\"string\\"}}}";\r\n        admin()\r\n            .indices()\r\n            .preparePutMapping(INDEX)\r\n            .setType("quota")\r\n            .setSource(propertiesSource)\r\n            .execute()\r\n            .actionGet();\r\n\r\n        index("1581354603", "quota", "{\\"allocatedQuantity\\":0.0000,\\"allocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"allocationStatus\\":\\"UNALLOCATED\\",\\"brand\\":null,\\"comments\\":\\"comments...\\",\\"commodity\\":\\"\\\\uFAAE\\\\u30D9\\\\uD9BA\\\\uDD57\\\\u854F\\\\u025B\\",\\"completedDate\\":null,\\"contract\\":\\"contract\\",\\"contractDutyPaid\\":null,\\"expectedSalesMonth\\":\\"2014-06-06\\",\\"grade\\":null,\\"groupCompany\\":\\"GROUP-COMPANY-NAME-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyCode\\":\\"GROUP-COMPANY-CODE-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyId\\":\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"invoiceQuantity\\":0.0000,\\"invoiceQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"invoiceStatus\\":\\"NOTFINALINVOICED\\",\\"isActive\\":true,\\"isAllocationNotInvoiced\\":false,\\"isCancelled\\":false,\\"isCompleted\\":false,\\"isFullyAllocated\\":false,\\"isInvoicedFinal\\":false,\\"isPurchase\\":false,\\"isZeroAllocated\\":true,\\"neptuneQuotaId\\":\\"neptune-quota-id\\",\\"quantity\\":1000.0000,\\"quantityNormalised\\":1000.0000,\\"quantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quotaId\\":\\"1581354603\\",\\"quotaRef\\":\\"2723.4\\",\\"quotaStatus\\":\\"OPEN\\",\\"quotaType\\":\\"SALES\\",\\"resolvedIds\\":[\\"5D2E1DF59DB54303819A7A1320DF6053\\",\\"D9B2681C5CEB4FC1AB34280B32FB1C0E\\",\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"18F42D923B444C3FB074D1A05548BE2B\\"],\\"shape\\":null,\\"shipmentDate\\":\\"2014-05-30\\",\\"tradeRef\\":\\"edm-trade-id\\",\\"trafficOperatorFullName\\":\\"James Brown\\",\\"trafficOperatorSid\\":\\"SID2\\",\\"unallocatedQuantity\\":1000.0000,\\"unallocatedQuantityNormalised\\":1000.0000,\\"unallocatedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"unallocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"uninvoicedQuantityNormalised\\":0.0000,\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\"}");\r\n        index("839452955", "quota", "{\\"allocatedQuantity\\":0.0000,\\"allocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"allocationStatus\\":\\"UNALLOCATED\\",\\"brand\\":null,\\"comments\\":\\"comments...\\",\\"commodity\\":\\"\\\\uFAAE\\\\u30D9\\\\uD9BA\\\\uDD57\\\\u854F\\\\u025B\\",\\"completedDate\\":null,\\"contract\\":\\"contract\\",\\"contractDutyPaid\\":null,\\"expectedSalesMonth\\":\\"2014-06-06\\",\\"grade\\":null,\\"groupCompany\\":\\"GROUP-COMPANY-NAME-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyCode\\":\\"GROUP-COMPANY-CODE-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyId\\":\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"invoiceQuantity\\":0.0000,\\"invoiceQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"invoiceStatus\\":\\"NOTFINALINVOICED\\",\\"isActive\\":true,\\"isAllocationNotInvoiced\\":false,\\"isCancelled\\":false,\\"isCompleted\\":false,\\"isFullyAllocated\\":false,\\"isInvoicedFinal\\":false,\\"isPurchase\\":false,\\"isZeroAllocated\\":true,\\"neptuneQuotaId\\":\\"neptune-quota-id\\",\\"quantity\\":1000.0000,\\"quantityNormalised\\":1000.0000,\\"quantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quotaId\\":\\"839452955\\",\\"quotaRef\\":\\"2723.1\\",\\"quotaStatus\\":\\"OPEN\\",\\"quotaType\\":\\"SALES\\",\\"resolvedIds\\":[\\"CE280D70F7CD4E16B871983420E7A136\\",\\"D9B2681C5CEB4FC1AB34280B32FB1C0E\\",\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"18F42D923B444C3FB074D1A05548BE2B\\"],\\"shape\\":null,\\"shipmentDate\\":\\"2014-05-30\\",\\"tradeRef\\":\\"edm-trade-id\\",\\"trafficOperatorFullName\\":\\"Current User \\\\u054C\\\\u0561\\\\u0581\\\\u058B\\\\u0545\\\\u0576\\\\u0565\\\\u054C\\\\u0550\\\\u0582\\",\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\",\\"unallocatedQuantity\\":1000.0000,\\"unallocatedQuantityNormalised\\":1000.0000,\\"unallocatedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"unallocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"uninvoicedQuantityNormalised\\":0.0000,\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\"}");\r\n        index("679144004", "quota", "{\\"allocatedQuantity\\":0.0000,\\"allocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"allocationStatus\\":\\"UNALLOCATED\\",\\"brand\\":null,\\"comments\\":\\"comments...\\",\\"commodity\\":\\"\\\\uFAAE\\\\u30D9\\\\uD9BA\\\\uDD57\\\\u854F\\\\u025B\\",\\"completedDate\\":null,\\"contract\\":\\"contract\\",\\"contractDutyPaid\\":null,\\"expectedSalesMonth\\":\\"2014-06-06\\",\\"grade\\":null,\\"groupCompany\\":\\"GROUP-COMPANY-NAME-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyCode\\":\\"GROUP-COMPANY-CODE-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyId\\":\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"invoiceQuantity\\":0.0000,\\"invoiceQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"invoiceStatus\\":\\"NOTFINALINVOICED\\",\\"isActive\\":true,\\"isAllocationNotInvoiced\\":false,\\"isCancelled\\":false,\\"isCompleted\\":false,\\"isFullyAllocated\\":false,\\"isInvoicedFinal\\":false,\\"isPurchase\\":false,\\"isZeroAllocated\\":true,\\"neptuneQuotaId\\":\\"neptune-quota-id\\",\\"quantity\\":1000.0000,\\"quantityNormalised\\":1000.0000,\\"quantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quotaId\\":\\"679144004\\",\\"quotaRef\\":\\"2723.5\\",\\"quotaStatus\\":\\"OPEN\\",\\"quotaType\\":\\"SALES\\",\\"resolvedIds\\":[\\"CE280D70F7CD4E16B871983420E7A136\\",\\"D9B2681C5CEB4FC1AB34280B32FB1C0E\\",\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"18F42D923B444C3FB074D1A05548BE2B\\"],\\"shape\\":null,\\"shipmentDate\\":\\"2014-05-30\\",\\"tradeRef\\":\\"edm-trade-id\\",\\"trafficOperatorFullName\\":\\"Current User \\\\u054C\\\\u0561\\\\u0581\\\\u058B\\\\u0545\\\\u0576\\\\u0565\\\\u054C\\\\u0550\\\\u0582\\",\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\",\\"unallocatedQuantity\\":1000.0000,\\"unallocatedQuantityNormalised\\":1000.0000,\\"unallocatedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"unallocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"uninvoicedQuantityNormalised\\":0.0000,\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\"}");\r\n        index("516594663", "quota", "{\\"allocatedQuantity\\":0.0000,\\"allocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"allocationStatus\\":\\"UNALLOCATED\\",\\"brand\\":null,\\"comments\\":\\"comments...\\",\\"commodity\\":\\"\\\\uFAAE\\\\u30D9\\\\uD9BA\\\\uDD57\\\\u854F\\\\u025B\\",\\"completedDate\\":null,\\"contract\\":\\"contract\\",\\"contractDutyPaid\\":null,\\"expectedSalesMonth\\":\\"2014-06-06\\",\\"grade\\":null,\\"groupCompany\\":\\"GROUP-COMPANY-NAME-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyCode\\":\\"GROUP-COMPANY-CODE-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyId\\":\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"invoiceQuantity\\":0.0000,\\"invoiceQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"invoiceStatus\\":\\"NOTFINALINVOICED\\",\\"isActive\\":true,\\"isAllocationNotInvoiced\\":false,\\"isCancelled\\":false,\\"isCompleted\\":false,\\"isFullyAllocated\\":false,\\"isInvoicedFinal\\":false,\\"isPurchase\\":false,\\"isZeroAllocated\\":true,\\"neptuneQuotaId\\":\\"neptune-quota-id\\",\\"quantity\\":1000.0000,\\"quantityNormalised\\":1000.0000,\\"quantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quotaId\\":\\"516594663\\",\\"quotaRef\\":\\"2723.3\\",\\"quotaStatus\\":\\"OPEN\\",\\"quotaType\\":\\"SALES\\",\\"resolvedIds\\":[\\"CE280D70F7CD4E16B871983420E7A136\\",\\"D9B2681C5CEB4FC1AB34280B32FB1C0E\\",\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"18F42D923B444C3FB074D1A05548BE2B\\"],\\"shape\\":null,\\"shipmentDate\\":\\"2014-05-30\\",\\"tradeRef\\":\\"edm-trade-id\\",\\"trafficOperatorFullName\\":\\"Current User \\\\u054C\\\\u0561\\\\u0581\\\\u058B\\\\u0545\\\\u0576\\\\u0565\\\\u054C\\\\u0550\\\\u0582\\",\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\",\\"unallocatedQuantity\\":1000.0000,\\"unallocatedQuantityNormalised\\":1000.0000,\\"unallocatedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"unallocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"uninvoicedQuantityNormalised\\":0.0000,\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\"}");\r\n        index("1677219499", "quota", "{\\"allocatedQuantity\\":0.0000,\\"allocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"allocationStatus\\":\\"UNALLOCATED\\",\\"brand\\":null,\\"comments\\":\\"comments...\\",\\"commodity\\":\\"\\\\uFAAE\\\\u30D9\\\\uD9BA\\\\uDD57\\\\u854F\\\\u025B\\",\\"completedDate\\":null,\\"contract\\":\\"contract\\",\\"contractDutyPaid\\":null,\\"expectedSalesMonth\\":\\"2014-06-06\\",\\"grade\\":null,\\"groupCompany\\":\\"GROUP-COMPANY-NAME-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyCode\\":\\"GROUP-COMPANY-CODE-\\\\uE815\\\\uD866\\\\uDD82d\\\\u549D\\\\uD8D3\\\\uDEF4\\\\uE9C3\\\\uD9A8\\\\uDDA5\\\\u716Fqd\\\\uB1B7\\",\\"groupCompanyId\\":\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"invoiceQuantity\\":0.0000,\\"invoiceQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"invoiceStatus\\":\\"NOTFINALINVOICED\\",\\"isActive\\":true,\\"isAllocationNotInvoiced\\":false,\\"isCancelled\\":false,\\"isCompleted\\":false,\\"isFullyAllocated\\":false,\\"isInvoicedFinal\\":false,\\"isPurchase\\":false,\\"isZeroAllocated\\":true,\\"neptuneQuotaId\\":\\"neptune-quota-id\\",\\"quantity\\":1000.0000,\\"quantityNormalised\\":1000.0000,\\"quantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"quotaId\\":\\"1677219499\\",\\"quotaRef\\":\\"2723.2\\",\\"quotaStatus\\":\\"OPEN\\",\\"quotaType\\":\\"SALES\\",\\"resolvedIds\\":[\\"D9B2681C5CEB4FC1AB34280B32FB1C0E\\",\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\",\\"18F42D923B444C3FB074D1A05548BE2B\\",\\"D41031D6EF9E4F829913E856FE2B8E89\\"],\\"shape\\":null,\\"shipmentDate\\":\\"2014-05-30\\",\\"tradeRef\\":\\"edm-trade-id\\",\\"trafficOperatorFullName\\":\\"Mary\\",\\"trafficOperatorSid\\":\\"SID1\\",\\"unallocatedQuantity\\":1000.0000,\\"unallocatedQuantityNormalised\\":1000.0000,\\"unallocatedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"unallocatedQuantityUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\",\\"uninvoicedQuantityNormalised\\":0.0000,\\"uninvoicedQuantityNormalisedUnitOfMeasure\\":\\"\\\\uF5FC\\\\u03FE\\\\u0105r\\\\u0440Z\\"}");\r\n        refresh();\r\n\r\n        String aggregationSource = "{\\"size\\":0,\\"aggregations\\":{\\"sales_quotas\\":{\\"filter\\":{\\"and\\":{\\"filters\\":[{\\"range\\":{\\"shipmentDate\\":{\\"from\\":null,\\"to\\":\\"2014-06-30\\",\\"include_lower\\":true,\\"include_upper\\":true}}},{\\"terms\\":{\\"groupCompanyId\\":[\\"DBF8E7284EC7423BBE1FCE8C0FDE9039\\"]}},{\\"type\\":{\\"value\\":\\"quota\\"}},{\\"term\\":{\\"isPurchase\\":false}},{\\"terms\\":{\\"quotaStatus\\":[\\"OPEN\\"]}}]}},\\"aggregations\\":{\\"shipmentDate\\":{\\"date_range\\":{\\"field\\":\\"shipmentDate\\",\\"ranges\\":[{\\"key\\":\\"Overdue\\",\\"to\\":\\"2014-05-01\\"},{\\"key\\":\\"May\\",\\"from\\":\\"2014-05-01\\",\\"to\\":\\"2014-06-01\\"},{\\"key\\":\\"June\\",\\"from\\":\\"2014-06-01\\",\\"to\\":\\"2014-07-01\\"}]},\\"aggregations\\":{\\"nothingAllocated\\":{\\"filter\\":{\\"term\\":{\\"isZeroAllocated\\":true}},\\"aggregations\\":{\\"ME\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}},\\"NOT_ME\\":{\\"filter\\":{\\"not\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}}}}}},\\"notFullyAllocated\\":{\\"filter\\":{\\"and\\":{\\"filters\\":[{\\"term\\":{\\"isZeroAllocated\\":false}},{\\"term\\":{\\"isFullyAllocated\\":false}}]}},\\"aggregations\\":{\\"ME\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}},\\"NOT_ME\\":{\\"filter\\":{\\"not\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}}}}}},\\"allocationNotFullyInvoiced\\":{\\"filter\\":{\\"term\\":{\\"isAllocationNotInvoiced\\":true}},\\"aggregations\\":{\\"ME\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}},\\"NOT_ME\\":{\\"filter\\":{\\"not\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}}}}}},\\"allocationNotFinalInvoiced\\":{\\"filter\\":{\\"and\\":{\\"filters\\":[{\\"term\\":{\\"isZeroAllocated\\":false}},{\\"term\\":{\\"isInvoicedFinal\\":false}}]}},\\"aggregations\\":{\\"ME\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}},\\"NOT_ME\\":{\\"filter\\":{\\"not\\":{\\"filter\\":{\\"term\\":{\\"trafficOperatorSid\\":\\"SID-IhNqzHDayeRxFVJHzoRy\\"}}}}}}},\\"commodity\\":{\\"terms\\":{\\"field\\":\\"commodity\\"},\\"aggregations\\":{\\"quantityNormalised\\":{\\"sum\\":{\\"field\\":\\"quantityNormalised\\"}},\\"unallocatedQuantityNormalised\\":{\\"sum\\":{\\"field\\":\\"unallocatedQuantityNormalised\\"}}}}}}}}}}";\r\n        SearchRequestBuilder request = client()\r\n            .prepareSearch(INDEX)\r\n            .setSource(aggregationSource);\r\n\r\n        System.out.println("request = " + prettyPrint(aggregationSource));\r\n\r\n        SearchResponse response = request\r\n            .execute()\r\n            .actionGet();\r\n\r\n        System.out.println("response = " + response);\r\n\r\n        SingleBucketAggregation salesQuotas = response.getAggregations().get("sales_quotas");\r\n        InternalDateRange dateRangeAggregation = salesQuotas.getAggregations().get("shipmentDate");\r\n\r\n        assertAllZero(dateRangeAggregation, "Overdue");\r\n        assertExpectedValues(dateRangeAggregation, "May");\r\n        assertAllZero(dateRangeAggregation, "June");\r\n    }\r\n\r\n    private void assertExpectedValues(InternalDateRange dateRangeAggregation, String name)\r\n    {\r\n        InternalDateRange.Bucket bucket = dateRangeAggregation.getBucketByKey(name);\r\n        assertFilter(bucket, name, "nothingAllocated", 5, 3, 2);\r\n        assertFilter(bucket, name, "notFullyAllocated", 0, 0, 0);\r\n        assertFilter(bucket, name, "allocationNotFullyInvoiced", 0, 0, 0);\r\n        assertFilter(bucket, name, "allocationNotFinalInvoiced", 0, 0, 0);\r\n\r\n        StringTerms commodity = bucket.getAggregations().get("commodity");\r\n        Collection<Terms.Bucket> buckets = commodity.getBuckets();\r\n        assertEquals("commodity buckets", 1, buckets.size());\r\n        assertSum(buckets, "quantityNormalised", 5000.0);\r\n        assertSum(buckets, "unallocatedQuantityNormalised", 5000.0);\r\n    }\r\n\r\n    private void assertSum(Collection<Terms.Bucket> buckets, String name, double expectedValue)\r\n    {\r\n        InternalSum quantityNormalised = buckets.iterator().next().getAggregations().get(name);\r\n        assertEquals(name, expectedValue, quantityNormalised.getValue(), 0.1);\r\n    }\r\n\r\n    private void assertAllZero(InternalDateRange dateRangeAggregation, String name)\r\n    {\r\n        InternalDateRange.Bucket bucket = dateRangeAggregation.getBucketByKey(name);\r\n        assertFilter(bucket, name, "nothingAllocated", 0, 0, 0);\r\n        assertFilter(bucket, name, "notFullyAllocated", 0, 0, 0);\r\n        assertFilter(bucket, name, "allocationNotFullyInvoiced", 0, 0, 0);\r\n        assertFilter(bucket, name, "allocationNotFinalInvoiced", 0, 0, 0);\r\n\r\n        Aggregations aggregations = bucket.getAggregations();\r\n        StringTerms commodity = aggregations.get("commodity");\r\n        assertEquals(name + " commodity buckets should be zero", 0, commodity.getBuckets().size());\r\n    }\r\n\r\n    private void assertFilter(InternalDateRange.Bucket bucket, String bucketName, String filterName, int expectedFilterCount, int expectedMeCount, int expectedNotMeCount)\r\n    {\r\n        String message = bucketName + ":" + filterName;\r\n\r\n        Aggregations aggregations = bucket.getAggregations();\r\n\r\n        InternalFilter filter = aggregations.get(filterName);\r\n        assertEquals(message, expectedFilterCount, filter.getDocCount());\r\n\r\n        assertBreakdowns(message, filter.getAggregations(), expectedMeCount, expectedNotMeCount);\r\n    }\r\n\r\n    private void assertBreakdowns(String message, Aggregations aggregations, int expectedMeCount, int expectedNotMeCount)\r\n    {\r\n        assertBreakdown(aggregations, message, "ME", expectedMeCount);\r\n        assertBreakdown(aggregations, message, "NOT_ME", expectedNotMeCount);\r\n    }\r\n\r\n    private void assertBreakdown(Aggregations aggregations, String message, String subFilterName, int expectedCount)\r\n    {\r\n        InternalFilter filter = aggregations.get(subFilterName);\r\n        assertEquals(message + ":" + subFilterName, expectedCount, filter.getDocCount());\r\n    }\r\n\r\n    private String prettyPrint(String aggregationSource) throws IOException\r\n    {\r\n        return json.writerWithDefaultPrettyPrinter().writeValueAsString(json.readValue(aggregationSource, Map.class));\r\n    }\r\n}\r\n\r\n```\r\n\r\nI hope this helps.'
6432,'colings86','Aggregations: Added percentile rank aggregation\nPercentile Rank Aggregation is the reverse of the Percentiles aggregation.  It determines the percentile rank (the proportion of values less than a given value) of the provided array of values.\r\n\r\nCloses #6386'
6429,'javanna','Index template API: unified PUT/POST behaviour in relation to create parameter\nThe put index template api supports the `create` parameter (defaults to false), which tells whether the template can replace an existing one with same name or not. When invoked through REST using the POST method, the `create` is currently forced to `true`, which is not consistent with our other apis.\r\n\r\nUnified its behaviour between PUT and POST method. Also added create parameter to the rest spec (was missing before) and a REST test for create true scenario.'
6424,'s1monw',"Internal: [FileSystem] Use XNativeFSLockFactory instead of the buggy Lucene 4.8.1 version\nThere is a pretty nasty [bug](https://issues.apache.org/jira/browse/LUCENE-5738) in the lock factory we use that can cause nodes to use the same data dir wiping each others data. Luckily this is unlikely to happen if the nodes are running in different JVM which they do unless they are embedded. We should place the fix for this as an X-Class until the fix is released.\r\n\r\nyet this might have other side-effects that we haven't uncovered yet so I mark it critical"
6421,'javanna',"Update post-filter.asciidoc\nRemove 'be' where it is not needed"
6417,'javanna','Add Javadoc\n'
6410,'s1monw','Bulk request against multiple indices fails on missing index instead of failing individual actions\nConsider the following repro use case:\r\n\r\n```json\r\nPUT /bulkindex1\r\nPUT /bulkindex2\r\nPOST /_bulk\r\n{"index":{"_id":"1","_type":"index1_type","_index":"bulkindex1"}}\r\n{"text": "hallo1" }\r\n{"index":{"_id":"1","_type":"index2_type","_index":"bulkindex2"}}\r\n{"text": "hallo2" }\r\nGET /bulkindex*/_search\r\nPOST /bulkindex2/_close\r\nPOST /_bulk\r\n{"index":{"_id":"1","_type":"index1_type","_index":"bulkindex1"}}\r\n{"text": "hallo1-update" }\r\n{"index":{"_id":"1","_type":"index2_type","_index":"bulkindex2"}}\r\n{"text": "hallo2" }\r\n```\r\n\r\nThe 2nd bulk action will certainly fail since bulkindex2 is closed.    However, when the _bulk request is submitted, ES fails the entire _bulk request:\r\n\r\n```json\r\n{\r\n   "error": "IndexMissingException[[bulkindex2] missing]",\r\n   "status": 404\r\n}\r\n```\r\n\r\nExpected behavior in this case is for ES to still process the request against the other index that is available and report the 404 as part of the response for the action against bulkindex2 like the following:\r\n\r\n```json\r\n{\r\n   "took": 14,\r\n   "errors": true,\r\n   "items": [\r\n      {\r\n         "index": {\r\n            "_index": "bulkindex1",\r\n            "_type": "index1_type",\r\n            "_id": "1",\r\n            "_version": 3,\r\n            "status": 200\r\n         }\r\n      },\r\n      {\r\n         "index": {\r\n            "_index": "bulkindex2",\r\n            "_type": "index2_type",\r\n            "_id": "1",\r\n            "status": 404,\r\n   "error": "IndexMissingException[[bulkindex2] missing]",\r\n         }\r\n      }\r\n   ]\r\n}\r\n```'
6409,'clintongormley',"Elastic Search Cluster becomes unresponsive \nHi,\r\n\r\nI have been facing this issue for couple of days. Here are the details -\r\n\r\nI have a cluster of 7 nodes, with 3 master eligible with no data and 4 only data nodes. I have set minimum master node property to 2. I have used KOPF/ElasticHQ/Head for monitoring. I have set up the cluster for testing so there is hardly any querying to going on. So it remains idle most of the time. After few hours of inactivity, the plugins cannot show the any details except for the green status (no other details like shard etc shown).  If I make any search query, it does not work. \r\n\r\ne.g. http://ironman:9200/_search?q=chrome\r\n\r\nI have to restart the master's elasticsearch service to make it responsive again. I checked the  $ tail -f /var/log/elasticsearch/avengers.log I don't see any failure/exceptions. I had elastic search 1.1 and updated to 1.2 as well. But I am still having this issue. \r\n\r\nPlease let me know if you need more details.\r\n\r\nThanks,\r\nNilesh.\r\n\r\n"
6407,'javanna','Add Javadoc\n'
6406,'javanna',"Scripting: Exposed _uid, _id and _type fields as stored fields (_fields notation)\nThe `_uid` field wasn't available in a script despite it's always stored. Made it available and made available also `_id` and `_type` fields that are deducted from it."
6403,'bleskes','Marvel plugin broken\nIf elasticsearch 1.2 is not listening on the ipv4 localhost the Marvel plugin does not work. Tried it in my dev environment on two machines.\r\n\r\nElasticsearch is bind to a ipv6 address. With 1.0 everything worked well.'
6402,'s1monw',"[BUILD] Promote artifacts from strings to their own type\n@s1monw Figured I would make this a formal PR.  Not sure what other errors you were getting during the release, but I've tested this end-to-end and it WFM."
6401,'jpountz','Guard the auto expand replicas setting against improper values\nThis is the PR for Issue #5752\r\n\r\nIt updates the documentation to be more precise, and adds guards in the code to ensure the values are recognized, and if not it emits a log message and throws an exception with a more helpful message.'
6399,'areek','Completion mapping type throws a misleading error on null value\nWhen a field is null, the completion mapping parser throws an error but it refers to the next field in the document, not the field with the completion type.\r\n\r\nAnother user had the same issue and reported it on your google group, but I don\'t see it here in the bug tracker. Here\'s the link he provided to reproduce the issue: \r\n\r\nhttps://gist.github.com/glade-at-gigwell/6408e0e4b69ddf2e8856\r\n\r\nAnd the stack trace:\r\n\r\n     {"acknowledged":true}[2014-05-18 13:40:24,150][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] creating index, cause [api], shards [1]/[0], mappings []\r\n     {"acknowledged":true}[2014-05-18 13:40:24,224][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] create_mapping [object]\r\n     {"acknowledged":true}[2014-05-18 13:40:24,245][INFO ][cluster.metadata         ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth] update_mapping [object] (dynamic)\r\n     {"_index":"completion_type_cant_handle_the_null_truth","_type":"object","_id":"1","_version":1,"created":true}[2014-05-18 13:40:24,265][DEBUG][action.index             ] [Aelfyre Whitemane] [completion_type_cant_handle_the_null_truth][0], node[k4lbsgzYSlWzynQkVGqMaw], [P], s[STARTED]: Failed to execute [index      {[completion_type_cant_handle_the_null_truth][object][2], source[{"field1" : null,"field2" : "nulls make me sad"}]}]\r\n     org.elasticsearch.index.mapper.MapperParsingException: failed to parse\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:540)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:462)\r\n\tat      org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:384)\r\n\tat      org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:203)\r\n\tat      org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)\r\n\tat      org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n     Caused by: org.elasticsearch.ElasticsearchIllegalArgumentException: Unknown field name[field2], must be one of [payload, input, weight, output]\r\n\tat      org.elasticsearch.index.mapper.core.CompletionFieldMapper.parse(CompletionFieldMapper.java:237)\r\n\tat      org.elasticsearch.index.mapper.object.ObjectMapper.serializeNullValue(ObjectMapper.java:505)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:465)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:515)\r\n\t... 8 more'
6398,'javanna','Update warmers.asciidoc\nFacets are slowly being deprecated\r\n(http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations.html). Documents should move to providing examples with aggregations instead of facets.'
6395,'martijnvg','Aggregations: Should refactor top_hits aggregation to extend MetricsAggregation\nNow that MetricsAggregations can support more than just doubles, we should refactor the top_hits aggregation to be a metrics aggregation since it is providing a metrics and not providing buckets to sub-aggregations.'
6389,'javanna','Reads better with a comma before the "but".\n'
6388,'javanna','Improved grammer\n'
6386,'colings86','Aggregations: Added the percentiles_rank aggregation\nThe percentiles aggregation currently allows to return a percentile given a percentage. However, the data-structure that we are using under the hoods is also able to do the [reverse operation](https://github.com/tdunning/t-digest/blob/master/src/main/java/com/tdunning/math/stats/TDigest.java#L112): given a value, what percentage of the values from my dataset are below it?\r\n\r\nFor example, if you have a dataset of response times and provided it with a response time of 134 (ms), it would be able to tell you that this is the 72th percentile.'
6383,'imotov','Snapshot/Restore: Allow deleting of interrupted snapshot\nIf a snapshot is interrupted by running of disk space or connection loss to the repository folder, it might be not possible to delete such snapshot.'
6380,'spinscale','CORS: Allowed to configure allow-credentials header to work via SSL\nElasticsearch supports CORS and Basic Authentication but not together. A browser sending a cross-origin request will omit credentials unless the XHR `withCredentials` field is set to `true`. If `withCredentials = true` there are two additional checks the browser must perform on the preflight response before sending the real request.\r\n\r\n- Preflight response `Access-Control-Allow-Origin` must be the Origin (host), not `*`\r\n- Preflight response must have header `Access-Control-Allow-Credentials: true`\r\n\r\nThe W3C spec [http://www.w3.org/TR/access-control/#resource-sharing-check-0](http://www.w3.org/TR/access-control/#resource-sharing-check-0) explains the CORS withCredentials check.\r\n\r\nThese headers can be added with 2 lines of code in `org.elasticsearch.http.netty.NettyHttpChannel.java` at line 95.\r\n\r\n```java\r\n// Add support for cross-origin Ajax requests (CORS)\r\nresp.headers().add("Access-Control-Allow-Origin", transport.settings().get("http.cors.allow-origin", HttpHeaders.getHeader(nettyRequest, "Origin", "*")));\r\nresp.headers().add("Access-Control-Allow-Credentials", transport.settings().get("http.cors.allow-credentials", "true"));\r\n```\r\n\r\nThis would make the `Access-Control-Allow-Origin` header fall back to the `Origin` header before falling back to `*`. It would also add the new header `Access-Control-Allow-Credentials: true`.\r\n\r\n## Reproduce steps\r\n1. Start an elasticsearch instance on `localhost:9200`\r\n2. Open a browser window with any domain other than `localhost:9200`. I\'m using an empty Chrome tab in my example.\r\n3. Run this JavaScript to send a request with basic authentication\r\n\r\n```js\r\nvar xhr = new XMLHttpRequest();\r\nxhr.open(\'POST\', \'http://user:passwd@localhost:9200\', true);\r\nxhr.withCredentials = true;\r\nxhr.setRequestHeader(\'Content-Type\', \'application/json\');\r\nxhr.send(\'{"query":{"match_all":{}}}\');\r\n```\r\n\r\nPreflight Request\r\n```\r\nOPTIONS http://localhost:9200/ HTTP/1.1\r\nHost: localhost:9200\r\nConnection: keep-alive\r\nCache-Control: no-cache\r\nAuthorization: Basic YWRtaW46d2VzdA==\r\nAccess-Control-Request-Method: POST\r\nPragma: no-cache\r\nOrigin: https://www.google.com\r\nUser-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/34.0.1847.116 Safari/537.36\r\nAccess-Control-Request-Headers: content-type\r\nAccept: */*\r\nAccept-Encoding: gzip,deflate,sdch\r\nAccept-Language: en-US,en;q=0.8,et;q=0.6\r\n```\r\n\r\nPreflight Response\r\n```\r\nHTTP/1.1 200 OK\r\nAccess-Control-Allow-Origin: *\r\nAccess-Control-Max-Age: 3\r\nAccess-Control-Allow-Methods: OPTIONS, HEAD, GET, POST, PUT, DELETE\r\nAccess-Control-Allow-Headers: X-Requested-With, Content-Type, Content-Length\r\nContent-Type: text/plain; charset=UTF-8\r\nContent-Length: 0\r\n```\r\n\r\nThe browser throws an error at this point saying the `Access-Control-Allow-Origin` and `Access-Control-Allow-Credentials` headers are bad.\r\n\r\n### Notes\r\nAfter you make the code changes the browser will start to cache preflight requests. When this happens you will only see the real request in Fiddler. To clear the cache, close the window and open a new one.\r\n\r\nThe XHR must be a [non-simple request](http://www.w3.org/TR/access-control/#simple-method). This is a rather vague spec but a POST with a content type of application/json should do it.\r\n\r\nIE skips most of these checks so don\'t bother testing in IE.\r\n\r\nThe example preflight request header `Authorization: Basic YWRtaW46d2VzdA==` is a bug in Chrome and shouldn\'t be included until the real request. There is a bug open for this here https://code.google.com/p/chromium/issues/detail?id=377541. Firefox works correctly and doesn\'t include the credentials on the preflight.'
6377,'s1monw',"Indexing: Lower the translog flush triggers to workaround #6363\nIf we don't do #6363 for 1.2.1 then we should consider lowering the translog flush triggers (which we increased in 1.2 to prevent too-frequent commits), because the versionMap is only cleared on flush not on refresh.  It can easily tie up a lot of RAM, put GC pressure, etc.; when Full GC does run it takes a looong time w/ that map."
6376,'spinscale','TransportClient: Improve logging, fix minor issue\nIn order to return more information to the client, in case a TransportClient\r\ncan not connect to the cluster, this commit adds logging and also returns the\r\nconfigured nodes in the NoNodeAvailableException\r\n\r\nAlso a minor bug has been fixed, which propagated exceptions wrong, so that an\r\ninvalid request was actually tried on every node, if a regular connection failure\r\non the first tried node had happened.'
6375,'jpountz',"Memory management: Raise the default max memory usage for BigArrays to 50%.\nThe value had been chosen by taking into account the fact that eg. the filter\r\ncache takes 10% and field data 60%. However, if you either don't need field\r\ndata or if you use doc values to store data on disk, then this would just leave\r\na lot of memory unused on your node. We will fortunately have a better story\r\nfor this once able to share memory accounting across several breakers."
6370,'javanna','fixing small typo\n'
6366,'javanna','Docs: Special attributes for allocation filtering\n'
6364,'mikemccand','Internal: Switch to ConcurrentHashMapV8\nConcurrentHashMapV8 from jsr166e has much better memory footprint and GC behavior.  We create large ConcurrentHashMaps with versionMap, so this should help.'
6362,'javanna',"removed slowest on single query benchmark requests\naccording to https://github.com/elasticsearch/elasticsearch/issues/5904 this shouldn't be present anymore."
6361,'javanna','Update request-body.asciidoc\nThere is no unit defined for timeout parameter.'
6358,'drewr','Update script - unable to resolve method: java.util.LinkedHashMap.distance(java.lang.Double, java.lang.Double)\nwhen having a document with a property like \r\n```\r\n"last_location" : {\r\n          "type" : "geo_point"\r\n        }\r\n```\r\ni am not able to use the ```.distance(lat, lon)``` function in a update (Java API)\r\n```\r\n"ctx._source.duration = ctx._source.last_location.distance(41.12, -71.34); "\r\n```\r\n\r\nthe exception mentions the cause:\r\n```\r\norg.elasticsearch.common.mvel2.PropertyAccessException: [Error: unable to resolve method: java.util.LinkedHashMap.distance(java.lang.Double, java.lang.Double) [arglength=2]]\r\n```'
6354,'mikemccand',"Add low level Lucene tool to see which types/fields are using the most space in the index\nAt the Hackfest after Berlin Buzzwords we created a simple diagnostic\r\ntool to see low-level index statistics (bytes used, total postings\r\nints) per-type and per-field.\r\n\r\nIt's just a standalone command-line (static main) tool for now, and it\r\ntakes some time to run (it visits all postings, docs only).  It\r\nproduces output like this:\r\n\r\n<pre>\r\nReader maxDoc=537472 delDocs=0 (0%)\r\n\r\nstored fields bytes by _type & field:\r\n  foo: 100.00%, 6665.5 MB\r\n    _source\r\n      98.62%, 6573.2 MB\r\n    _uid\r\n      1.38%, 92.3 MB\r\n\r\npostings terms bytes:\r\n  foo: 100.00%, 142.7 MB\r\n    _all\r\n      38.58%, 55.1 MB\r\n    message\r\n      38.31%, 54.7 MB\r\n    _uid\r\n      9.34%, 13.3 MB\r\n    request\r\n      7.31%, 10.4 MB\r\n    ...\r\n\r\npostings total ints:\r\n  foo: 100.00%, 446.1 M\r\n    _all\r\n      norms,DOCS_AND_FREQS_AND_POSITIONS\r\n      46.01%, 205.2 M\r\n    message\r\n      norms,DOCS_AND_FREQS_AND_POSITIONS\r\n      38.55%, 172.0 M\r\n    agent\r\n      norms,DOCS_AND_FREQS_AND_POSITIONS\r\n      4.07%, 18.1 M\r\n    ...\r\n</pre>\r\n\r\nIt works by first visiting all _uid terms to make a mapping from docID\r\n-> _type, and then visits stored fields & postings to gather up stats\r\nby type and field.\r\n\r\nIt's just a start, but it should be useful when you want some\r\nlow-level details about which types are contributing to which field's\r\nindex bytes usage, how fields were indexed, etc.\r\n"
6350,'spinscale','Packaging: Remove java-6 directories from debian init script\nAs we only support java 7 from 1.2 on, we should backport this down into 1.2.'
6339,'javanna','fixed typo\n'
6338,'colings86','Aggregations: Added GeoBounds Aggregation\nThe GeoBounds Aggregation is a new single bucket aggregation which outputs the coordinates of a bounding box containing all the points from all the documents passed to the aggregation as well as the doc count.\r\n\r\nCloses #5634'
6336,'dadoonet','Title in Batch file (Windows)\nPlease add a title to the batch file in windows version (and possibly in linux version as well) so that we can differentiate between a regular cmd window and elasticsearch also which version of elasticsearch is running. - The title could be the elasticsearch jar name itself!'
6335,'dadoonet','docs-delete-by-query is not working with elasticsearch 1.2.0\nhttp://elasticsearch-users.115913.n3.nabble.com/docs-delete-by-query-is-not-working-with-elasticsearch-1-2-0-td4056615.html\r\n\r\ndocs-delete-by-query is not working with elasticsearch 1.2.0\r\n\r\ni did version up  elasticsearch from 0.90.7  to 1.2.0 \r\nand i did regressionTest. \r\n\r\nafter that. doc-delete-by-query is not working. \r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-delete-by-query.html\r\n\r\n\r\n- request \r\norg.apache.http.wire[72] - http-outgoing-5 >> "DELETE /test_10.70.16.183_708246/temp/_query HTTP/1.1[\\r][\\n]" \r\n\r\ne.g) \r\n{ \r\n    "query_string": { \r\n        "default_field": "playId", \r\n        "query": "9eaee1fb-c8ab-4ef3-9e37-5c3f68ba21e1 OR 6b6352c1-209e-4d0c-be85-c7048453ad03ORdbe9e6b3-895f-4bd4-8943-b50ddd8712 .. OR ... more than 200 docu" \r\n    } \r\n} \r\n\r\n- response \r\n\r\n{ \r\n    "_indices":{ \r\n        "test_10.70.16.183_708246":{ \r\n            "_shards":{ \r\n                "total":2, \r\n                "successful":0, \r\n                "failed":2, \r\n                "failures":[ \r\n                    { \r\n                        "index":"test_10.70.16.183_708246", \r\n                        "shard":0, \r\n                        "reason":"RemoteTransportException[ \r\n                            [ \r\n                                10.99.198.150_21000 \r\n                            ][ \r\n                                inet[ \r\n                                    /10.99.198.150:21000 \r\n                                ] \r\n                            ][ \r\n                                deleteByQuery/shard \r\n                            ] \r\n                        ]; nested: QueryParsingException[ \r\n                            [ \r\n                                test_10.70.16.183_708246 \r\n                            ] request does not support [ \r\n                                simple_query_string \r\n                            ] \r\n                        ]; " \r\n                    }, \r\n                    { \r\n                        "index":"test_10.70.16.183_708246", \r\n                        "shard":1, \r\n                        "reason":"RemoteTransportException[ \r\n                            [ \r\n                                10.99.198.150_21001 \r\n                            ][ \r\n                                inet[ \r\n                                    /10.99.198.150:21001 \r\n                                ] \r\n                            ][ \r\n                                deleteByQuery/shard \r\n                            ] \r\n                        ]; nested: QueryParsingException[ \r\n                            [ \r\n                                test_10.70.16.183_708246 \r\n                            ] request does not support [ \r\n                                simple_query_string \r\n                            ] \r\n                        ]; " \r\n                    } \r\n                ] \r\n            } \r\n        } \r\n    } \r\n} \r\n\r\nwhat\'s worng? \r\nplz somebody help me. '
6332,'jpountz','Memory management: do not enforce the BigArrays limit on the network layer and the tranlog.\nBigArrays byte accounting (https://github.com/elasticsearch/elasticsearch/pull/6050) applies all the time. Yet, we might want to disable it for cluster-management-related operations so that they would not be impacted eg. in case of heavy search requests.'
6330,'spinscale','StemmerTokenFilterFactory has two entries for language="portuguese"\nThere are two checks for Portuguese; one returns a PortugueseStemFilter and one returns a SnowballFilter. The one returning a SnowballFilter comes first so there\'s no way to get a PortugueseStemFilter.'
6329,'alexksikes',"More Like This Query: Adds the ability to specify the analyzer used for each Field\nWhen using multiple items, the user may want to specify which analyzer to use\r\nfor each field. Previously, either the analyzer specified by 'analyzer' would\r\nbe used for all the fields, or if not set, the analyzer associated with the\r\nfield would be chosen. This commit provides the ability to fine grain which\r\nanalyzer should be used for each field by providing a new 'fields_analyzer'\r\nparameter to the More Like This Query."
6327,'clintongormley',"[docs] Store documentation doesn't explain how to change storage type\nThe [store documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-store.html) describes the different storage types but not how to switch them."
6325,'spinscale','Serialization of queue size is broken\nThis issue exists in both in the 1.1 branch and head.\r\n\r\nI have set the threadpool.get.queue_size to -1 using the REST API.\r\nI am running the elasticsearch server with assertions enable (-ea).\r\nIf then a TransportClient with "sniff=false" connects and tries to send a query it gets a org.elasticsearch.client.transport.NoNodeAvailableException\r\nThe underlying exception is:\r\nCaused by: java.lang.AssertionError\r\n\tat org.elasticsearch.common.io.stream.StreamOutput.writeVLong(StreamOutput.java:176)\r\n\tat org.elasticsearch.common.io.stream.AdapterStreamOutput.writeVLong(AdapterStreamOutput.java:126)\r\n\tat org.elasticsearch.common.unit.SizeValue.writeTo(SizeValue.java:211)\r\n\tat org.elasticsearch.threadpool.ThreadPool$Info.writeTo(ThreadPool.java:643)\r\n\tat org.elasticsearch.threadpool.ThreadPoolInfo.writeTo(ThreadPoolInfo.java:74)\r\n\tat org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:291)\r\n\tat org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse.writeTo(NodesInfoResponse.java:68)\r\n\tat org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:246)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:241)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.finishHim(TransportNodesOperationAction.java:227)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onOperation(TransportNodesOperationAction.java:202)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$900(TransportNodesOperationAction.java:102)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\n\r\nThat is the server tries to use OutputStream.writeVLong on a negative number.\r\n\r\nPlease let me know if you need more information.'
6324,'colings86','Internal: Added plugins to .gitignore\nSince plugins should never be committed to the core codebase and it is useful to be able to add plugins to the development environment adding plugins folder to the .gitignore file will stop it from appearing in the unstaged changes'
6323,'clintongormley','[docs] Update documentation on pull request policy\nAfter talking to some Elasticsearch folks they mentioned that they now like pull requests to be made up of multiple commits rather then amending the same commit.  Before the pull request is merged it\'d be squashed but during review it\'d keep growing.\r\n\r\nIt\'d be cool to document this on CONTRIBTUING.md and http://www.elasticsearch.org/contributing-to-elasticsearch/ .\r\n\r\nIf Elasticsearch grows pull request guidelines then can we add:  always make the subject of your pull request descriptive of what it does rather then what issue it closes.  These subjects turn into email subjects and it is a lot easier to filter what you need to read if the subject is descriptive like "Splort the sort sprocket to make the sort faster" or "Fix overflow in the foo" then "Closes #1234".\r\n'
6322,'javanna',"[docs] contributing page doesn't match CONTRIBUTIING.md\nThe [contributing page](http://www.elasticsearch.org/contributing-to-elasticsearch/) doesn't match CONTRIBUTING.md.  I figured the page would be built from the file but I don't believe that is the case?\r\n\r\nThis is the section that is missing from the page but in the file\r\n```\r\n* Don't worry too much about imports.  Try not to change the order but don't worry about fighting your IDE to stop it from switching from * imports to specific imports or from specific to * imports.\r\n```\r\n\r\n"
6321,'dadoonet',"Plugins: Fix github download link when using specific version\nRemoves the extraneous 'v' character which could have been used previously for Github links.\r\n\r\nDoes not include an updated test case since the PluginManagerTests class skips tests with invalid downloads (try-catch block in singlePluginInstallAndRemove). In addition, the PluginManager logs to System.out, which is removed during tests."
6318,'s1monw','Search: Search template not replacing parameter after initial failure in parameter substitution\nSteps to reproduce:\r\n\r\n1) Restart ES\r\n\r\n2) Run this (which returns an error as expected):\r\n\r\n```\t\r\nGET _search/template\r\n{\r\n  "template": {\r\n    "query": { "match_all": {}},\r\n    "size": "{{my_size}}"\r\n  }\r\n}\r\n```\r\n\r\n3) Then run this (which still returns an error - not expected):\r\n\r\n```\t\r\nGET _search/template\r\n{\r\n  "template": {\r\n    "query": { "match_all": {}},\r\n    "size": "{{my_size}}"\r\n  },\r\n  "params": {\r\n    "my_size": 1\r\n  }\r\n}\r\n```'
6314,'s1monw',"BulkProcessor's close ignores in-flight bulkRequests\nI have observed on a number of occasions that when I have large batch sizes / concurrent requests that I don't always get the same number of documents out of ES that I put in. Looking at:\r\n \r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L204\r\n\r\n```java\r\n    /**\r\n     * Closes the processor. If flushing by time is enabled, then its shutdown. Any remaining bulk actions are flushed.\r\n     */\r\n    public synchronized void close() {\r\n        if (closed) {\r\n            return;\r\n        }\r\n        closed = true;\r\n        if (this.scheduledFuture != null) {\r\n            this.scheduledFuture.cancel(false);\r\n            this.scheduler.shutdown();\r\n        }\r\n        if (bulkRequest.numberOfActions() > 0) {\r\n            execute();\r\n        }\r\n    }\r\n```\r\n\r\nWhat happens if there are concurrent requests in-flight before the `close` call is made? Shouldn't this method block on those finishing? Or is the requirement / expectation that the client call `close`\r\n on `TransportClient` which will block on the in-flight requests for up to 10 seconds? \r\n\r\nAs far as I can tell, there is no reliable way to know it is safe to shutdown the JVM."
6311,'alexksikes',"More Like This Query: Added syntax for single item specification.\nUsers are more likely to search for only one document than for multiple ones.\r\nThis commit extends the current syntax with 'id' and 'doc' for single item\r\nsearch. Additionally, 'ids' and 'docs' now support a single value or a single\r\nobject respectively. For consitency with 'like_text', alternative syntax such\r\nas 'like_doc', 'like_docs', 'like_id' and 'like_ids' are also possible."
6306,'clintongormley','Adding Hebrew analyzer\n'
6301,'jpountz','Routing + filter aggregation + extended_bounds = ReduceSearchPhaseException (503)\n#### Description\r\nI\'m using a date histogram aggregation (with the extended bounds option) nested inside of a filter aggregation along with routing. Here are the combinations that work:\r\n* `!routing, filter, extended_bounds`\r\n* `routing, !filter, extended_bounds`\r\n* `routing, filter, !extended_bounds`\r\n\r\nCombining all three results in the following response from elasticsearch:\r\n```\r\n{"error":"ReduceSearchPhaseException[Failed to execute phase [merge], [reduce] ]; nested: UnsupportedOperationException; ","status":503}\r\n```\r\nand the following trace:\r\n```\r\n[2014-05-23 10:32:16,017][DEBUG][action.search.type       ] [Jumbo Carnation] failed to reduce search\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [merge], [reduce] \r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryAndFetchAction.java:79)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:404)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:198)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:174)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$1.onResult(TransportSearchTypeAction.java:171)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:526)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.UnsupportedOperationException\r\n\tat java.util.Collections$EmptyListIterator.add(Collections.java:3059)\r\n\tat org.elasticsearch.search.aggregations.bucket.histogram.InternalHistogram.reduce(InternalHistogram.java:290)\r\n\tat org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:160)\r\n\tat org.elasticsearch.search.aggregations.bucket.InternalSingleBucketAggregation.reduce(InternalSingleBucketAggregation.java:69)\r\n\tat org.elasticsearch.search.aggregations.InternalAggregations.reduce(InternalAggregations.java:146)\r\n\tat org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:545)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryAndFetchAction.java:89)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryAndFetchAction.java:77)\r\n\t... 8 more\r\n```\r\n\r\n#### Environment\r\nElasticsearch versions tested:\r\n* `Version: 1.1.1, Build: f1585f0/2014-04-16T14:27:12Z, JVM: 1.7.0_45`\r\n* `Version: 1.2.0, Build: c82387f/2014-05-22T12:49:13Z, JVM: 1.7.0_45`\r\n\r\nOperating system:\r\n```\r\nProductName:\tMac OS X\r\nProductVersion:\t10.9.2\r\nBuildVersion:\t13C64\r\n```\r\n\r\n#### Steps to reproduce\r\nCreate index:\r\n```\r\ncurl -XPUT localhost:9200/foo -d \'\r\n{\r\n  "mappings": {\r\n    "charges": {\r\n      "_routing": {\r\n        "require": true,\r\n        "path": "account_id"\r\n      },\r\n      "properties": {\r\n        "account_id": {\r\n          "type": "integer"\r\n        },\r\n        "amount": {\r\n          "type": "integer"\r\n        },\r\n        "created_at": {\r\n          "type": "date"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n```\r\n{"acknowledged":true}\r\n```\r\n\r\nQuery:\r\n```\r\ncurl -XGET localhost:9200/foo/charges/_search?routing=1 -d \'\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "term": {\r\n                "account_id": 1\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "aggs": {\r\n    "charges": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "term": {\r\n                "_type": "charges"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "aggs": {\r\n        "monthly_stats": {\r\n          "date_histogram": {\r\n            "field": "created_at",\r\n            "interval": "month",\r\n            "min_doc_count": 0,\r\n            "extended_bounds": {\r\n              "min": "2014-01-01",\r\n              "max": "2014-12-31"\r\n            }\r\n          },\r\n          "aggs": {\r\n            "revenue": {\r\n              "sum": {\r\n                "field": "amount"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n```\r\n{"error":"ReduceSearchPhaseException[Failed to execute phase [merge], [reduce] ]; nested: UnsupportedOperationException; ","status":503}\r\n```'
6300,'s1monw','Query DSL: Fix MatchQueryParser not parsing fuzzy_transpositions\n'
6297,'imotov','_cat/nodes causes NullPointerException in 1.2.0\ncurl -s \'http://localhost:9200/_cat/nodes\'\r\n{"error":"NullPointerException[null]","status":500}\r\n\r\nNothing else is logged on the masters or data nodes.\r\nWe have Logstash 1.4.1 connected to the cluster, but this happens regardless of whether the Logstash nodes are connected or not.'
6296,'dakrone',"Internal: Guava doesn't explicitly remove entries when clearing the entire cache\nA call to `.cleanUp()` is needed to ensure the entries have been removed."
6292,'brwe','Query DSL: Function score without function throws NPE\n    PUT /t/test/1\r\n    {\r\n      "text": "baseball bats"\r\n    }\r\n\r\n    GET /t/test/_search?explain\r\n    {\r\n      "query": {\r\n        "function_score": {\r\n          "score_mode": "sum",\r\n          "boost_mode": "replace",\r\n          "functions": [\r\n            {\r\n              "filter": {\r\n                "term": {\r\n                  "text": "baseball"\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n\r\nThrows NPE.  With a function (eg boost_factor) it doesn\'t:\r\n\r\n    GET /t/test/_search?explain\r\n    {\r\n      "query": {\r\n        "function_score": {\r\n          "score_mode": "sum",\r\n          "boost_mode": "replace",\r\n          "functions": [\r\n            {\r\n              "filter": {\r\n                "term": {\r\n                  "text": "baseball"\r\n                }\r\n              }\r\n            },\r\n            "boost_factor": 1\r\n          ]\r\n        }\r\n      }\r\n    }\r\n'
6291,'javanna','[docs] Add imap river and security plugin and community supported MSI installer to docs\nAdd imap river and security plugin to docs\r\nAdd community supported MSI installer to docs'
6287,'javanna','Update repositories.asciidoc\nupgrade to 1.2 to be consistent with the default version on the download page.'
6286,'jpountz',"Limit guava caches to 31.9GB\nGuava's caches have overflow issues around 32GB with our default segment\r\ncount of 16 and weight of 1 unit per byte.  We give them 100MB of headroom\r\nso 31.9GB.\r\n\r\nThis limits the sizes of both the field data and filter caches, the two\r\nlarge guava caches.\r\n\r\nCloses #6268"
6285,'javanna','Update doc to match changes in IndexResponse API\nIndexResponse.id() ->  IndexResponse.getId()\r\n...'
6284,'javanna','Correcting typo in Javadoc\nReplacing\r\nNote: the number of objects passed to this method must be and even number.\r\nWith\r\nNote: the number of objects passed to this method must be an even number.'
6282,'colings86','Geo: Issue with polygons near date line\nIf a polygon is constructed which overlaps the date line but has a hole which lies entirely one to one side of the date line, JTS error saying that the hole is not within the bounds of the polygon because the code which splits the polygon either side of the date line does not add the hole to the correct component of the final set of polygons.  The fix ensures this selection happens correctly.\r\n\r\nCloses #6179'
6279,'jpountz',"Nested: queries/filters/aggregations expect FixedBitSets, yet it isn't the case with NoneFilterCache\nThe nested queries, filters and aggregations expect that filters produce `FixedBitSet` instances. However, if the filter cache is disabled, you might get an index-based `DocsEnum` directly, so the set of documents needs to be loaded into a `FixedBitSet` before running the query."
6278,'jpountz','Aggregations: `ReverseNestedAggregator` does not compute parent documents correctly\nIn order to compute parent documents based on a child document, `ReverseNestedAggregator` does the following:\r\n\r\n```java\r\nint parentDoc = parentDocs.advance(childDoc);\r\n```\r\n\r\nBut the behavior of `advance` is undefined when the target is less than or equal to the target document, which can happen if you have 2 matching child documents that have the same parent.\r\n\r\nThis works fine in most cases when the filter cache is enabled since `FixedBitSet` is permissive and would go the the next set bit after `childDoc`. But if the filter cache is disabled then you might run into trouble.'
6270,'javanna','Update network.asciidoc\nFix typo'
6268,'jpountz','Internal: Filter cache size limit not honored for 32GB or over\nHi,\r\n\r\nWe are running an elasticsearch 1.1.1 6 node cluster with 256GB of ram, and using 96GB JVM heap sizes. I\'ve noticed that when I set the filter cache size to 32GB or over with this command:\r\n```\r\ncurl -XPUT "http://localhost:9200/_cluster/settings" -d\'\r\n{\r\n    "transient" : {\r\n       "indices.cache.filter.size" : "50%"\r\n    }\r\n}\'\r\n```\r\nThe field cache size keeps growing above and beyond the indicated limit. The relevant node stats show that the filter cache size is about 69GB in size, which is over the configured limit of 48GB\r\n```\r\n"filter_cache" : {\r\n    "memory_size_in_bytes" : 74550217274,\r\n    "evictions" : 8665179\r\n},\r\n```\r\nI\'ve enable debug logging on the node itself and it looks like the cache itself is getting created with the correct values:\r\n```\r\n[2014-05-21 00:31:57,215][DEBUG][indices.cache.filter     ] [ess02-006] using [node] weighted filter cache with size [50%], actual_size [47.9gb], expire [null], clean_interval [1m]\r\n```\r\nWhats strange is that when I set the limit to 31.9GB, the limit is enforced, which leads me to believe there is some sort of overflow going on.\r\n\r\nThanks,\r\nDaniel'
6267,'jpountz',"Plain highlighter to use analyzer defined on a document level \n\r\nAt the moment plain highligher only uses an analyzer defined for on the type level. However, during the indexing stage it is possible to define analyzer on per document level, for example mapping '_analyzer' to another field, containing required name. This commit attempts to make sure that highlighting works correctly in this scenario.\r\n\r\nImportant: the field containing the document analyzer is hardcoded to 'language_analyzer' at the moment. This should use document mappings instead. I just could not figure out a way to do it without introducing too many changes on DocumentMapper.\r\n\r\nCloses #5497"
6266,'colings86','Aggregations: Fixed conversion of date field values when using multiple date formats\nWhen multiple date formats are specified using the || syntax in the field mappings the date_histogram aggregation breaks.  This is because we are getting a parser rather than a printer from the date formatter for the object we use to convert the DateTime values back into Strings.  Simple fix to get the printer form the date format and test to back it up'
6264,'clintongormley','Creating an index that already exists should return 409\n* Create an index named "conflict"\r\n* Create an index named "conflict" again\r\n* Expected: 409 Conflict\r\n* Actual: 400 Bad Request\r\n\r\n```\r\nC:\\>curl -v -XPUT "http://localhost:9200/conflict"\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1... connected\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> PUT /conflict HTTP/1.1\r\n> User-Agent: curl/7.21.7 (amd64-pc-win32) libcurl/7.21.7 OpenSSL/0.9.8r zlib/1.2.5\r\n> Host: localhost:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 21\r\n<\r\n{"acknowledged":true}* Connection #0 to host localhost left intact\r\n* Closing connection #0\r\n\r\nC:\\>curl -v -XPUT "http://localhost:9200/conflict"\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1... connected\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> PUT /conflict HTTP/1.1\r\n> User-Agent: curl/7.21.7 (amd64-pc-win32) libcurl/7.21.7 OpenSSL/0.9.8r zlib/1.2.5\r\n> Host: localhost:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 400 Bad Request\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 79\r\n<\r\n{"error":"IndexAlreadyExistsException[[conflict] already exists]","status":400}* Connection #0 to host localhost left intact\r\n* Closing connection #0\r\n```'
6260,'s1monw','FieldData: Global ordinals cause ClassCastExceptions if used with a bounded fielddata cache\nI randomized the fielddata settings today and run into these exceptions:\r\n\r\n```\r\nInternalGlobalOrdinalsIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.AtomicFieldData\r\n  1> \tat org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$FieldDataWeigher.weigh(IndicesFieldDataCache.java:102)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2160)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.storeLoadedValue(LocalCache.java:3142)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2351)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2318)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2280)\r\n  1> \tat com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2195)\r\n  1> \tat com.google.common.cache.LocalCache.get(LocalCache.java:3934)\r\n  1> \tat com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4736)\r\n  1> \tat org.elasticsearch.indices.fielddata.cache.IndicesFieldDataCache$IndexFieldCache.load(IndicesFieldDataCache.java:163)\r\n  1> \tat org.elasticsearch.index.fielddata.plain.AbstractBytesIndexFieldData.loadGlobal(AbstractBytesIndexFieldData.java:75)\r\n```\r\n\r\nthis causes the Circ. Breaker to not be reset to 0 etc. '
6257,'javanna','Restored MetaData#concreteIndices(String[] indices) method\nRestored MetaData#concreteIndices(String[] indices) method, deprecated instead or removed for backwards compatibility in 1.x\r\n\r\nRelates to #6059'
6250,'javanna','[DOCS] Fielddata cat API added in 1.2.0\nI hope the formatting works! :)'
6249,'javanna','Update http.asciidoc\nTypo fix'
6247,'jpountz','XFilteredQuery defaults to Query First strategy\nIn XFilteredQuery the strategy seems to almost always default to QueryFirst: https://github.com/elasticsearch/elasticsearch/blob/9ed34b5a9e9769b1264bf04d9b9a674794515bc6/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L226-L231\r\n\r\nThere the docIdSet is usually a NotDeletedDocIdSet or BitsFilteredDocIdSet built by ApplyAcceptedDocsFilter, and is not detected as a fast iterator.\r\n\r\nOn wide a query with a highly filtering filter, the performance impact is important, I have to manually force the "strategy" to something like "random_access_100".\r\n'
6243,'javanna','Fix grammar in dynamic mappings\n'
6239,'colings86','Aggregations: date_histogram aggregation breaks on date fields with multiple formats\nThe date format docs says you can separate different date formats using a double bar and that the first will be used to format any stored dates: [Date Format Docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html)\r\n\r\nWhen using multiple date formats, however, the date_histogram aggregation throws "UnsupportedOperationException[Printing not supported]" which seems to be a Joda exception when a format doesn\'t have a printer. If I change the mapping to be a single format, the exception isn\'t thrown, so my guess is that the first format isn\'t parsed out to retrieve the printer.\r\n\r\nI\'m using ES 1.1.1, though I\'ve also observed this on 1.1.0. \r\n\r\nCreate the index:\r\n\r\n    ➜  ~  curl localhost:9200/my_index -XPUT -d\'{"settings":{},"mappings":{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime||mm-DD-yyyy"}}}}}\' \r\n   \r\n    {"acknowledged":true}%       \r\n\r\nAdd a document with a date:\r\n                                                   \r\n    ➜  ~  curl localhost:9200/my_index/my_type/1 -XPOST -d \'{"my_date":"12-13-2014"}\'\r\n\r\n    {"_index":"my_index","_type":"my_type","_id":"1","_version":1,"created":true}%  \r\n\r\nPerform a date histogram facet (works):\r\n\r\n    ➜  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d \'{"facets":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}\'\r\n\r\n    {"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"my_index","_type":"my_type","_id":"1","_score":1.0, "_source" : {"my_date":"12-13-2014"}}]},"facets":{"dates":{"_type":"date_histogram","entries":[{"time":1389571200000,"count":1}]}}}%      \r\n\r\nPerform an aggregation facet (fails with UnsupportedOperationException):\r\n\r\n     ➜  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d \'{"aggs":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}\'\r\n\r\n    {"error":"UnsupportedOperationException[Printing not supported]","status":500}% \r\n\r\nChange the mapping to contain only a single date format:\r\n\r\n    ➜  ~  curl localhost:9200/my_index/my_type/_mapping -XPUT -d \'{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime"}}}}\'\r\n\r\n    {"acknowledged":true}%                                                          \r\n\r\nCheck that the mapping changed:\r\n\r\n    ➜  ~  curl localhost:9200/my_index/my_type/_mapping      \r\n                      \r\n    {"my_index":{"mappings":{"my_type":{"properties":{"my_date":{"type":"date","format":"dateOptionalTime"}}}}}}%          \r\n\r\nRun the original aggregation again, and see that it returns the same result as the facet:\r\n\r\n    ➜  ~  curl localhost:9200/my_index/my_type/_search -XPOST -d \'{"aggs":{"dates":{"date_histogram":{"field":"my_date","interval":"day"}}}}\'\r\n\r\n    {"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"my_index","_type":"my_type","_id":"1","_score":1.0, "_source" : {"my_date":"12-13-2014"}}]},"aggregations":{"dates":{"buckets":[{"key_as_string":"2014-01-13T00:00:00.000Z","key":1389571200000,"doc_count":1}]}}}%                '
6232,'mikemccand',"Use Lucene's query rescorer\nA query rescorer was added to lucene that basically does the same as our\r\nrescorer. In-fact we designed it to work pretty similar. This commit\r\ncuts over to the lucene version shipped with 4.8"
6231,'s1monw','Fixing invalid jsons\n'
6223,'aleph-zero','Benchmark: Do not throw an exception on unavailable nodes.\nChanged behavior to not throw an exception on a status or abort request\r\nwhen there are no available benchmark nodes.\r\n\r\nCloses #6146'
6222,'aleph-zero','Benchmark abort accepts wildcard patterns\nThis adds support for sending a list of benchmark names and/or wildcard\r\npatterns when submitting an abort request. Standard wildcards such as:\r\n"*", "*xxx", and "xxx*" are supported. Multiple benchmark names and\r\nwildcard patterns can be submitted together as a comma-separated list\r\nfrom the REST API or as a string array from the Java API.\r\n\r\nCloses #6185'
6221,'s1monw',"Don't report terms as live if all it's docs are filtered out\nFilterableTermsEnum allows to filter stats by supplying per segment\r\nbits. Today if all docs are filtered out the term is still reported as\r\nlive but shouldn't.\r\n\r\nRelates to #6211"
6218,'javanna','Update nodes-stats.asciidoc\nsimple changes in stats example i have correct the ? with & for pretty option not so much :)'
6215,'s1monw','Search by integer NullPointerException\nI have field on index is Long type, when I try to search, I\'m getting NullPointerException. \r\n\r\nElasticsearch: Version 1.1.1\r\nHere\'s my traceback:\r\n\r\n[2014-05-17 21:09:18,922][DEBUG][action.search.type       ] [Orbit] [skoob][0], node[LlsPNTswRgGWnzxnGrgBMQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2f53db06] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [index3][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\r\n   "query": {\r\n      "multi_match": {\r\n         "query": 2014,\r\n         "fields": [\r\n            "year"\r\n         ]\r\n      }\r\n   },\r\n   "size": 20\r\n}\r\n]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:507)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.index.search.MultiMatchQuery.forceAnalyzeQueryString(MultiMatchQuery.java:266)\r\n\tat org.elasticsearch.index.search.MatchQuery.parse(MatchQuery.java:178)\r\n\tat org.elasticsearch.index.search.MultiMatchQuery.parseAndApply(MultiMatchQuery.java:57)\r\n\tat org.elasticsearch.index.search.MultiMatchQuery.parse(MultiMatchQuery.java:71)\r\n\tat org.elasticsearch.index.query.MultiMatchQueryParser.parse(MultiMatchQueryParser.java:164)\r\n\tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:223)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:330)\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:260)\r\n\tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)\r\n\t... 11 more\r\n'
6212,'mikemccand',"Indexing: Versions.loadDocIdAndVersion should reuse TermsEnums\nToday, every version lookup creates a new TermsEnum for each segment in the index, but this is quite costly, e.g. on NIOFSDir it must clone the IO buffer, and because BlockTreeTermsReader has a lot of internal state.\r\n\r\nWe'd need a ThreadLocal somewhere/somehow... I have a start at a utility class here: https://svn.apache.org/repos/asf/lucene/dev/branches/lucene5675/lucene/test-framework/src/java/org/apache/lucene/index/PerThreadPKLookup.java maybe we can adapt/use this."
6211,'dadoonet','[TEST] NPE with seed ED695B6A3A249CA6\nWith -Dtests.seed=ED695B6A3A249CA6, in `FreqTermsEnumTests` term has been skipped so not available in reference array.'
6209,'dakrone','Allocation: Allow primaries that have never been allocated to be allocated if under the low watermark\n(In the DiskThresholdDecider)\r\n\r\nFixes #6196'
6206,'clintongormley','added install instruction with apt\n'
6205,'clintongormley','fixed typo\n'
6203,'clintongormley','put-mapping typo fix\nsplit backwardscompatibility to backwards compatibility'
6196,'dakrone','Allocation: Disk-aware allocation decider should allow initial primary allocation if under the high watermark\nFor _initial_ primary allocation, it should still be allowed if above the low watermark, and denied if above the high watermark.'
6194,'s1monw','Randomize MergeScheduler Settings\nConcurrentMS has settings we should randomize in the index template just like LuceneTestCase does.'
6192,'s1monw',"Mustache dependency not shaded\nThe ```com.github.spullara.mustache.java:compiler``` dependency is not being shaded in the Elasticsearch fat jar:\r\n\r\n```\r\n$ curl -s http://repo1.maven.org/maven2/org/elasticsearch/elasticsearch/1.1.1/elasticsearch-1.1.1.jar | jar tv | grep -i mustache\r\n     0 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/\r\n  5705 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/MustacheScriptEngineService.class\r\n  1598 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/JsonEscapingMustacheFactory.class\r\n  3246 Wed Apr 16 14:28:38 EDT 2014 org/elasticsearch/script/mustache/MustacheScriptEngineService$MustacheExecutableScript.class\r\n     0 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/\r\n   175 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/Binding.class\r\n   639 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/Code.class\r\n     0 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/\r\n  6514 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DefaultCode.class\r\n  1951 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DefaultMustache.class\r\n   758 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/DepthLimitedWriter.class\r\n  4121 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ExtendCode.class\r\n  1025 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ExtendNameCode.class\r\n  1737 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/IterableCode$1.class\r\n  5438 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/IterableCode.class\r\n  2183 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/NotIterableCode.class\r\n  3939 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/PartialCode.class\r\n  1772 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ValueCode$1.class\r\n  5351 Wed Apr 16 14:28:42 EDT 2014 com/github/mustachejava/codes/ValueCode.class\r\n[...]\r\n```\r\n\r\nStrange since it is being included in the list of shaded dependencies: https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L585\r\n\r\nMaybe it's because it is declared as an optional dependency? https://github.com/elasticsearch/elasticsearch/blob/master/pom.xml#L176\r\n"
6191,'bleskes',"Resiliency: Before deleting a local unused shard copy, verify we're connected to the node it's supposed to be on\nThis is yet another safety guard to make sure we don't delete data if the local copy is the only one (even if it's not part of the cluster state any more)"
6185,'aleph-zero','Benchmark: Benchmark API should accept match patterns\nIf I run abort with `*` as the index it barfs with `"Benchmark with name [*] not found"` we should support the wildcards to abort all benchmarks and simple patterns.'
6182,'imotov','Switch to shared thread pool for all snapshot repositories\n Closes #6181'
6181,'imotov','Unregistering snapshot repositories causes thread leaks\n'
6179,'colings86','Geo:Valid Polygon crossing dateline fails to parse\nAttempt to upload polygon at https://gist.github.com/anonymous/7f1bb6d7e9cd72f5977c\r\nfails with \r\norg.elasticsearch.index.mapper.MapperParsingException: failed to parse [geometry]\r\n...\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: -1\r\n\r\nRepro\r\n\r\ncurl -XDELETE \'http://localhost:9200/test\'\r\n\r\ncurl -XPOST \'http://localhost:9200/test\' -d \'{                                                                                                                                                                                                                                                                                                                            \r\n  "mappings":{                                                                                                                                                                                                                                                                                                                                                            \r\n    "test":{                                                                                                                                                                                                                                                                                                                                                              \r\n      "properties":{                                                                                                                                                                                                                                                                                                                                                      \r\n        "geometry":{                                                                                                                                                                                                                                                                                                                                                      \r\n          "type":"geo_shape",                                                                                                                                                                                                                                                                                                                                             \r\n          "tree":"quadtree",                                                                                                                                                                                                                                                                                                                                              \r\n          "tree_levels":14,                                                                                                                                                                                                                                                                                                                                               \r\n          "distance_error_pct":0.0                                                                                                                                                                                                                                                                                                                                        \r\n        }                                                                                                                                                                                                                                                                                                                                                                 \r\n      }                                                                                                                                                                                                                                                                                                                                                                   \r\n    }                                                                                                                                                                                                                                                                                                                                                                     \r\n  }                                                                                                                                                                                                                                                                                                                                                                       \r\n}\'\r\n\r\ncurl -XPOST \'http://localhost:9200/test/test/1\' -d \'{                                                                                                                                                                                                                                                                                                                     \r\n  "geometry": {                                                                                                                                                                                                                                                                                                                                                           \r\n    "type": "Polygon",                                                                                                                                                                                                                                                                                                                                                    \r\n    "coordinates": [[[-186,0],[-176,0],[-176,3],[-183,3],[-183,5],[-176,5],[-176,8],[-186,8],[-186,0]],[[-185,1],[-181,1],[-181,2],[-184,2],[-184,6],[-178,6],[-178,7],[-185,7],[-185,1]],[[-179,1],[-177,1],[-177,2],[-179,2],[-179,1]],[[-180,0],[-180,-90],[-180,90],[-180,0]]]                                                                                        \r\n  }                                                                                                                                                                                                                                                                                                                                                                       \r\n}\'\r\n\r\nAdditionally, there is a unit test at:\r\nhttps://github.com/marcuswr/elasticsearch-dateline/commit/cbf9db12615c55ba8a8801aa3eaa3704ac2943c6'
6176,'s1monw',"Randomize the nodes number of processors \nWe configure the threadpools according to the number of processors which is different on every machine. Yet, we just had some test failures related to #6174 that only happened reproducibly on a node with 1 CPU (available processors returned 1). We should:\r\n\r\n * sometimes randomize the number of available processors\r\n * if we don't randomize we should set the actual number of available processors in the settings on the test node\r\n * always print out the num of processors when a test fails to make sure we can reproduce the thread pool settings with the commandline"
6174,'aleph-zero','Benchmark: Use GENERIC thread pool for status/abort benchmark calls\nOn a small machine, the BENCH thread pool can get sized to 1, leaving no threads available to perform other tasks such as listing and aborting.'
6169,'javanna',"Unified MetaData#concreteIndices methods into a single method that accepts indices (or aliases) and indices options\nThe goal of this PR is to have a single method in `MetaData` that allows to resolve aliases or indices (eventually containing wildcard expressions) to concrete indices. That way all the apis will just refer to this same new method passing the proper indices options as argument.\r\n\r\nIn order to achieve that a new internal flag has been added to IndicesOptions, which tells whether aliases can be resolved to multiple indices or not.\r\n\r\nAlso it's now possible to decide not to expand wildcards at all via indices options, while previously it was possible to decide whether to expand them to open indices, closed indices, or both.\r\n\r\nNote that the PR is against 1.x to handle backwards compatibility properly when needed. Bw comp checks will be removed when porting to master."
6164,'spinscale',"Security: Make JSONP responses optional.\nThis poses a significant security threat by being on by default. If an attacker can entice a user to load a legitimate ElasticSearch query as a script tag, they can effectively bypass SOP and exfiltrate the contents of a local ElasticSearch instance (or bypass firewall rules by using a victim's browser to talk to a remote instance). \r\n\r\nThis would be trivial to implement from an attackers perspective, and I'll be submitting a pull request to the BeEF project (https://github.com/beefproject/beef) to automate this sort of attack, as it'll be useful during a pentesting engagement. \r\n\r\nIf you have any questions or change requests please let me know, and I'll be more than happy to accommodate. \r\n\r\nThanks!\r\nFitblip"
6161,'clintongormley','Fix typo in network docs\n'
6159,'aleph-zero','Cat recovery API documentation\nThe API documentation for _cat/recovery is out of date.'
6157,'clintongormley','Update nested-query.asciidoc\nAdded note that fields inside a nested query must be full qualified.'
6155,'javanna','Function Score: Add missing whitespace in error message when throwing exception\nDecayFunctionParser throws a parse exception with a string containing "scaleand origin", this fixes the spacing issue.'
6153,'clintongormley',"Documentation: fuzzy query's max_expansion fix\nSee line org.elasticsearch.index.query.FuzzyQueryParser:70\r\n```java\r\n  int maxExpansions = FuzzyQuery.defaultMaxExpansions;\r\n```\r\n\r\nThe contents of org.apache.lucene.search.FuzzyQuery\r\n```java\r\n  public final static int defaultPrefixLength = 0;\r\n  public final static int defaultMaxExpansions = 50;\r\n```"
6150,'dadoonet','Allow sorting on nested sub generated field\nWhen you have a nested document and want to sort on its fields, it\'s perfectly doable on regular fields but not on "generated" sub fields.\r\n\r\nHere is a SENSE recreation:\r\n\r\n```\r\nDELETE /tmp\r\n\r\nPUT /tmp\r\n\r\nPUT /tmp/doc/_mapping\r\n{\r\n  "properties": {\r\n    "flat": {\r\n      "type": "string",\r\n      "index": "analyzed",\r\n      "fields": {\r\n        "sub": {\r\n          "type": "string",\r\n          "index": "not_analyzed"\r\n        }\r\n      }\r\n    },\r\n    "nested": {\r\n      "type": "nested",\r\n      "properties": {\r\n        "foo": {\r\n          "type": "string",\r\n          "index": "analyzed",\r\n          "fields": {\r\n            "sub": {\r\n              "type": "string",\r\n              "index": "not_analyzed"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT /tmp/doc/1\r\n{\r\n  "flat":"bar",\r\n  "nested":{\r\n    "foo":"bar"\r\n  }\r\n}\r\n```\r\n\r\nWhen sorting on `flat.sub` sub field, everything is fine:\r\n\r\n```\r\nGET /tmp/doc/_search\r\n{\r\n  "sort": [\r\n    {\r\n      "flat.sub": {\r\n        "order": "desc"\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n# gives\r\n# "sort": [\r\n#               "bar"\r\n#            ]\r\n```\r\n\r\nWhen sorting on `nested` field, everything is fine:\r\n\r\n```\r\nGET /tmp/doc/_search\r\n{\r\n  "sort": [\r\n    {\r\n      "nested.foo": {\r\n        "order": "desc"\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n# gives\r\n# "sort": [\r\n#               "bar"\r\n#            ]\r\n```\r\n\r\nBut when sorting on `nested.sub` field, sorting is incorrect:\r\n\r\n```\r\nGET /tmp/doc/_search\r\n{\r\n  "sort": [\r\n    {\r\n      "nested.foo.sub": {\r\n        "order": "desc"\r\n      }\r\n    }\r\n  ]\r\n}\r\n\r\n# gives\r\n# "sort": [\r\n#               null\r\n#            ]\r\n'
6149,'bleskes',"Allow 0 as a valid external version\nUntil now all version types have officially required the version to be a positive long number. Despite of this has being documented, ES versions <=1.0 did not enforce it when using the `external` version type. As a result people have succesfully indexed documents with 0 as a version. In 1.1. we introduced validation checks on incoming version values and causing indexing request to fail if the version was set to 0. While this is strictly speaking OK, we effectively have a situation where data already indexed does not match the version invariant.\r\n\r\nTo be lenient and adhere to spirit of our data backward compatibility policy, we have decided to allow 0 as a valid external version type. This is somewhat complicated as 0 is also the internal value of `MATCH_ANY`, which indicates requests should succeed regardles off the current doc version. To keep things simple, this commit changes the internal value of `MATCH_ANY` to `-3` for all version types.\r\n\r\nSince we're doing this in a minor release (and because versions are stored in the transaction log), the default `internal` version type still accepts 0 as a `MATCH_ANY` value. This is not a problem for other version types as `MATCH_ANY` doesn't make sense in that context.\r\n\r\nCloses #5662"
6146,'aleph-zero','Benchmark: Benchmark list() should return empty result if no bench nodes are available\ntoday we fail a list benchmark request which seems odd to me. Should we just return an empty list in that case?'
6140,'clintongormley','Clean up Java-API docs.\nFixed some awkward wording and subject/verb agreement while reading through the Java API documentation.'
6139,'imotov','Add ability to snapshot replicating primary shards\nThis change adds a new cluster state that waits for the replication of a shard to finish before starting snapshotting process. Because this change adds a new snapshot state, it will not be possible to do rolling upgrade from 1.1 branch to 1.2 with snapshot is being executed in the cluster.\r\n\r\nCloses #5531'
6137,'javanna','MetaData#concreteIndices to throw exception with a single index argument if allowNoIndices == false\nFixed MetaData#concreteIndices to throw exception with a single index argument in case allowNoIndices == false and ignoreUnavailable == true.\r\n\r\nThe current behaviour is inconsistent as 2 or more non existing indices would cause an exception to be thrown:\r\n\r\n```\r\nGET /a,b/_search?allow_no_indices=false&ignore_unavailable=true\r\n\r\n{\r\n   "error": "IndexMissingException[[[a, b]] missing]",\r\n   "status": 404\r\n}\r\n```\r\n\r\nOn the other hand \r\n```\r\nGET /a/_search?allow_no_indices=false&ignore_unavailable=true\r\n```\r\n\r\nreturns 0 results and no exception regardless of the `allow_no_indices` value.\r\n\r\n\r\nTook also the chance to improve the javadocs for `IndicesOptions` class.'
6130,'dakrone','Track the number of times the circuit breaker has tripped\nWe should add a stat for the number of times that the circuit breaker has been tripped.'
6129,'dakrone','Switch memory breaking in process requests to use CircuitBreaker infrastructure\nFollowup for the work from #6050, we should use the circuit breaker infrastructure for the memory limiting.'
6127,'jpountz',"Internal: Add support for Byte and BytesRef to the XContentBuilder\nHi all,\r\n\r\ncurrently the XContentBuilder will call .ToString()  on the object as fallback if it isn't known.\r\nThis will cause Byte instances to become a String instead of staying a number and BytesRef have some kind of hex representation which is wrong.\r\n\r\nWe could work around this if course by converting the values beforehand, but for performance reasons it would be nice to have the XContentBuilder handling the cases correctly.\r\n"
6122,'clintongormley','Update object-type.asciidoc\nFix small typo.'
6120,'mikemccand','Remove SerialMergeScheduler\nIn master we can just remove it, and for 1.2 I think we should "translate" it into the equivalent ConcurrentMergeScheduler (max_thread_count=1, max_merge_count=1).\r\n\r\nThis merge scheduler is not practical for real usage: it only allows one merge to run at a time, and it\'s scync\'d so it blocks any other incoming threads.\r\n\r\nIt also doesn\'t work with the upcoming index throttling (https://github.com/elasticsearch/elasticsearch/tree/enhancement/throttle_engine ), and while we could "solve" this by just documenting that apps shouldn\'t use SMS, I\'d prefer to remove/remap it instead.'
6119,'colings86','Added `filters` aggregation\nA multi-bucket aggregation where multiple filters can be defined (each filter defines a bucket). The buckets will collect all the documents that match their associated filter.\r\n\r\nThis aggregation can be very useful when one wants to compare analytics between different criterias. It can also be accomplished using multiple definitions of the single filter aggregation, but here, the user will only need to define the sub-aggregations only once.\r\n\r\nCloses #6118'
6118,'colings86','Aggregations: Add `filters` aggregation\nAdd a `filters` (note the `s`) aggregation that takes a list of filters and uses them to construct multiple buckets.\r\n\r\nTechnically the user could create multiple `filter` aggregations, however this gets messy if they have several sub aggregations as they would need to copy the contents into each of the top level aggregations.'
6116,'javanna','Validate query ignores type filter\nSteps to reproduce:\r\n\r\n```\r\nPUT twitter\r\n\r\nGET twitter/tweet/_validate/query?explain\r\n{\r\n  "query" : {\r\n    "term" : {"field":"value"}        \r\n  }\r\n}\r\n\r\n{\r\n   "valid": true,\r\n   "_shards": {\r\n      "total": 1,\r\n      "successful": 1,\r\n      "failed": 0\r\n   },\r\n   "explanations": [\r\n      {\r\n         "index": "twitter",\r\n         "valid": true,\r\n         "explanation": "field:value"\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\nThe resulting above explanation should be wrapped into a filtered query `_type:tweet` but it isn\'t.'
6115,'clintongormley','Minor: update aggregations.asciidoc\nFix plural/singular forms.'
6114,'javanna','Fixed validate query parsing issues\nMade sure that a match_all query is used when no query is specified and ensure no NPE is thrown either.\r\nAlso used the same code path as the search api to ensure that alias filters are taken into account, same for type filters.\r\n\r\nCloses #6111 Closes #6112 Closes #6116\r\n'
6113,'aleph-zero','Percent bytes recovered greater than 100%\nThe recovery API sometimes reports percentages > 100 for bytes recovered. This is confusing and makes no sense. '
6112,'javanna','Validate query ignores alias filters\n    PUT /foo/t/1\r\n    {"foo": "bar"}\r\n\r\n    PUT /foo/_alias/bar\r\n    {\r\n      "filter": {"term": { "foo": "bar"}}\r\n    }\r\n\r\n    GET /foo/_validate/query?explain\r\n    {"query": { "match_all": {}}}\r\n\r\nReturns:\r\n\r\n      {\r\n         "index": "foo",\r\n         "valid": true,\r\n         "explanation": "ConstantScore(*:*)"\r\n      }\r\n\r\nIt should include the filter from the alias.'
6111,'javanna','Validate query without a body throws an NPE\n    GET /_validate/query\r\n\r\n    failed to executed [[[]][], source[_na_], explain:false]\r\n    java.lang.NullPointerException\r\n        at org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction.shardOperation(TransportValidateQueryAction.java:185)\r\n        at org.elasticsearch.action.admin.indices.validate.query.TransportValidateQueryAction.shardOperation(TransportValidateQueryAction.java:63)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:170)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n\r\nShould just default to a `match_all` query'
6104,'dakrone',"Log script change/add and removal at INFO level\nNow that dynamic scripting is disabled, it's extremely helpful to know if/when a script is added or removed via the `ScriptChangesListener`, previously these were only TRACE level messages, they now look like:\r\n\r\n```\r\n[2014-05-09 12:01:25,393][INFO ][script                   ] [Plunderer] compiling script file [/Users/hinmanm/src/elasticsearch/config/scripts/calculate-score.mvel]\r\n[2014-05-09 12:02:25,397][INFO ][script                   ] [Plunderer] compiling script file [/Users/hinmanm/src/elasticsearch/config/scripts/test.mvel]\r\n[2014-05-09 12:03:25,399][INFO ][script                   ] [Plunderer] removing script file [/Users/hinmanm/src/elasticsearch/config/scripts/test.mvel]\r\n```\r\n\r\nTo give an indication as to when the script changes have noticed and the scripts added/reloaded/removed."
6102,'spinscale','add link to staltstack module\n'
6101,'clintongormley','Fixed a typo. Changed "should" to "optional" to match the docs for minimum_should_match.\nIt also might be clearer to change it to \'or\' to keep consistent with the language of the section.'
6095,'martijnvg','Pc min should match\nAdds a `minimum_children` parameter to the `has_child` query and filter.  \r\n\r\n* For the query with `score_mode: sum|avg|max`, it was possible to support this directly in ChildrenQuery.\r\n* For the filter and the query with `score_mode: none`, I added CountChildrenConstantScoreQuery as it seemed a cleaner solution than trying to make either ChildrenQuery or ChildrenConstantScoreQuery fit\r\n\r\nYAML tests have been added, but not sure what to do the Java tests.  Also still needs docs.\r\n\r\nCloses #6019'
6094,'aleph-zero','Benchmark: BenchmarkIntegrationTest failure fixed\nFollowing tests are failing:\r\nBenchmarkIntegrationTest#testSubmitBenchmark\r\nBenchmarkIntegrationTest#testAbortBenchmark'
6091,'aleph-zero','Benchmark: Fix abort benchmark test\nIn some cases a benchmark may complete before we have a chance to abort\r\nit. Just log these cases and continue.'
6089,'aleph-zero','Benchmark: Fix for benchmark test\n- Fix bug where repeatedly calling computeSummaryStatistics() could\r\n  accumulate some values incorrectly.\r\n- Fix check for number of responsive nodes on list is <= number of\r\n  candidate benchmark nodes.'
6082,'javanna','[DOCS] Update fielddata.asciidoc\nSpelling correction and corrected breaker limit default value.'
6071,'aleph-zero','Fix _cat/allocation rest test\nThe rest test for _cat/allocation was failing due to a regular\r\nexpression not accounting for space-padded right-justified text.'
6070,'aleph-zero','Fix for build failure #847\nThis fixes a stack overflow in the test for the _cat/recovery API.\r\nThe regular expression that tests the response body was modified to\r\nhandle large responses properly.'
6068,'javanna','Unify IndicesOptions constants\nRenamed IndicesOptions#strict and IndicesOptions#lenient to make it clearer what they actually return, reused methods and introduced new one\r\n\r\nRelates to #6059, where two new constants were introduced in IndicesOptions. There were already two constants there though, one of which we could have reused. This commit tries to unify them.'
6066,'mikemccand','Throttling incoming indexing when Lucene merges fall behind\nLucene has low-level protection that blocks incoming segment-producing threads (indexing threads, NRT reopen threads, commit, etc.) when there are too many merges running.\r\n\r\nBut this is too harsh for Elasticsearch, so it\'s entirely disabled, but this means merges can fall far behind under heavy indexing, and this results in too many segments in the index, which causes all sorts of problems (slow version lookups, too much RAM, etc.).\r\n\r\nSo we need to do something "softer"; Simon has a good starting patch, which I tested and confirmed (after https://issues.apache.org/jira/browse/LUCENE-5644 is fixed) at least in one use-case that it prevents too many segments in the index:\r\n\r\nBefore Simon\'s + Lucene\'s fix: http://people.apache.org/~mikemccand/lucenebench/base.html\r\n\r\nSame test with the fix: http://people.apache.org/~mikemccand/lucenebench/throttled.html\r\n\r\nSegment counts stay essentially flat.\r\n\r\nHere\'s Simon\'s prototype patch: https://github.com/s1monw/elasticsearch/commit/2de96f9176ee471d30fab7ab7ff40348b9a5bf57'
6064,'clintongormley','Update getting-started.asciidoc\n'
6059,'javanna',"Made it mandatory to specify IndicesOptions when calling MetaData#concreteIndices\nRemoved MetaData#concreteIndices variations that didn't require an IndicesOptions argument. Every caller should use a single method and specify how indices should be resolved to concrete indices based on the indices options argument."
6058,'clintongormley','Update repositories.asciidoc\n'
6054,'javanna','Admin: Remove field names in stats url\n#5671'
6049,'clintongormley','Clarify `missing` behavior.\n'
6048,'clintongormley','Update asciifolding-tokenfilter.asciidoc\nTypo'
6047,'spinscale','Startup: Reset locale to C in bin/elasticsearch\nBecause the NetworkExceptionHelper class relies on the english language in\r\norder to extract information and decide whether a certain exception is a\r\nnetwork problem, we need to set the english locale on startup in order\r\nto prevent other locales to circumvent this check.'
6041,'brwe','Add `shard_min_doc_count` parameter for significant terms similar to `shard_size`\nSignificant terms internally maintain a priority queue per shard with a size potentially\r\nlower than the number of terms. This queue uses the score as criterion to determine if\r\na bucket is kept or not. If many terms with low subsetDF score very high\r\nbut the `min_doc_count` is set high, this might result in no terms being\r\nreturned because the pq is filled with low frequent terms which are all sorted\r\nout in the end.\r\n\r\nThis can be avoided by increasing the `shard_size` parameter to a higher value.\r\nHowever, it is not immediately clear to which value this parameter must be set\r\nbecause we can not know how many terms with low frequency are scored higher that\r\nthe high frequent terms that we are actually interested in.\r\n\r\nOn the other hand, if there is no routing of docs to shards involved, we can maybe\r\nassume that the documents of classes and also the terms therein are distributed evenly\r\nacross shards. In that case it might be easier to not add documents to the pq that have\r\nsubsetDF <= `shard_min_doc_count` which can be set to something like\r\n`min_doc_count`/number of shards  because we would assume that even when summing up\r\nthe subsetDF across shards `min_doc_count` will not be reached.\r\n'
6038,'clintongormley','Make _search on all indices switchable?\nHi,\r\n\r\nA real story.\r\n\r\nFirst off: my code was flawed. It triggered a "POST /_search" (query + timerange filter + sort on timestamp) resulting in a search on all shards (4000 pri shards) filling fielddata cache to its maximum causing my nodes to dive into full gc and become unresponsive.\r\n\r\nMaybe its an idea to disallow search on all indices? To act as safeguard for bugs like mine?\r\n\r\nRegards,\r\nRenzo'
6035,'jpountz',"Update allocation.asciidoc\nIt's minor, but I've added the percentage representations in brackets as it took me a couple of attempts to figure out I wasn't supposed to be using an actual percentage, eg 80%/90%, but a decimal representation of the value, eg 0.80/0.90.\r\n\r\nThat should make it a lot clearer for others."
6034,'jpountz','Fix bug in PropertyPlaceholder and add unit tests\nThis pull request includes the following changes:\r\n* Fix bug in `PropertyPlaceholder` when prefix and suffix have same length or suffix is longer than prefix\r\n* Add unit tests for the `PropertyPlaceholder` class. `testNestedSameLengthPrefixSuffix` and `testNestedShorterPrefix` failed before this bug fix.'
6031,'clintongormley','Added Bloodhound Haskell client/DSL to docs\n'
6030,'clintongormley','Update getting-started.asciidoc\nFixed "Jone Done" to "Jone Doe"'
6029,'clintongormley','Correcting gramma\n'
6023,'aleph-zero',"Test: Integration tests for benchmark API.\nAdds randomized integration tests for the benchmark API.\r\nAdds negative tests for cases where the cluster cannot run benchmarks.\r\nSmall fixes for NPE's exposed during testing.\r\n\r\nCloses #6003"
6022,'martijnvg','Scroll api reduce phase fails if shard failures occur\nOnly occurs in 1.x and master branches and not in any released versions.'
6018,'mikemccand','Store IO throttling throttles far more than asked\nI\'ve been digging into the "merges can fall behind" at high indexing\r\nrates, and I discovered some serious issues with the IO throttling,\r\nwhich we recently (#5902) up\'d from 20 MB/sec to 50 MB/sec by default.\r\n\r\nNet/net I think when we ask for 50 MB/sec today we are really\r\nthrottling at something like 8 MB/sec!\r\n\r\nDetails:\r\n\r\nI indexed a bunch of small log-file type docs into 1 shard, 0\r\nreplicas, using 1 sync _bulk client, to the point where it did it\'s\r\nfirst big-ish merge (611 MB, 440K docs); the merge does not use CFS so\r\nit\'s really writing 611 MB.  I\'m using a fast SSD.\r\n\r\nWith no throttling (index.store.throttle.type=none), the merge takes\r\n20.8 seconds.\r\n\r\nWith the default 50 MB/sec merge throttling, it takes 72.1 sec, which\r\nfar too long (611 MB / 50 = 12.2 sec).  The rate limiter enforces the\r\ninstantaneous rate, so at worse the merge time should have been 20.8 +\r\n12.2 = 33 sec but likely much less than that because merging takes\r\nCPU time.\r\n\r\nSo I dug in and discovered one problem, I think caused by the\r\nsuper.flush and then delegate.flush in BufferedChecksumIndexOutput,\r\nwhere the RateLimiter is always alternately called first on 8192 bytes\r\nthen on 0 bytes.  If I fix RateLimiter to just ignore those 0 bytes,\r\nthe merge time with 50 MB/sec throttle drops to 49.9 sec: better, but\r\nstill too long.  (I think once we cutover to Lucene\'s checksums this 0\r\nbyte issue will be fixed?)\r\n\r\nSystem.nanoTime is actually quite costly, so I suspect the overhead of\r\njust checking whether to pause, and of calling Thread.sleep, is way\r\ntoo much when the pause time is small.  So I change SimpleRateLimiter to\r\njust accumulate the incoming bytes and then once it crosses 1 msec\r\nworth at the specified rate, invoke the pause logic.\r\n\r\nThis really improved it: now the merge takes 25.7 sec at 50 MB/sec\r\nthrottle, and 64.9 sec at 10 MB/sec throttle.  These times seem correct.\r\n\r\nI\'ll also open a Lucene issue to fix this, and make an XRateLimiter\r\nfor ES in the meantime.\r\n'
6017,'spinscale',"Reworded Note about shorthand suggest syntax\nThe existing Note about the shorthand suggest syntax was poorly worded and confusing. Please check whether the way I've phrased it now is still correct as to what the shorthand form actually does and doesn't do: the original wording did not provide me enough information to be sure.\r\nThanks!"
6015,'dadoonet','Update shade-plugin to 2.3\nShade-plugin 2.2 does not work with JDK8 (see http://jira.codehaus.org/browse/MSHADE/fixforversion/19828)\r\n\r\nNot sure if we need to apply that patch in 1.2 branch as well?'
6013,'dadoonet','Plugins: bin/plugin tests for missing plugin name when passing --url\nWhen setting the `--url`, it now sets the implied action--`INSTALL`. Therefore, if the user fails to supply the install flag themselves, then the plugin name will be caught as missing (also added to remove incase a future scenario allows that) and fail immediately.\r\n\r\nAdding code to test for unset plugin names to fail fast with descriptive error messages (for all flags that require a value). Also simplified the series of `if` statements checking for the commands by using a `switch` (now that it\'s using Java 7), added tests, and updated random exceptions with the up-to-date flag names (e.g., "--verbose" instead of "-verbose").\r\n\r\n@dadoonet Note: This will cause or have merge conflicts with #5977. I have no problem with my changes just being incorporated into that PR or doing the merge here.\r\n\r\nAlso, while messing with the unit tests, I noticed that the package for the tests is `org.elasticsearch.plugin` while the non-test code is `org.elasticsearch.plugins`. That\'s probably worth a cleanup by whoever loses the merge; I avoided doing it upfront to simplify any potential merge.\r\n\r\nCloses #5976'
6009,'spinscale','_timestamp: enabled=true doesn\'t work when applied via index template\nI have the following index template set:\r\n\r\n```json\r\n{\r\n    "template" : "foo-*",\r\n    "settings" : {\r\n        "number_of_shards" : 1\r\n    },\r\n    "mappings" : {\r\n\t\t"_default_" : {\r\n\t\t\t"_timestamp" : { "enabled" : true },\r\n\t\t\t"dynamic_templates": [\r\n                { "standard_string": {\r\n                      "match":              "*", \r\n                      "match_mapping_type": "string",\r\n                      "mapping": {\r\n                          "type":           "string",\r\n                          "index":\t\t\t"not_analyzed"\r\n                      }\r\n                }},\r\n                { "analyzed_string": {\r\n                      "match":              "*_analyzed", \r\n                      "match_mapping_type": "string",\r\n                      "mapping": {\r\n                          "type":           "string",\r\n                          "analyzer":       "english"\r\n                      }\r\n                }}\r\n            ]\r\n\t\t}\r\n    }\r\n}\r\n```\r\n\r\nWhen creating a new index `foo-bar` I can verify the mapping is applied correctly, and the `_timestamp` setting is indeed persisted in the index mapping for each and every type in it, but no timestamps are recorded for any document that\'s being added to that index.'
6008,'jpountz',"optimize_bbox for geo_distance filters can cause missing results\nWhen optimize_bbox is enabled for geo_distance filters, it can cause missing results:\r\n\r\nhttps://gist.github.com/jtibshirani/1e42809a52be9ac651fc\r\n\r\nThis issue occurs on ES 1.1.1, and also in ES 1.0.3 and below, before the upgrade to Lucene 4.7. It seems the distance calculation for the bounding box uses DistanceUnit#getEarthRadius(), which is the radius at the semi-major axis, whereas the actual geo_distance filter uses SloppyMath to do the calculation. In Lucene 4.6 SloppyMath uses the average earth radius, and in 4.7 it averages the radii at the two points.\r\n\r\nThe same problem exists for both 'memory' and 'indexed' bounding boxes."
6004,'GaelTadh',"Add time based UUIDs\nSee #5941.\r\n\r\n\r\nUsing SecureRandom as a UUID generator is slow and doesn't allow us\r\nto take adavantage of some lucene optimizations around ids with common\r\nprefixes. This commit will allow us to use a\r\ntimestamp64bit-macAddr-counter UUID. Since the macAddr may be shared\r\namong several nodes running on the same hardware we use an xor of the\r\nmacaddr with a SecureRandom number generated on startup."
6003,'aleph-zero','Test: Benchmark API needs integration tests\nThe benchmark API needs thorough tests for submitting, listing, and aborting benchmarks.'
6000,'martijnvg','Add include/exclude support to global ordinals based terms and significant terms aggregations\n'
5997,'GaelTadh',"Delete mapping sometimes hangs\nThis test fails occasionally when running the Perl test suite:\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/indices.delete_mapping/10_basic.yaml\r\n\r\nEssentially, the test creates three indices, doesn't wait for status yellow, then runs a delete mapping request.  Normally this succeeds but sometimes it just hangs. After 30s my Perl client times out, and shortly thereafter I see a null pointer exception in Elasticsearch. \r\n\r\nLogs of requests from the Perl client, and trace logs from Elasticsearch are available here: https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9\r\n\r\nI think the problem is that not all shards are live when the delete mapping request starts, so the delete-by-query request never runs on the unstarted shards.  See https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9#file-eslogs-L1353 Elasticsearch waits for successful responses from all delete-by-query requests, and so just hangs until the requests time out, and we get the NPE: https://gist.github.com/clintongormley/1a4b4eae386f271e4dc9#file-eslogs-L1642\r\n\r\n"
5993,'jpountz','Update phrase-suggest.asciidoc\nGrammatical error'
5992,'dadoonet',"Can't start the elasticsearch server after upgrade\nWhen i upgraded the elasticsearch from 1.1.0 to 1.1.1, some nodes can't start at all.\r\nI found some error messages in node logs,such as:\r\n\r\n```\r\n[2014-04-30 14:49:18,134][ERROR][bootstrap                ]\r\n {1.1.1}: Initialization Failed ...\r\n1) A binding to org.elasticsearch.indices.analysis.smartcn.SmartChineseIndicesAnalysis was already configured at _unknown_.\r\nubuntu@np8:/mnt/avos/logs/elasticsearch$ /usr/share/elasticsearch/bin/elasticsearch -v\r\n```\r\n\r\nI don't know the meaning of `SmartChineseIndicesAnalysis was already configured at _unknown_`, any help? Thanks."
5982,'martijnvg','Percolation request parameters order dependent\nThis request returns aggregations:\r\n\r\n    GET /t/mytype/_percolate\r\n    {\r\n      "aggs": {\r\n        "colors": {\r\n          "terms": {\r\n            "field": "color"\r\n          }\r\n        }\r\n      },\r\n      "doc": { "foo": "bar baz"}\r\n    }\r\n\r\nThis version doesn\'t:\r\n\r\n    GET /t/mytype/_percolate\r\n    {\r\n      "doc": { "foo": "bar baz"},\r\n      "aggs": {\r\n        "colors": {\r\n          "terms": {\r\n            "field": "color"\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n/cc @martijnvg '
5981,'alexksikes','More Like This API would not take into account size and from in request ...\n...body parameters. Instead these values would always be overridden by the default values of REST parameters search_size and search_from.'
5976,'dadoonet',"bin/plugin -u url fails silently\nWhen installing a plugin using `-u` ( `--url`) option, it fails silently when you don't provide `-i` (`--install`)  option:\r\n\r\n```sh\r\n$ bin/plugin -u http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip\r\n$ bin/plugin -l\r\nInstalled plugins:\r\n    - No plugin detected in elasticsearch/plugins\r\n```\r\n\r\nWith `-i`:\r\n\r\n```sh\r\n$ bin/plugin -u http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip -i rssriver\r\n-> Installing rssriver...\r\nTrying http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/river/rssriver/1.1.0/rssriver-1.1.0.zip...\r\nDownloading ......................................DONE\r\nInstalled rssriver into /Users/dpilato/Documents/Elasticsearch/apps/elasticsearch/elasticsearch-2.0.0-SNAPSHOT/plugins/rssriver\r\n$ bin/plugin -l\r\nInstalled plugins:\r\n    - rssriver\r\n```\r\n"
5974,'spinscale','Analysis: Default analyzer includes stopwords\nUsing the default analyzer:\r\n\r\n    GET /_analyze?text=The fox\r\n\r\nRemoves stopwords:\r\n\r\n    {\r\n       "tokens": [\r\n          {\r\n             "token": "fox",\r\n             "start_offset": 4,\r\n             "end_offset": 7,\r\n             "type": "<ALPHANUM>",\r\n             "position": 2\r\n          }\r\n       ]\r\n    }\r\n\r\nUsing the `standard` analyzer:\r\n\r\n    GET /_analyze?text=The fox&analyzer=standard\r\n\r\nKeeps stopwords:\r\n\r\n    {\r\n       "tokens": [\r\n          {\r\n             "token": "the",\r\n             "start_offset": 0,\r\n             "end_offset": 3,\r\n             "type": "<ALPHANUM>",\r\n             "position": 1\r\n          },\r\n          {\r\n             "token": "fox",\r\n             "start_offset": 4,\r\n             "end_offset": 7,\r\n             "type": "<ALPHANUM>",\r\n             "position": 2\r\n          }\r\n       ]\r\n    }\r\n\r\n\r\n\r\n\r\n\r\n\r\n'
5972,'clintongormley','Inconsistent results when mixing `fieldname: { fieldname: {` and `fieldname.fieldname` for mapping and indexing\nThe handling of objects in a mapping seems inconsistent. When adding fields with dots, for example `afield.number`, then, when indexing a document \r\n\r\n```\r\n"afield": {\r\n\t"number": 4\r\n}\r\n```\r\n\r\nthe type of `"afield": { "number": ` is found correctly, but a second mapping for the object is added. This happens even if the indexing operation fails in case the type of `"afield": { "number": ` does not correspond with `"afield.number"` as defined in the mapping.\r\n\r\nAn example is here: https://gist.github.com/brwe/11394327\r\n\r\nThere are two issue with this:\r\n\r\n1. In STEP 2 (indexing the object) the indexing operation is rejected but the mapping for the object is still added. I do not think this should be so (This might be related to [#5798](https://github.com/elasticsearch/elasticsearch/issues/5798))\r\n2. It seems to me that when creating or merging mappings, it should be checked if pathes exist twice. For example, the following yields no error:\r\n\r\n```\r\nPUT /testindex/sometype/_mapping\r\n{\r\n   "sometype": {\r\n      "properties": {\r\n         "afield.number": {\r\n            "type": "integer"\r\n         },\r\n         "afield.text": {\r\n            "type": "string"\r\n         },\r\n         "afield": {\r\n            "properties": {\r\n               "number": {\r\n                  "type": "string"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n'
5971,'mikemccand','Documentation wrong for store throttling\nThe documentation for indices.store.throttle.type says that it can be all, merge or not (http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-translog.html) but it is all, merge or none.'
5966,'imotov','Fix for hanging aborted snapshot during node shutdown\nIf a node is shutdown while a snapshot that runs on this node is aborted, it might cause the snapshot process to hang.\r\n\r\nCloses #5958'
5965,'jpountz','Update client.asciidoc\nshould be classpath rather than classloader, no?'
5963,'jpountz','Fixed a typo\n'
5959,'dadoonet','time_zone should set pre AND post _zone in aggs\nUntil now, with aggregations and facets, when users use `time_zone` we get back some inaccurate results.\r\n\r\nThe cause is that we set only `pre_zone` when `time_zone` is set. So after computing values, we don\'t apply `post_zone` to results, which gives "strange" results.\r\n\r\nHere is a full recreation:\r\n\r\nCreate the test\r\n------------\r\n\r\n```\r\nDELETE /test\r\n\r\nPUT /test/doc/1\r\n{\r\n  "date" : "2014-04-25T21:05:00"\r\n}\r\n\r\nPUT /test/doc/2\r\n{\r\n  "date" : "2014-04-25T22:06:00"\r\n}\r\n\r\nPUT /test/doc/3\r\n{\r\n  "date" : "2014-04-25T23:07:00"\r\n}\r\n\r\nPUT /test/doc/4\r\n{\r\n  "date" : "2014-04-26T00:08:00"\r\n}\r\n```\r\n\r\nGroup by date\r\n-----------\r\n\r\n```\r\nGET /test/doc/_search\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "bydate": {\r\n      "date_histogram": {\r\n        "field": "date",\r\n        "interval": "day",\r\n        "time_zone": "Europe/Paris"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis gives\r\n\r\n```\r\n   "aggregations": {\r\n      "bydate": {\r\n         "buckets": [\r\n            {\r\n               "key_as_string": "2014-04-25T00:00:00.000Z",\r\n               "key": 1398384000000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T00:00:00.000Z",\r\n               "key": 1398470400000,\r\n               "doc_count": 3\r\n            }\r\n         ]\r\n      }\r\n   }\r\n```\r\n\r\nWhen `pre_zone` and `post_zone` are used:\r\n\r\n```\r\nGET /test/doc/_search\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "bydate": {\r\n      "date_histogram": {\r\n        "field": "date",\r\n        "interval": "day",\r\n        "pre_zone": "Europe/Paris",\r\n        "post_zone": "Europe/Paris"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWe get:\r\n\r\n```\r\n   "aggregations": {\r\n      "bydate": {\r\n         "buckets": [\r\n            {\r\n               "key_as_string": "2014-04-25T02:00:00.000Z",\r\n               "key": 1398391200000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T02:00:00.000Z",\r\n               "key": 1398477600000,\r\n               "doc_count": 3\r\n            }\r\n         ]\r\n      }\r\n   }\r\n```\r\n\r\nGrouping by hours\r\n--------------\r\n\r\nIt\'s even more obvious when grouping by hours:\r\n\r\n```\r\nGET /test/doc/_search\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "bydate": {\r\n      "date_histogram": {\r\n        "field": "date",\r\n        "interval": "hour",\r\n        "time_zone": "Europe/Paris"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\ngives\r\n\r\n```\r\n   "aggregations": {\r\n      "bydate": {\r\n         "buckets": [\r\n            {\r\n               "key_as_string": "2014-04-25T21:00:00.000Z",\r\n               "key": 1398459600000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-25T22:00:00.000Z",\r\n               "key": 1398463200000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-25T23:00:00.000Z",\r\n               "key": 1398466800000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T00:00:00.000Z",\r\n               "key": 1398470400000,\r\n               "doc_count": 1\r\n            }\r\n         ]\r\n      }\r\n   }\r\n```\r\n\r\nand\r\n\r\n```\r\nGET /test/doc/_search\r\n{\r\n  "size": 0,\r\n  "aggs": {\r\n    "bydate": {\r\n      "date_histogram": {\r\n        "field": "date",\r\n        "interval": "hour",\r\n        "pre_zone": "Europe/Paris",\r\n        "post_zone": "Europe/Paris"\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\ngives\r\n\r\n```\r\n   "aggregations": {\r\n      "bydate": {\r\n         "buckets": [\r\n            {\r\n               "key_as_string": "2014-04-25T23:00:00.000Z",\r\n               "key": 1398466800000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T00:00:00.000Z",\r\n               "key": 1398470400000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T01:00:00.000Z",\r\n               "key": 1398474000000,\r\n               "doc_count": 1\r\n            },\r\n            {\r\n               "key_as_string": "2014-04-26T02:00:00.000Z",\r\n               "key": 1398477600000,\r\n               "doc_count": 1\r\n            }\r\n         ]\r\n      }\r\n   }\r\n```\r\n\r\nThis patch fix this.'
5958,'imotov','Snapshot aborted but but still in progress\nHi\r\n\r\nI\'m running elasticsearch 1.1.1 on centos 6 AWS instances with java 7. We have a snapshot that the api call "_snapshot/<repo_name>/_all" is listing as IN_PROGRESS, however when attempting to delete that snapshot, it doesn\'t delete. In fact the delete call just hangs and never returns. When I check the current status of all the snapshots using "_snapshot/<repo_name>/_status" that to just hangs and does not return. \r\n\r\nI then looked at the cluster state and that tells me that the snapshot is in an "ABORTED" state. I can\'t actually create a new snapshot right now. Any ideas how I can resolve this and is it a bug?\r\n\r\nThanks\r\n\r\n'
5954,'javanna',"Run tests through forbidden apis\nCheck the tests for forbidden api's.  Start with simple URL#getPath and URL#getFile that have issues accessing resources with spaces in the path.\r\n\r\n\r\nThis PR has #5950 merged in."
5951,'jpountz','A couple small typos and one clarification.\nTypos.'
5950,'javanna','Use URI vs URL accessing File from classpath.\nURL escapes special characters such as spaces which\r\ncauses the resource to not be found when used to create\r\na File object.  Use URI.\r\n\r\nCloses #5915'
5949,'javanna',"[TEST] randomly introduced a client node within test cluster\nThe default number of clients nodes is 1, applied to all cluster scopes (global, suite and test). Can be changed though through the newly added `@ClusterScope#numClientNodes`.\r\n\r\nIn our tests we currently refer to nodes in a generic way. All the tests that either stop or start nodes rely on the fact that those nodes hold data though. Made that clearer as that becomes more important when introducing other types of nodes within the test cluster. Reflected this by adapting and renaming the following methods in `TestCluster`:\r\n\r\n- `ensureAtLeastNumNodes` to `ensureAtLeastNumDataNodes`\r\n- `ensureAtMostNumNodes` to `ensureAtMostNumDataNodes`\r\n- `stopRandomNode` to `stopRandomDataNode`\r\n\r\nand the following ones in `ElasticsearchIntegrationTest`:\r\n- `dataNodes` to `numDataNodes`.\r\n- `@ClusterScope#numNodes` to `numDataNodes`\r\n- `@ClusterScope#minNumNodes` to `minNumDataNodes`\r\n- `@ClusterScope#maxNumNodes` to `maxNumDataNodes`\r\n\r\nAdded facilities to be able to deal with data nodes specifically, like for instance retrieve a client to a data node, or retrieve an instance of a class through guice only from data nodes.\r\n\r\nAdapted existing tests to successfully run although there's a node client around.\r\n\r\nFixed _cat/allocation REST tests to make disk.total, disk.avail and disk.percent optional as client nodes won't return that info."
5948,'javanna','_cat/allocation returns -1 as disk.total for clients nodes\nNo data gets allocated on client nodes, and a subest of the nodes stats information is not available for client nodes. That is why some columns in `_cat/allocation` might be empty, but there should be consistency on how null values are returned. The example below contains `0b` for `disk.used`, no value for `disk.available`, and `-1b` for `disk.total`:\r\n\r\n```\r\n0     0b             -1b    Lucas-MacBook-Air.local 192.168.0.13 Hazard\r\n6 92.5gb 140.4gb 232.9gb 39 Lucas-MacBook-Air.local 192.168.0.13 Ningal\r\n```\r\n\r\nI think it should be as follows instead, with no value instead of `-1`, leaving `0b` for `disk.used` though:\r\n\r\n```\r\n0     0b                    Lucas-MacBook-Air.local 192.168.0.13 Hazard\r\n6 92.5gb 140.4gb 232.9gb 39 Lucas-MacBook-Air.local 192.168.0.13 Ningal\r\n```\r\n\r\n'
5947,'javanna','Postings highlighter breaks on "." Full recreation attached\nPostings highlighter in 1.0.3 only captures part of a field value before or after "." if the "." is present in the searched string\r\n\r\nFor example John M.Smith when searched on John will produce ```"<em>John</em> M."``` highlight while search on Smith ```"<em>Smith</em>"```\r\n\r\nPlain highlighter produces ```"<em>John</em> M. Smith"``` as expected\r\n\r\nhttps://gist.github.com/roytmana/11308459\r\n\r\ncc @javanna '
5944,'markharwood','Significant_terms agg: added option for a backgroundFilter \nAllows for a narrowed background context in analysis of term frequencies.\r\n'
5941,'GaelTadh',"Explore using timebased decentralized UUID for autogenerated IDs\nToday we use UUIDs which are very costly to compute since they use `SecureRandom` and they have some properties that might not play very well / or as well as they could with the Lucene internal datastructures since they don't share a common prefix. With a time based ID we can potentially remove the need for bloom fitlers on the ID fields since the shared prefixes across the IDs might be all in memory and that would speedup updates dramatically. Something along the lines of [flake ids](http://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/) would be very interesting since it has this property"
5940,'brwe','Function score multi values\nDecay functions currently only use the first value in a field that contains\r\nmultiple values to compute the distance to the origin. Instead, they should\r\nconsider all distances if more values are in the field and then use\r\none of min/max/sum/avg which is defined by the user.\r\n\r\ncloses #3960\r\n'
5937,'javanna','Fix code typo in FieldSortBuilder.java\ntypo'
5932,'s1monw','Upgrade to Lucene 4.8.0\nLucene 4.8 included several bugfixes and API Improvements. The most relevant for Elasticsearch are:\r\n\r\n * LUCENE-4747, LUCENE-5514: Move to Java 7 as minimum Java version.\r\n * LUCENE-5516: MergeScheduler#merge() now accepts a MergeTrigger as\r\n   well as a boolean that indicates if a new merge was found in the\r\n   caller thread before the scheduler was called.\r\n * LUCENE-5487: Separated bulk scorer (new Weight.bulkScorer method)\r\n   from normal scoring (Weight.scorer) for those queries that can do\r\n   bulk scoring more efficiently, e.g. BooleanQuery in some cases.\r\n   This also simplified the Weight.scorer API by removing the two\r\n   confusing booleans.\r\n * LUCENE-5497: HunspellStemFilter properly handles escaped terms and\r\n   affixes without conditions.\r\n * LUCENE-5468: HunspellStemFilter uses 10 to 100x less RAM. It also\r\n   loads all known openoffice dictionaries without error, and supports\r\n   an additional longestOnly option for a less aggressive approach.\r\n * LUCENE-5505: HunspellStemFilter ignores BOM markers in dictionaries\r\n   and handles varying types of whitespace in SET/FLAG commands.\r\n * LUCENE-5507: Fix HunspellStemFilter loading of dictionaries with\r\n   large amounts of aliases etc before the encoding declaration.\r\n * LUCENE-5111: Fix WordDelimiterFilter to return offsets in correct order.\r\n * LUCENE-4848: Use Java 7 NIO2-FileChannel instead of RandomAccessFile\r\n   for NIOFSDirectory and MMapDirectory. This allows to delete open files\r\n   on Windows if NIOFSDirectory is used, mmapped files are still locked.\r\n\r\nThe changes to WordDelimiterFilter causes a behavior change in the next Elasticsearch release. The previous version of this tokenfilter will still be available for indices that are created in previous version or with the according lucene version.'
5931,'brwe','Mappings lost after cluster rolling restart\nHi,\r\n\r\nWe have 3-node ELK cluster running 1.0.1. To activate a heapchange we did a node-by-node restart.\r\n\r\nUnwanted result is loss of mappings on a small number of indices, causing errors like below when querying with Kibana:\r\n```\r\nCaused by: org.elasticsearch.search.SearchParseException: [logstash-pro-endeca-2014.04.13][3]: query[filtered((@source_host:"pro end app 006"))->cache(@timestamp:[2014-04-10T14:07:21+02:00 TO 2014-04-24T14:07:21+02:00])],from[0],size[50]: Parse Failure [No mapping fo\r\nund for [@timestamp] in order to sort on]\r\n        at org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:198)\r\n        at org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:172)\r\n        at org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:90)\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:583)\r\n        ... 10 more\r\n```\r\nAfter copying the mapping from a sibling index everything is OK again.\r\n\r\n'
5930,'brwe','Cannot aggregate on completion suggester field in elasticsearch\nIn the mapping, the field named domain (String value) is a completion suggester as below:\r\n```JSON\r\n"domain": {\r\n                  "max_input_length": 50,\r\n                  "preserve_separators": true,\r\n                  "payloads": false,\r\n                  "analyzer": "simple",\r\n                  "preserve_position_increments": true,\r\n                  "type": "completion"\r\n               }\r\n```\r\nCurrently, I would like to aggregate based on the value in the field "domain". The example of a query in Sense is shown below. \r\nlocalhost:9200\r\n```JSON\r\nPOST /ourindex/_search\r\n{\r\n   "query": {\r\n     "match_all": {}\r\n   },\r\n   "aggs": {\r\n      "test": {\r\n         "terms": {\r\n            "field": "domain"\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\nWhen the query is run, it returns NullPointerException. The detailed response is following.\r\n```LOG\r\n{\r\n"error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[iQfn8u55QoqZSnlXEGGUlw][ourindex][4]: SearchParseException[[ourindex][4]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\r\\n   \\"query\\": {\\r\\n     \\"match_all\\": {}\\r\\n   },\\r\\n   \\"aggs\\": {\\r\\n      \\"test\\": {\\r\\n         \\"terms\\": {\\r\\n            \\"field\\": \\"domain\\"\\r\\n         }\\r\\n      }\\r\\n   }\\r\\n}\\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][2]: SearchParseException[[ourindex][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\r\\n   \\"query\\": {\\r\\n     \\"match_all\\": {}\\r\\n   },\\r\\n   \\"aggs\\": {\\r\\n      \\"test\\": {\\r\\n         \\"terms\\": {\\r\\n            \\"field\\": \\"domain\\"\\r\\n         }\\r\\n      }\\r\\n   }\\r\\n}\\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][3]: SearchParseException[[ourindex][3]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\r\\n   \\"query\\": {\\r\\n     \\"match_all\\": {}\\r\\n   },\\r\\n   \\"aggs\\": {\\r\\n      \\"test\\": {\\r\\n         \\"terms\\": {\\r\\n            \\"field\\": \\"domain\\"\\r\\n         }\\r\\n      }\\r\\n   }\\r\\n}\\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][0]: SearchParseException[[ourindex][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\r\\n   \\"query\\": {\\r\\n     \\"match_all\\": {}\\r\\n   },\\r\\n   \\"aggs\\": {\\r\\n      \\"test\\": {\\r\\n         \\"terms\\": {\\r\\n            \\"field\\": \\"domain\\"\\r\\n         }\\r\\n      }\\r\\n   }\\r\\n}\\n]]]; nested: NullPointerException; }{[iQfn8u55QoqZSnlXEGGUlw][ourindex][1]: SearchParseException[[ourindex][1]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\r\\n   \\"query\\": {\\r\\n     \\"match_all\\": {}\\r\\n   },\\r\\n   \\"aggs\\": {\\r\\n      \\"test\\": {\\r\\n         \\"terms\\": {\\r\\n            \\"field\\": \\"domain\\"\\r\\n         }\\r\\n      }\\r\\n   }\\r\\n}\\n]]]; nested: NullPointerException; }]",\r\n   "status": 400\r\n}\r\n```\r\n\r\nDoes the type of the suggester matter? How can it work to aggregate by the values of domain field?'
5924,'s1monw','Resiliency: Use Lucene built-in checksumming \nLucene 4.8 added checksumming for all files using `CRC32`. We use `Adler` at this point on top of the Direcotory API to check integrity on recovery etc. We should try to cut over to the lucene impl since it comes "for free" now.'
5923,'colings86','Scriptable Metric Aggregation\nIt would be great if we could have a script-based metric aggregation that can \r\nrefer to other metric aggregations from within the script.  For example, given \r\nthe data and query given in this gist: \r\n\r\nhttps://gist.github.com/mattweber/71033b1bf2ebed1afd8e\r\n\r\nIt would like to be able to do something like the following for a "profit" calculation:\r\n\r\n```\r\n"cost_agg": {\r\n    "filter": {\r\n        "term": {\r\n            "type": "cost"\r\n        }\r\n    },\r\n    "aggs": {\r\n        "cost_sum_agg": {\r\n            "sum": {\r\n                "field": "value"\r\n            }\r\n        }\r\n    }\r\n},\r\n"sale_agg": {\r\n    "filter": {\r\n        "term": {\r\n            "type": "sale"\r\n        }\r\n    },\r\n    "aggs": {\r\n        "sale_sum_agg": {\r\n            "sum": {\r\n                "field": "value"\r\n            }\r\n        }\r\n    }\r\n},\r\n"profit_agg": {\r\n    "metric_script": {\r\n        "script": "sale_agg>sale_sum_agg - cost_agg>cost_sum_agg"\r\n    }\r\n}\r\n```\r\n\r\nSo using the data from the gist we would get a response such as:\r\n\r\n```\r\n{\r\n  "key" : "foobar",\r\n  "doc_count" : 4,\r\n  "sale_agg" : {\r\n    "doc_count" : 2,\r\n    "sale_sum_agg" : {\r\n      "value" : 65.48\r\n    }\r\n  },\r\n  "cost_agg" : {\r\n    "doc_count" : 2,\r\n    "cost_sum_agg" : {\r\n      "value" : 24.68\r\n    }\r\n  },\r\n  "profit_agg": {\r\n    "value": 40.80\r\n  }\r\n}\r\n```\r\n\r\n\r\nI believe something like this would be possible given the fact that the\r\nthe `order` parameter of a Terms Aggregation can refer to metric sub-aggregation\r\nvalues.  The logic and retrieving the values should be very similar.  Exposing\r\nthem to a script might be the hard part.\r\n\r\nI will look into this further but I wanted to open the issue in case you are\r\nworking on similar functionality or have any suggestions for me.\r\n\r\n/cc @uboness @jpountz '
5921,'GaelTadh',"Scripting: Allow search templates stored in an index to be retrieved and used at search time\nAllow running search templates stored in an index at search time\r\n\r\nThis change allows search templates stored in an index to be used at search time.\r\nYou can run an indexed template by\r\n```\r\nGET /_search/template\r\n{\r\n    'templatename' : '/index/language/id',\r\n    'params' : {\r\n        'param1' : 'foo'\r\n    }\r\n}\r\n```\r\nRunning the template will fail if the template cannot be found, if the templatename\r\nstarts with a '/' but does not conform to the /index/type/id triple\r\nIf the named template cannot be found an error will be raised. Currently the\r\ntemplates are NOT cached. We also use TransportGetAction directly instead of using\r\nClient to avoid a circular dependency between Client and ScriptService\r\n\r\nSee #5637"
5920,'javanna','Test: Start nodes using node.bench true to enable benchmark api\nEnabling benchmark apis is required to run our REST tests for it. Added the parameter to node creation in `TestCluster` as well in the release script, when starting the node to run the smoke tests against.\r\n\r\nCloses #5910'
5919,'aleph-zero','Benchmark: NullPointerException thrown by benchmark run against multiple nodes through REST layer\nWhen using the benchmark api against multiple nodes through the REST layer, depending on the node hit by the request, a `NullPointerException` might be thrown and returned:\r\n\r\n```\r\n{\r\n  "error" : "NullPointerException[null]",\r\n  "status" : 500\r\n}\r\n```\r\n\r\nThe reason for this is that the [`CompetitionResult#toXContent`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bench/CompetitionResult.java#L211) method reads from the `competitionSummary` variable which is `null` when the object gets created through the empty constructor as the `readFrom` method doesn\'t initialize it based on the received information.\r\n\r\nRunning the REST tests with our java runner reproduces this, once nodes are started with `node.bench=true` (relates to #5910).'
5917,'s1monw',"Don't lookup version for auto generated id and create\nWhen a create document is executed, and its an auto generated id (based on UUID), we know that the document will not exists in the index, so there is no need to try and lookup the version from the index.\r\nFor many cases, like logging, where ids are auto generated, this can improve the indexing performance, specifically for lightweight documents where analysis is not a big part of the execution."
5915,'javanna','test failures\nLately I have been unable to run the tests without any errors.  With java 1.7.0_55 and 1.8.0_05.  I have put the exceptions and commands to reproduce below.  Let me know if this is expected of if I should open a ticket for each failure.\r\n\r\nSuite: org.elasticsearch.index.query.TemplateQueryTest\r\n```\r\nmvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.index.query.TemplateQueryTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Des.logger.level=INFO -Des.node.local=true -Dtests.heap.size=512m\r\n```\r\n```\r\nERROR   0.53s | TemplateQueryTest.testTemplateInFile <<<\r\n   > Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[L3om1MHxTAKSpWUQy21U5A][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][5]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][2]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][8]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][8]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][9]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][9]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][7]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][0]: RemoteTransportException[[node_1][inet[/192.168.1.121:9301]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query":{"template":{"query":"storedTemplate","params":{"template":"all"}}}}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }\r\n   > \tat __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:C7881C13C67531D2]:0)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)\r\n   > \tat org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n   > \tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n   > \tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n```\r\nERROR   0.63s | TemplateQueryTest.testRawFSTemplate <<<\r\n   > Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[L3om1MHxTAKSpWUQy21U5A][test][4]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][5]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][8]: SearchParseException[[test][8]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][9]: SearchParseException[[test][9]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][7]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[L3om1MHxTAKSpWUQy21U5A][test][0]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }{[fldhqviNRyK_6JLp8pH8tQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query_binary":"eyJ0ZW1wbGF0ZSI6IHsicXVlcnkiOiAic3RvcmVkVGVtcGxhdGUiLCJwYXJhbXMiIDogeyJ0ZW1wbGF0ZSIgOiAiYWxsIn19fQ=="}]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e]; }\r\n   > \tat __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:43A1FBBC1F4C432F]:0)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)\r\n   > \tat org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n   > \tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n   > \tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n```\r\nERROR   0.81s | TemplateQueryTest.testThatParametersCanBeSet <<<\r\n   > Throwable #1: org.elasticsearch.action.search.SearchPhaseExecutionException: Failed to execute phase [dfs], all shards failed; shardFailures {[fldhqviNRyK_6JLp8pH8tQ][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][5]: SearchParseException[[test][5]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[L3om1MHxTAKSpWUQy21U5A][test][2]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][6]: SearchParseException[[test][6]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][7]: SearchParseException[[test][7]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[L3om1MHxTAKSpWUQy21U5A][test][0]: RemoteTransportException[[node_0][inet[/192.168.1.121:9300]][search/phase/dfs]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }{[fldhqviNRyK_6JLp8pH8tQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [_na_]]]; nested: ElasticsearchParseException[Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@f0a4a93c]; }\r\n   > \tat __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:8330A16E27FBB878]:0)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:291)\r\n   > \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:239)\r\n   > \tat org.elasticsearch.search.action.SearchServiceTransportAction$4.handleException(SearchServiceTransportAction.java:209)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handleException(MessageChannelHandler.java:181)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)\r\n   > \tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:123)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n   > \tat org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n   > \tat org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n   > \tat org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n   > \tat org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n   > \tat org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n   > \tat org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n   > \tat org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n   > \tat org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n   > \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\nSuite: org.elasticsearch.test.rest.test.FileUtilsTests\r\n```\r\nmvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.test.rest.test.FileUtilsTests -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.method="testLoadMultipleYamlSuites" -Des.logger.level=INFO -Des.node.local=true -Dtests.heap.size=512m\r\n```\r\n```\r\nFAILURE 0.10s | FileUtilsTests.testLoadMultipleYamlSuites <<<\r\n   > Throwable #1: java.lang.AssertionError: \r\n   > Expected: <1>\r\n   >      got: <0>\r\n   > \r\n   > \tat __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:5726FA52A0F1DF9F]:0)\r\n   > \tat org.junit.Assert.assertThat(Assert.java:780)\r\n   > \tat org.junit.Assert.assertThat(Assert.java:738)\r\n   > \tat org.elasticsearch.test.rest.test.FileUtilsTests.testLoadMultipleYamlSuites(FileUtilsTests.java:54)\r\n   > \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n   > \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n   > \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n   > \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)\r\n   > \tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\r\n   > \tat org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\r\n   > \tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n   > \tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\r\n   > \tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)\r\n   > \tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n   > \tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\r\n   > \tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n\r\nSuite: org.elasticsearch.index.query.TemplateQueryParserTest\r\n```\r\nmvn test -Dtests.seed=CFC58CB69C73A534 -Dtests.class=org.elasticsearch.index.query.TemplateQueryParserTest -Dtests.prefix=tests -Dfile.encoding=UTF-8 -Duser.timezone=America/Los_Angeles -Dtests.method="testParserCanExtractTemplateNames" -Des.logger.level=INFO -Dtests.heap.size=512m\r\n```\r\n```\r\nERROR   1.30s | TemplateQueryParserTest.testParserCanExtractTemplateNames <<<\r\n   > Throwable #1: org.elasticsearch.ElasticsearchParseException: Failed to derive xcontent from org.elasticsearch.common.bytes.PagedBytesReference@88846b7e\r\n   > \tat __randomizedtesting.SeedInfo.seed([CFC58CB69C73A534:B1A8F5E0111DEFFC]:0)\r\n   > \tat org.elasticsearch.common.xcontent.XContentFactory.xContent(XContentFactory.java:259)\r\n   > \tat org.elasticsearch.index.query.TemplateQueryParser.parse(TemplateQueryParser.java:67)\r\n   > \tat org.elasticsearch.index.query.TemplateQueryParserTest.testParserCanExtractTemplateNames(TemplateQueryParserTest.java:121)\r\n   > \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n   > \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n   > \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n   > \tat java.lang.reflect.Method.invoke(Method.java:606)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:826)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:862)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:876)\r\n   > \tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\r\n   > \tat org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\r\n   > \tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n   > \tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\r\n   > \tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)\r\n   > \tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)\r\n   > \tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n   > \tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\r\n   > \tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:70)\r\n   > \tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\r\n   > \tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n   > \tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n   > \tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n'
5912,'mikemccand',"use Lucene's defaults for CMS settings\n#5882\r\n\r\nLucene changed the settings a while back to be more conservative but ES has the old hardwired defaults."
5910,'javanna','Run tests with node.bench=true\nThe benchmark tests are failing because the nodes are not set to run with\r\n\r\n    -Des.node.bench=true\r\n\r\nDisabled tests for now with https://github.com/elasticsearch/elasticsearch/commit/640085c45d112884cd8eedd7c1cd214bb874aafc which should be reverted once tests fixed'
5907,'javanna','Add REST API spec for /_search_shards endpoint\n'
5906,'aleph-zero','Benchmark: Missing benchmark should return 404\nCurrently, aborting a non-existent benchmark returns 200 OK, eg:\r\n\r\n    curl -XPOST \'http://localhost:9200/_bench/abort/my_benchmark?pretty=1\'\r\n\r\n    Response: 200, Took: 1 ms\r\n    {   "error" : "Benchmark with id [my_benchmark] not found" }\r\n\r\nit should return a 404 not found'
5904,'aleph-zero','Benchmark: Remove "slowest" if only one request in benchmark API\nWhile each competitor in the benchmark API can accept multiple requests, two competitors can be used to compare two variations of the same query, ie there is only one request per competitor.\r\n\r\nIn this case it makes sense to not output the `slowest` request as there is only one.  '
5903,'aleph-zero','Benchmark: Use index/type in benchmark API\nCurrently the benchmark API supports the `indices` parameter which actually takes a path-like `index/type` setting.\r\n\r\nFor consistency, we should instead accept `index` and `type` parameters (which can be single or multiple, and handle the usual wildcards) and whose values default to the values provided in the URL.'
5897,'javanna','[TEST] Allow to disable randomization of shards and replicas via system property\nNeeded for REST backwards compatibility tests, since we need to run older tests with the latest runner, which randomizes shards and replicas, but the tests rely on defaults (5,1).'
5896,'javanna','[TEST] Randomized number of replicas between 0 and the number of data nodes - 1\nRandomized number of replicas between 0 and the number of data nodes - 1 (rather than just between 0 and 1) when creating the random index template before each test'
5891,'mikemccand',"Enable turning on IndexWriter's infoStream\nFor diagnosing low level IndexWriter issues, it's sometimes useful to enable IndexWriter's infoStream output, but one cannot do this from Elasticsearch today."
5885,'jpountz','Update service.bat\ncorrected typo, "Exiting..." instead of "Existing..."'
5882,'mikemccand',"ConcurrentMergeSchedulerProvider should use Lucene's default settings\nIn https://issues.apache.org/jira/browse/LUCENE-4661 we changed CMS's default maxMergeCount / maxMergeThreads to be less aggressive because the previous settings (allowing too many concurrent merges) would make merging less efficient.\r\n\r\nBut Elasticsearch's ConcurrentMergeSchedulerProvider is still using the old defaults.\r\n\r\nI think we should just cutover to using whatever Lucene defaults to?"
5881,'javanna','[TEST] Added blacklist to be able to skip specific REST tests\nThe blacklist can be provided through `-Dtests.rest.blacklist` and supports a comma separated list of globs\r\ne.g. `-Dtests.rest.blacklist=get/10_basic/*,index/*/*`\r\n\r\nAlso added some missing docs and made it clearer that the suite/test descriptions effectively contains their (relative) path (api/yaml_file/test section)'
5880,'jpountz','Add support for conditional highlighting\nAdds a "conditional" object under each highlighting field that can contain\r\nother fields.  Those fields are highlighted based on if there was a match\r\nin the containing field.\r\n\r\nIt\'ll let you do things like this example in the docs:\r\n\r\nThis example extracts extracts the first 100 characters from `text` if there is\r\na match in `title`, otherwise it highlights `text` as normal.\r\n```js\r\n{\r\n    "highlight": {\r\n        "fields": {\r\n            "title": {\r\n                "conditional": {\r\n                    "match": {\r\n                        "text": {\r\n                            "no_match_size": 100,\r\n                            "skip_matching":  true\r\n                        }\r\n                    }\r\n                    "no_match": {\r\n                        "text": {\r\n                            "no_match_size": 100\r\n                        }\r\n                    },\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAlso adds a setting to instruct the highlighter to skip its normal match\r\nlogic and just do its no_match_size stuff.   This is very useful with\r\nconditional highlighting "chains" that end in a no_match_size extracting\r\na previous entry in the chain.  Like this example from the docs:\r\n```js\r\n{\r\n    "highlight": {\r\n        "fields": {\r\n            "text": {\r\n                "conditional": {\r\n                    "no_match": {\r\n                        "auxiliary_text": {\r\n                            "conditional": {\r\n                                "no_match": {\r\n                                    "file_text": {\r\n                                        "conditional": {\r\n                                           "no_match": {\r\n                                               "text": {\r\n                                                    "no_match_size": 100,\r\n                                                    "skip_matching": true\r\n                                                }\r\n                                            }\r\n                                        }\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nCloses #4649'
5876,'imotov',"Don't block stats for read-only indices\nCloses #5855"
5875,'colings86','Update datehistogram-aggregation.asciidoc\nDocs really need to explain how to suppress the hits portion of the response to shrink it down and speed it up. I had to do a lot of digging to figure out how and actual found it based on how to do it in facets. Although this probably effects all the various bucket aggregations. Perhaps the change should be made somewhere else in the docs. But made it here since this was where I needed it and tested it.\r\n\r\nHope this is helpful. Thanks!'
5870,'uboness','Percentiles aggregations are always keyed and suggestion on non keyed response\nI realize the percentiles aggregations is still experimental which is probably the cause for this: \r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/PercentilesParser.java#L60\r\n\r\nThe routine that is currently in place to write the percentiles `non_keyed` output will write the aggregation like this:\r\n\r\n```\r\n"aggs" : {\r\n  "my_percentiles": [\r\n       { .. }, \r\n       { .. }\r\n   ]\r\n}\r\n```\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/metrics/percentiles/InternalPercentiles.java#L131\r\n\r\nMaking it the only aggregation to directly return an array instead of within a wrapped object. \r\n\r\n```\r\n"aggs" : {\r\n    "my_percentiles": { \r\n        percentiles: [\r\n           { .. }, \r\n           { .. }\r\n       ]\r\n    }\r\n}\r\n```\r\n\r\nWhich makes the response very similar the non keyed range aggregation response:\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-range-aggregation.html#search-aggregations-bucket-range-aggregation\r\n'
5868,'clintongormley','[DOCS] Added vertx elasticsearch integration\n'
5865,'martijnvg','Return missing (404) if a scroll_id is cleared that no longer exists.\nPR for #5730'
5864,'brwe','Error handling on invalid mapping data\n```json\r\nDELETE /test1\r\nPUT /test1\r\nPUT /test1/test/_mapping\r\n{\r\n  "mappings":\r\n  {\r\n    "test":{\r\n      "properties":{\r\n        "prop1":{\r\n          "type": "string",\r\n          "index" : "not_analyzed"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\nGET /test1/test/_mapping\r\n```\r\n\r\nThe 2nd PUT request above is invalid but it is still accepted/acknowledged by the engine.  Would be nice to add some validation to the above use case to return an error.'
5863,'s1monw','Update forbidden-apis to 1.5.1 and remove the relaxed failOnMissingClasses setting, fix typo\n'
5857,'alexksikes','Added searching for multiple similar documents with More Like This Query...\n... DSL.\r\n\r\nThe documents are specfied by a list of ids. The index and type where each document is\r\nfetched from is assumed to be the first specified in the URL.\r\n\r\nRelates #4075'
5854,'martijnvg','Improve global ordinals on low-cardinality fields\nOn low-cardinality fields, it is very likely that the large segments are going to contain the same set of values as the whole index. This means that the segments ordinals are already global and that the `segmentOrdToGlobalOrdLookup` is going to be an identity map.\r\n\r\nWe could detect such situations, and directly expose the segment ordinals as global ordinals in order to remove one layer of abstraction.'
5853,'dakrone',"Change default for script.disable_dynamic\nPlease make setting script.disable_dynamic=true the default.\r\nThe current default: 'false' allows for nasty business via a _search api call. The docs already mention this, but does not mention the high level of nastiness. More details available. PM is probably better suited.\r\n\r\n"
5848,'jpountz','Unable to aggregate on _index\nWhen using the aggregations module I am unable to get aggregated doc counts per index.\r\n\r\nThe following query **DO NOT** return the aggregated result per index:\r\n```\r\n{\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "size": 0,\r\n  "aggs": {\r\n    "type": {\r\n      "terms": {\r\n        "field": "_index"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhen I use **Facets** instead of aggs, I get the desired result:\r\n```\r\n{\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "size": 0,\r\n  "facets": {\r\n    "type": {\r\n      "terms": {\r\n        "field": "_index"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIs this a known issue?\r\n'
5844,'jpountz',"Path-based routing doesn't work with doc values\nIf you use routing on a field that has doc values, you might hit a `ClassCastException`.\r\n\r\nThe reason is that `RoutingFieldMapper` does unchecked casts to the `oal.document.Field` class although for doc values we use classes that only implement `oal.document.IndexableField`."
5843,'martijnvg','The percolator needs to take deleted percolator documents into account.\nThis bug only occurs in non-realtime mode when query, filter, facet or aggs is used.\r\n\r\nPR for #5840'
5842,'seang-es',"Remove spaces from commented config lines in elasticsearch.yml and logging.yml\nIf a user has a mix of indented and unindented lines in elasticsearch.yml or logging.yml, elasticsearch will not parse the YML file correctly on startup.  Any indented lines after an unindented line will be treated as children of the unindented line, and all of those lines will not parse correctly.\r\n\r\nThe sample config comments in elasticsearch.yml and logging.yml include space characters after the '#' character at the beginning of the lines.  This encourages users to uncomment them by removing the '#' character without touching the space.  This can lead to trouble if users later add other config keys without including the space.  It would be better if the space characters were removed."
5840,'martijnvg','Duplicate ids in percolator results\nIf you index the same percolator query twice without a `refresh` in between, then it will show up twice in the percolator results.  Also, the percolator count API will return the wrong number of results (will return 2).\r\n\r\nHere is a test case showing the issue (uses the Elasticsearch Ruby gem):\r\nhttps://gist.github.com/cjbottaro/10920007'
5834,'jpountz','Randomize field data impls and global ordinals via random index template\nwe randomize these settings in dedicated tests but not via the random index template - we should do it for FD impls and eager and lazy global ords'
5828,'martijnvg','index corruption through delete_by_query of child documents\nwe experienced index corruption (indices having UNASSIGNED shards)\r\nduring incremental indexing.\r\n\r\nThe index contains one main document type and three other document\r\ntypes for child documents.\r\n\r\nThe corruption is related to delete by query requests for the child\r\ndocuments.\r\nWe had cases where the index was recovered on ES restart and cases \r\nwhere ES could not fix the index any more. This might be related to\r\nthe number of replicas (in cases where recovery worked we only had\r\none index copy on one instance) but we did not do an exhaustive \r\nanalysis on that.\r\n\r\nAfter we changed the delete by query requests into a client side\r\ndelete by query (search the child documents, send bulk requests for\r\ndelete by document id then) the issues stopped.\r\n\r\nElastic search version is 1.0.2, client operates through the ruby\r\nbindings using http (should not matter). OS is linux, the index\r\nhad 6 shards, ~ 12 mio documents (1.7 mio "main" documents,\r\nthe rest is child documents of three different types) in ~ 7.3GB.\r\nThe server has 16 GB memory, ES runs with 8 GB heap memory and \r\n65535 file descriptors.\r\n\r\nSorry, we could not try ES 1.1 because of an error regarding empty\r\ngeo points (seems to be fixed in master but not released).\r\n\r\nPart of the problem might be, that we are not too strict to ensure\r\nreferential integrity between parent and child documents.\r\nMy - perhaps naive - expectation was, that this should not matter. \r\nSo there might be child documents naming a parent, where the\r\nparent document does not exist.\r\n\r\nWhen we ran the incremental updates on a partial index containing\r\nonly a handful of documents (while the incremental stream was on\r\nchanges in all data) the issue did not show up.\r\nSo it\'s either related to the index size or (more probably (?)) to \r\nthe question if the delete by query finds something to delete or not.\r\n\r\nWe only had one process working on the updates strictly sequential.\r\nSo it should not be a race condition between serveral changes the same\r\ntime. There were parallel updates to other indices though.\r\n\r\nThe error itself is not too enlightening (at least for a pure\r\nes user): the indexer dies from a http timeout in an indexing request.\r\n\r\nThe ES log shows an error stating\r\nfailed to merge\r\nand\r\norg.apache.lucene.store.AlreadyClosedException: this IndexReader is closed\r\nsee full stack trace below.\r\nRaising ES log level to debug did not provide additional information,\r\nwhy the index reader was closed.\r\n\r\nI\'m afraid I cannot provide a full sample, how to reproduce the\r\nproblem.\r\n\r\nbest\r\n  Morus\r\n\r\nPS:\r\nThe initial error messages look like\r\n[WARN ][index.merge.scheduler    ] [pjpp-production mas\r\nter] [candidates_v0004][5] failed to merge\r\norg.apache.lucene.store.AlreadyClosedException: this IndexReader is closed\r\n        at org.apache.lucene.index.IndexReader.ensureOpen(IndexReader.java:252)\r\n        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.ja\r\nva:102)\r\n        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.ja\r\nva:56)\r\n        at org.apache.lucene.index.IndexReader.leaves(IndexReader.java:502)\r\n        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.cont\r\nains(DeleteByQueryWrappingFilter.java:122)\r\n        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.getD\r\nocIdSet(DeleteByQueryWrappingFilter.java:81)\r\n        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDoc\r\nIdSet(ApplyAcceptedDocsFilter.java:45)\r\n        at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(Con\r\nstantScoreQuery.java:142)\r\n        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.fil\r\nteredScorer(FilteredQuery.java:533)\r\n        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:13\r\n3)\r\n        at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFi\r\nlter.java:59)\r\n        at org.apache.lucene.index.BufferedUpdatesStream.applyQueryDeletes(Buffe\r\nredUpdatesStream.java:546)\r\n        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(\r\nBufferedUpdatesStream.java:284)\r\n        at org.apache.lucene.index.IndexWriter._mergeInit(IndexWriter.java:3844)\r\n        at org.apache.lucene.index.IndexWriter.mergeInit(IndexWriter.java:3806)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3659)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMe\r\nrgeScheduler.java:405)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(Trac\r\nkingConcurrentMergeScheduler.java:107)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(Conc\r\nurrentMergeScheduler.java:482)\r\n[WARN ][index.engine.internal    ] [pjpp-production mas\r\nter] [candidates_v0004][5] failed engine\r\norg.apache.lucene.index.MergePolicy$MergeException: org.apache.lucene.store.Alre\r\nadyClosedException: this IndexReader is closed\r\n        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvi\r\nder$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler\r\nProvider.java:109)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(Conc\r\nurrentMergeScheduler.java:518)\r\nCaused by: org.apache.lucene.store.AlreadyClosedException: this IndexReader is closed\r\n        at org.apache.lucene.index.IndexReader.ensureOpen(IndexReader.java:252)\r\n        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.java:102)\r\n        at org.apache.lucene.index.CompositeReader.getContext(CompositeReader.java:56)\r\n        at org.apache.lucene.index.IndexReader.leaves(IndexReader.java:502)\r\n        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.contains(DeleteByQueryWrappingFilter.java:122)\r\n        at org.elasticsearch.index.search.child.DeleteByQueryWrappingFilter.getDocIdSet(DeleteByQueryWrappingFilter.java:81)\r\n        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)\r\n        at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:142)\r\n        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:533)\r\n        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)\r\n        at org.apache.lucene.search.QueryWrapperFilter$1.iterator(QueryWrapperFilter.java:59)\r\n        at org.apache.lucene.index.BufferedUpdatesStream.applyQueryDeletes(BufferedUpdatesStream.java:546)\r\n        at org.apache.lucene.index.BufferedUpdatesStream.applyDeletesAndUpdates(BufferedUpdatesStream.java:284)\r\n        at org.apache.lucene.index.IndexWriter._mergeInit(IndexWriter.java:3844)\r\n        at org.apache.lucene.index.IndexWriter.mergeInit(IndexWriter.java:3806)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3659)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:107)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\r\n'
5827,'jpountz','Cardinality not working as expected\nI have a query like this.\r\n\r\n```ruby\r\n  aggregations: {\r\n                      by_month: {\r\n                          date_histogram: {\r\n                              field:    "time_stamp",\r\n                              interval: "1M",\r\n                              format:   "yyyy-MM-dd HH:mm"\r\n                          },\r\n                          aggregations:   {\r\n                              by_node_mac: {\r\n                                  terms:        {\r\n                                      field: "node_mac"\r\n                                  },\r\n                                  aggregations: {\r\n                                      cardinality: {field: \'device_mac\'}\r\n                                  }\r\n\r\n                              }\r\n                          }\r\n                      }\r\n                  }\r\n}\r\n\r\n```\r\nbut I seem to be getting the wrong answers.  I am using fake data which should give me very low numbers for the cardinality but it actually seems to be counting the number of rows not the number of distinct items. The numbers are outrageously high.\r\n\r\nI tried a precision threshold of 1000 and 100 but it seems to make no difference.'
5821,'jpountz','Instantiate facets/aggregations during the QUERY phase.\nIn case of a DFS_QUERY_THEN_FETCH request, facets and aggregations are currently\r\ninstantiated during the DFS phase while they only become useful during the QUERY\r\nphase. By instantiating during the QUERY phase instead, we can make better use\r\nof recycling since objects will have a shorter life out of the recyclers.'
5807,'rmuir',"Release should fail if test are annotated with AwaitsFix\ntoday we don't check `AwaitsFix` if we do a `mvn deploy`. We should maybe extend forbidden APIs to do that and fail hard if we try to release with `AwaitsFix`"
5802,'s1monw','Upgrade to Lucene 4.7.2\nLucene 4.7.2 is available on mvn central \r\n\r\nlets get it in!'
5801,'s1monw','Check for no open issues before build release\nthis commit adds a basic check in the release script that checks if there are any open issues on github for the release we are trying to build.'
5800,'s1monw','Ensure close is called under lock in the case of an engine failure\nUntil today we did close the engine without aqcuireing the write lock\r\nsince most calls were still holding a read lock. This commit removes\r\nthe code that holds on to the readlock when failing the engine which\r\nmeans we can simply call #close()'
5793,'imotov','Add Partial snapshot state\nCurrently even if some shards of the snapshot are not snapshotted successfully, the snapshot is still marked as "SUCCESS". Users may miss the fact the there are shard failures present in the snapshot and think that snapshot was completed. This change adds a new snapshot state "PARTIAL" that provides a quick indication that the snapshot was only partially successful.\r\n\r\nCloses #5792'
5792,'imotov','Snapshot/Restore: Add "PARTIAL" snapshot status\nCurrently even if some shards of the snapshot are not snapshotted successfully, the snapshot is still marked as "SUCCESS". Users may miss the fact the there are shard failures present in the snapshot and think that snapshot was completed successfully (#5657 and #5742). A new "PARTIAL" snapshot status should be added to provides a quick indication that the snapshot was only partially successful. '
5790,'imotov','Snapshot Status failing without repository\nThe snapshot status request \r\n```\r\n curl -XGET "localhost:9200/_snapshot/_status"\r\n```\r\nreturns an error\r\n```\r\n{"error":"ActionRequestValidationException[Validation Failed: 1: repository is missing;]","status":500}\r\n```\r\nIt should return all a list of currently running snapshots in all repositories instead.\r\n'
5789,'jpountz','Common fields aggregation / bucketing\nAggregations are great! It would be even more cool to have an aggregation that buckets by field occurrence (based on field name) from a given path, allowing us to apply sub-aggs on these *dynamically found* fields.\r\n\r\nSay we have the following product index:\r\n```\r\ncurl -XPOST \'localhost:9200/store/product/1\' -d \'{"name":"Galaxy S4","manufacturer":"Samsung","type":"phone","specs":{"os":"Android","core":4,"ram":"2GB"}}\'\r\ncurl -XPOST \'localhost:9200/store/product/2\' -d \'{"name":"iPhone 5","manufacturer":"Apple","type":"phone","specs":{"os":"iOS","core":2,"ram":"1GB"}}\'\r\ncurl -XPOST \'localhost:9200/store/product/3\' -d \'{"name":"WF210ANW/XAA","manufacturer":"Samsung","type":"washing-machine","specs":{"capacity":3.5,"loadType":"front","presets":6}}\'\r\ncurl -XPOST \'localhost:9200/store/product/4\' -d \'{"name":"WT4801CW","manufacturer":"LG","type":"washing-machine","specs":{"capacity":3.7,"loadType":"top","presets":9}}\'\r\n```\r\n\r\nHere we have two different nature of products (phone and washing machine), they have consequently different features (the K/V pairs in the `specs` field).\r\nIf such an aggregation was available, we could apply it to retrieve the top *most frequent* fields for a resulting data set and compute any kind of sub-agg over them. Combined with well-minded query/filter this could end to a nice magic agg effect :)\r\n\r\nI tried to build my own aggs tree using [terms aggregation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html), but I did not found any way to access the parent bucket key from a child aggregation. I only succeed to achieve the first part, ie. find the top-n field names sorted by occurence count. Anyway this seems not viable as it looks like a 2-step process: doing it this way, we need to evaluate the number of occurrences of each fields first and then compute sub-aggs on the selected ones.\r\n\r\n```\r\n... query and filters ...\r\n"aggs" : {\r\n  "dynamic_agg_step1": {\r\n    "terms": { \r\n      "script": "_source.fields.keySet()",\r\n      "exclude": "type|subtype"\r\n    }\r\n    "aggs": {\r\n      "dynamic_agg_step2": {\r\n        "terms": { \r\n          "script": "_source.fields.get(_myMagicBucketKey?_)"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nA simple (and probably naive) approach would be to compute declared sub-aggs on all fields seen from a given path (here the `specs` path) while keeping track of field occurrences and finally only retain agg trees from the *most frequent* top-n fields. This represents extra workload and more memory to hold aggs for all fields but if the field cardinality remains reasonable, it mights worth it. We could also imagine to exclude some fields that we know we\'re not interested in (as we can currently [do](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-terms-aggregation.html#_filtering_values) with the terms agg). Obviously, the considered path necessarily have to point to a JSON hash to run correctly.\r\n\r\nTell me if that makes sense to you?\r\nMaybe I missed something or the feature already exists, and if so please tell me :)\r\n\r\nNicolas'
5788,'spinscale','Updated date_formats in dynamic_mapping docs to new dynamic_date_formats\n'
5784,'spinscale','Added Twitter Storehaus client\nAdded Twitter Storehaus client'
5783,'dakrone','HasChild query picks wrong type\nGiven the following setup:\r\n```\r\ncurl -XPUT \'http://localhost:9200/haschildtest/\'\r\n\r\ncurl -XPUT \'localhost:9200/haschildtest/posts/_mapping\' -d \'\r\n{\r\n   "posts":{\r\n      "_parent":{\r\n         "type":"features"\r\n      },\r\n      "_routing":{\r\n         "required":true\r\n      }\r\n   }\r\n}\'\r\n\r\ncurl -XPUT \'localhost:9200/haschildtest/features/feature1\' -d \'\r\n{\r\n   "title": "feature title 1"\r\n}\'\r\n\r\ncurl -XPUT \'localhost:9200/haschildtest/posts/post1?parent=feature1\' -d \'\r\n{\r\n   "specials":{\r\n      "title": "jack"\r\n   }\r\n}\'\r\n\r\ncurl -XPUT \'localhost:9200/haschildtest/specials/special1\' -d \'\r\n{\r\n   "title": "this somehow interferes with the has_child query"\r\n}\'\r\n```\r\nThis query runs correctly:\r\n```\r\ncurl -XPOST \'localhost:9200/haschildtest/features/_search?pretty=true\' -d \'\r\n{\r\n   "query": {\r\n      "has_child": {\r\n         "type": "posts",\r\n         "query": {\r\n            "match": {\r\n               "specials.title": "jack"\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\'\r\n```\r\nWhereas this query with the parent type specifed after the query, fails:\r\n```\r\ncurl -XPOST \'localhost:9200/haschildtest/features/_search?pretty=true\' -d \'\r\n{\r\n   "query": {\r\n      "has_child": {\r\n         "query": {\r\n            "match": {\r\n               "specials.title": "jack"\r\n            }\r\n         },\r\n         "type": "posts"\r\n      }\r\n   }\r\n}\'\r\n```\r\nSomehow the second example is picking the `specials` type `title` field instead of the type specified in the `has_child` block. I have tested this on ES 1.0.2 and took me a while to figure out what was going on...'
5782,'javanna','Fix detection of unsupported fields with validate API\nThe validate API was failing to reject JSON input that had unsupported fields placed after a supported field. This was causing invalid requests to be reported as valid.\r\n\r\nFirst commit adds a test that triggers the issue. Second commit fixes it.\r\n\r\nRef. #5685'
5778,'costin','_cat plugin endpoint throws an exception\nA left-over header causes the _cat/plugin endpoint to throw : \r\n```\r\ncurl localhost:9200/_cat/plugins\r\n{"error":"ElasticsearchIllegalStateException[mismatch on num\r\n}\r\n```'
5777,'spinscale','Update core-types.asciidoc\nMissing bracket'
5773,'colings86','Geo: Valid complex polygons fail to parse\nposting certain valid geojson polygons results in the following exception:\r\n\r\norg.elasticsearch.index.mapper.MapperParsingException: failed to parse [geometry] at org.elasticsearch.index.mapper.geo.GeoShapeFieldMapper.parse(GeoShapeFieldMapper.java:249)\r\n...\r\n\r\ncurl -XDELETE \'http://localhost:9200/test\'\r\n\r\ncurl -XPOST \'http://localhost:9200/test\' -d \'{\r\n  "mappings":{\r\n    "test":{\r\n      "properties":{\r\n        "geometry":{\r\n          "type":"geo_shape",\r\n          "tree":"quadtree",\r\n          "tree_levels":14,\r\n          "distance_error_pct":0.0\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XPOST \'http://localhost:9200/test/test/1\' -d \'{\r\n  "geometry":{\r\n    "type":"Polygon",\r\n    "coordinates":[\r\n      [[-85.0018514,37.1311314],\r\n       [-85.0016645,37.1315293],\r\n       [-85.0016246,37.1317069],\r\n       [-85.0016526,37.1318183],\r\n       [-85.0017119,37.1319196],\r\n       [-85.0019371,37.1321182],\r\n       [-85.0019972,37.1322115],\r\n       [-85.0019942,37.1323234],\r\n       [-85.0019543,37.1324336],\r\n       [-85.001906,37.1324985],\r\n       [-85.001834,37.1325497],\r\n       [-85.0016965,37.1325907],\r\n       [-85.0016011,37.1325873],\r\n       [-85.0014816,37.1325353],\r\n       [-85.0011755,37.1323509],\r\n       [-85.000955,37.1322802],\r\n       [-85.0006241,37.1322529],\r\n       [-85.0000002,37.1322307],\r\n       [-84.9994,37.1323001],\r\n       [-84.999109,37.1322864],\r\n       [-84.998934,37.1322415],\r\n       [-84.9988639,37.1321888],\r\n       [-84.9987841,37.1320944],\r\n       [-84.9987208,37.131954],\r\n       [-84.998736,37.1316611],\r\n       [-84.9988091,37.131334],\r\n       [-84.9989283,37.1311337],\r\n       [-84.9991943,37.1309198],\r\n       [-84.9993573,37.1308459],\r\n       [-84.9995888,37.1307924],\r\n       [-84.9998746,37.130806],\r\n       [-85.0000002,37.1308358],\r\n       [-85.0004984,37.1310658],\r\n       [-85.0008008,37.1311625],\r\n       [-85.0009461,37.1311684],\r\n       [-85.0011373,37.1311515],\r\n       [-85.0016455,37.1310491],\r\n       [-85.0018514,37.1311314]],\r\n      [[-85.0000002,37.1317672],\r\n       [-85.0001983,37.1317538],\r\n       [-85.0003378,37.1317582],\r\n       [-85.0004697,37.131792],\r\n       [-85.0008048,37.1319439],\r\n       [-85.0009342,37.1319838],\r\n       [-85.0010184,37.1319463],\r\n       [-85.0010618,37.13184],\r\n       [-85.0010057,37.1315102],\r\n       [-85.000977,37.1314403],\r\n       [-85.0009182,37.1313793],\r\n       [-85.0005366,37.1312209],\r\n       [-85.000224,37.1311466],\r\n       [-85.000087,37.1311356],\r\n       [-85.0000002,37.1311433],\r\n       [-84.9995021,37.1312336],\r\n       [-84.9993308,37.1312859],\r\n       [-84.9992567,37.1313252],\r\n       [-84.9991868,37.1314277],\r\n       [-84.9991593,37.1315381],\r\n       [-84.9991841,37.1316527],\r\n       [-84.9992329,37.1317117],\r\n       [-84.9993527,37.1317788],\r\n       [-84.9994931,37.1318061],\r\n       [-84.9996815,37.1317979],\r\n       [-85.0000002,37.1317672]]]\r\n  }\r\n}\'\r\n\r\n\r\nExpected:\r\n  {"ok":true,"_index":"test","_type":"test","_id":"1","_version":1}\r\nActual:\r\n  {"error":"MapperParsingException[failed to parse [geometry]]; nested: ArrayIndexOutOfBoundsException[-1]; ","status":400}\r\n\r\nThis is an issue with es-1.1.0.  The same requests execute successfully against es-0.2.4.\r\n\r\nIt is possible to view and validate the data in qgis.\r\n\r\n![screen shot 2014-04-10 at 5 01 56 pm](https://cloud.githubusercontent.com/assets/6935249/2675061/cc6cfc7a-c10d-11e3-9829-7c80f8075fe8.png)\r\n \r\n'
5772,'brwe','Improve handling of store parameter updates post indexing\n```json\r\nDELETE /myindex\r\nPOST /myindex/mytype/1\r\n{\r\n  "prop1":"test1"\r\n}\r\nGET /myindex/_search\r\n{\r\n  "fields": [\r\n    "_timestamp"\r\n  ], \r\n  "query": {\r\n    "match_all": {}\r\n  }\r\n}\r\nPUT /myindex/mytype/_mapping\r\n{\r\n\r\n  "mytype": {\r\n    "_timestamp": {\r\n    "enabled": "true",\r\n    "store": "true"\r\n    }\r\n  }\r\n}\r\nGET /myindex/mytype/_mapping\r\n```\r\n\r\nIf you create an index, put a doc into it, and then try to update the mapping to enable _timestamp and set store:true, you will notice that the response comes back as acknowledged:true with no error messages.  But if you get the _mapping back, you will notice that store:true is not set (by design because we do not allow changing the store parameter post indexing).  \r\n\r\nFor regular properties, if you attempt to update its store parameter post indexing, it will actually throw a 400 error indicating a MergeMappingException because of differences in store values.  \r\n\r\nIt will be nice to add the same validation and throw a similar error when users attempt to update the store value of _timestamp post-indexing.'
5769,'markharwood','NPE during significant_terms search\nI got this exception in elasticsearch 1.1:\r\n\r\n    [2014-04-10 12:31:16,678][DEBUG][action.search.type       ] [Lunatica] [ebooktracker][0], node[29zEP2OsR_6Qd-BaUeWKGA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3137ad6b]\r\n    java.lang.NullPointerException\r\n            at org.elasticsearch.search.aggregations.bucket.significant.SignificantTermsAggregatorFactory.getBackgroundFrequency(SignificantTermsAggregatorFactory.java:190)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator.buildAggregation(SignificantStringTermsAggregator.java:87)\r\n            at org.elasticsearch.search.aggregations.bucket.significant.SignificantStringTermsAggregator$WithOrdinals.buildAggregation(SignificantStringTermsAggregator.java:129)\r\n            at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:135)\r\n            at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:136)\r\n            at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:330)\r\n            at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:304)\r\n            at org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:71)\r\n            at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n            at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n            at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n            at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n            at java.lang.Thread.run(Thread.java:744)\r\n            \r\nWhile doing this search:\r\n\r\n    POST /ebooktracker/tweet/_search\r\n    {\r\n        "fields": ["text"],\r\n        "query" : {\r\n            "terms" : {"text": ["kindle"]}\r\n        },\r\n        "aggregations" : {\r\n            "tags" : {\r\n                "significant_terms" : { "field" : "hashtags" }\r\n            }\r\n        }\r\n    }\r\n'
5767,'jpountz',"[Docs]  Add experimental highlighter plugin\nAdd documentation for highlighter plugin I'm working on.  Works reasonably well but still rough around the edges."
5765,'spinscale','Plugins: Properly quote $JAVA in bin/plugin\nThe all new Oracle Java 7 on OSX has\r\nJAVA_HOME=/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home\r\nNote the happy space in "Internet Plug-Ins" - it gives plugin\r\nmajor agida...'
5763,'jpountz','Update percolate.asciidoc\n'
5755,'aleph-zero','Benchmark: Fix for hung clients on cluster without benchmark nodes\nThis is a fix for a bug whereby a cluster that has no nodes started with\r\n-Des.node.bench=true will cause clients to hang if they attempt to\r\nsubmit a benchmark.\r\n\r\nCloses #5754'
5750,'martijnvg','Percolation does not seem to work fully on dynamically templated fields\nHi all,\r\n\r\n  it looks like when using dynamic_templates with percolation, new fields introduced in the percolated document do not get properly picked up. Here is the test scenario:\r\n\r\nCreate an index with a type with all `custom.*` fields not analysed:\r\n```\r\nPUT /myindex\r\nPUT /myindex/mytype/_mapping\r\n{\r\n    "mytype": {\r\n        "dynamic" : false,\r\n        "properties" : {\r\n            "custom": {\r\n                "dynamic": true, "type": "object", "include_in_all": false\r\n            }\r\n        },\r\n        "dynamic_templates": [ {\r\n            "custom_fields": {\r\n                "path_match": "custom.*",\r\n                "mapping": {\r\n                    "index": "not_analyzed"\r\n                }\r\n            }\r\n        }]\r\n    }\r\n}\r\n```\r\n\r\nThen register two queries one matching docs with `color:red` and the other `color:blue`\r\n```\r\nPUT /myindex/.percolator/redperco\r\n{\r\n    "query": {\r\n        "query_string": {\r\n           "query": "color:red"\r\n        }\r\n    }\r\n}\r\n\r\nPUT /myindex/.percolator/blueperco\r\n{\r\n    "query": {\r\n        "query_string": {\r\n           "query": "color:blue"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThen percolate. Note that we did not insert any document in the index. It will be the first time it sees the `custom.color` field.\r\n```\r\nPOST /myindex/mytype/_percolate\r\n{\r\n    "doc" : {\r\n        "custom": {\r\n            "color": "blue"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nWe get no match when it should find one.\r\n\r\nInteresting notes:\r\n- if you use the fully qualified field name `custom.color` in the queries it works\r\n- if you re-put the mapping and re-register the queries it works\r\n- if you index the same document before registering the queries it works\r\n\r\nTests run on OSX 10.9.2.\r\nElasticsearch Info:\r\n```\r\n{\r\n   "status": 200,\r\n   "name": "Silver Fox",\r\n   "version": {\r\n      "number": "1.1.0",\r\n      "build_hash": "2181e113dea80b4a9e31e58e9686658a2d46e363",\r\n      "build_timestamp": "2014-03-25T15:59:51Z",\r\n      "build_snapshot": false,\r\n      "lucene_version": "4.7"\r\n   },\r\n   "tagline": "You Know, for Search"\r\n}\r\n```\r\n\r\nThanks,\r\nEmmanuel'
5743,'aleph-zero','Empty HTTP body returned from _recovery API on empty cluster\nWhen the `_recovery` API is executed against a cluster with no indices, an empty HTTP body is returned, causing errors in clients expecting a valid JSON response:\r\n\r\n```\r\n$ elasticsearch --cluster.name=recovery_response_error --http.port=9280\r\n# ...\r\n$ curl -i localhost:9280/_recovery\r\nHTTP/1.1 200 OK\r\nContent-Type: application/json; charset=UTF-8\r\nContent-Length: 0\r\n```\r\n\r\nAn empty JSON object (`{}`) is expected here.'
5742,'imotov','Snapshot/Restore: Add ability to restore partial snapshots\nIs there a way to copy a single index from a snapshot marked as "incomplete" by the restore function to the data dir of a single node elastic cluster in order to restore that index data by reopening it as a closed index?\r\n\r\neg, to restore a single index from a multi-index snapshot (an index marked as incompleted).\r\n\r\nfrom whatever source location with a dir structure like: \r\n```\r\nsnapshotname/indices/myindexname/*\r\n```\r\n\r\nto: \r\n```\r\n/var/lib/elasticsearch/mycluster/nodes/0/indices/myindexname/*\r\n```\r\nwill elasticsearch see this as a closed index and provide the ability to online it again?\r\n\r\nFailing that, is there any other possible way to salvage data from a snapshotted index that is marked as incomplete, especially if the size of the index appears to be correct?'
5737,'dadoonet','ArrayIndexOutOfBoundsException\nmapping setting:\r\nclean_body: {\r\ntype: "string",\r\nstore: true,\r\nanalyzer: "smartcn"\r\n}\r\nWhen I post some data, I will got error below sometime.\r\n{"error":"ArrayIndexOutOfBoundsException[null]","status":500}\r\nBut when I delete the mapping and post the same data, there will be no any error.'
5733,'clintongormley','Update "Character classes" part\nFew changes in "Character classes" part. The same thing was written two times.'
5730,'martijnvg','clear scroll throws 500 on array out of bounds exception\n```\r\nDELETE http://127.0.0.1:9200/_search/scroll/asdasdadasdasd HTTP/1.1\r\n```\r\n\r\nReturns the following response\r\n\r\n```\r\nHTTP/1.1 500 Internal Server Error\r\nContent-Type: application/json; charset=UTF-8\r\nContent-Length: 73\r\n\r\n{\r\n  "error" : "ArrayIndexOutOfBoundsException[1]",\r\n  "status" : 500\r\n}\r\n```\r\n\r\nWhile a 404 is expected. \r\n\r\nIt would also be nice if we can allow the scroll id to be posted. I\'ve had people hit problems with scroll ids that are too big in the past:\r\n\r\nhttps://github.com/elasticsearch/elasticsearch-net/issues/318'
5729,'spinscale','Missing scroll ID no longer returns exception\nAs of commit 705c7e2469546fcb241f119570265a76262eac75 running a scroll request on a bad scroll ID no longer returns a 500 request error.  Instead, each shard returns a failure but the overall request is a 200 OK.\r\n\r\n/cc @kimchy '
5727,'dadoonet','README.textile Searching\nThe README contains this excerpt:\r\n\r\nWe can also use the JSON query language Elasticsearch provides instead of a query string:\r\n\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/_search?pretty=true\' -d \'\r\n{ \r\n    "query" : { \r\n        "text" : { "user": "kimchy" }\r\n    } \r\n}\'\r\n\r\nBut when I try to run it I get this error:\r\n\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[mp8It5yKTLSkvuIfiAX0pw][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{ \\n    \\"query\\" : { \\n        \\"text\\" : { \\"user\\": \\"kimchy\\" }\\n    } \\n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][4]: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{ \\n    \\"query\\" : { \\n        \\"text\\" : { \\"user\\": \\"kimchy\\" }\\n    } \\n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{ \\n    \\"query\\" : { \\n        \\"text\\" : { \\"user\\": \\"kimchy\\" }\\n    } \\n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{ \\n    \\"query\\" : { \\n        \\"text\\" : { \\"user\\": \\"kimchy\\" }\\n    } \\n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }{[mp8It5yKTLSkvuIfiAX0pw][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{ \\n    \\"query\\" : { \\n        \\"text\\" : { \\"user\\": \\"kimchy\\" }\\n    } \\n}]]]; nested: QueryParsingException[[twitter] No query registered for [text]]; }]",\r\n  "status" : 400\r\n}\r\n\r\n changed "text" in the query to "term" and it worked. When I looked up the query DSL there was no keyword "text" either.'
5726,'martijnvg','Clear scroll should accept scroll_id in body\nIn https://github.com/elasticsearch/elasticsearch-perl/issues/24 a user was generating scroll IDs which were too long for the intervening proxy to handle in the URL or query string. The same problem would apply to the clear-scroll API.\r\n\r\nThe clear-scroll API should optionally accept the scroll_id in the body as well as in the URL'
5724,'dakrone','Provide more context variables in update scripts\nIn addition to `_source`, the following variables are available through the `ctx` map: `_index`, `_type`, `_id`,  `_version`, `_routing`, `_parent`, `_timestamp`, `_ttl`.\r\n\r\nSome of these fields are more useful still within the context of an Update By Query, see #1607, #2230, #2231.'
5718,'dakrone',"Include name of field that caused circuit breaking exception\nWe could include the name of the field that is potentially blowing out memory with the exception so it's easier for people to tell which field is using the memory.\r\n\r\nThis could still be a bit misleading when loading more than one field at a time, but may be more useful than not including any field name."
5715,'dadoonet','ElasticsearchIllegalStateException when invoking _cat plugins\nReproducible using ES 1.1.0.  \r\n/_cat/plugins works fine when no plugins are installed.  But once a plugin is installed (eg. latest marvel), it throws an exception indicating that the resulting table constructed has 7 fields in the header row but not enough fields in the data rows to make it a valid table.\r\n\r\n{\r\n  "status" : 500,\r\n  "error" : "ElasticsearchIllegalStateException[mismatch on number of cells 6 in a row compared to header 7]"\r\n}\r\n\r\n'
5711,'aleph-zero','Rename readPrimitive*Array()/writePrimitive*Array() methods\nRename methods to have shorter names.'
5705,'imotov','After restart elasticsearch reads double as array of string\nHello, first of all I want to say you making crazy useful things, guys, thank you!\r\n\r\nI have two elasticsearch twin (versions, configuration, everything) servers with the same data, replicated by logstash: ES1 and ES2. ES1 I restarted for a few hours ago and now I have a problems.\r\n\r\nMy mapping for logstash type, same for both servers.\r\n```\r\n"nginx_accesslog": {\r\n        "dynamic_templates": [\r\n                {\r\n                        "string_template": {\r\n                                "mapping": { "index": "not_analyzed", "type": "string" },\r\n                                "match": "*",\r\n                                "match_mapping_type": "string"\r\n                        }\r\n                }\r\n        ],\r\n        "_all": { "enabled": false },\r\n        "_source": { "compress": true },\r\n        "properties": { \r\n                "@timestamp": { "type": "date", "format": "dateOptionalTime" },\r\n                "@version": { "type": "long" },\r\n                "api_key": { "type": "string", "index": "not_analyzed" },\r\n                "body_bytes_sent": { "type": "long" },\r\n                "host": { "type": "string", "index": "not_analyzed" },\r\n                "http_host": { "type": "string", "index": "not_analyzed" },\r\n                "http_method": { "type": "string", "index": "not_analyzed" },\r\n                "http_referer": { "type": "string", "index": "not_analyzed" },\r\n                "http_user_agent": { "type": "string", "index": "not_analyzed" },\r\n                "http_version": { "type": "string", "index": "not_analyzed" },\r\n                "http_x_forwarded_for": { "type": "string", "index": "not_analyzed" },\r\n                "message": { "type": "string", "index": "not_analyzed" },\r\n                "path": { "type": "string", "index": "not_analyzed" },\r\n                "remote_addr": { "type": "string", "index": "not_analyzed" },\r\n                "remote_user": { "type": "string", "index": "not_analyzed" },\r\n                "request": { "type": "string", "index": "not_analyzed" },\r\n                "request_time": { "type": "double" },\r\n                "status": { "type": "long" },\r\n                "tags": { "type": "string", "index": "not_analyzed" },\r\n                "type": { "type": "string", "index": "not_analyzed" }\r\n        }\r\n}\r\n```\r\n\r\nQuery (I\'m using Sense add-on)\r\n```\r\nPOST /logstash-2014.04.07/_search\r\n{\r\n    "script_fields": {\r\n       "s_request_time": {\r\n          "script": "doc[\'request_time\'].value"\r\n       }\r\n    },\r\n    "size": 20\r\n}\r\n```\r\n\r\nES2, everything is normal:\r\n```\r\n{\r\n   "took": 63,\r\n   "timed_out": false,\r\n   "_shards": {\r\n      "total": 4,\r\n      "successful": 4,\r\n      "failed": 0\r\n   },\r\n   "hits": {\r\n      "total": 17041240,\r\n      "max_score": 1,\r\n      "hits": [\r\n         {\r\n            "_index": "logstash-2014.04.07",\r\n            "_type": "nginx_accesslog",\r\n            "_id": "LSfAaBwSSDS5rL6utSHQJA",\r\n            "_score": 1,\r\n            "fields": {\r\n               "s_request_time": 0.014\r\n            }\r\n         },\r\n...\r\n```\r\n\r\nES1: Ooops!\r\n```\r\n{\r\n   "took": 51,\r\n   "timed_out": false,\r\n   "_shards": {\r\n      "total": 4,\r\n      "successful": 4,\r\n      "failed": 0\r\n   },\r\n   "hits": {\r\n      "total": 17041131,\r\n      "max_score": 1,\r\n      "hits": [\r\n         {\r\n            "_index": "logstash-2014.04.07",\r\n            "_type": "nginx_accesslog",\r\n            "_id": "tjZo1_JmRpOu5kE2WRfURw",\r\n            "_score": 1,\r\n            "fields": {\r\n               "s_request_time": [\r\n                  " \\u0001?PZ\\u000e+\\u0001\\u0003\\t\\u001c" <-- WAT?!\r\n               ]\r\n            }\r\n         },\r\n```\r\n\r\nIt\'s most obvious demonstration of my problem. Another thing is when I\'m using data_histogram facet (in kibana) I got ClassCastException[org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData.\r\n\r\nPurging all indexes fixes problem. Not only new, but old data too misenterpreted as arrays of one string.\r\nWell, I want my doubles back. :)'
5704,'javanna','[DOCS] Update URI of mapping\nThe method `PUT /{index}/{type}/_mapping` seems to be out of date.\r\n("\x01\x01\x01still supported for backwardscompatibility.")\r\n\r\nSubstituted them with `PUT /{index}/_mapping/{type}`.'
5701,'javanna','add lucene LMSimilarity\ncloses #5697'
5698,'kimchy',"Optimizing with wait_for_merge=false is no longer asynchronous\nAfter the upgrade to 1.1.0, the _optimize command doesn't seem to acknowledge wait_for_merge=false as being an asynchronous operation, and blocks until it times out."
5697,'javanna',"Lucene LMSimilarity seems to be missing in Elasticsearch\nNowadays Lucene has a configurable similarity module (actually a set of similarity modules) to compute a ranking over (query, document) pairs.\r\n\r\nThe list of similarities which are available in Lucene 4.7.1 (and have been since 4.0.0 as far as i can tell) is:\r\n- DefaultSimilarity\r\n- BM25Similarity\r\n- DFR\r\n- Information based models\r\n- language models\r\n\r\n(according to the documentation at: http://lucene.apache.org/core/4_7_1/core/org/apache/lucene/search/similarities/package-summary.html#sims)\r\n\r\n\r\nElasticSearch makes these Lucene similarities available via its similarity module. The list of similarities that ElasticSearch 1.0/1.1 supports is:\r\n- default similarity\r\n- bm25 similarity\r\n- dfr similarity\r\n- ib similarity\r\n\r\n(this list according to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-similarity.html#_available_similarities)\r\n\r\nSo the language modeling approach seems to be missing from ElasticSearch (or maybe they are available, but they're undocumented.)\r\n"
5692,'jpountz','Update LongHash to work like BytesRefHash\n'
5685,'javanna','Query validation doesn\'t detect extra JSON properties after the query property\n```\r\n$ curl -s \'http://localhost:9200/\'|grep number\r\n    "number" : "1.1.0",\r\n```\r\n\r\nWhen validating a query, adding an extra property **after** the ``query`` does not trigger a validation error:\r\n\r\n```shell\r\n$ curl -XPOST \'http://localhost:9200/test/_validate/query?pretty\' -d \'\r\n{"query": {"term" : { "user" : "kimchy" }}, "foo": "bar"}\r\n\'\r\n```\r\n```json\r\n{\r\n  "valid" : true,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  }\r\n}\r\n```\r\n\r\nIf the extra property is placed **before** the ``query`` property, a validation error is reported.\r\n\r\n```shell\r\n$ curl -XPOST \'http://localhost:9200/test/_validate/query?pretty\' -d \'\r\n{"foo": "bar", "query": {"term" : { "user" : "kimchy" }}}\r\n\'\r\n```\r\n\r\n```json\r\n{\r\n  "valid" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  }\r\n}\r\n```\r\nWhen doing an actual search however, both queries fail:\r\n\r\n```shell\r\n$ curl \'http://localhost:9200/test/_search?pretty\' -d \'\r\n{"foo": "bar", "query": {"term" : { "user" : "kimchy" }}}\r\n\'\r\n```\r\n```json\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[FsbLm57PSNeCm5gRjO-uLA][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"foo\\": \\"bar\\", \\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}}\\n]]]; nested: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"foo\\": \\"bar\\", \\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}}\\n]]]; nested: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"foo\\": \\"bar\\", \\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}}\\n]]]; nested: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"foo\\": \\"bar\\", \\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}}\\n]]]; nested: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"foo\\": \\"bar\\", \\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}}\\n]]]; nested: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }]",\r\n  "status" : 400\r\n}\r\n```\r\n```shell\r\n$ curl \'http://localhost:9200/test/_search?pretty\' -d \'\r\n{"query": {"term" : { "user" : "kimchy" }}, "foo": "bar"}\r\n\'\r\n```\r\n```json\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[FsbLm57PSNeCm5gRjO-uLA][test][4]: SearchParseException[[test][4]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}, \\"foo\\": \\"bar\\"}\\n]]]; nested: SearchParseException[[test][4]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][3]: SearchParseException[[test][3]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}, \\"foo\\": \\"bar\\"}\\n]]]; nested: SearchParseException[[test][3]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][2]: SearchParseException[[test][2]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}, \\"foo\\": \\"bar\\"}\\n]]]; nested: SearchParseException[[test][2]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][0]: SearchParseException[[test][0]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}, \\"foo\\": \\"bar\\"}\\n]]]; nested: SearchParseException[[test][0]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }{[FsbLm57PSNeCm5gRjO-uLA][test][1]: SearchParseException[[test][1]: query[user:kimchy],from[-1],size[-1]: Parse Failure [Failed to parse source [\\n{\\"query\\": {\\"term\\" : { \\"user\\" : \\"kimchy\\" }}, \\"foo\\": \\"bar\\"}\\n]]]; nested: SearchParseException[[test][1]: query[user:kimchy],from[-1],size[-1]: Parse Failure [No parser for element [foo]]]; }]",\r\n  "status" : 400\r\n}\r\n```'
5680,'dadoonet',"geo_point doesn't allow null values as of 1.1.0\nAfter upgrading to 1.1.0, I found that my backend processes that perform bulk inserts/updates to ElasticSearch started failing with the following error:\r\n\r\nMapperParsingException[failed to parse]; nested: ElasticsearchParseException[geo_point expected]; \r\n\r\nIt seems that geo_point fields now require a non-null value for every document? Is there a way to bypass this behavior without having to change my ETL process to create fake geo_point coordinates?"
5678,'javanna','minor typo\n'
5676,'javanna','Adding javadoc to UpdateRequestBuilder for a couple of details \nCloses #4904 (this PR is nothing more than a squashed single-commit version of #4904).'
5673,'clintongormley','[DOCS] Updated ruby clients\nmarked (re)tire as retired and added searchkick'
5671,'javanna','[DOCS] fix incorrect field data statistics doc\n'
5669,'jpountz','add doc value for binary field\nadd doc values support for binary field\r\n\r\nin my image plugin, the score function need to get the binary field for all documents, using doc values will be much faster than using store field\r\n'
5667,'s1monw','NPE in PagedBytesReference\nI rebased #3278 to latest master and one of my benchmarks is throwing a NPE.  Traced it down to ref.bytes being null for the array copy.  See [PagedBytesReference.java#L448](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/bytes/PagedBytesReference.java#L448).\r\n\r\nNot sure why, but this is the chunk of code that triggers this in my PR is:\r\n\r\n[TermsFilterParser.java#L135](https://github.com/mattweber/elasticsearch/blob/terms_lookup_by_query/src/main/java/org/elasticsearch/index/query/TermsFilterParser.java#L135)\r\n```\r\n if ("filter".equals(currentFieldName)) {\r\n    lookupFilter = XContentFactory.contentBuilder(parser.contentType());\r\n    lookupFilter.copyCurrentStructure(parser);\r\n}\r\n```\r\n\r\nI can push up the rebased PR that is failing if you need it to test.  Let me know.\r\n\r\n/cc @s1monw @hhoffstaette \r\n\r\n'
5663,'bleskes','A Get request with a version set should always validate for equality\nCurrently we use the same version checking logic for get and indexing requests. The leads to the wrong semantics for the `EXTERNAL` and the `EXTERNAL_TYPE` versioning systems.  For example, doing `GET index/type/1?version=10&version_type=external` will return the document if it has a version of 9 or less. \r\n\r\nWe should make it return documents only if their version match the specified version.'
5662,'bleskes','Indexing a document fails when setting `version=0` &  `version_type=external`\nThis is a regression introduced in 4e0e40644d53c701c2a7525dda7179829ac75d6c , which was release with 1.1'
5661,'bleskes',"Calling the Update API using EXTERNAL(_GTE) version type should throw a validation error\nThe update API is utility to do a get-change-and-index cycle while guarantying the document didn't change between the get and the index phase of the operation. We do so using Elasticsearch's versioning system which allows you to fail an indexing request if the document doesn't have the expected version (i.e., changed). \r\n\r\nAt the moment, using the `EXTERNAL` or `EXTERNAL_GTE` version types break this guaranties. These should be disabled. In the future we can extend the index API to allow supplying both an expected exiting version and a version to be index at which point we could have a proper support in the update API."
5659,'jpountz',"Search: Speed up `exists` and `missing` filters on high-cardinality fields\nThe way that the `exists` filter works is by merging all postings lists. `missing` just wraps an `exists` filter into a `not` filter.\r\n\r\nMerging all postings lists can however be very slow on high-cardinality fields. I think there are two ways to fix it:\r\n 1. make these filters run on top of field data,\r\n 2. or add a new metadata field that we could eg. call `_field_names` that would index all field names of a document.\r\n\r\nWorking on field data has the drawback of requiring a lot of stuff to be loaded into memory if the field doesn't have doc values, and the returned filter cannot skip.\r\n\r\nI tend to like indexing field names because it would not load anything into memory with a default setup, and the returned filter could skip efficiently since it would be based on a postings list. But unfortunately it could not be used on indices that have been created before we introduce this new metadata field."
5657,'imotov','Snapshot succeeds even when some nodes cannot access shared repository\nHi,\r\n I am testing out using the snapshot/restore feature in ES 1.0.0. I have created the following setup.\r\nAn ES cluster with three nodes. A shared file system which can only be accessed by Node 1[Master] and Node 2. This shared fs is registered as the repository for taking backups on Node 1.\r\n\r\nI noticed that even if I \r\na) force some of my data to be present only on Node 3, and \r\nb) ensure that Node 3 cannot access the shared repository\r\n\r\nTaking a snapshot of the entire cluster still reports a success. However, when I browse the contents of the snapshot folder, I do not see any of the data from Node 3. I was expecting a "RepositoryMissing" exception to be thrown by Node 3. Have I misunderstood how ES snapshotting works?\r\n\r\nThanks!\r\n'
5655,'s1monw','error w/ arcDistance* in script\nBeen experimenting w/ ES 1.1.0 and seeing this error:\r\n\r\n[2014-03-31 18:37:47,807][DEBUG][action.search.type       ] [Spidercide] [_river][0], node[8g5DHgKoR6G-3lwIojQcPQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@15029ae] lastShard [true]\r\norg.elasticsearch.search.SearchParseException: [_river][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"sort":[{"modified":{"order":"desc"}},"_score"],"from":0,"script_fields":{"distance":{"params":{"lat":37.7749295,"lon":-122.4194155},"script":"doc[\'location2.coordinates\'].arcDistanceInMiles(lat, lon)"}},"fields":["_source"],"facets":{"services":{"facet_filter":{"and":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]},"terms":{"field":"services","size":200}},"proficiencies":{"facet_filter":{"and":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]},"terms":{"field":"good_at","size":50}}},"filter":{"bool":{"must":[{"terms":{"services":["52431dc3c61e364f922ce716"],"execution":"and"}}]}},"query":{"match_all":{}},"size":20}]]\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:634)\r\n\tat org.elasticsearch.search.SearchService.createContext(SearchService.java:507)\r\n\tat org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:480)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:252)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: org.elasticsearch.search.SearchParseException: [_river][0]: from[-1],size[-1]: Parse Failure [No mapping found for [modified] in order to sort on]\r\n\tat org.elasticsearch.search.sort.SortParseElement.addSortField(SortParseElement.java:198)\r\n\tat org.elasticsearch.search.sort.SortParseElement.addCompoundSortField(SortParseElement.java:172)\r\n\tat org.elasticsearch.search.sort.SortParseElement.parse(SortParseElement.java:80)\r\n\tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:622)\r\n\t... 11 more\r\n[2014-03-31 18:37:47,840][DEBUG][action.search.type       ] [Spidercide] [9733] Failed to execute fetch phase\r\njava.lang.RuntimeException: cannot invoke method: arcDistanceInMiles\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessorNest.getValue(MapAccessorNest.java:54)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\r\n\tat org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)\r\n\tat org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:74)\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:452)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)\r\n\t... 17 more\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.index.fielddata.ScriptDocValues$GeoPoints.arcDistanceInMiles(ScriptDocValues.java:365)\r\n\t... 21 more\r\n[2014-03-31 18:37:47,840][DEBUG][action.search.type       ] [Spidercide] [9732] Failed to execute fetch phase\r\njava.lang.RuntimeException: cannot invoke method: arcDistanceInMiles\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessorNest.getValue(MapAccessorNest.java:54)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\r\n\tat org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:191)\r\n\tat org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:74)\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:452)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.reflect.InvocationTargetException\r\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:606)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)\r\n\t... 17 more\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.index.fielddata.ScriptDocValues$GeoPoints.arcDistanceInMiles(ScriptDocValues.java:365)\r\n\t... 21 more\r\n\r\n\r\n\r\nThe error doesn\'t cause anything to actually not work -- which is to show the distance to the vendor listing.  Here is the query that produces the error:\r\n```\r\n{\r\n  "sort": [\r\n    {\r\n      "modified": {\r\n        "order": "desc"\r\n      }\r\n    },\r\n    "_score"\r\n  ],\r\n  "from": 0,\r\n  "script_fields": {\r\n    "distance": {\r\n      "params": {\r\n        "lat": 37.7749295,\r\n        "lon": -122.4194155\r\n      },\r\n      "script": "doc[\'location2.coordinates\'].arcDistanceInMiles(lat, lon)"\r\n    }\r\n  },\r\n  "fields": [\r\n    "_source"\r\n  ],\r\n  "facets": {\r\n    "services": {\r\n      "facet_filter": {\r\n        "and": [\r\n          {\r\n            "terms": {\r\n              "services": [\r\n                "52431dc3c61e364f922ce716"\r\n              ],\r\n              "execution": "and"\r\n            }\r\n          }\r\n        ]\r\n      },\r\n      "terms": {\r\n        "field": "services",\r\n        "size": 200\r\n      }\r\n    },\r\n    "proficiencies": {\r\n      "facet_filter": {\r\n        "and": [\r\n          {\r\n            "terms": {\r\n              "services": [\r\n                "52431dc3c61e364f922ce716"\r\n              ],\r\n              "execution": "and"\r\n            }\r\n          }\r\n        ]\r\n      },\r\n      "terms": {\r\n        "field": "good_at",\r\n        "size": 50\r\n      }\r\n    }\r\n  },\r\n  "filter": {\r\n    "bool": {\r\n      "must": [\r\n        {\r\n          "terms": {\r\n            "services": [\r\n              "52431dc3c61e364f922ce716"\r\n            ],\r\n            "execution": "and"\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  },\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "size": 20\r\n}\r\n```\r\n\r\nWhen, I remove the offending script, the error goes away but so does the functionality!  Any idea what\'s going on here?\r\n'
5654,'javanna',"[TEST] Replaced RestTestSuiteRunner with parametrized test that uses RandomizedRunner\nElasticsearchRestTests extends now ElasticsearchIntegrationTest and makes use of our ordinary test infrastructure, in particular all randomized aspects now come for free instead of having to maintain a separate (custom) tests runner\r\n\r\nWe previously parsed only the tests that needed to be run given the version of the cluster the tests are running against. This doesn't happen anymore as it didn't buy much and it would be harder to support as the tests get now parsed before the test cluster gets started. Thus all the tests are now parsed regardless of their skip sections, afterwards the ones that don't need to be run will be skipped through assume directives.\r\n\r\nFixed REST tests that rely on a specific number of shards as this change introduces also random number of shards and replicas (through randomIndexTemplate)"
5651,'seang-es','Improved upgrade docs\n'
5646,'jpountz',"ScriptDocValues.EMPTY doesn't implement `getValue`\nThis might not look like an issue since getting a single value on something that is empty makes no sense but it introduces an inconsistency between documents that have no value, depending on whether they are on a segment that has no value at all (which will use `ScriptDocValues.EMPTY`), or on a segment where at list one document has a value. In the latter case, `ScriptDocValues.Longs` (or `Doubles`) will be used and these classes implement `getValue` and return a default value (`0`) for documents with no value.\r\n\r\nSee https://github.com/elasticsearch/elasticsearch/pull/4684#issuecomment-39167864 for the original bug report."
5643,'s1monw',"TribeNode throws NPE if index doesn't exist\n```\r\n[2014-04-01 08:00:24,976][WARN ][tribe                    ] [Keith Kilham] failed to process [cluster event from t2, local-disco-receive(from master)]\r\njava.lang.NullPointerException\r\n\tat org.elasticsearch.tribe.TribeService$TribeClusterStateListener$1.execute(TribeService.java:315)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:309)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\n```"
5638,'javanna','fix typo\n'
5637,'GaelTadh','Add predefined search templates endpoint\nNew feature #5122 allows storing query templates on local nodes.\r\n\r\nWe should add some endpoints to allow:\r\n\r\n## Put/Update a search template:\r\n\r\n```\r\nPUT _search/template/templatename\r\n{\r\n    "template" : {\r\n        "content" : {\r\n            "query": {\r\n                "match_{{te_1}}": {}\r\n            },\r\n            "fac{{te_2}}": {\r\n            "list-id": {\r\n                "terms": {\r\n                    "field": "list-{{te_3}}",\r\n                    "size": 10\r\n                }\r\n            }\r\n            }, \r\n            "fields": ["subject", "from"]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n## Get a search template:\r\n\r\n```\r\nGET _search/template/templatename\r\n```\r\n\r\n## Remove a search template:\r\n\r\n```\r\nDELETE _search/template/templatename\r\n```\r\n\r\n## Run the template:\r\n\r\n```\r\nGET /_search/template\r\n{\r\n    "template": "templatename" ,\r\n    "params": {\r\n        "te_1" : "all",\r\n        "te_2" : "ets",\r\n        "te_3" : "id"\r\n    }\r\n}\r\n```\r\n\r\n## Search for all templates:\r\n\r\n```\r\nGET _search/template\r\nGET _search/template/_all\r\nGET _search/template/*\r\n```\r\n\r\n'
5636,'spinscale','Update JNA to 4.1.0, properly warn on error, hint at noexec mount\nSee issue #5493\r\n'
5634,'colings86','Aggregations: Support bounding box aggregation on geo_shape/geo_point data types.\nFor applications that index spatial data it would be very useful to be able to request an aggregation that would return the bounding box (extent) of the matching hits.  A typical use case would be to zoom a map to area containing matching hits. '
5630,'javanna',"[TEST] added ExternalTestCluster that allows to run tests against an external cluster\nAll the ordinary test operations happen based on the `ImmutableTestCluster` base class and are executed via transport client. Will be used especially for the REST tests once migrated to the standard randomized runner.\r\n\r\nAdded new httpAddresses method to ImmutableTestCluster to be able to retrieve the http addresses to connect to for the REST tests. Both versions will look inside the cluster to figure out which nodes are available for http calls and their addresses.\r\n\r\nThe external cluster is used as global cluster if the `tests.cluster` system property is available. The property needs to contain a comma separated list of available elasticsearch nodes that will be used to connect to the cluster (e.g. localhost:9300,localhost:9301).\r\n\r\nOnly a subset of the integration tests can currently be run successfully against the external cluster, for more precision the ones that don't modify the cluster layout (don't require `cluster()` functionalities but rely only on `immutableCluster()`). Also at least two data nodes are required otherwise the `ensureGreen` calls cannot succeed."
5620,'javanna',"[TEST] introduced ImmutableTestCluster abstract base class for TestCluster\nThe new base class contains all the immutable methods for a cluster, which will be extended by the future ExternalCluster impl that relies on an external cluster and won't be adding nodes etc. but only sending requests to an existing cluster whose layout never changes"
5618,'clintongormley','support `F` as false\naccording to document `F` should be treat as `false`\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#boolean\r\n\r\ncloses #2075'
5615,'imotov','SnapshotMetaData.fromXContent does not match toXContent, but throws an Exception\nIs there any particular reason why ``fromXContent`` should raise an Exception, while ``toXContent`` serializes the information?\r\n\r\nSource at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/metadata/SnapshotMetaData.java#L358\r\n\r\nThis seems to cause problems for certain gateways, such as https://github.com/elasticsearch/elasticsearch-cloud-aws/issues/68'
5613,'uboness','Fixing questionable PNRG behavior\nCloses #5454 and #5578\r\n\r\nThis strengthens and simplifies the PNRG used by `random_score` by more closely mirroring the `Random.nextFloat()` method, rather than a mix of that, `nextInt` and `nextDouble`. The `docBase` or `docId` are no longer used as they were biasing the result (particularly if it was `0`, which consistently made it the highest scoring result in tests), which partially defeats the purpose of random scoring.'
5610,'s1monw','Update percolate.asciidoc\nfix typo'
5607,'imotov','add _meta for all mapping fields\nadd field level `_meta` support for all fields in mapping\r\n\r\ncloses #2857'
5606,'javanna','Replaces usage of `StringBuffer` with `StringBuilder`\nCloses #5605\r\n\r\nAlso fixes stray single quote in the same `PluginInfo#toString()` method.'
5605,'javanna',"StringBuilder should be used in place of StringBuffer\n`StringBuilder` was made as an API compatible, drop-in replacement of `StringBuffer` without the generally unnecessary synchronization of every method.\r\n\r\nCurrently, `StringBuffer` is being used by `PluginInfo#toString()` (not vital, but still unnecessary) and `FullRestartStressTest` (less of an issue as it's test code)."
5602,'s1monw','Add MediaWiki integration to list\nMediaWiki can use Elasticsearch to power its search backend via the CirrusSearch extension.'
5593,'imotov',"Snapshot/Restore API: Snapshot checksum verification\nThe snapshot process should verify checksums for each file that is being snapshotted to make sure that created snapshot doesn't contain corrupted files. If a corrupted file is detected, the snapshot should fail with an error.\r\n\r\nIn order to implement this feature we need to have correct and verifiable checksums stored with segment files, which is only possible for files that were written by append-only codecs. All officially supported codecs that are currently in use are append-only. If there are old 3rd party codecs that are not, such codecs will no longer work with Elasticsearch. "
5592,'colings86','Indexing a polygon geo-shape where polygons share are a starting point throws an exception\nWhen we index a polygon shape we allow to supply holes in that polygon. Those are supplied as extra polygon as the geojson spec dictates. If a hole start on the same coordinate as the boundary polygon we throw an ArrayIndexOutOfBoundsException.\r\n\r\nSee https://gist.github.com/mcuelenaere/c3370ac8356b7ec80724 for a reproduction.\r\n'
5589,'hhoffstaette','Restore streamInput() performance over PagedBytesReference.\nThe initial implementation of bulk-reading a streamInput() over PagedBytesReference was slow (byte-by-byte reading when bulk copying).\r\n\r\nTimes in µs for bulk-reading a stream over plain vs. paged, averaged over 1000 runs:\r\n\r\n| MB  | plain µs | paged µs | Ratio |\r\n|----:|---------:|---------:|-------|\r\n|   1 |       72 |     2048 | 28.6  |\r\n|   2 |      140 |     4127 | 29.4  |\r\n|   3 |      218 |     6208 | 28.4  |\r\n|   4 |      430 |     8396 | 19.5  |\r\n|   5 |      700 |    10525 | 15.0  |\r\n|  10 |     1739 |    21055 | 12.1  |\r\n|  20 |     3481 |    42063 | 12.1  |\r\n|  50 |     8701 |   105337 | 12.1  |\r\n| 100 |    17409 |   210848 | 12.1  |\r\n\r\nThis changeset restores performance:\r\n\r\n| MB  | plain µs | paged µs | Ratio |\r\n|----:|---------:|---------:|-------|\r\n|   1 |       65 |       68 | 1.05  |\r\n|   2 |      134 |      141 | 1.04  |\r\n|   3 |      198 |      235 | 1.18  |\r\n|   4 |      456 |      418 | 0.91  |\r\n|   5 |      750 |      761 | 1.01  |\r\n|  10 |     1736 |     1743 | 1.00  |\r\n|  20 |     3514 |     3497 | 0.99  |\r\n|  50 |     8706 |     8700 | 0.99  |\r\n| 100 |    17608 |    17731 | 1.00  |\r\n\r\nThe performance jitters slightly due to the usual Hotspot variances, OS scheduling etc. For all practical purposes the performance is now back to what it was before.\r\n'
5586,'colings86','Aggregations: DateHistogramBuilder uses wrong data type for pre_offset and post_offset\nThe DateHistogramBuilder stores and builds the DateHistogram [with a `long` value for the `pre_offset` and `post_offset`](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramBuilder.java#L44), which is neither what the [API docs specify](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-datehistogram-aggregation.html#_pre_post_offset_2) (which specify the format is the data format `1s`, `2d`, etc.) nor what the [DateHistogramParser expect](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/aggregations/bucket/histogram/DateHistogramParser.java#L122).\r\n\r\nThis forces the improper construction of DateHistogram requests when using the Java API to construct queries.  Both the `preOffset` and `postOffset` variables should be converted to `Strings`.'
5585,'jpountz','check "store" for binary mapper and check "index_name" for all mappers\ncloses #5474\r\n\r\nthe "index_name" is also ignored without throw exception, added check for that as well'
5584,'javanna','A few grammar and word use corrections.\n'
5580,'drewr','[DOCS] Update _cat/nodes documentation with all headers\nThe current [_cat/nodes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/cat-nodes.html) documentation shows a few examples of possible output, but it does not provide the full list of possible options (headers) available.\r\n\r\nAdding all of the options will make this page itself more user friendly and search friendly because people trying to find such information will hopefully get hits from the exhaustive list of headers and descriptions when searching, rather than potentially passing over `_cat/nodes` without realizing that it has many, many more useful fields.'
5578,'uboness','RandomScoreFunction.PRNG generates weak random numbers\nHere is some code extracted from `PRNG.random(int)`:\r\n```java\r\nlong rand = doc;\r\nrand |= rand << 32;\r\nrand ^= rand;\r\nreturn nextFloat(rand);\r\n```\r\nThe issue is that `rand ^= rand;` is equivalent to `rand = 0;` so in the end, the random score generation completely discards the doc ID that was provided.'
5575,'dadoonet','`flush` method for BulkProcessor class.\nThis is for #5570.\r\n\r\nThere is no explicit method `flush/execute` in [BulkProcessor][1] class. This can be useful in certain scenarios. Currently it requires to close and create a new BulkProcessor if one wants an immediate flush.\r\n\r\n[1]: http://javadoc.kyubu.de/elasticsearch/v0.90.0/org/elasticsearch/action/bulk/BulkProcessor.html'
5570,'dadoonet','[JAVA-API] BulkProcessor flush/execute\nThere is no explicit method `flush/execute` in [BulkProcessor][1] class. This can be useful in certain scenarios. Currently it requires to close and create a new BulkProcessor if one wants an immediate flush.\r\n\r\n[1]: http://javadoc.kyubu.de/elasticsearch/v0.90.0/org/elasticsearch/action/bulk/BulkProcessor.html'
5568,'javanna','Add configured thread pool sizes to _cat/thread_pool\nAdding support for reporting the `*.min` (`*mi`) and `*.max` (`*ma`) from `ThreadPool#Info`.\r\n\r\nCloses #5366'
5567,'drewr','[DOCS] Update nodes documentation with all headers\nAdds a table with the exhaustive list of all available headers with a brief description (mostly from `org.elasticsearch.rest.action.cat.RestNodesAction`) so that people do not need to go searching for them in the code like I did, or search through `nodes?help`.\r\n\r\nCloses #5580 '
5566,'dadoonet','Exposed a PluginManager.isPluginInstalled(String name) method for easier embedded use\nExposed a PluginManager.isPluginInstalled(String name) method to make it a bit cleaner using the PluginManager embedded (when running elastic embedded).\r\n\r\nDid a bit of a refactor to reduce some duplication (that the above change introduced - and little bit of existing).\r\nNot sure if it was intentional - but there was a bit of mixed usage of IllegalArgumentException and ElasticsearchIllegalArgumentException - so I standardised on ElasticsearchIllegalArgumentException.\r\nThere was also some validation on the name when removing a plugin - that is worthwhile having when installing a plugin - so I made that common.\r\nIssue #5565'
5564,'javanna','fix dynamic_type in dynamic_template\ncloses #5256'
5563,'s1monw','Add suggest stats\ncloses #4032'
5559,'jpountz','IndexShardRoutingTable might barf if it has handled lots of searches...\nA user reported this on the ML:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/AxRU1UQP24U\r\n\r\n```\r\nCaused by: java.lang.IndexOutOfBoundsException: index (-2) must not be negative\r\n        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:306)\r\n        at org.elasticsearch.common.base.Preconditions.checkElementIndex(Preconditions.java:285)\r\n        at org.elasticsearch.common.collect.RegularImmutableList.get(RegularImmutableList.java:65)\r\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferNodeActiveInitializingShardsIt(IndexShardRoutingTable.java:378)\r\n        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.preferenceActiveShardIterator(PlainOperationRouting.java:210)\r\n        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.getShards(PlainOperationRouting.java:80)\r\n        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:80)\r\n        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:42)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.<init>(TransportShardSingleOperationAction.java:121)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.<init>(TransportShardSingleOperationAction.java:97)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:74)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:49)\r\n        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:85)\r\n        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:174)\r\n        ... 9 more\r\n```\r\n\r\nthe code that causes this is:\r\n\r\n```Java\r\n\r\n  private int pickIndex() {\r\n        return Math.abs(counter.incrementAndGet());\r\n  }\r\n```\r\n\r\nwhich might return a negative number. `Math.abs()` returns `-1` for `Integer.MIN_VALUE` which causes the AIOOB mentioned above. The usage of this method seems to be pretty broken along those lines and we might need to think about fixing this generally...'
5556,'spinscale','search_template does not support ?source=\nAll GET-with-body endpoints should support passing the body as the `source` parameter in the query string.  `search_template` does not currently support this'
5551,'s1monw','match_phrase_prefix broken\nThis breaks in 1.1:\r\n\r\nCreation of an index and one document in it\r\n```Json\r\ncurl -XPOST \'localhost:9200/twitter/tweet/1\' -d \'\r\n{\r\n"user" : "kimchy",\r\n"post_date" : "2009-11-15T14:12:12",\r\n"message" : "trying out Elasticsearch"\r\n}\'\r\n``` \r\n \r\nFollowing will work on 1.0.2 but fail on 1.1.0\r\n```Json\r\ncurl -XPOST \'localhost:9200/twitter/tweet/_search\' -d \'\r\n{\r\n"query": {\r\n"match_phrase_prefix": {\r\n"message": "try"\r\n}\r\n}\r\n}\'\r\n```'
5550,'spinscale',"Spec query string params for search_template \nThe current JSON spec for the `search_template` endpoint doesn't list any query string parameters, eg `routing` etc.\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/api/search_template.json\r\n\r\nShould these parameters be the same as for the `search` endpoint?"
5549,'hhoffstaette','Assorted fixes for bugs in the PagedBytesReference tests\nVarious fixes for the PBR tests, plus a typo where the wrong variable was used for a calculation.\r\n'
5547,'s1monw','Adding Kopf to community list of monitoring tools.\nAdding versatile monitoring and administration tool Kopf to the community section of the documentation.'
5545,'javanna','REST API: Removed support for aliases as part of index settings\nNow that we have explicit support for aliases when creating indices and as part of index templates, we may remove support for aliases (only names) as part of index settings. The change is marked as breaking as the following calls:\r\n\r\n```\r\ncurl -XPUT localhost:9200/index -d \'{\r\n  "settings" : {\r\n    "aliases" : [ "alias1"]\r\n  }\r\n}\r\n```\r\n\r\nand\r\n\r\n```\r\ncurl -XPUT localhost:9200/index -d \'{\r\n  "settings" : {\r\n    "index.aliases" : [ "alias1"]\r\n  }\r\n}\r\n```\r\n\r\nwon\'t be supported anymore and will need to be replaced with\r\n\r\n```\r\ncurl -XPUT localhost:9200/index -d \'{\r\n  "aliases" : {\r\n    "alias1": {}\r\n  }\r\n}\r\n```'
5544,'javanna','add human readable start_time and refresh_interval\ncloses #5280'
5543,'hhoffstaette','Fix for zero-sized content throwing off toChannelBuffer().\nAlso short-circuit writeTo(..) accordingly to avoid unnecessary work. This fixes an issue where 0-sized ContentLength replies (e.g. 404) would make some REST tests fail.\r\n'
5542,'javanna','[TEST] Moved wipe* methods, randomIndexTemplate & ensureEstimatedStats to TestCluster\nThis is the first to make it possible to have a different impl of TestCluster (e.g. based on an external cluster) that has the same methods but a different impl for them (e.g. it might use the REST API to do the same instead of the Java API)'
5531,'imotov','Add an ability to snapshot relocating primary shards\nCurrently the snapshot process fails immediately with an error if one of the indices involved in the snapshot process has a relocating primary shard.'
5529,'hhoffstaette','Let ByteArray/BigByteArray.get() indicate whether a byte[] was materialized.\nNecessary and cleaner instead of letting client code "guess" whether pages are shared or copied.\r\n'
5525,'spinscale','Context Suggester Issues/Exceptions\nThe context suggester seems to have a couple of bugs in the geo impl - when working according to the docs I could not create working suggestions.\r\n\r\nThis example does not return any suggestions (but it should return the one in Amsterdam)\r\n\r\n```JSON\r\nDELETE venues\r\nPUT venues\r\n\r\nGET venues/_mapping\r\nPUT venues/poi/_mapping\r\n{\r\n  "poi" : {\r\n    "properties" : {\r\n      "suggest_field": {\r\n        "type": "completion",\r\n        "context": {\r\n          "loc": { \r\n            "type": "geo",\r\n            "precision" : "10km"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT venues/poi/1\r\n{\r\n  "suggest_field": {\r\n    "input": ["Hotel Amsterdam" ],\r\n    "context": {\r\n      "loc": {\r\n        "lat": 52.529172, \r\n        "lon": 13.407333\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET venues/poi/2\r\nPUT venues/poi/2\r\n{\r\n  "suggest_field": {\r\n    "input": ["Hotel Berlin in AMS" ],\r\n    "context": {\r\n      "loc": {\r\n        "lat": 52.363389, \r\n        "lon": 4.888695\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /venues/_search\r\n\r\nGET /venues/_suggest\r\n{\r\n  "suggest" : {\r\n    "text" : "h",\r\n    "completion" : {\r\n      "field" : "suggest_field",\r\n      "context": {\r\n        "loc": {\r\n          "lat": 52.36,\r\n          "lon": 4.88\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nThis returns a parsing exception (but is mentioned in the docs like that)\r\n\r\n```JSON\r\nDELETE venues\r\nPUT venues\r\n\r\nGET venues/_mapping\r\nPUT venues/poi/_mapping\r\n{\r\n  "poi" : {\r\n    "properties" : {\r\n      "suggest_field": {\r\n        "type": "completion",\r\n        "context": {\r\n          "location": {\r\n              "type": "geo",\r\n              "precision": ["1km", "5m"],\r\n              "neighbors": true,\r\n              "path": "pin",\r\n              "default": {\r\n                  "lat": 0.0,\r\n                  "lon": 0.0\r\n              }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis is also directly from the docs, using the `value` field, results in a parse exception\r\n\r\n```JSON\r\nDELETE venues\r\nPUT venues\r\n\r\nGET venues/_mapping\r\nPUT venues/poi/_mapping\r\n{\r\n  "poi" : {\r\n    "properties" : {\r\n      "suggest_field": {\r\n        "type": "completion",\r\n        "context": {\r\n          "loc": { \r\n            "type": "geo",\r\n            "precision" : "10km"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGET /venues/_suggest\r\n{\r\n    "suggest" : {\r\n        "text" : "m",\r\n        "completion" : {\r\n\r\n            "field" : "suggest_field",\r\n            "size": 10,\r\n            "context": {\r\n                "location": {\r\n                    "value": {\r\n\r\n                        "lat": 0,\r\n                        "lon": 0\r\n                    },\r\n                    "precision": "1km"\r\n                }\r\n            }\r\n        }\r\n\r\n    }\r\n}\r\n```\r\n\r\nAlso the category suggester seems to have an issue, when you specify a path in the mapping for the context, but that path is not set on document indexing\r\n\r\n```JSON\r\nDELETE services\r\nPUT services\r\nPUT services/service/_mapping\r\n{\r\n    "service": {\r\n        "properties": {\r\n            "suggest_field": {\r\n                "type": "completion",\r\n                "context": {\r\n                    "color": { \r\n                        "type": "category",\r\n                        "path": "color_field"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nGET services/service/_mapping\r\n\r\nPUT services/service/1\r\n{\r\n    "suggest_field": {\r\n        "input": ["knacksack", "backpack", "daypack"]\r\n    }\r\n}\r\n```\r\n\r\nThis was tested on the 1.x branch and master'
5524,'costin',"allow plugins to be upgraded in place/live without first uninstalling\nSince all we're doing is wiping/removing a particular subdir under plugins/ dir, can't we do the remove and install in a single step?\r\n\r\nSo instead of doing an uninstall then an install, allow an upgrade that does both in one step.\r\n\r\nThere might be some limitation if a plugin is unable to run two different versions on two nodes at the same time, but perhaps a method should be provided for plugin authors to prevent upgrade method and instead force uninstall/install if required."
5522,'martijnvg','fix include_in_all for multi field\nAs in document http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#_include_in_all\r\n\r\nThe `include_in_all` should be ignored for multi-field. Currently it will set the `includeInAll` to `null`, then it will be default to `true` if `index` is not set to `no`. I think it should be set to `false` instead of `null`.\r\n\r\nbtw, seems `AllFieldMapper.IncludeInAll.unsetIncludeInAll()` will not be used anywhere after this change, should that be removed?\r\n\r\ncloses #5364'
5510,'jpountz','fix return incorrect XContentParser\n'
5509,'s1monw','add CBOR data format support\nadd CBOR data format support using https://github.com/FasterXML/jackson-dataformat-cbor\r\n\r\ncloses #4860'
5506,'jpountz','merge GeoPoint specific mapping properties\ncloses #5505\r\nI assume only `validate_lat` and `validate_lon` can be updated, other fields will throw exceptions. Please let me know if I was wrong.\r\n\r\nAlso added `precision_step` to the document page'
5505,'jpountz','GeoPointFieldMapper doesn\'t merge GeoPoint specific properties\n```sh\r\n# add mapping\r\ncurl -XPUT localhost:9200/test/test/_mapping -d \'{\r\n  "test": {\r\n    "properties": {\r\n      "testGeo": {\r\n        "type": "geo_point",\r\n        "validate": false\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n# update validate to true\r\ncurl -XPUT localhost:9200/test/test/_mapping -d \'{\r\n  "test": {\r\n    "properties": {\r\n      "testGeo": {\r\n        "type": "geo_point",\r\n        "validate": true\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nAfter update, the `validate` is still false and no exception is thrown. Other fields like `store` etc. works.'
5485,'martijnvg','Add reverse nested aggregation \nSimilar to the `nested` aggregator, but in reverse. It allows to create buckets and metrics based on the parent docs in the buckets of child documents.\r\n\r\nThis aggregation only makes sense inside a `nested` aggregator. Assume a ticket system with where the comments are embedded into the issue document. The `reverse_nested` aggregator would look like this:\r\n\r\n```json\r\n"aggs" : {\r\n   "top_commenters_to_issues" : {\r\n        "nested" : {"path" : "comments"},\r\n        "aggs" : {\r\n            "top_commenters" : {\r\n                "terms" : { "field" : "username" },\r\n                "aggs" : {\r\n                    "top_tags_to_comments" : {\r\n                        "reverse_nested" : {\r\n                            "path" : "comments"\r\n                        },\r\n                        "aggs" : {\r\n                            "top_tags" : { "terms" : { "field" :  "tags"}}\r\n                        }    \r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis aggregation would return top username buckets and per username bucket the top tags. The important constraint that the `nested` and `reverse_nested` add here is that only tag buckets are associated to username buckets that reside in the nested structure.'
5484,'GaelTadh','Scripting: Allow to run scripts/templates stored in .scripts index\nThis will be handy if search templates for example are stored in .scripts/templates index or something of the like. This can allow for recursive execution of templates, as well as "user defined queries" more easily, since changing the query/search request is just a matter of updating the query.\r\n\r\nThis will also allow to more easily disable dynamic scripts, and just allow loading them form FS or another index, with the other index allowing to easily change existing scripts.'
5481,'javanna','fix typo joda-time link\nFound broken link in doc of time format.'
5480,'drewr','Node version sometimes empty in _cat/nodes\nSometimes a node will not list its version in `_cat/nodes`.\r\n\r\n    drewr@client-002 Thu 20 17:04:28 30 0 ~()% curl localhost:9200/_cat/nodes\\?h=host,ip,name,version,jvm\r\n    data-001   100.67.136.16 Nick Fury      1.0.1\r\n    client-001 100.67.182.91 Melody Guthrie\r\n    client-002 100.67.184.5  Nightwind      1.0.1\r\n\r\n`client-001` has the same version of ES:\r\n\r\n    drewr@client-002 Thu 20 17:04:31 31 0 ~()% curl -s client-001:9200\r\n    {\r\n      "status" : 200,\r\n      "name" : "Melody Guthrie",\r\n      "version" : {\r\n        "number" : "1.0.1",\r\n        "build_hash" : "5c03844e1978e5cc924dab2a423dc63ce881c42b",\r\n        "build_timestamp" : "2014-03-17T02:02:39Z",\r\n        "build_snapshot" : false,\r\n        "lucene_version" : "4.6"\r\n      },\r\n      "tagline" : "You Know, for Search"\r\n    }\r\n'
5473,'s1monw','Mustache templates should escape JSON, not HTML\nCurrently we\'re using the default  `escape` method from Mustache, which is intended for escaping HTML, not JSON.\r\n\r\nThis results in things like `"` -> `&quot;`\r\n\r\nInstead, we should be using these escapes:\r\n\r\n    \\b  Backspace (ascii code 08)\r\n    \\f  Form feed (ascii code 0C)\r\n    \\n  New line\r\n    \\r  Carriage return\r\n    \\t  Tab\r\n    \\v  Vertical tab\r\n    \\"  Double quote\r\n    \\\\  Backslash \r\n\r\n\r\n'
5472,'s1monw','Add hasClauses method to BoolFilterBuilder\nWould be nice to have the hasClauses() method as in BoolQueryBuilder.'
5470,'s1monw','Allow iteration over MultiGetRequest#Item instances\nCloses #3061'
5458,'jpountz','filtering aggregations \nWhen doing a nested aggregation against a specific doc type, and with a filter of a specific doc type, the results still return unfiltered results.  Here\'s an example:\r\n\r\n```\r\nPOST _all/summary_phys/_search\r\n{\r\n  "aggs": {\r\n    "summary_phys_events": {\r\n      "filter": {\r\n        "type": {"value": "summary_phys"}\r\n      },\r\n      "aggs": {\r\n        "events_by_date": {\r\n          "date_histogram": {\r\n            "field": "@timestamp",\r\n            "interval": "300s",\r\n            "min_doc_count": 0\r\n          },\r\n          "aggs": {\r\n            "events_by_host": {\r\n              "terms": {\r\n                "field": "host.raw",\r\n                "min_doc_count": 0\r\n              },\r\n              "aggs": {\r\n                "avg_used": {\r\n                  "avg": {\r\n                    "field": "used"\r\n                  }\r\n                },\r\n                "max_used": {\r\n                  "max": {\r\n                    "field": "used"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI get buckets with entries matching hosts that do not show up in this doc type.  For example, I have only 3 values for host in this doc type [compute-4, compute-2, compute-3], but I will get buckets back with hosts from other doc types like:\r\n\r\n```\r\n"events_by_host": {\r\n                  "buckets": [\r\n                     {\r\n                        "key": "compute-4",\r\n                        "doc_count": 11,\r\n                        "max_used": {\r\n                           "value": 4608\r\n                        },\r\n                        "avg_used": {\r\n                           "value": 3677.090909090909\r\n                        }\r\n                     },\r\n                     {\r\n                        "key": "compute-2",\r\n                        "doc_count": 8,\r\n                        "max_used": {\r\n                           "value": 4608\r\n                        },\r\n                        "avg_used": {\r\n                           "value": 2304\r\n                        }\r\n                     },\r\n                     {\r\n                        "key": "compute-3",\r\n                        "doc_count": 2,\r\n                        "max_used": {\r\n                           "value": 4608\r\n                        },\r\n                        "avg_used": {\r\n                           "value": 4608\r\n                        }\r\n                     },\r\n                     {\r\n                        "key": "10.10.11.22:49509",\r\n                        "doc_count": 0,\r\n                        "max_used": {\r\n                           "value": null\r\n                        },\r\n                        "avg_used": {\r\n                           "value": null\r\n                        }\r\n                     },\r\n                     {\r\n                        "key": "controller",\r\n                        "doc_count": 0,\r\n                        "max_used": {\r\n                           "value": null\r\n                        },\r\n                        "avg_used": {\r\n                           "value": null\r\n                        }\r\n                     },\r\n                     {\r\n                        "key": "object-1",\r\n                        "doc_count": 0,\r\n                        "max_used": {\r\n                           "value": null\r\n                        },\r\n                        "avg_used": {\r\n                           "value": null\r\n                        }\r\n                     }\r\n                  ]\r\n            }\r\n```\r\n\r\nI believe that the extra hosts should be picked up by the aggregation filter if not by the URL path.'
5453,'clintongormley','Fixing context suggest documentation, missing comma after path in catego...\nFixing context suggest documentation.  Thanks to Sean Shubin for pointing out missing comma in sample.'
5448,'spinscale','"geohash_precision" in mapping throws error when using format like "1km"\nThe doc (http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/mapping-geo-point-type.html#_mapping_options) says you can specify the precision using strings like "1km" or "1m".\r\n\r\nHowever, doing so returns an error, only specifying the precision using a number seems to be allowed.\r\n\r\nE.g., create a new index like this:\r\n```\r\ncurl -XPUT \'http://localhost:9200/test\' -d \'{\r\n  "mappings": {\r\n    "pin": {\r\n      "properties": {\r\n        "location": {\r\n          "type": "geo_point",\r\n          "geohash_precision": "1m"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nand you\'ll get an error like this:\r\n```\r\n{\r\n  "error" : "MapperParsingException[mapping [pin]]; nested: NumberFormatException[For input string: \\"1m\\"]; ",\r\n  "status" : 400\r\n}\r\n```\r\n\r\nI\'m using 1.0.0.'
5434,'spinscale','Packaging: Export JAVA_HOME in RPM init script\nAdded missing line to init script. '
5433,'electrical','ES RedHat/Centos Init script not exporting JAVA_HOME\nHi,\r\n\r\nexport JAVA_HOME is missing from the init script in ES RPM 1.0.1-1 noarch.\r\n\r\nwhen trying to make ES use a specific version of Java it fails until one "fixes" the init script to include : \r\n\r\n\r\n[...]\r\n\r\n[ -e /etc/sysconfig/$prog ] && . /etc/sysconfig/$prog\r\n\r\nexport ES_HEAP_SIZE\r\nexport ES_HEAP_NEWSIZE\r\nexport ES_DIRECT_SIZE\r\nexport ES_JAVA_OPTS\r\nexport JAVA_HOME\r\n\r\nlockfile=/var/lock/subsys/$prog\r\n\r\n[...]\r\n\r\nAlex'
5428,'dadoonet','Enforce java version 1.7\nWhen building elasticsearch, we now require to use java 1.7 ( #5421 ).\r\n\r\nMaven will check that before compiling any class. If Java version is incorrect, you will get the following message:\r\n\r\n```\r\n[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:\r\nDetected JDK Version: 1.6.0-65 is not in the allowed range [1.7,).\r\n```\r\n\r\n'
5427,'hhoffstaette','New class PagedBytesReference: BytesReference over pages\nBytesRefererence over a BigArray/ByteArray list of pages. Passes both a boatload of new tests and the full test suite. About as compatible as it can be for now; minor perf improvements require extensions to BigArrays (see #5420)\r\n'
5422,'imotov','Remove deprecated Gateways\nwe deprecated Gateways before `1.0` and given the snapshot and restore feature they became obsolete. We gave quite some time to migrate and I think we should move away  from it in `1.2.0`. On master we can remove it already I guess but it might make sense to  merge it together? @imotov I think you should remove the gateways ;)'
5421,'s1monw',"Move to JAVA 1.7 with the once 1.1 is released following the upcoming Lucene 4.8 Release\nLucene moved to Java 1.7 as the minimum requirement here is the reasoning / official announcement:\r\n\r\n```\r\nthe Apache Lucene/Solr committers decided with a large majority on the vote to require Java 7 for the next minor release of Apache Lucene and Apache Solr (version 4.8)!\r\nSupport for Java 6 by Oracle  already ended more than a year ago and Java 8 is coming out in a few days.\r\n\r\nThe next release will also contain some improvements for Java 7:\r\n- Better file handling (especially on Windows) in the directory implementations. Files can now be deleted on windows, although the index is still open - like it was always possible on Unix environments (delete on last close semantics).\r\n- Speed improvements in sorting comparators: Sorting now uses Java 7's own comparators for integer and long sorts, which are highly optimized by the Hotspot VM..\r\n\r\nIf you want to stay up-to-date with Lucene and Solr, you should upgrade your infrastructure to Java 7. Please be aware that you must use at least use Java 7u1.\r\nThe recommended version at the moment is Java 7u25. Later versions like 7u40, 7u45,... have a bug causing index corrumption. Ideally use the Java 7u60 prerelease, which has fixed this bug. Once 7u60 is out, this will be the recommended version.\r\nIn addition, there is no Oracle/BEA JRockit available for Java 7, use the official Oracle Java 7. JRockit was never working correctly with Lucene/Solr (causing index corrumption), so this should not be an issue for you. Please also review our list of JVM bugs: http://wiki.apache.org/lucene-java/JavaBugs\r\n\r\nApache Lucene and Apache Solr were also heavily tested with all prerelease versions of Java 8, so you can also give it a try! Looking forward to the official Java 8 release next week - I will run my indexes with that version for sure!\r\n```\r\n\r\nWe should move as well and make Java 1.7 the minimum requirement to run Elasticsearch once we move to `Lucene 4.8`. Note, this is not yet release and it will roughly take a month or so until it will. I am putting es version `1.2.0` on this issue and mark it breaking to notify users. This means everything `>= v1.2.0` will need Java 1.7 and will not run on `1.6` to be absolutely clear."
5420,'hhoffstaette','BigArray/ByteArray improvements\nWe could use some extensions to BigArray/ByteArray:\r\n\r\n* copy() - similar to System.arrayCopy()\r\n* equals()/hashCode()\r\n\r\nThe latter is mostly driven by PagedBytesReference which currently materializes the pages into one big byte[], which is unnecessary.\r\n\r\n@jpountz Suggestions where and how to add equals/hashCode?\r\n'
5414,'jpountz','Scripts in aggs can\'t return more than 4 values\nDocument scripts can return an array of up to 4 values, but more than 4 cause an array out of bounds exception to be thrown:\r\n\r\n    DELETE /myindex\r\n    PUT /myindex/t/1\r\n    {}\r\n\r\n    GET /myindex/_search\r\n    {\r\n      "aggs": {\r\n        "foo": {\r\n          "date_histogram": {\r\n            "script": "[1388534400000,1388534400000,1388534400000,1388534400000]",\r\n            "interval": "hour"\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nThis throws an exception:\r\n\r\n    GET /myindex/_search\r\n    {\r\n      "aggs": {\r\n        "foo": {\r\n          "date_histogram": {\r\n            "script": "[1388534400000,1388534400000,1388534400000,1388534400000,1388534400000]",\r\n            "interval": "hour"\r\n          }\r\n        }\r\n      }\r\n    }'
5412,'spinscale','minor nits on aggs doc\n'
5407,'aleph-zero','Benchmark API\n'
5404,'javanna','Upgrade randomized-testing to 2.1.1\nNote that the standard `atLeast` implementation has now Integer.MAX_VALUE as upper bound, thus it behaves differently from what we expect in our tests, as we never expect the upper bound to be that high.\r\nAdded our own `atLeast` to `AbstractRandomizedTest` so that it has the expected behaviour with a reasonable upper bound.\r\nSee https://github.com/carrotsearch/randomizedtesting/issues/131'
5402,'dadoonet','Nested multi_field type wrapped by a custom type passing \'external values\' doesn\'t get values passed\nThis issue came up when I tried to combine the **Attachment** type (elasticsearch-mapper-attachments plugin) with a nested **multi_field** type. The intention behind that was to use the fulltext content extracted by the Attachment type for creating shingles and nGrams as well.\r\n\r\n### Prerequisites\r\nInstall the **Attachment** type plugin:\r\n\r\n```\r\n$ elasticsearch-1.0.1/bin/plugin --install elasticsearch/elasticsearch-mapper-attachments/2.0.0.RC1\r\n```\r\n\r\n### Index configuration\r\nI\'ve prepared a sample configuration for an elasticsearch index \'attachment\' as listed below:\r\n\r\n```sh\r\n# delete the index\r\n$ curl -XDELETE \'http://localhost:9200/attachment\'\r\n\r\n# create the index with some analyzers to be used later on\r\n$ curl -XPUT http://localhost:9200/attachment?pretty -d \' \r\nindex :\r\n    number_of_shards : 1\r\n    number_of_replicas : 1\r\n            \r\n    analysis:\r\n        analyzer:\r\n            default:\r\n                type: custom\r\n                tokenizer: standard\r\n            ngram_analyzer:\r\n                type : custom\r\n                tokenizer : standard\r\n                filter: [ngram_filter]\r\n            e_ngram_analyzer:\r\n                type : custom\r\n                tokenizer : standard\r\n                filter: [e_ngram_filter]\r\n            shingle_analyzer:\r\n                type : custom\r\n                tokenizer : standard\r\n                filter: [shingle_filter]\r\n            \r\n        filter: \r\n            shingle_filter:\r\n                type : shingle\r\n                min_shingle_size : 2\r\n                max_shingle_size : 5\r\n                output_unigrams : true\r\n            ngram_filter:\r\n                type : nGram\r\n                min_gram : 1\r\n                max_gram : 20\r\n            e_ngram_filter:\r\n                type : edgeNGram\r\n                min_gram : 1\r\n                max_gram : 20                \r\n            \r\n    mapping:\r\n        attachment:\r\n            ignore_errors: false\r\n\'\r\n\r\n# create the mapping for the index\'s documents\r\n$ curl -XPUT http://localhost:9200/attachment/document/_mapping?pretty -d \'\r\n{\r\n    "document" : {\r\n        "properties" : {\r\n            "file" : {\r\n                "type" : "attachment",\r\n                "path" : "full",\r\n                "fields" : {\r\n                    "file" : {\r\n                        "type" : "multi_field",\r\n                        "fields" : {\r\n                            "file" : {\r\n                                "type" : "string",\r\n                                "store" : true,\r\n                                "index" : "not_analyzed"\r\n                            },\r\n                            "shingle" : {\r\n                                "type" : "string",\r\n                                "store" : true,\r\n                                "analyzer" : "shingle_analyzer"\r\n                            },\r\n                            "ngram" : {\r\n                                "type" : "string",\r\n                                "store" : true,\r\n                                "analyzer" : "ngram_analyzer"\r\n                            },\r\n                            "e_ngram" : {\r\n                                "type" : "string",\r\n                                "store" : true,\r\n                                "analyzer" : "e_ngram_analyzer"\r\n                            }\r\n                        }\r\n                    },\r\n                    "author" : {\r\n                        "type" : "string",\r\n                        "store" : true\r\n                    },\r\n                    "title" : {\r\n                        "type" : "string",\r\n                        "store" : true\r\n                    },\r\n                    "name" : {\r\n                        "type" : "string",\r\n                        "store" : true\r\n                    },\r\n                    "date" : {\r\n                        "type" : "date",\r\n                        "store" : true,\r\n                        "format" : "dateOptionalTime"\r\n                    },\r\n                    "keywords" : {\r\n                        "type" : "string",\r\n                        "store" : true\r\n                    },\r\n                    "content_type" : {\r\n                        "type" : "string",\r\n                        "store" : true\r\n                    },\r\n                    "content_length" : {\r\n                        "type" : "integer",\r\n                        "store" : true\r\n                    }\r\n                }   \r\n            }\r\n        }\r\n    }\r\n}\r\n\r\n\'\r\n```\r\n\r\n### Adding document to the index\r\nAfter adding a document, we will see, that the defined *multi_field* fields for shingles and nGrams do NOT receive any data.\r\n\r\n```sh\r\n# Adding a document to the index, as needed by the Attachment plugin binary data as base64 encoded string\r\n$ curl -XPUT \'http://localhost:9200/attachment/document/1?pretty\' -d \'\r\n{\r\n    "file" : "VGhpcyBpcyBhIHRlc3QgZG9jdW1lbnQgdG8gZGVtb25zdHJhdGUgdGhlIGJlaGF2aW9yIG9mIHRoZSBBdHRhY2htZW50IHR5cGUgcGx1Z2luIAp3aGljaCBkb2Vzbid0IHBhc3MgdGhpcyBkb2N1bWVudCdzIGNvbnRlbnQgdG8gdGhlIGRlZmluZWQgbmVzdGVkIG11bHRpX2ZpZWxkIHBhcnRzIApmb3IgY3JlYXRpbmcgbkdyYW1zIGFuZCBzaGluZ2xlcy4="\r\n}\r\n\'\r\n\r\n# Searching for documents afterwards shows, that the multi_field fields are not receiving the data provided\r\n$ curl -XGET \'http://localhost:9200/attachment/_search?q=*&fields=*&pretty\'\r\n\r\n# Result:\r\n{\r\n  "took" : 48,  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "attachment",\r\n      "_type" : "document",\r\n      "_id" : "1",\r\n      "_score" : 1.0,\r\n      "fields" : {\r\n        "file.e_ngram" : [ "" ],\r\n        "file.content_length" : [ 200 ],\r\n        "file.ngram" : [ "" ],\r\n        "file.file" : [ "" ],\r\n        "file.content_type" : [ "text/plain; charset=ISO-8859-1" ],\r\n        "file.shingle" : [ "" ]\r\n      }\r\n    } ]\r\n  }\r\n}\r\n\r\n```\r\n\r\n### Problem in elasticsearch code\r\nI\'ve started investigations on this issue, as I could find discussions online about similar problems, but no solution. Digging into the depths of the elasticsearch code I\'ve detected the issue and built a workaround inside the Attachment plugin\'s code for my local needs which produced the desired result. From my point of view this should be fixed inside the elasticsearch code and I will try to explain how.\r\n\r\nFrom what I hopefully got right, there are two ways the **org.elasticsearch.index.mapper.ParseContext** may provide values to any subclass of **org.elasticsearch.index.mapper.core.AbstractFieldMapper<T>** for parsing:\r\n* accessing the next token from the provided JSON stream: **org.elasticsearch.index.mapper.ParseContext.parser().currentToken()**\r\n* checking for a so called \'external value\' to be used instead: **org.elasticsearch.index.mapper.ParseContext.externalValue()**\r\n\r\nInside the **org.elasticsearch.index.mapper.core.StringFieldMapper.parseCreateFieldForString(ParseContext, String, float)** method this can be seen, where the \'external value\' is tried to be **consumed** (emphasis intended) by calling *org.elasticsearch.index.mapper.ParseContext.externalValue()*. I am using the term \'consumed\' as this is literally what happens as the call to the *externalValue()* method sets a boolean flag to \'false\' which is actually used for checking for existence of such an \'external value\'. \r\n\r\nSo, this is where the problem resides and I overcame it by wrapping the *ParseContext* passed to the Attachment mapper plugin with my own implementation, that actually just delegates any call to the original context except for the *org.elasticsearch.index.mapper.ParseContext.externalValueSet()* method which I\'ve overwritten to not only check the boolean flag but also check for the \'external value\' to be not *null*. This way it is assured that as long as the *multi_field* fields are processed all of them get the content.\r\n\r\n\r\n### Desired output after fixing the code\r\nJust to make sure what the expected result should look like:\r\n\r\n```sh\r\n# result of the above document being indexed with my local changes, where we can see that the nGram and shingle fields properly get content\r\n{\r\n  "took" : 12,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "attachment",\r\n      "_type" : "document",\r\n      "_id" : "1",\r\n      "_score" : 1.0,\r\n      "fields" : {\r\n        "file.e_ngram" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \\nwhich doesn\'t pass this document\'s content to the defined nested multi_field parts \\nfor creating nGrams and shingles.\\n" ],\r\n        "file.content_length" : [ 200 ],\r\n        "file.ngram" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \\nwhich doesn\'t pass this document\'s content to the defined nested multi_field parts \\nfor creating nGrams and shingles.\\n" ],\r\n        "file.file" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \\nwhich doesn\'t pass this document\'s content to the defined nested multi_field parts \\nfor creating nGrams and shingles.\\n" ],\r\n        "file.content_type" : [ "text/plain; charset=ISO-8859-1" ],\r\n        "file.shingle" : [ "This is a test document to demonstrate the behavior of the Attachment type plugin \\nwhich doesn\'t pass this document\'s content to the defined nested multi_field parts \\nfor creating nGrams and shingles.\\n" ]\r\n      }\r\n    } ]\r\n  }\r\n}\r\n```\r\n\r\nHopefully my explanations are clear to you guys. As I am not totally sure which way one should solve this issue in order to avoid any side-effects I am relying on you to get this thing fixed.\r\n\r\nThanks in advance\r\nTom\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'
5401,'spinscale',"NPE when TransportClient times out\nElastic Search version: 1.0.0\r\nOperating System: Windows 7 Enterprise\r\nJava Version: 1.7.0_45\r\nServer setup:\r\n- Windows Server 2003\r\n- Java 1.7.0_45\r\n- Running Elastic Search as a service\r\n- 1 Node with 1 index and 5 shards with 1 replica \r\n\r\nHere is the original issue:\r\nhttps://groups.google.com/forum/#!msg/elasticsearch/QJx0nzVci1s/uqzE2YhKGJIJ\r\n\r\nWe are using a an instance of a Transport Client to connect to our Elastic Search server.  Since the instance is going to be living on a webserver, we need it to stay open so it is ready for whenever a user needs to perform a search, or data needs to be indexed. The problem is that if no operation is performed within an hour or so of the last operation on the transport client, it throws this error. The other scenario is if no operation is performed within 10-15 minutes of the Transport Client's last operation, and another operation is performed, it will throw the same error. \r\n\r\nIf this error is thrown, sometimes it recovers and continues the operation, but a majority of the time it does not do this at all. Here is the stack trace:\r\n\r\n```   \r\nMar 08, 2014 1:15:37 AM org.elasticsearch.client.transport\r\nINFO: [Elven] failed to get node info for [#transport#-1][WIN7-113-00726][inet[/159.140.213.87:9300]], disconnecting...\r\norg.elasticsearch.transport.RemoteTransportException: [Server_Dev1][inet[/159.140.213.87:9300]][cluster/nodes/info]\r\nCaused by: java.lang.NullPointerException\r\nat org.elasticsearch.http.HttpInfo.writeTo(HttpInfo.java:82)\r\nat org.elasticsearch.action.admin.cluster.node.info.NodeInfo.writeTo(NodeInfo.java:301)\r\nat org.elasticsearch.action.admin.cluster.node.info.NodesInfoResponse.writeTo(NodesInfoResponse.java:63)\r\nat org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:244)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$TransportHandler$1.onResponse(TransportNodesOperationAction.java:239)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.finishHim(TransportNodesOperationAction.java:225)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.onOperation(TransportNodesOperationAction.java:200)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction.access$900(TransportNodesOperationAction.java:102)\r\nat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\nat java.lang.Thread.run(Thread.java:744)\r\n```"
5400,'chilling','Distance Script\nUpdated docs for distance scripting and added\r\nmissing geohash distance functions\r\nCloses #5397'
5396,'s1monw',"Speed up phrase suggestion scoring\nTwo changes:\r\n1.  In the StupidBackoffScorer only look for the trigram if there is a bigram.\r\n2.  Cache the frequencies in WordScorer rather so we don't look them up\r\nagain and again and again.\r\n\r\nThis provides a pretty substantial speedup when there are many candidates.\r\n\r\nCloses #5395"
5394,'javanna',"[TEST] Randomized number of replicas used for indices created during tests\nIntroduced two levels of randomization for the number of replicas when running tests:\r\n\r\n1) through the existing random index template, which now sets a random number of replicas that can either be 0 or 1 that is shared across all the indices created in the same test method unless overwritten\r\n\r\n2)  through createIndex and prepareCreate methods, between 0 and the number of data nodes available, similar to what happens using the indexSettings method, which changes for every createIndex or prepareCreate unless overwritten (overwrites index template for what concerns the number of replicas)\r\n\r\nAdded the following facilities to deal with the random number of replicas:\r\n\r\n- made it possible to retrieve how many data nodes are available in the `TestCluster`\r\n- added common methods similar to indexSettings, to be used in combination with createIndex and prepareCreate method and explicitly control the second level of randomization: numberOfReplicas, minimumNumberOfReplicas and maximumNumberOfReplicas\r\n\r\nTests that specified the number of replicas have been reviewed:\r\n- removed manual replicas randomization where present, replaced with ordinary one that's now available\r\n- adapted tests that didn't need a specific number of replicas to the new random behaviour\r\n- also done some more cleanup, used common methods like assertAcked, ensureGreen, refresh, flush and refreshAndFlush where possible"
5392,'jpountz','Corrected issue with throttle type setting not respected upon updates\nThe setting "index.store.throttle.type" is not able to be updated on a live running instance using a PUT request to _settings'
5384,'spinscale',"[DOCS] Fix minor error in cluster stats example\n`?human` doesn't pretty print for me (Elasticsearch 1.0.1)"
5366,'javanna',"Add configured thread pool sizes to _cat/thread_pool\n`_cat/thread_pool` has a `*.size` column for each thread pool.  That maps to `ThreadPoolStats.getThreads()`, which, if there hasn't been any activity in the thread pool, makes the number misleading for checking your settings.\r\n\r\n    % curl localhost:9200/_cat/thread_pool\\?h=h,i,bulk.size\\&v\r\n    h          i            bulk.size\r\n    iota.local 192.168.1.68         0\r\n\r\nWe should add `*.max` and `*.min` columns mapped respectively to `ThreadPool$Info.getMax()` and `ThreadPool$Info.getMin()` for each pool."
5364,'martijnvg','"include_in_all": false not working for nested multi-field mappings\nI\'m running Elasticsearch 1.0.1 using the following $ES_HOME/config/default-mapping.json:\r\n\r\n    {\r\n      "_default_": {\r\n        "properties": {\r\n          "foo": {\r\n            "type": "nested",\r\n            "include_in_all": false,\r\n            "properties": {\r\n              "bar": {\r\n                "type": "string",\r\n                "index": "not_analyzed",\r\n                "include_in_all": false,\r\n                "fields": {\r\n                  "lower": {\r\n                    "analyzer": "standard",\r\n                    "type": "string"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nWhen I post the following, the include_in_all does not seem to be recognized at any level:\r\n\r\n     {\r\n       "foo": {\r\n         "bar": "Elasticsearch rules!"\r\n       }\r\n     }\r\n\r\nThe resulting metadata is the following:\r\n\r\n    {\r\n        state: open\r\n        settings: {\r\n            index: {\r\n                uuid: S2hmo2d3SGWFP51PXs6XbA\r\n                number_of_replicas: 0\r\n                number_of_shards: 1\r\n                version: {\r\n                    created: 1000199\r\n                }\r\n            }\r\n        }\r\n        mappings: {\r\n            foobar: {\r\n                properties: {\r\n                   foo: {\r\n                       include_in_all: false\r\n                        properties: {\r\n                            bar: {\r\n                                include_in_all: false\r\n                                index: not_analyzed\r\n                                type: string\r\n                                fields: {\r\n                                    lower: {\r\n                                        analyzer: standard\r\n                                        type: string\r\n                                    }\r\n                                }\r\n                            }\r\n                        }\r\n                        type: nested\r\n                    }\r\n                }\r\n            }\r\n        }\r\n        aliases: [ ]\r\n    }\r\n\r\nI verified that _all does include the "elasticsearch rules" via the following aggregation search:\r\n\r\n    {\r\n      "aggs": {\r\n        "foobar": {\r\n          "terms": {\r\n            "field": "_all"\r\n          }\r\n        }\r\n      },\r\n      "size": 0\r\n    }\r\n\r\nThe aggs search result is the following:\r\n\r\n    {\r\n        took: 36\r\n        timed_out: false\r\n        _shards: {\r\n            total: 1\r\n            successful: 1\r\n            failed: 0\r\n        }\r\n        hits: {\r\n            total: 1\r\n            max_score: 0\r\n            hits: [ ]\r\n        }\r\n        aggregations: {\r\n            foo: {\r\n                buckets: [\r\n                    {\r\n                        key: elasticsearch\r\n                        doc_count: 1\r\n                    }\r\n                    {\r\n                        key: rule\r\n                        doc_count: 1\r\n                    }\r\n                ]\r\n            }\r\n        }\r\n    }\r\n\r\nSo, _all should be empty, but it is not.\r\n'
5363,'imotov','terms filter returning wrong results, maybe cache issue\nI\'m seeing a strange behavior using elasticsearch 0.90.12.\r\n\r\nThe index contains 180269 docs\r\n\r\nI try to perform the following query:\r\n\r\n    {\'query\':\r\n            {\'filtered\':\r\n                    {\'filter\': {\r\n                            \'terms\': {\'id\': [188915, 189067, 183817, 188969, 188425]}\r\n                    },\r\n                    \'query\': {\'match_all\': {}}\r\n                    }\r\n            }\r\n    }\r\n\r\nIt returns more or less randomly the following list of doc ids: \r\n\r\n[183817, 188915, 188969, 189067, 188231]\r\nor\r\n[183817, 188915, 188969, 189067, 188425]\r\n\r\nAs you can see the first return value is entirely unexpected. If i add "_cache: false" to the terms filter it consistently returns the correct value. It also works as expected when i choose a bool execution strategy. It also work if i slightly change the list of requested ids (remove one, add one), only that specific list triggers the problem.\r\n\r\nI can\'t reduce the problem locally, it seems to depend on the exact state of that index and that exact lis of ID.\r\n\r\nCould it be some kind of cache collision because of the hashing algorithm used to generate the cache key, or the bitset ?\r\n\r\nIn any case this could lead to data leak since i can\'t trust that my filter will work as expected.. Thanks for any advice on how to debug this properly!'
5362,'spinscale','Add integration client support for Spring Data Elasticsearch\nI added this earlier but may be with new site this was removed.\r\n\r\ncan you guys add it back ?\r\n\r\nThanks \r\nMohsin'
5358,'spinscale','[docs] Updating scripting docs for geo functions\nThe docs for the geo functions are currently out of sync with the implementation in Elasticsearch. I have removed the geohash functions, added the factor functions, and updated the existing geo distance functions.'
5357,'spinscale','RemoteTransportException when trying to access :9200/_nodes\nI have a 3 node ES cluster. I just upgraded from 0.90.11 to 1.0.1 and started experiencing these exceptions. When I try to access ```curl \'http://server:9200/_nodes?pretty=true\'``` on any of my nodes I get this exception in ES logs:\r\n```\r\n[2014-03-06 03:52:23,848][DEBUG][action.admin.cluster.node.info] [logserver3-la] failed to execute on node [iPvGOBIQTuOV_YhNAmLAUg]\r\norg.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]\r\nCaused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:148)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:108)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:318)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 7711\r\n\tat org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)\r\n\tat org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)\r\n\tat org.elasticsearch.common.io.stream.StreamInput.readString(StreamInput.java:276)\r\n\tat org.elasticsearch.common.io.stream.HandlesStreamInput.readString(HandlesStreamInput.java:61)\r\n\tat org.elasticsearch.threadpool.ThreadPool$Info.readFrom(ThreadPool.java:597)\r\n\tat org.elasticsearch.threadpool.ThreadPoolInfo.readFrom(ThreadPoolInfo.java:65)\r\n\tat org.elasticsearch.threadpool.ThreadPoolInfo.readThreadPoolInfo(ThreadPoolInfo.java:55)\r\n\tat org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:224)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:146)\r\n\t... 23 more\r\n```\r\n\r\nI created a gist of the output of _nodes that I do get here:\r\nhttps://gist.github.com/daledude/c6c0fb018d06d1e45a62\r\n\r\nThe exception in the logs is the same for all nodes. Using ES 1.0.1 and Java HotSpot(TM) 64-Bit Server VM 1.7.0_25 on all nodes.\r\n\r\nThis is my config which is the same for all nodes except the hosts, rack, zone:\r\n```\r\ncluster.name: mycluster\r\nnode.name: "logserver1-chi"\r\nnode.rack: chi1\r\nnode.zone: chi\r\nnode.master: true\r\nnode.data: true\r\n\r\nindex.number_of_replicas: 0\r\n\r\n# cluster discovery\r\ndiscovery.zen.fd.ping_interval: 15s\r\ndiscovery.zen.fd.ping_timeout: 60s\r\ndiscovery.zen.fd.ping_retries: 5\r\ndiscovery.zen.ping.multicast.enabled: false\r\ndiscovery.zen.ping.unicast.hosts: ["logserver3-la.domain.com", "logserver2.domain.com"]\r\ncluster.routing.allocation.awareness.attributes: zone\r\n\r\nindices.memory.index_buffer_size: 20%\r\nindex.translog.flush_threshold_ops: 50000\r\nindices.fielddata.cache.size: 30%\r\nbootstrap.mlockall: true\r\n\r\nthreadpool.search.type: fixed\r\nthreadpool.search.size: 20\r\nthreadpool.search.queue_size: -1\r\n\r\nthreadpool.index.type: fixed\r\nthreadpool.index.size: 60\r\nthreadpool.index.queue_size: -1\r\n\r\naction.disable_delete_all_indices: false\r\n```\r\n'
5356,'javanna',"[TEST] Randomized number of shards used for indices created during tests\nIntroduced two levels of randomization for the number of shards (between 1 and 10) when running tests:\r\n\r\n1) through the existing random index template, which now sets a random number of shards that is shared across all the indices created in the same test method unless overwritten\r\n\r\n2) through `createIndex` and `prepareCreate` methods, similar to what happens using the `indexSettings` method, which changes for every `createIndex` or `prepareCreate` unless overwritten (overwrites index template for what concerns the number of shards)\r\n\r\nAdded the following facilities to deal with the random number of shards:\r\n- `getNumShards` to retrieve the number of shards of a given existing index, useful when doing comparisons based on the number of shards and we can avoid specifying a static number. The method returns an object containing the number of primaries, number of replicas and the expected total number of shards for the existing index\r\n\r\n- added `assertFailures` that checks that a shard failure happened during a search request, either partial failure or total (all shards failed). Checks also the error code and the error message related to the failure. This is needed as without knowing the number of shards upfront, when simulating errors we can run into either partial (search returns partial results and failures) or total failures (search returns an error)\r\n\r\n- added common methods similar to `indexSettings`, to be used in combination with `createIndex` and `prepareCreate` method and explicitly control the second level of randomization: `numberOfShards`, `minimumNumberOfShards` and `maximumNumberOfShards`. Added also `numberOfReplicas` despite the number of replicas is not randomized (default not specified but can be overwritten by tests)\r\n\r\nTests that specified the number of shards have been reviewed:\r\n- removed number_of_shards in node settings, ignored anyway as it would be overwritten by both mechanisms above\r\n- remove specific number of shards when not needed\r\n- removed manual shards randomization where present, replaced with ordinary one that's now available\r\n- adapted tests that didn't need a specific number of shards to the new random behaviour\r\n- fixed a couple of test bugs (e.g. 3 levels parent child test could only work on a single shard as the routing key used for grand-children wasn't correct)\r\n- also done some cleanup, shared code through shard size facets and aggs tests and used common methods like `assertAcked`, `ensureGreen`, `refresh`, `flush` and `refreshAndFlush` where possible"
5355,'spinscale','[docs] Fix a typo in the reference doc. SuSe -> SUSE.\n`SUSE`, as a Linux distribution, reads better if all upper case. Please refer to http://en.wikipedia.org/wiki/SUSE_Linux_distributions for details. \r\n\r\nfixes #5354'
5354,'spinscale',"[doc] Fix a typo in reference doc.\nIn `docs/reference/setup/as-a-service.asciidoc`, there's a minor typo : Distributions like `SuSe` do not use... I think `SUSE` reads better, since `SUSE`, as a Linux distribution, is never lower cased. http://en.wikipedia.org/wiki/SUSE_Linux_distributions"
5323,'jpountz','Percentiles aggregation\nA `percentiles` aggregation would allow to compute (approximate) values of arbitrary percentiles based on the [t-digest](https://github.com/tdunning/t-digest) algorithm. Computing exact percentiles is not reasonably feasible as it would require shards to stream all values to the node that coordinates search execution, which could be gigabytes on a high-cardinality field. On the other hand, t-digest allows to trade accuracy for memory by trying to summarize the set of values that have been accumulated with interesting properties/features:\r\n - compression is configurable, meaning that if you can configure it to have better accuracy at the cost of a higher memory usage,\r\n - accuracy is excellent for extreme percentiles,\r\n - percentiles are going to be accurate if few values were accumulated.\r\n\r\nExample:\r\n```json\r\n{\r\n    "aggs" : {\r\n        "load_time_outlier" : {\r\n            "percentiles" : {\r\n                "field" : "load_time",\r\n                "percents" : [95, 99, 99.9] \r\n            }\r\n        }\r\n    }\r\n}\r\n```'
5309,'spinscale','doc updates for date histogram interval \ncloses  #5308 '
5308,'spinscale','doc update: date histogram interval corrections\nAs per DateHistogramParser ctor \r\n-month interval in aggregations\r\n-second interval in facets\r\n\r\nLet me see if I can submit doc changes request'
5305,'javanna',"Logging configuration is not documented\nThere is almost no documentation for the logging.yml configuration file. Changing the logging level is simple enough -- but as someone unfamiliar with log4j, I have absolutely no idea what the options in the logging.yml file do or how to configure elasticsearch to not rotate it's own logs so that I can use a standard system service to manage log rotation.  I would rather not have to read the source code in order to understand how to configure the logging mechanism."
5303,'jpountz',"Terms aggregations: don't use ordinals on high-cardinality fields\nOrdinals help performance a lot on low-cardinality fields since they help avoid costly string comparisons. However, when the field has a high cardinality, bytes need to be compared anyway and ordinals only add overhead.\r\n\r\nWe should probably have a basic heuristic to disable ordinals on fields that have a very high cardinality.\r\n\r\nThis would of course only apply to defaults and it would still be possible to use ordinals on high-cardinality fields by passing `execution_hint=ordinals` to the aggregator parser."
5299,'jpountz','BigArrays: reuse pages more agressively\nCurrent implementation starts using paged arrays when the size of the array is\r\ngreater than the size of a page. This means that if an array has a length of\r\n${page_size}+1, it will use two pages, wasting about half of the memory.\r\n\r\nI think this behavior should be symetric when the size is less than the page\r\nsize but higher than half of the page size. This means that we would start\r\nreusing memory at ${page_size}/2 instead of ${page_size}.'
5293,'electrical','Added SLES11 SP3 Init Script\nSUSE Enterprise Server uses still the old init.d and the "daemon" function isn\'t avaible. There for is "startproc" to start processes as daemons. The "lockfile"-mechanism isn\'t necessary since rh_status manages that there aren\'t multple instance of ES.\r\n\r\nTested on Suse Enterprise Server 11 ServicePack 3'
5292,'s1monw','Edit distance allowed values for fuzzy_like_this query\nThe doc here says http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/common-options.html#fuzziness\r\n\r\n> Note: in all APIs except for the Fuzzy Like This Query, the maximum allowed edit distance is 2.\r\n\r\nHowever when one tries to do this in Java API:\r\n```\r\nfuzzyLikeThisQuery(fieldName).likeText(searchString).fuzziness(Fuzziness.fromEdits(4));\r\n```\r\n\r\nAn exception is thrown from the Fuzziness.fromEdits(4) method call\r\n>  org.elasticsearch.ElasticsearchIllegalArgumentException: Valid edit distances are [0, 1, 2] but was [4]\r\n\r\nWe thought that maybe this is a bug in the Java API and tried the same thing with the REST API. The query was this:\r\n```\r\n{\r\n        "flt": {\r\n            "fields": [\r\n            "comment"\r\n        ],\r\n        "like_text": "FFFdfds",\r\n        "fuzziness": "4"\r\n    }\r\n}\r\n```\r\nthe result was this:\r\n> ElasticsearchIllegalArgumentException[Can\'t get similarity from fuzziness [4]]; }]\r\n\r\nBut the query works fro values 1 and 2.\r\nSo I guess either the documentation is mistaken or the implementation. We were previously using the edit distances of up to 4-5 characters. After the update we\'re kind of lost for now :)'
5290,'imotov','Make sure that currently running snapshot is cancelled before closing the shard\n'
5281,'javanna',"Java API does not have a way to set global highlighting settings\nThere is no method in the Java API on SearchRequestBuilder to set the global highlighting options for 'fragment_size' or 'number_of_fragments'.\r\n\r\nThe documentation lists this ability in the 0.90.X series as well as 1.0.X series:\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/0.90/search-request-highlighting.html#highlighting-settings\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-highlighting.html#highlighting-settings\r\n\r\nI am working around this by using the method:\r\n\r\naddHighlightedField(name, fragmentSize, numberOfFragments)\r\n\r\nBut that is not ideal since my fragmentSize and numberOfFragments doesn't change on a per field basis.\r\n\r\nPlease see: https://groups.google.com/forum/#!topic/elasticsearch/a4K8yfMqim4"
5280,'javanna','Add human readable JVM start_time and process refresh_interval\nIn the JVM and process display, the ``human`` flag sets add human-readable heap data, but does not include human-readable start_time and refresh_interval.  Adding human-readable data here would be helpful.'
5279,'s1monw',"Upgrade Spatial4j to 0.4.1 and JTS to 1.13\nSpatial4j 0.4.1 is required by Lucene-spatial 4.7.0's new SerializedDVStrategy, plus it has various bug fixes and performance improvements (especially index() use of JTS PreparedGeometry).\r\n\r\nJTS 1.13 is JTS's latest version which has various improvements.  Thread-safety of PreparedGeometry comes to mind.\r\n\r\n(I'm working on these things right now on my fork)"
5278,'spinscale','Change sentence fragments into headings\nAdd a whitespace to make these separate paragraphs rather than repetitive sentence fragments.'
5276,'dakrone','Add circuit breaker functionality to parent/child id field data cache\nNow that the parent/child id cache is part of the field data instead of a dedicated cache, we should add the circuit breaker functionality to it to prevent OOMEs when it is loaded.'
5275,'spinscale',"[docs] Attempt to reword clear-scroll sentence\nThis may not be the best fix but the sentence didn't read correctly the way it was.\r\nChange it as you see fit."
5272,'javanna','Minor doc issue in docs/reference/query-dsl/queries/range-query.asciidoc\ns/bool/boost/ in the line\r\n\r\n    `boost`:: \tSets the bool value of the query, defaults to `1.0`\r\n'
5267,'s1monw','Make master aka. Elasticsearch 2.0 use Java 1.7 or higher\nIt seems Java 8 is months out and `1.6` has reached EOL. We should move `master` to `1.7` only. `1.x` will still run on `1.6` but once `2.0` is released Elasticsearch will only run on Java 7 or higher.'
5256,'javanna','Possible  issue with dynamic mapping in Elasticsearch\nElasticSearch 1.0.\r\n\r\nWhen using dynamic_template described here:\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-root-object-type.html#_dynamic_templates\r\n\r\nthe MapperParsingException occurs. The gist with an example identical as in documentation is here:\r\n\r\nhttps://gist.github.com/nnegativ/9213854\r\n'
5246,'s1monw',"MultiMatchQuery fails to highlight with new cross field mode\nthe new BlendedTermQuery doesn't implement:\r\n```\r\n@Override\r\npublic void extractTerms(Set<Term> terms) {\r\n //...\r\n}\r\n```\r\n\r\nwhich causes an unsupported op exception to be thrown. This code has not yet been released."
5243,'jpountz',"Force single-segment merges\nAlthough not recommended in general, the `optimize` API currently has two main use-cases:\r\n - making read-only shards slightly more efficient by merging them into a single segment,\r\n - reclaiming space by expunging deleted documents.\r\n\r\nBut forcing merges can also be useful to force a codec change, for example:\r\n - if you want to make sure to not have Lucene 3.x segments anymore\r\n - if you are using an unsupported codec and would like to switch back to the default one before an upgrade\r\n\r\nHowever the optimize API won't do anything if there is already a single segment in the shard. So it would be useful to be able to force a merge, no matter how many segments there are in a shard.\r\n"
5242,'imotov','The delete snapshot operation on a running snapshot may take a long time on large shards\nThe delete snapshot operation on a running snapshot should cancel the snapshot execution. However, it interrupts the snapshot only when currently running snapshot files are completely copied, which might take a long time for large files. '
5240,'imotov','The get snapshot operation can take a long time when large shards are snapshotted\nThe get snapshot operation is using the same blob store thread pool to obtain snapshot information. If all thread in the pool are used to snapshot large shard segment files, it might appear that the get snapshot operation hangs. '
5238,'electrical','RPM requires jre 1.6 even it 1.7 is installed\nHello,\r\n\r\nif i want to install the RPM i have to install Java 1.6 even if 1.7 is already installed and the JAR file works with Java 1.7.\r\n\r\n```\r\nroot@logstash01 ~]# /bin/rpm -i /opt/logstash/swdl/logstash-1.3.3-1_centos.noarch.rpm \r\nerror: Failed dependencies:\r\n\tjre >= 1.6.0 is needed by logstash-1.3.3-1_centos.noarch\r\n\r\nroot@logstash01 ~]# java -version\r\njava version "1.7.0_40"\r\nJava(TM) SE Runtime Environment (build 1.7.0_40-b43)\r\nJava HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)\r\n```\r\n'
5236,'jpountz','Terms aggregations order wrong when sorting NaN\'s\nI have a strong believe there is an issue in the sorting of term aggregations.\r\n\r\nHave a look [here](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L215). If we look at the comment above it indicates that it would like to push NaN\'s to the bottom of the list (which would be the correct behaviour according to me). But when I test this out it does not work\r\n\r\nLoading the following test data:\r\n\r\n    $ curl -XDELETE \'localhost:9200/sorting?pretty=true\'\r\n    $ curl -XPOST \'localhost:9200/_bulk?pretty=true\' -d \'\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "a", a:1, b:1 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "a", a:2, b:4 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "a", a:3, b:9 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "b", a:4, b:16 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "b", a:5, b:25, c: 42 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "b", a:6, b:36, c: 50 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "c", a:7, b:49 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "c", a:8, b:64 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "c", a:9, b:81 }\r\n    { index: { _index: "sorting", _type: "object" }}\r\n    { t: "c", a:10, c:100 }\r\n    \'\r\n\r\nYou can start running some aggregations:\r\n\r\n    $ curl \'localhost:9200/sorting/_search?pretty=true\' -d \'\r\n    {\r\n      "size": 0,\r\n      "aggs": {\r\n        "t": {\r\n          "terms": {\r\n            "field": "t",\r\n            "order": {\r\n              "aStats.min": "desc"\r\n            }\r\n          },\r\n          "aggs": {\r\n            "aStats": {\r\n              "stats": {\r\n                "field": "c"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \'\r\n\r\nWhen we run this aggregations if turns out that a term without value\'s in the `c` field ends in the top.\r\n\r\n    {\r\n      "aggregations": {\r\n        "t": {\r\n          "buckets": [\r\n            {\r\n              "key": "a",\r\n              "doc_count": 3,\r\n              "aStats": {\r\n                "count": 0,\r\n                "min": null,\r\n                "max": null,\r\n                "avg": null,\r\n                "sum": null\r\n              }\r\n            },\r\n            {\r\n              "key": "c",\r\n              "doc_count": 4,\r\n              "aStats": {\r\n                "count": 1,\r\n                "min": 100,\r\n                "max": 100,\r\n                "avg": 100,\r\n                "sum": 100\r\n              }\r\n            },\r\n            {\r\n              "key": "b",\r\n              "doc_count": 3,\r\n              "aStats": {\r\n                "count": 2,\r\n                "min": 42,\r\n                "max": 50,\r\n                "avg": 46,\r\n                "sum": 92\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n\r\nWhen running the same analysis with facetting you would see that the term without the values in `c` are pushed to the bottom of the list\r\n\r\n    $ curl \'localhost:9200/sorting/_search?pretty=true\' -d \'\r\n    {\r\n      "size": 0,\r\n      "facets": {\r\n        "t": {\r\n          "terms_stats": {\r\n            "key_field": "t",\r\n            "value_field": "c",\r\n            "order": "min"\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \'\r\n\r\nThis looks to me as correct behaviour.\r\n\r\nI have done some research on why this is happening, and in fact the if statement referenced above is comparing the aggregated metric to `Double.NaN`. In java [it turns out](http://stackoverflow.com/questions/8819738/why-does-double-nan-double-nan-return-false) that NaN is not equal to NaN :), luckily the guys working at java thought of this and added a function to check for NaN values `Double.isNaN`. Changing the line accordingly makes the return statement next work since it is skipped always at the moment. But...\r\n\r\nOn line [216](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L216) it returns 1 or -1 depending on the ordering provided. This would result in NaN floating to the top of the list when ordering descending. Which has the strange effect that NaN\'s would be at the top of the list. My believe is that is should always `return 1` if `v1` is `NaN`.\r\n\r\nLast part of the bug is that only `v1` is being checked to be `NaN`. You would also need to check `v2` for being `NaN` and `return -1`(!) if so. This would, as the comment suggest always push \'NaN\' values to the bottom of the sorted list. This resembles the most to how facets sort at the moment.\r\n\r\nConcrete effects of this bug is that we are not able to use aggregations for a table like view (which is the main benefit of using aggs, since you can get multiple columns at once) to show terms sorted descending on the avg of a sparse field we have in our collection of documents which was able when using facets.\r\n\r\nPlease have a look, and note that this error is made on two spots in the same file. ([second spot is on line 230](https://github.com/elasticsearch/elasticsearch/blob/ad8a482d19/src/main/java/org/elasticsearch/search/aggregations/bucket/terms/InternalOrder.java#L230)).\r\n\r\nI tested (not via the test suite, but by hand) out a fix locally and that seems works for us. If you would like to share my patch by opening a pull request I could, although I need to check my code against your guidelines, and add some automated tests :)'
5235,'uboness','feature request : aggregation reducer functions (aka buffer functions / window functions)\ni would like to do group-by type aggregations e.g. given documents :\r\n\r\n```\r\n{name: "foo", location: "london", revenue: 100}\r\n{name: "foo", location: "paris", revenue: 500}\r\n{name: "bar", location: "sydney", revenue: 15}\r\n{name: "bar", location: "new york", revenue: 23}\r\n```\r\n\r\nto produce output by grouping on name, collecting locations and summing revenues: \r\n\r\n```\r\n{name: "foo", location: ["london","paris"], revenue: 600}\r\n{name: "bar", location: ["sydney", "new york"], revenue: 38}\r\n```\r\n\r\nthis is difficult with the currently available aggregations\r\n\r\nto my naive eye it seems like it might be relatively easy to create a metric aggregation which operates very similarly to cascalog\'s defparallelagg https://github.com/nathanmarz/cascalog/wiki/Guide-to-custom-operations#wiki-aggregators\r\n\r\nso the metric aggregation would define two scripts : one to initialise an aggregate container given a document, and another to combine aggregate containers\r\n\r\ne.g.\r\n\r\n```\r\n{aggs:\r\n  {name: \r\n    {terms: {field: "name", size: 0},\r\n     aggs: \r\n       {location_revenue: \r\n         {reduce: {init: "{name: doc.name, \r\n                           location: [doc.location], \r\n                           revenue: doc.revenue}", \r\n                   combine: "{name: doc1.name, \r\n                              location: doc1.location.concat(doc2.location), \r\n                              revenue: doc1.revenue+doc2.revenue}"}}}}}}\r\n```'
5234,'uboness',"Init script should allow the user to investigate stdout and stderr if initializing fails\nI was trying to test some changes on a virtual machine that didn't have enough ram to start Elasticsearch in the configuration I had it set in but didn't see the error messages because the init script just died:\r\n```\r\n * Starting ElasticSearch Server\r\n```\r\nStarting Elasticsearch manually using the same configuration spat out the helpful:\r\n```\r\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x000000077ad30000, 2060255232, 0) failed; error='Cannot allocate memory' (errno=12)\r\n#\r\n# There is insufficient memory for the Java Runtime Environment to continue.\r\n# Native memory allocation (malloc) failed to allocate 2060255232 bytes for committing reserved memory.\r\n# An error report file with more information is saved as:\r\n# /tmp/hs_err_pid8570.log\r\n```\r\nIt'd be nice to be able to read that message if it fails to start via the init script."
5233,'s1monw','Remove nanoTime in global cluster randomization.\nRemove nanoTime in global cluster randomization in favor of deriving the seed from the main master seed.\r\n\r\nThe problem here is that if you don\'t give cluster\'s seed then test times\r\nfluctuate oddly, even for a fixed -Dtests.seed=... This shouldn\'t be the\r\ncase -- ideally, the test ran with the same master seed should reproduce\r\npretty much with the same execution time (and internal logic, obviously).\r\n\r\nFrom the code point of view "global" variables are indeed a problem\r\nbecause JUnit has no notion of before-suite hooks. And RandomizedRunner\r\ndoesn\'t support context accesses in static class initializers (this is\r\nintentional because there is no way to determine when such initializers\r\nwill be executed). A workaround is to move such static global variables to\r\nlazily-initialized methods and invoke them (once) in @BeforeClass hooks.\r\n\r\nI\'ve changed the code slightly so that nanoTime is *not* used if\r\ntests.cluster_seed is not provided. Instead, tests.cluster_seed becomes\r\nthe current master seed. This will work both from IDEs and from Maven. I\r\nfelt tempted to remove the entire tests.cluster_seed layer (because any\r\nstatic seed should/ could be derived from the master seed) but I don\'t\r\nknow what you guys are doing with it so I leave this decision to a\r\nfollow-up commit.'
5228,'drewr','Minor improvements to Table class and add tests\nThis pull request includes the following changes:\r\n- Replace `ElasticsearchIllegalArgumentException` with `ElasticsearchIllegalStateException` which seems more appropriate.\r\n- Add a few more preliminary checks to some methods. Some may be too pedantic and may not worth it. Feel free to remove those\r\n  if you think the little overhead is not justified.\r\n- Add unit tests for the `Table` class.'
5225,'martijnvg','Make sure get field mapping request is executed on node hosting the index\nPR for #5177'
5224,'uboness',"Create empty buckets in date_/histogram aggregation at the edges, beyond the value space of the data \nThis issue exists for a long time since the date_histogram facet, it can be circumvented in some situations with `min_doc_count` but it don't work in all cases.\r\n\r\nElasticSearch should fill the gaps for filter/query ranges or at least, add an option on date_histogram to enable this.\r\n"
5223,'javanna','Made SearchContextHighlight.Field class immutable to prevent from unwanted updates\nA Field instance can map to multiple actual fields when using wildcard expressions. Each actual field should use the proper highlighter depending on the available data structure (e.g. term_vectors), while we currently select the highlighter for the first field and we keep using the same for all the fields that match the wildcard expression.\r\n\r\nModified also how the PercolateContext sets the forceSource option, in a global manner now rather than per field.\r\n\r\nCloses #5175'
5221,'javanna','Highlighting on a wildcard field name causes the wildcard expression to be returned rather than the actual field name\nWildcards expressions are supported to specify which fields need to be highlighted. When using a wildcard expression, in some cases the highlighted fragments generated by the fast vector highlighter contain the wildcard expression as field name instead of the actual field name.\r\n\r\nThis happens also with the other when returning errors like:\r\n- "the field [field*] should be indexed with positions and offsets in the postings list to be used with postings highlighter"\r\n- "the field [field*] should be indexed with term vector with position offsets to be used with fast vector highlighter"\r\n- "source is forced for field [field*] but type [type1] has disabled _source"\r\n\r\nIn all the above cases, the actual field name should be returned (e.g. `field1` rather than `field*`)\r\n\r\nRelates to #5175 .'
5220,'javanna',"forceSource highlighting field option doesn't have any effect when set using the Java API\nThe `forceSource` highlighting option can either be specified for all the fields or for specific fields. However, changing the per-field option through Java API doesn't have any effect."
5218,'colings86','Geo: Geo bounding box filter returning no results\nMy search:\r\n```json\r\n{\r\n  "filter": {\r\n    "geo_bounding_box": {\r\n      "location": {\r\n        "top_left": {\r\n          "lat": 90,\r\n          "lon": -180\r\n        },\r\n        "bottom_right": {\r\n          "lat": -90,\r\n          "lon": 180\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nIt returns no results.\r\n\r\nIf I change a value, it works.'
5217,'hhoffstaette','Recycler: better lifecycle control for pooled instances\n Recycler: better lifecycle control for pooled instances\r\n\r\nChanges & additional test for all changes. Successfully ran full test suite in both eclipse and mvn. #5214'
5216,'uboness',"Fixes two minor typos.\nFixes to minor issues Aggregations' docs."
5214,'hhoffstaette','Recycler: better lifecycle control for pooled instances\nThe default DequeueRecycler.close() currently simply clears its internal queue and drops all instances on the floor for GC. During work for issue #5159 I found that this can be improved:\r\n\r\n* closing a recycler should properly dispose of the pooled instances via the factory\r\n* the factory currently conflates the functionality of per-instance "clean up & return to pool" and "destroy instance" behind the (according to Adrien badly named) cleanup() callback method.\r\n\r\nSo I suggest that we:\r\n\r\n* rename Recycler.C.clear(T) to recycle(T)\r\n* add a Recycler.C.destroy(T) to indicate that this particular instance needs to die\r\n* destroy() can then also be used when the recycler wants to kill off excess instances (on return).\r\n\r\nWhile we\'re renaming things I\'d also like to vote to rename the Recycler.C and Recycler.V interfaces to something more descriptive. :)\r\n'
5212,'imotov',"Restore of an existing index using rename doesn't completly open the index after restore\n\r\nTo reproduce: \r\n- snapshot an index A\r\n- create index B with the same number of shards as A\r\n- close index B\r\n- restore A while renaming it to B\r\n- search index B\r\n\r\nObserved behavior:\r\n- the search fails with `ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]` error\r\n\r\nExpected behavior:\r\n- the search should work"
5210,'imotov','Restore of an existing index doesn’t restore mappings and settings\nTo reproduce: \r\n- snapshot an index\r\n- modify mappings and settings of the index\r\n- close the index\r\n- restore the index\r\n\r\nObserved behavior:\r\n- the mappings and settings are not reverted back to the original (snapshotted) state\r\n\r\nExpected behavior:\r\n- the mappings and settings should match the state in the snapshot\r\n'
5208,'dakrone','Refactor SimpleQueryParser settings into separate Settings class, add "lenient" option\nFixes #5011'
5207,'jpountz','Fix possible exception in toCamelCase method\n'
5206,'jpountz','Remove useless URL instantiation\n'
5201,'jpountz',"Work around Lucene 3.x segments' costly RAM usage estimations\nThis is a follow-up to [LUCENE-5462](https://issues.apache.org/jira/browse/LUCENE-5462), Lucene 3.x segments happen to be very costly when it comes to memory usage estimations and this can make the stats API use too much CPU and memory.\r\n\r\nThis will be fixed in Lucene 4.7 but until then we should disable memory usage estimation on 3.x segments."
5200,'dadoonet','[TEST] Plugins: wait for REST Service to start\nWhen running tests for site plugins, it could happen that the REST Service is not fully started and not ready immediately to serve HTTP requests.\r\nIt gives `503 Service Unavailable` error in that case.\r\n\r\nThis patch will gives 5 seconds before failing the test.'
5195,'dadoonet',"NPE in PluginsService when starting elasticsearch with a wrong user\nWhen starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.\r\n\r\nTo reproduce:\r\n\r\n* create a plugins directory with `rwx` rights for root user only\r\n* launch elasticsearch from another account (elasticsearch for example)\r\n\r\nIt was supposed to be fixed with #4186, but sadly it's not :-(\r\n\r\n"
5192,'jpountz','Improve geo distance accuracy\nGeo distance computations assume that the earth is round although it is an ellipsoid. Taking the ellipsoid shape into account would be costly but thanks to [LUCENE-5271](https://issues.apache.org/jira/browse/LUCENE-5271), `SloppyMath.haversin` is now going to use an approximate value of the diameter of the earch at the average of the two latitudes instead of assuming a uniform diameter. We should use the same trick for the non-sloppy `arc` distance computation.'
5190,'uboness','"Missing" aggregation fails when object containing aggregation field is missing as well\nsay our data structure is\r\n\r\n```\r\ninvoice:{\r\n  vendor:{\r\n    id:10\r\n    name: "google"\r\n  }\r\n}\r\n```\r\n\r\nvendor object is optional to the invoice and may be absent\r\n\r\nwhen calculating missing aggregation on vendor.id it fails with \r\n\r\n```\r\nearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[mr2aF25CTPGrkvzftHq9Rg][award][0]: ClassCastException[org.elasticsearch.search.aggregations.support.FieldDataSource$Bytes$FieldData cannot be cast to org.elasticsearch.search.aggregations.support.FieldDataSource$Numeric]}]\r\n``` \r\n\r\nI would expect missing aggregation to treat missing parent objects of the field the aggregation is calculated for as if the field itself is missing or it would be virtually impossible to guarantee that such aggregation would finish successfully over deeply nested graphs where any part of the path to the missing field may be absent'
5189,'s1monw',"Leaking file handles?\nI know this issue has been raised a couple times in the past, but none of them has provided me a solution so far. I've also seen similar issues being raised in forums.\r\n\r\nWe're running ES 0.90.7 on a two-node EC2 cluster with the following settings:\r\n * Total RAM 17GB\r\n * ES_HEAP_SIZE 9GB\r\n * open files (ulimit -n) 65535\r\n\r\nWe have five different indexes (13GB, 7GB, 4GB, 22MB, 4MB), we are running about 150K indexing operations per day (mostly new docs, but also some updates). Depending on days, we're running between 60K and 120K search operations.\r\n\r\nHere is a recurrent pattern that has been consistently happening over the past couple months and which requires us to restart the cluster every 3/4 days or so.\r\nThe graphics below show the evolution of the heap size, the process CPU and the open file handles from one restart (~12PM on Feb 15th) to the next one (~8PM on Feb 19th). Initially, everything runs smoothly. Node 2 was the master in that run and was the one processing the ActiveMQ river. \r\n\r\nThen on Feb 18th around 12PM (orange vertical bar), something starts happening. At that time, the same things keep happening on the master node:\r\n * the process CPU starts increasing and stays high\r\n * the file handles count keeps increasing (but more steeply)\r\n * The GC doesn't seem to be able to release memory\r\n\r\n![capture decran 2014-02-20 a 05 26 24](https://f.cloud.github.com/assets/1280019/2215402/70a7e0ba-99f0-11e3-8756-05c31daa3561.png)\r\n\r\nEverything keeps running more or less smoothly for a day or so up until the file handles are exhausted. Just before they do, we restart the problematic node (green vertical bar), the second node becomes the master and the same pattern repeats with that node.\r\n\r\nAnother thing worth noting is that about 90% of the open file handles are marked (deleted) and belong 99% to the biggest index, i.e. the one that is 13GB.\r\n\r\nWe're going to upgrade to 0.90.11 anytime soon to see if Lucene 4.6.1 will be of any help, but in the meantime, we'd appreciate any kind of insights as to why this kind of things is happening."
5187,'javanna','Togglable stacktrace display\nMake it flaggable with system property whether ES will display \r\nonly Throwable message or full stack trace on startup, as \r\nsuggested by @imotov in #5103 discussion. Defaults to \r\ndisplaying stack trace, but it can be suppressed by \r\n````-Des.suppress-stack-traces=true```` .'
5186,'jpountz','Fix yamlBuilder() to return YAML builder instead of SMILE\nCloses #5185'
5184,'jpountz',"Allow using the FVH and Postings Highlighter without storing extra data\nRight now this is a work in progress more for review then anything.  I'll squash it and make it better later.\r\n\r\nI'm sure there are tons of things wrong with this but it could save me a ton of disk space and speed up highlighting.  But, yeah, it is a huge hack.\r\n\r\nCloses #5183"
5183,'uboness',"Allow using the FVH and Postings Highlighter without storing extra data\nIt'd be cool to be able to force the fvh or postings highlighter even if you haven't stored the data required for them to run.  Elasticsearch would re-analyze the field and twist the results appropriately for the highlighter.\r\n\r\nFurther, it'd be cool if you could set a minimum field length for which to save term vectors.  Combine this with the above and you the fvh actually gets much faster on small fields.  Not as fast as the plain highlighter or the postings highlighters, but if you are addicted to other fvh features it is nice.  Also, this allows you to hit time cpu cost for index time cpu and disk space by cranking that size up beyond where it is more efficient to re-analyze.  Again, this is really for folks addicted to the fvh."
5182,'uboness','[Suggest] Bug when mapping have a source field at the higher level of the document\nHello,\r\n\r\nI was trying to play with the suggest feature and get a headache when my first basic example was working but my complexe one wasn\'t giving results at all. \r\nI just figured out that when you have a source field at the higher level of your mapping/document, the suggest feature doesn\'t return anything. \r\n\r\nHere is my mapping which the suggest feature doesn\'t work : \r\n\r\n```\r\n{\r\n  "source" : {\r\n    "properties" : {\r\n      "domain" : {\r\n        "type" : "completion",\r\n        "analyzer" : "autocomplete",\r\n        "payloads" : true,\r\n        "preserve_separators" : false,\r\n        "preserve_position_increments" : false,\r\n        "max_input_len" : 50\r\n      },\r\n      "forum" : {\r\n        "type" : "completion",\r\n        "analyzer" : "autocomplete",\r\n        "payloads" : true,\r\n        "preserve_separators" : false,\r\n        "preserve_position_increments" : false,\r\n        "max_input_len" : 50\r\n      },\r\n      "source" : {\r\n        "properties" : {\r\n          "name" : {\r\n            "type" : "completion",\r\n            "analyzer" : "autocomplete",\r\n            "payloads" : true,\r\n            "preserve_separators" : false,\r\n            "preserve_position_increments" : false,\r\n            "max_input_len" : 50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nHere is my new working one. I just replaced source by toto. \r\n\r\n```\r\n{\r\n  "source" : {\r\n    "properties" : {\r\n      "domain" : {\r\n        "type" : "completion",\r\n        "analyzer" : "autocomplete",\r\n        "payloads" : true,\r\n        "preserve_separators" : false,\r\n        "preserve_position_increments" : false,\r\n        "max_input_len" : 50\r\n      },\r\n      "forum" : {\r\n        "type" : "completion",\r\n        "analyzer" : "autocomplete",\r\n        "payloads" : true,\r\n        "preserve_separators" : false,\r\n        "preserve_position_increments" : false,\r\n        "max_input_len" : 50\r\n      },\r\n      "toto" : {\r\n        "properties" : {\r\n          "name" : {\r\n            "type" : "completion",\r\n            "analyzer" : "autocomplete",\r\n            "payloads" : true,\r\n            "preserve_separators" : false,\r\n            "preserve_position_increments" : false,\r\n            "max_input_len" : 50\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```'
5180,'javanna','Added support for aliases to index templates\nThis PR finalizes the work that has been done in #2739 towards adding support for aliases to index templates.\r\n\r\nAliases can now be specified when creating an index template as follows:\r\n\r\n```\r\ncurl -XPUT localhost:9200/_template/template_1 -d \'\r\n{\r\n    "template" : "te*",\r\n    "settings" : {\r\n        "number_of_shards" : 1\r\n    },\r\n    "aliases" : {\r\n        "alias1" : {},\r\n        "alias2" : {\r\n            "filter" : {\r\n                "term" : {"user" : "kimchy" }\r\n            },\r\n            "routing" : "kimchy"\r\n        },\r\n        "{index}-alias" : {}\r\n    }\r\n}\r\n\'\r\n```\r\n\r\nThe `{index}` placeholder within the alias name will be replaced with the actual index name that the template gets applied to during index creation.'
5179,'uboness','date_histogram against empty index results in ArrayIndexOutOfBoundsException\nHi,\r\n\r\nI am using 1.0.0. and see:\r\n<code>\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[kwtJyjumSJ-DRx68nXaVhw][data][1]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][0]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][4]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][3]: ArrayIndexOutOfBoundsException[0]}{[kwtJyjumSJ-DRx68nXaVhw][data][2]: ArrayIndexOutOfBoundsException[0]}]",\r\n  "status" : 500\r\n}\r\n</code>\r\n\r\nwhen running date_histogram query. It happens when I run the query on the whole index while index is empty, or when I run the query on some type which has no documents (while other types in the index have docs).\r\n\r\nSteps to reproduce:\r\n<pre>\r\n<code>\r\ncurl -XDELETE \'http://localhost:9200/_all/\'\r\n</code>\r\n</pre>\r\n<pre>\r\n<code>\r\ncurl -XPUT \'http://localhost:9200/data/\' -d \'{    "mappings" : {\r\n      "_default_" : {\r\n        "_source" : { "enabled" : false },\r\n        "_timestamp" : { "enabled" : true, "store" : true },\r\n        "_all" : {"enabled" : false},\r\n        "properties" : {\r\n          "timestamp" : { "type" : "date", "index" : "not_analyzed", "store": "true" },\r\n          "message" : { "type" : "string", "index" : "analyzed", "analyzer" : "standard", "store": "true" },\r\n          "type" : { "type" : "string", "index" : "not_analyzed", "store": "true" }\r\n        }\r\n      }\r\n    }    \r\n}\'\r\n</code>\r\n</pre>\r\n<pre>\r\n<code>\r\ncurl "http://localhost:9200/data/_search?fields=*&pretty" -d \'{"query":{"filtered":{"filter":{"range":{"timestamp":{"gte":1392026400000,"lte":1392631200000}}},"query":{"query_string":{"query":"mesage:*"}}}},"aggs":{"data":{"date_histogram":{"field":"timestamp","interval":"60m"},"aggs":{"data_type":{"terms":{"field":"_type"}}}}}}\'\r\n</code>\r\n</pre>\r\n\r\n'
5177,'martijnvg','GetFieldMappings API will not return field mapping if the index is not hosted on the node executing it\nThe current implementation of the GetFieldMapping API uses information from the index service which is only available on a node if that node actively hosts a shard from that index. If the information is missing the call will act as if the type/field was not found and will not return information for it.\r\n\r\nDuring a rolling upgrade from <= 0.90.11 or 1.0.0 the get field mapping api might fail. This has to do with the way this issue has been fixed. The way how internally the request got handled has changed in order completely to fix it properly at the cost that during a rolling upgrade this api may fail.'
5175,'javanna','Highlighting on a wildcard field name uses the same highlighter for all fields that match\n```HighlightPhase.hitExecute(SearchContext context, HitContext hitContext)```\r\n\r\nAssume that the field object represents a wildcard field (i.e. field.field() returns "*")\r\nAssume that the index contains two fields:\r\nfield_1 with "index_options":"freq"\r\nfield_2 with "index_options":"offsets"\r\nAssume that a highlighter has not been explicitly specified (i.e. ```field.highlighterType()==null```)\r\n\r\nThe object fieldNamesToHighlight will contain both of the above field names.\r\nThe first pass through the for (String fieldName : fieldNamesToHighlight) loop will evaluate the statement: ```if (field.highlighterType() == null){}``` to true.\r\nThe block will evaluate the index_options of the first field (from a HashSet, so essentially random choice) to determine the appropriate highlighter. The highlighter will then be set using ```field.highlighterType("postings")```.\r\n\r\nSubsequent executions of the fieldNamesToHighlight for loop will evaluate the statement: ```if (field.highlighterType() == null){}``` to false, based on the settings from the first execution.\r\n\r\nResult: The highlighter chosen for the (arbitrary) first field will be used for all subsequent fields. If the selected highlighter is not the plain highlighter then there is the potential for the code to throw an IllegalArgumentException due to a mismatch between the highlighter and the indexing options of the field.\r\n\r\nThe solution is to treat the user provided configuration (the field object) as immutable. For each iteration of the fieldNamesToHighlight loop, extract field.highlighterType() to a local variable. Test that local variable for null - and set to a selected highlighter as appropriate.'
5170,'s1monw','Issue 5165: Fix SearchContext occasionally closed prematurely \nPR for #5165'
5169,'dakrone',"remove RoutingAllocation.Result from cluster state\nWe should remove `RoutingAllocation.Result` from the cluster state, as it doesn't really provide any useful data, and just inflates the size of the cluster state."
5164,'dadoonet','Can\'t delete all indexes from the Java API in 1.0.0\nIn 0.90.x i was able to delete all my indices from the java api by calling \r\n\r\n    client.admin().indices().prepareDelete(new String[] {}).execute().actionGet();\r\n\r\nHowever this fails in 1.0.0 with \r\n\r\norg.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: index / indices is missing;\r\n\tat org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)\r\n\tat org.elasticsearch.action.admin.indices.delete.DeleteIndexRequest.validate(DeleteIndexRequest.java:72)\r\n\t*snip long stacktrace*\r\n\r\nwhich points me to\r\n\r\n    public ActionRequestValidationException validate() {\r\n        ActionRequestValidationException validationException = null;\r\n        if (indices == null || indices.length == 0) {\r\n            validationException = addValidationError("index / indices is missing", validationException);\r\n        }\r\n        return validationException;\r\n    }\r\n\r\nSo that\'s what now throws the error, however the documentation still says:\r\n\r\n    /**\r\n     * Deletes an index based on the index name.\r\n     *\r\n     * @param indices The indices to delete. Empty array to delete all indices.\r\n     */\r\n    DeleteIndexRequestBuilder prepareDelete(String... indices);\r\n\r\n\r\nIs this a bug, or is there a new preferred way to delete all indices that isn\'t reflected yet in the documentation?\r\n\r\n(In case anyone is wondering: I use this in my unit test test setup to make sure I have a clean slate every time).\r\n\r\n\r\n'
5162,'s1monw','Reduce number of warnings throughout the code\nA newly imported tree results in a project with ~4090 warnings (in Eclipse). Even though the majority of these may be harmless (i.e. not result in runtime failures or wrong logic), they exist for a reason and are a problem because they might hide actual warnings we could/should care about.\r\n\r\nTodo: analyze severity/frequency & suggest mitigation. Should be mostly menial work (ideal for one-fix-a-day) and very often a single fix will fix hundreds of warnings (e.g. non-generic use of a class that is supposed to be used as generic).\r\n\r\nRelates to issue #5160.\r\n'
5161,'bleskes','New mapping error: _mapping/field/* 404\n```\r\ncurl -v \'http://web245:9200/statistics-20140216/_mapping/field/*\'\r\n* About to connect() to web245 port 9200 (#0)\r\n*   Trying 192.168.0.245...\r\n* connected\r\n* Connected to web245 (192.168.0.245) port 9200 (#0)\r\n> GET /statistics-20140216/_mapping/field/* HTTP/1.1\r\n> User-Agent: curl/7.26.0\r\n> Host: web245:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 404 Not Found\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 2\r\n<\r\n* Connection #0 to host web245 left intact\r\n{}* Closing connection #0\r\n```\r\n```\r\n# curl -v \'http://web245:9200/statistics-20140216/_mapping\'\r\n* About to connect() to web245 port 9200 (#0)\r\n*   Trying 192.168.0.245...\r\n* connected\r\n* Connected to web245 (192.168.0.245) port 9200 (#0)\r\n> GET /statistics-20140216/_mapping HTTP/1.1\r\n> User-Agent: curl/7.26.0\r\n> Host: web245:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 1648\r\n<\r\n{"statistics-20140216":{"mappings":{"markers":{"_all":{"enabled":false},"properties":{"@message":{"type":"string"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}},"precise":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{"@key":{"type":"string","index":"not_analyzed"},"@precise":{"type":"double"},"@timestamp":{"type":"date","format":"dateOptionalTime"}}},"events":{"_all":{"enabled":false},"_routing":{"required":true,"path":"@key"},"properties":{/* skipped properties here */}}}}}* Closing connection #0\r\n```\r\n\r\nKibana complains that my proxy is misconfigured.\r\n\r\nThis happened after 0.90.10 -> 1.0.0 upgrade on one of our clusters. All staging clusters were ok, except this one in production :)\r\n\r\nWhat is wrong and how to fix it?'
5152,'dakrone','Unbound threadpools considered harmful\nElasticSearch server and java client both have an unbound, undocumented threadpool that can cause lockup of the whole JVM process and take down the whole server machine: #5151.\r\n\r\nThe minimum change is documenting the unbound generic pool & instructions for reconfiguring it at http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html.\r\n\r\nIMHO unbound threadpool is a Bad Idea from the start. Bound pool with some reasonable size queue and abort policy would not cause jvm/server lockup in a runaway case, instead you would get some meaningful information in the log.'
5150,'spinscale','wrong loading via homebrew\nI have ran elasticsearch via homebrew, but get errors as follow when execute elasticsearch --config=/usr/local/opt/elasticsearch/config/elasticsearch.yml\r\n\r\ngetopt: illegal option -- -\r\ngetopt: illegal option -- c\r\ngetopt: illegal option -- o\r\ngetopt: illegal option -- n\r\ngetopt: illegal option -- i\r\ngetopt: illegal option -- g\r\ngetopt: illegal option -- =\r\ngetopt: illegal option -- /\r\ngetopt: illegal option -- u\r\ngetopt: illegal option -- s\r\ngetopt: illegal option -- r\r\ngetopt: illegal option -- /\r\ngetopt: illegal option -- l\r\ngetopt: illegal option -- o\r\ngetopt: illegal option -- c\r\ngetopt: illegal option -- a\r\ngetopt: illegal option -- l\r\ngetopt: illegal option -- /\r\ngetopt: illegal option -- o\r\n\r\nwhen elasticsearch has been running as well, but it is create new folder after launch.'
5148,'javanna',"Add support for char filters in the analyze API\nAllow char filters to be used in the analyze API. Potentially breaks AnalyzeRequest serialization. The REST action contains the now ambiguous 'filter' parameter, which will denote a 'token_filter', not a 'char_filter'.\r\n\r\nOne additional item I noticed is the exception message for invalid token filters. To me it appears like an overzealous copy and paste, but should the exception contain the token filter name, not the tokenizer? I can fix this item as well.\r\n\r\nExample:\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/admin/indices/analyze/TransportAnalyzeAction.java?source=cc#L173\r\n"
5143,'javanna','Fixed multi term queries support in postings highlighter for non top-level queries\nIn #4052 we added support for highlighting multi term queries using the postings highlighter. That worked only for top-level queries though, and not for multi term queries that are nested for instance within a bool query, or filtered query, or a constant score query.\r\n\r\nThe way we can make this work is by walking the query structure and temporarily overriding the query rewrite method with a method that allows for multi terms extraction.\r\n\r\nCloses #5127 '
5142,'jpountz','_exists_ doesn\'t work on objects\nAt least in 1.0.0\r\n\r\n```shell\r\n# create index and object\r\ncurl -X PUT \'http://127.0.0.1:9200/wtf/?pretty\'\r\ncurl -X PUT \'http://127.0.0.1:9200/wtf/test/1?pretty\' -d \'{"complex": { "object": "it is" }, "simple": "value"}\'\r\n\r\n# no results\r\ncurl -X POST \'http://127.0.0.1:9200/wtf/test/_search?pretty&q=_exists_:complex\'\r\n# has results\r\ncurl -X POST \'http://127.0.0.1:9200/wtf/test/_search?pretty&q=_exists_:simple\'\r\n\r\n# has results\r\ncurl -X POST \'http://127.0.0.1:9200/wtf/test/_search?pretty\' -d \'{ "query": { "filtered": { "filter": { "not": { "missing": { "field": "complex" } } } } } }\'\r\n\r\n# has results\r\ncurl -X POST \'http://127.0.0.1:9200/wtf/test/_search?pretty\' -d \'{ "query": { "filtered": { "filter": { "not": { "missing": { "field": "simple" } } } } } }\'\r\n```\r\n\r\nThis behaviour  should be either fixed or noted in docs.'
5137,'imotov','Query DSL: Throw parsing exception if terms filter or query has more than one field\nCloses #5014'
5136,'spinscale',"JAVA_HOME is not checked in ubuntu start script\nI've installed on Ubuntu 12.04 from the elasticsearch.org apt repository. According to the instructions on [the setup page of the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup.html), the start script should be checking JAVA_HOME. What actually happens is that it only checks for JAVA_HOME in the /etc/default/elasticsearch file, not in the current environment. \r\n\r\nI'm not sure whether it's the documentation or the start script that is wrong but one of them should be updated. "
5135,'spinscale',"apt-get installs wrong 1.0.0 version of elasticsearch\nHi,\r\n\r\nRecently discovered that apt-get upgrade elasticsearch installs 1.0.0RC2 and *not* 1.0.0. At attempt to force the version causes a warning that the package is about to be downgraded\r\n\r\n```\r\n# apt-get install elasticsearch=1.0.0\r\nReading package lists... Done\r\nBuilding dependency tree       \r\nReading state information... Done\r\nThe following packages will be DOWNGRADED:\r\n  elasticsearch\r\n0 upgraded, 0 newly installed, 1 downgraded, 0 to remove and 0 not upgraded.\r\n```\r\n\r\nA brief chat on irc shows that this is known and fix is coming (however, I couldn't seem to find a github ticket so that I can watch when this is fixed - hence this ticket.)\r\n\r\n```\r\n18:20 <@honzakral> dharrigan: it is,we are aware of that and fix is coming - it's based on the sorting algorithm apt uses to determine the latest version\r\n```\r\n\r\nThank you and eagerly looking forward to this! :-)\r\n\r\n-=david=-"
5133,'dadoonet','Source filtering with wildcards broken when given multiple patterns\n```\r\ncurl -XPUT \'http://localhost:9200/twitter/tweet/1\' -d \'{\r\n    "user" : "kimchy",\r\n    "post_date" : "2009-11-15T14:12:12",\r\n    "message" : "trying out Elasticsearch", "retweeted": false\r\n}\'\r\n```\r\n\r\nNo source fields delivered:\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/1?_source=*.id,retweeted&pretty=yes\'\r\n```\r\n\r\n`retweeted` returned:\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/1?_source=retweeted,*.id&pretty=yes\'\r\n```\r\n\r\nCloses #5132.'
5132,'dadoonet','Source filtering with wildcards broken when given multiple patterns\n```\r\ncurl -XPUT \'http://localhost:9200/twitter/tweet/1\' -d \'{\r\n    "user" : "kimchy",\r\n    "post_date" : "2009-11-15T14:12:12",\r\n    "message" : "trying out Elasticsearch", "retweeted": false\r\n}\'\r\n```\r\n\r\nNo source fields delivered:\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/1?_source=*.id,retweeted&pretty=yes\'\r\n```\r\n\r\n`retweeted` returned:\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/1?_source=retweeted,*.id&pretty=yes\'\r\n```\r\n\r\nThis happens because the filter [breaks out of the loop](https://github.com/elasticsearch/elasticsearch/blob/f2710c16ebd918f646be9d0ab64b4871c25be4c2/src/main/java/org/elasticsearch/common/xcontent/support/XContentMapValues.java#L178) instead of continuing to check whether any of the other `includes` may match. Replacing the `break` with a `continue` fixes this. But I hesitate to submit a pull request because I didn\'t have time to fully understand this code.\r\n\r\nI understand that this is a corner case but it\'s a new feature in 1.0 and the example in the documentation does not work due to this bug.'
5130,'colings86','ESLogger logs wrong class name\nESLogger logs wrong class name, method name, line number.\r\n\r\nversion: 1.0.0\r\n\r\nconfiguration:\r\n  log4j\r\n      conversionPattern: "[%d{ISO8601}][%-5p][%-25c][%C.%M:%L] %m%n"\r\n  slf4j + logback\r\n    <pattern>[%date{ISO8601}][%-5.5level][%-25.25logger][%class{36}.%method:%line] %msg%n</pattern>\r\n\r\nESLogger logs this.(wrong class name!):\r\n  org.elasticsearch.common.logging.log4j.Log4jESLogger.internalInfo:114\r\n\r\n\r\n\r\nESLogger always logs the class name "Log4jESLogger".\r\nI thought Elasticsearch can avoid it using other log4j/slf4j methods and I changed.'
5128,'dakrone','query_string and simple_query_string should allow selective a Locale\n`query_string` currently defaults to `Locale.getDefault()` while `simple_query_string` uses `Locale.ROOT`.\r\n\r\nWe should add a "locale" option to both queries that allows the user to select a locale, and unify the defaults to be the same.'
5127,'javanna','Postings Highlighter does not highlight trailing wildcard matches\nIn 1.0 Postings highlighter does not highlight trailing wildcard matches. I tried with both simple_query_string and query_string and things like photo* does not get highlighted'
5125,'javanna','Exposed shard id related to a failure in delete by query\nRelates to #5095, where we want to expose the potential shard failures obtained from the delete by query api. Although the failure is available, the shard id where it happened is not (always `-1`).\r\n\r\nRefactored TransportIndexReplicationOperationAction to be able to expose the shard id related to a shard failure.\r\n\r\nThe `ShardOperationFailedException` is now created within `TransportIndexReplicationAction` passing in the current shard id as a constructor argument.\r\nAlso replaced `AtomicReferenceArray<Object>` with `AtomicReferenceArray<ShardActionResult>`, where `ShardActionResult` wraps the `ShardResponse` or the failure, containing all the needed info.'
5121,'javanna',"Update get.asciidoc\nMinor improvements.\r\n\r\ncurl -XHEAD doesn't actually print anything so I've changed to use -I which actually prints the headers received."
5118,'colings86','Add _cat/segments\nCurrently displays the same information as the _segments API'
5115,'s1monw','Add preserve original token option to ASCIIFolding\nCloses #4931'
5114,'spinscale','Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version 1.0.0\nI got an error using elasticsearch 1.0.0 ( with version 0.90.x without any problem).\r\n\r\n```\r\n[root@logger01 /Software/bin]# Exception in thread "main" java.lang.NoClassDefFoundError: org/elasticsearch/bootstrap/Elasticsearch\r\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.bootstrap.Elasticsearch\r\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:217)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:205)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:323)\r\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)\r\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:268)\r\nCould not find the main class: org.elasticsearch.bootstrap.Elasticsearch. Program will exit.\r\n```\r\nOS FreeBSD 10.0 x64\r\n\r\njava -version:\r\n```\r\njava -version\r\nopenjdk version "1.6.0_32"\r\nOpenJDK Runtime Environment (build 1.6.0_32-b30)\r\nOpenJDK 64-Bit Server VM (build 23.25-b01, mixed mode)\r\n```\r\n\r\nthanks for any help.\r\nStefan\r\n'
5113,'javanna','Fix typo in similarity docs\nDRF similarity -> DFR similarity'
5111,'javanna',"Docs for 1.0 search breaking changes\n[Breaking changes](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_search_requests.html) say that top-level query parameter is *required* and links to: \r\n\r\n* [Count api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-count.html) which says: `NOTE: in contrast to Search the query being sent in the body must not be nested in a query key.`\r\n\r\n* [Validate api](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/search-validate.html) which shows example *without* top-level query parameter.\r\n\r\nIs that should be fixed or I'm wrong?"
5109,'javanna','[DOCS] should use setPostFilter instead of setFilter\n'
5106,'drewr','Documentation error:  _cat?help\nThe documentation for the cat API in MASTER and 1.0.0 says you can GET `_cat?help` to retrieve the full list of commands.  This just returns the cute little cat ASCII art.  Digging into the code comments, it looks like the actual command is `_cat?h`, which works nicely.\r\n\r\n> All the cat commands accept a query string parameter help to see all the headers and info they provide, and the /_cat?help command lists all the available commands.\r\n\r\n```\r\n$ http localhost:9200/_cat?help\r\nHTTP/1.1 200 OK\r\nContent-Length: 6\r\nContent-Type: text/plain; charset=UTF-8\r\n\r\n=^.^=\r\n```\r\n\r\n```\r\n$ http localhost:9200/_cat?h\r\nHTTP/1.1 200 OK\r\nContent-Length: 280\r\nContent-Type: text/plain; charset=UTF-8\r\n\r\n=^.^= try:\r\n/_cat/allocation\r\n/_cat/shards\r\n/_cat/shards/{index}\r\n/_cat/master\r\n/_cat/nodes\r\n/_cat/indices\r\n/_cat/indices/{index}\r\n/_cat/count\r\n/_cat/count/{index}\r\n/_cat/recovery\r\n/_cat/recovery/{index}\r\n/_cat/health\r\n/_cat/pending_tasks\r\n/_cat/aliases\r\n/_cat/aliases/{alias}\r\n/_cat/thread_pool\r\n```'
5104,'jpountz','DocValues and ConcurrentModificationException\nPossibly a bug in Lucene rather than Elasticsearch:\r\n```\r\n[2014-02-12 19:46:55,768][DEBUG][action.admin.indices.stats] [hostname] [logstash-2014.02.12][0], node[idCsLCeJQOuc_GEON7LCCQ], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@10e4c9b1]\r\norg.elasticsearch.transport.RemoteTransportException: [hostname][inet[/ip.add.re.ss:9300]][indices/stats/s]\r\nCaused by: java.util.ConcurrentModificationException\r\n        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)\r\n        at java.util.HashMap$ValueIterator.next(HashMap.java:954)\r\n        at org.apache.lucene.codecs.lucene45.Lucene45DocValuesProducer.ramBytesUsed(Lucene45DocValuesProducer.java:291)\r\n        at org.apache.lucene.codecs.perfield.PerFieldDocValuesFormat$FieldsReader.ramBytesUsed(PerFieldDocValuesFormat.java:308)\r\n        at org.apache.lucene.index.SegmentDocValues.ramBytesUsed(SegmentDocValues.java:103)\r\n        at org.apache.lucene.index.SegmentReader.ramBytesUsed(SegmentReader.java:555)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.getReaderRamBytesUsed(InternalEngine.java:1123)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.segmentsStats(InternalEngine.java:1135)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.segmentStats(InternalIndexShard.java:532)\r\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:161)\r\n        at org.elasticsearch.action.admin.indices.stats.ShardStats.<init>(ShardStats.java:49)\r\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:197)\r\n        at org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:413)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:399)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n```\r\n\r\nElasticsearch 1.0.0 (GA), when trying to bulk index.'
5103,'javanna','Closes #5102 - Show stacktrace of startup exception\nhttps://github.com/elasticsearch/elasticsearch/issues/5102\r\n\r\nThis makes it a lot easier to debug a problem\r\nlike\r\n\r\n{2.0.0-SNAPSHOT}: Startup Failed ...\r\n- NumberFormatException[For input string: ""]\r\n\r\nbecause now you see from where the error comes\r\nfrom (which might be from a plugin!).'
5102,'javanna','Display full stack trace of exception that occurs on startup\nIf an exception is raised by code triggered from Bootstrap class, only its message (and the message of the possible cause stack) is displayed, not the full stack trace.\r\n\r\nThis means that if for example some plugin reads a System property that it expects to be numeric but which is accidentally set as empty, you only see something like this\r\n\r\n    {2.0.0-SNAPSHOT}: Startup Failed ...\r\n    - NumberFormatException[For input string: ""]\r\n\r\nI propose the whole stack trace would be displayed in these cases, either on System.err when no logging is configured or on ERROR level via log4j. This would make these kinds of configuration errors loads easier to debug, with the small price of uglier error messages.\r\n\r\nPull request is coming up.'
5101,'javanna','fixed markup and typo\n'
5100,'clintongormley','multi-field terms aggregation\nIt would be nice if the aggregation could be done on multiple fields to get a list of unique keys. The result should include the fields per key (where it found the term):\r\n   "example" : {\r\n      "buckets" : [ {\r\n         "key" : "java",\r\n         "doc_count" : 5\r\n         "fields": ["island", "programming language"]\r\n       }\r\n       ...\r\n      ]\r\n   }'
5095,'javanna','add failures reason to delete by query response\ncloses #5093\r\n\r\nIt will return an array of failures. e.g.\r\n```json\r\n "tweet-36": {\r\n    "_shards": {\r\n      "total": 5,\r\n      "successful": 4,\r\n      "failed": 1,\r\n      "failures": [\r\n        {\r\n          "index": "tweet-36",\r\n          "reason": "EsRejectedExecutionException[rejected execution (queue capacity 200) on org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1@176685ee]"\r\n        }\r\n      ]\r\n    }\r\n  }\r\n```'
5093,'javanna',"Delete by query doesn't return failure reasons\nIf we run delete by query and there are some shards failed, we only got failed shards count but doesn't contains failure reasons\r\n\r\ne.g. #5083 "
5091,'dakrone','[0.9.11] simple_query_string does not recognize fields by their nested name but only by index_name\n```\r\n{\r\n  simple_query_string: {\r\n    query: \'hello world\',\r\n    fields:[\'award.title\']\r\n  }\r\n}\r\n```\r\n\r\nreturns no results while\r\n\r\n```\r\n{\r\n  simple_query_string: {\r\n    query: \'hello world\',\r\n    fields:[\'award_title\']\r\n  }\r\n}\r\n```\r\nreturns expected results. the issue is with fields:[] in first case I use logical (dot-notation) field name while ion the second I use actual lucene name (index_name in the mappings below)\r\n\r\n```\r\n"award": {\r\n  "properties":{\r\n            "title": {\r\n              "type": "multi_field",\r\n              "path": "just_name",\r\n              "fields": {\r\n                "title": {\r\n                  "type": "string",\r\n                  "index_analyzer": "stemmed",\r\n                  "search_analyzer": "stemmed",\r\n                  "index_options": "offsets",\r\n                  "index_name": "award_title",\r\n                  "boost": 3\r\n                },\r\n                "all": {\r\n                  "type": "string",\r\n                  "index_name": "i_all",\r\n                  "boost": 3\r\n                },\r\n                "all_shingle": {\r\n                  "type": "string",\r\n                  "analyzer": "not_stemmed_shingle",\r\n                  "index_name": "i_all_shingle",\r\n                  "boost": 3\r\n                },\r\n                "all_stem": {\r\n                  "type": "string",\r\n                  "index_analyzer": "stemmed",\r\n                  "search_analyzer": "stemmed",\r\n                  "index_name": "i_all_stem",\r\n                  "boost": 3\r\n                }\r\n              }\r\n            }\r\n...\r\n  }\r\n}\r\n```'
5089,'spinscale','[Java API] IdsQueryBuilder allow merging with list\nCurrently IdsQueryBuilder does support only appending arrays of identifiers and converting them to lists:\r\n\r\n````\r\npublic IdsQueryBuilder addIds(String... ids) {\r\n        values.addAll(Arrays.asList(ids));\r\n        return this;\r\n    }\r\n````\r\n\r\nIn cases of huge numbers of IDs submitted to builder it could be more wise of performance to merge with lists directly. Or are there any better ideas?'
5085,'chilling','Enhance DistanceUnit to recognize nautical miles.\nThe DistanceUnit class should add the DistanceUnit.NAUTICALMILES enumeration. For example:\r\n\r\nNAUTICALMILES(1852.0, "NM", "nmi"),\r\n\r\nI\'ll let someone else decide what the second, longer, string should actually be. Most aeronautical charts use "nm" but I\'ve also seen "nmi".'
5083,'s1monw','Delete by query does not delete all documents\nI am new to elasticsearch and facing an issue with \'delete by query\' api. Please correct me if something wrong with approach or understanding\r\n\r\nSteps to reproduce\r\n1. Create about 50 documents in 50 different indices\r\n2. Use delete by query api to delete all the documents created above.\r\n3. Verify if all documents are deleted\r\n\r\nExpected: all documents in all indices should be deleted\r\nActual: Few documents are left over, there is no consistency as to which ones ...\r\n\r\nGist "https://gist.github.com/ssanghavi/8935108" can help reproduce the issue\r\n\r\nAlso tried to delete using "_all/query" but even that does not work\r\nIf I delete documents from each index at a time i.e. loop over, then it works fine\r\n\r\nI am using a single node on my desktop'
5075,'javanna',"Variable renamings to reduce unnecessary variable naming diversity\nI've been working on a research machine learning-based tool (link: http://groups.inf.ed.ac.uk/naturalize/ ) tool that analyzes source code identifiers and makes suggestions for renaming them. The goal is to reduce unnecessary diversity in variable naming and improve code readability. This pull request is only a small sample of the suggestions made for elasticsearch.\r\n\r\nNo functional changes were made in any of the commits"
5072,'spinscale',"Migrating NodesInfo API to use plugins instead of singular plugin\nIn order to be consistent (and because in 1.0 we switched from\r\nparameter driven information to specifzing the metrics as part of the URI)\r\nthis patch moves from 'plugin' to 'plugins' in the Nodes Info API.\r\n\r\nImportant: This breaks BWC for 1.0 a bit more than the switch already done, but is now documented in the migration document as well."
5071,'spinscale','Add note to statistical facet docs about documents missing faceted field\nHey,\r\n\r\nI was not sure how the computing of statistical facets worked in terms of how it treated documents that were missing the faceted field. I visited the #elasticsearch channel on irc.freenode.net, where @HonzaKral was very helpful with what happens - `"documents with missing fields are excluded"` - so I thought it would be a good idea to add a note to the docs and share this knowledge with the rest of the community in an accessible way :)\r\n\r\nCheers!'
5068,'javanna','Made possible to dynamically update `discovery.zen.publish_timeout` cluster setting\n`discovery.zen.publish_timeout` controls how long the master node is going to try and wait for all the nodes to respond to a cluster state publish before going ahead with the following updates in the queue (default 30s). Up until now changing the settings required restarting each node. The setting is now dynamic and can be changed through the cluster update settings api.'
5066,'javanna','[DOCS] add DynamoDB river plugin\nAdd DynamoDB river plugin to plugins page\r\n\r\nhttps://github.com/kzwang/elasticsearch-river-dynamodb'
5063,'javanna','Allow to dynamically change discovery.zen.publish_timeout using cluster update settings\n'
5058,'javanna','added local flag & master_timeout support to cluster pending tasks api (0.90)\nThis PR was created against 0.90 branch so that we can backport to 0.90, in a backwards compatible manner, the support for `local` flag and `master_timeout` in cluster pending tasks api.\r\n\r\nWith #3345 we added support for the `local` flag across the board to all cluster state read operations. The change was only applied to master and 1.x branches though, while having `local` flag and `master_timeout` support backported to 0.90 for cluster pending tasks api would help a lot. \r\n'
5057,'uboness','date_histogram aggregation and  time_zone\nAccording to http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-datehistogram-aggregation.html\r\n\r\nthe time_zone attribute can take integer value but in DateHistogramParser.java this attribute is only parsed when the value is a string.\r\n\r\nSo for example: \r\n  time_zone: 1 \r\ngives: \r\n\r\nParse Failure [Unknown key for a VALUE_NUMBER in [agg_name]: [time_zone].]]\r\n\r\nVersion: ES 1.0RC2 and 1.0 branch.'
5056,'javanna','[DOCS] Add GitHub community river plugin\n'
5053,'spinscale','Mapping: Fix possibility of losing meta configuration on field mapping update\nCreate a new schema with a field and the _timestamp field enabled:\r\n\r\ncurl -XPOST http://localhost:9200/foo -d \'\r\n{\r\n        "mappings": {\r\n                "product": {\r\n                        "_timestamp" : { "enabled" : true },\r\n                        "properties": {\r\n                                "field1": {  "type": "integer" }\r\n                        }\r\n                }\r\n       }\r\n}\'\r\n\r\nRetrieve the mapping, all looks good:\r\ncurl http://localhost:9200/foo/\\_mapping\r\n{"foo":{"product":{"_timestamp":{"enabled":true},"properties":{"field1":{"type":"integer"}}}}}\r\n\r\nNow add another field:\r\ncurl -XPUT http://localhost:9200/foo/product/\\_mapping -d \'\r\n{\r\n       "product" : {\r\n                "properties": {\r\n                        "field2" : {"type" : "integer" }\r\n                }\r\n        }\r\n}\r\n\'\r\nGet the mapping again:\r\ncurl http://localhost:9200/foo/\\_mapping\r\n{"foo":{"product":{"properties":{"field1":{"type":"integer"},"field2":{"type":"integer"}}}}}\r\n\r\nThe _timestamp field is gone!\r\n\r\n\r\n'
5049,'s1monw','AssertionError during percolation using constant score query\nWhen elasticsearch is running with assertions enabled, a constant score query that doesn\'t match a record can cause AssertionError to be thrown. To reproduce start elasticsearch with assertions enabled and execute the following script\r\n\r\non 0.90 branch:\r\n\r\n```\r\ncurl -XDELETE localhost:9200/test-idx\r\ncurl -XPUT \'localhost:9200/test-idx/\'\r\ncurl -XPUT \'localhost:9200/_percolator/test-idx/1\' -d \'{\r\n    "query" : {\r\n        "constant_score":{\r\n            "filter": {\r\n                "and": [{\r\n                    "query": {\r\n                        "query_string" : {\r\n                            "query" : "root"\r\n                        }\r\n                    }\r\n                }, {\r\n                    "term" : {\r\n                        "message" : "tree"\r\n                    }\r\n                }]\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XGET \'localhost:9200/test-idx/message/_percolate\' -d \'{\r\n    "doc" : {\r\n        "message" : "A new bonsai tree in the office"\r\n    }\r\n}\'\r\n```\r\n\r\nto reproduce on master:\r\n```\r\ncurl -XDELETE localhost:9200/test-idx\r\ncurl -XPUT \'localhost:9200/test-idx/.percolator/1\' -d \'{\r\n    "query" : {\r\n        "constant_score":{\r\n            "filter": {\r\n                "and": [{\r\n                    "query": {\r\n                        "query_string" : {\r\n                            "query" : "root"\r\n                        }\r\n                    }\r\n                }, {\r\n                    "term" : {\r\n                        "message" : "tree"\r\n                    }\r\n                }]\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XGET \'localhost:9200/test-idx/message/_percolate\' -d \'{\r\n    "doc" : {\r\n        "message" : "A new bonsai tree in the office"\r\n    }\r\n}\'\r\n```\r\n\r\nOn the master the issue is not as prominent since the error is not returned to the user. On the current master the result of execution of the script above is the following error in the log:\r\n\r\n```\r\n[2014-02-07 10:49:32,594][WARN ][percolator               ] [Blizzard II] [[31]] failed to execute query\r\njava.lang.AssertionError\r\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:61)\r\n\tat org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:256)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:621)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\r\n\tat org.elasticsearch.percolator.PercolatorService$4.doPercolate(PercolatorService.java:543)\r\n\tat org.elasticsearch.percolator.PercolatorService.percolate(PercolatorService.java:232)\r\n\tat org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:194)\r\n\tat org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:55)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$2.run(TransportBroadcastOperationAction.java:226)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\n```'
5048,'jpountz','aggregation error ArrayIndexOutOfBoundsException\nmapping extract\r\n\r\n                    "extension": {\r\n                        "type": "string", // eg .xls, no empty fields\r\n                        "index": "not_analyzed",\r\n                    },\r\n                    "sharepath": {\r\n                        "type": "string", // eg //1.2.3.4/Someshare/, no empty fields\r\n                        "index": "not_analyzed",\r\n                    },\r\n                    "doc_type": {\r\n                        "type": "string", // eg Spreadsheet Files, no empty fields\r\n                        "index": "not_analyzed"\r\n                    },\r\n\r\n\r\nquery works\r\n\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "and": [\r\n              {\r\n                "range": {\r\n                  "modified": {\r\n                    "lt": 1391775892000\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          "query": {\r\n            "match_all": {}\r\n          }\r\n        }\r\n      },\r\n      "aggs": {\r\n        "sharepath": {\r\n          "terms": {\r\n            "field": "sharepath",\r\n            "size": 2147483647\r\n          },\r\n          "aggs": {\r\n            "total_size_sharepath": {\r\n              "filter": {\r\n                  "term": {\r\n                      "doc_type": "Spreadsheet Files"\r\n                  }\r\n              },\r\n              "aggs": {\r\n                "total_size": {\r\n                  "stats": {\r\n                    "field": "size"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      "size": 0\r\n    }\r\n\r\n\r\nquery fails (more or less same query as before)\r\n\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "and": [\r\n              {\r\n                "range": {\r\n                  "modified": {\r\n                    "lt": 1391775892000\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          "query": {\r\n            "match_all": {}\r\n          }\r\n        }\r\n      },\r\n      "aggs": {\r\n        "extension": {\r\n          "terms": {\r\n            "field": "extension",  // previously \'sharepath\' which works\r\n            "size": 2147483647\r\n          },\r\n          "aggs": {\r\n            "total_size_extension": {\r\n              "filter": {\r\n                "term": {\r\n                  "doc_type": "Spreadsheet Files"\r\n                }\r\n              },\r\n              "aggs": {\r\n                "total_size": {\r\n                  "stats": {\r\n                    "field": "size"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      "size": 0\r\n    }\r\n\r\nStacktrace\r\n\r\n\r\n    [2014-02-07 14:07:35,301][DEBUG][action.search.type       ] [Copycat] [files_v1][3], node[XlVxAUsKRNinZGxwgkLyeg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@71c8e67c]\r\n    java.lang.ArrayIndexOutOfBoundsException: 51\r\n      at org.elasticsearch.common.util.BigArrays$LongArrayWrapper.get(BigArrays.java:118)\r\n      at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketDocCount(BucketsAggregator.java:79)\r\n      at org.elasticsearch.search.aggregations.bucket.filter.FilterAggregator.buildAggregation(FilterAggregator.java:73)\r\n      at org.elasticsearch.search.aggregations.bucket.BucketsAggregator.bucketAggregations(BucketsAggregator.java:88)\r\n      at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:121)\r\n      at org.elasticsearch.search.aggregations.bucket.terms.StringTermsAggregator.buildAggregation(StringTermsAggregator.java:41)\r\n      at org.elasticsearch.search.aggregations.AggregationPhase.execute(AggregationPhase.java:132)\r\n      at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:137)\r\n      at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:230)\r\n      at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\r\n      at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n      at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n      at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n      at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n      at java.lang.Thread.run(Thread.java:701)\r\n'
5044,'dadoonet','delete by query appears to be broken\nTested with 1.0.0.RC2 and RC1.\r\n\r\nTo repro:\r\ncurl -XPUT \'http://localhost:9200/test/\'\r\ncurl -XPUT \'http://localhost:9200/test/mytype/1\' -d \'{"name":"jack"}\'\r\ncurl -XDELETE \'http://localhost:9200/test/mytype/_query\' -d \'{"ids":{"values":["1"]}}\'\r\n\r\n{"_indices":{"test":{"_shards":{"total":5,"successful":0,"failed":5}}}}\r\n\r\nIn the log, I see:\r\n\r\n[2014-02-06 22:19:53,701][DEBUG][action.deletebyquery     ] [Shathra] [test][2], node[EGFe578KStSDkYbFLtXUCw], [P], s[STARTED]: Failed to execute [delete_by_query {[test][mytype], query [{"ids":{"values":["1"]}]}]\r\norg.elasticsearch.index.query.QueryParsingException: [test] request does not support [ids]\r\n\tat org.elasticsearch.index.query.IndexQueryParserService.parseQuery(IndexQueryParserService.java:303)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.prepareDeleteByQuery(InternalIndexShard.java:444)\r\n\tat org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnPrimary(TransportShardDeleteByQueryAction.java:118)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n'
5041,'javanna','Fixed the string() code literal in the java client index api doc.\nJust a simple fix of the missing code blocks.'
5038,'javanna','Java API: BulkProcessor does not call afterBulk when bulk throws eg NoNodeAvailableException\nWhen using a BulkProcessor with setting concurrentRequests > 0 and an exception occurs on row 283 (client.bulk(...)) other than InterruptedException then it is not caught and subsequently the afterBulk function is never called.\r\n\r\nIt is reproducible by running:\r\n* a transport client without an elasticsearch to connect to\r\n* with the BulkProcessor configured with concurrentRequests > 0\r\n* sending in enough documents so that the bulk is sent\r\n\r\nThis problem leads to that you only get exception for 1 of the documents in the bulk and there is no way to know that the other documents also failed.'
5035,'javanna','Add Marvel and Paramedic to config file\n'
5033,'spinscale','Packaging: Use the new command line syntax in the init script\nHi,\r\n\r\nI\'ve noticed that in the init script provided by the 1.0.0.RC2 Debian package, the daemon is started with pre-1.0 arguments : \r\n\r\n```\r\nDAEMON_OPTS="-d -p $PID_FILE -Des.default.config=$CONF_FILE -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR"\r\n```\r\n\r\nAccording to [the documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/_system_and_settings.html) we could use a simpler/shorter invocation : \r\n\r\n```\r\nDAEMON_OPTS="-d -p $PID_FILE --default.config=$CONF_FILE --default.path.home=$ES_HOME --default.path.logs=$LOG_DIR --default.path.data=$DATA_DIR --default.path.work=$WORK_DIR --default.path.conf=$CONF_DIR"\r\n```\r\n\r\nIt\'s definitely not a blocker, but it would be nice to be consistent.'
5032,'dadoonet','Improve documentation\nThe way it was written before I was unsure what the actual config parameter was because of the redundant "multicast" in there. I think it makes more sense to have the "enabled" parameter in the table as well.'
5030,'dadoonet','Upgrading analysis plugins fails\nWhen an analysis plugins provides default index settings using `PreBuiltAnalyzerProviderFactory`,  `PreBuiltTokenFilterFactoryFactory`, `PreBuiltTokenizerFactoryFactory` or `PreBuiltCharFilterFactoryFactory ` it fails when upgrading it with elasticsearch superior or equal to 0.90.5.\r\n\r\nRelated issue: #4936 \r\n\r\nFix is needed in core. But, in the meantime, analysis plugins developers can fix that issue by overloading default prebuilt factories.\r\n\r\nFor example:\r\n\r\n```java\r\npublic class StempelAnalyzerProviderFactory extends PreBuiltAnalyzerProviderFactory {\r\n\r\n    private final PreBuiltAnalyzerProvider analyzerProvider;\r\n\r\n    public StempelAnalyzerProviderFactory(String name, AnalyzerScope scope, Analyzer analyzer) {\r\n        super(name, scope, analyzer);\r\n        analyzerProvider = new PreBuiltAnalyzerProvider(name, scope, analyzer);\r\n    }\r\n\r\n    @Override\r\n    public AnalyzerProvider create(String name, Settings settings) {\r\n        return analyzerProvider;\r\n    }\r\n\r\n    public Analyzer analyzer() {\r\n        return analyzerProvider.get();\r\n    }\r\n}\r\n``` \r\n\r\nAnd instead of:\r\n\r\n```java\r\n    @Inject\r\n    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {\r\n        super(settings);\r\n        indicesAnalysisService.analyzerProviderFactories().put("polish", new PreBuiltAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));\r\n    }\r\n```\r\n\r\ndo \r\n\r\n```java\r\n    @Inject\r\n    public PolishIndicesAnalysis(Settings settings, IndicesAnalysisService indicesAnalysisService) {\r\n        super(settings);\r\n        indicesAnalysisService.analyzerProviderFactories().put("polish", new StempelAnalyzerProviderFactory("polish", AnalyzerScope.INDICES, new PolishAnalyzer(Lucene.ANALYZER_VERSION)));\r\n    }\r\n```\r\n'
5025,'seang-es',"Cluster API command to rebuild a shard in place\nIt would be useful to have the ability to command a node to rebuild an existing replica shard in place, deleting the local copy and re-initializing it from the master.  The same effect can currently be obtained by moving the replica to another host and moving it back, but rebuilding in place would be simpler.  This probably makes the most sense as a 'rebuild' or 'resync' option under the _cluster/reroute command.\r\n\r\nEDIT:  The cancel command can be used to tear down an existing replica and force a rebuild somewhere else in the cluster.  This is not clearly specified in the reference docs.  I suggest rewording the following:\r\nCancel allocation of a shard (or recovery). Accepts index and shard for index name and shard number, and node for the node to cancel the shard allocation on. It also accepts allow_primary flag to explicitly specify that it is allowed to cancel allocation for a primary shard.\r\nto read:\r\nCancel allocation of a shard (or recovery). Accepts index and shard for index name and shard number, and node for the node to cancel the shard allocation on. It also accepts allow_primary flag to explicitly specify that it is allowed to cancel allocation for a primary shard.  This can be used to force resynchronization of existing replicas from the primary shard by cancelling them and allowing them to be reinitialized through the standard reallocation process."
5023,'dadoonet','[DOCS]\xa0add azure and gce discovery plugins\nClean EC2 disco doc\r\nAdd Azure disco doc\r\nAdd Google Compute Engine doc'
5021,'jpountz','Aggregations return different counts when invoked twice in a row\nHi,\r\n\r\nA couple of days ago I started a thread on the mailing list (https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/c_xLCPOpvjc) about this issue, and the responses on it are slim.\r\n\r\nThe problem exists in the aggregations api since version 1.0.0.RC1 and is confirmed by me to also occur in 1.0.0.RC2.\r\n\r\nThe problem is that when you do a terms aggregation on an index sharded in multiple shards (10 in my case) it start to return inconsistent numbers. With this I mean that the numbers are different the second time compared to the first time. You cannot show these numbers to users as when they reload the analytics it shows totally different numbers than before without anything changing to the data.\r\n\r\nI created a test suit as a gist for you to recreate the problem your self. It is hosted at: https://gist.github.com/thanodnl/8803745.\r\n\r\nBut since it contains datafiles it is kind of bugged in the web interface of github. Best you can clone this gist by running: `$ git clone https://gist.github.com/8803745.git`\r\n\r\ncd into the newly created directory and run: `$ ./aggsbug.load.sh` to load the test set into your local database. This can take a couple of minutes since it is loading ~1M documents. I tried to recreate it with a smaller set, but then the issue is not appearing.\r\n\r\nOnce the data is loaded you can run a contained test with: `$ ./aggsbug.test.sh`. This will call the same aggregation twice, store the output, and later print the diff of the output.\r\n\r\nIf you recreated the bug the output of the test should be something like:\r\n\r\n    $ ./aggsbug.test.sh\r\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                     Dload  Upload   Total   Spent    Left  Speed\r\n    100  1088  100   950  100   138    192     27  0:00:05  0:00:04  0:00:01   206\r\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                     Dload  Upload   Total   Spent    Left  Speed\r\n    100  1086  100   948  100   138   2867    417 --:--:-- --:--:-- --:--:--  2872\r\n    diff in 2 aggs calls:\r\n    2c2\r\n    <   "took" : 4918,\r\n    ---\r\n    >   "took" : 325,\r\n    18c18\r\n    <         "doc_count" : 3599\r\n    ---\r\n    >         "doc_count" : 3228\r\n    21c21\r\n    <         "doc_count" : 2517\r\n    ---\r\n    >         "doc_count" : 2254\r\n    24c24\r\n    <         "doc_count" : 2207\r\n    ---\r\n    >         "doc_count" : 2007\r\n    27c27\r\n    <         "doc_count" : 2207\r\n    ---\r\n    >         "doc_count" : 1971\r\n    30c30\r\n    <         "doc_count" : 1660\r\n    ---\r\n    >         "doc_count" : 1478\r\n    33c33\r\n    <         "doc_count" : 1534\r\n    ---\r\n    >         "doc_count" : 1401\r\n    36c36\r\n    <         "doc_count" : 1468\r\n    ---\r\n    >         "doc_count" : 1330\r\n    39c39\r\n    <         "doc_count" : 1079\r\n    ---\r\n    >         "doc_count" : 952\r\n\r\nWhen ran against 1.0.0.Beta2 the output is what is to be expected:\r\n\r\n    $ ./aggsbug.test.sh\r\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                     Dload  Upload   Total   Spent    Left  Speed\r\n    100  1087  100   949  100   138    208     30  0:00:04  0:00:04 --:--:--   208\r\n      % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                     Dload  Upload   Total   Spent    Left  Speed\r\n    100  1086  100   948  100   138   1525    222 --:--:-- --:--:-- --:--:--  1526\r\n    diff in 2 aggs calls:\r\n    2c2\r\n    <   "took" : 4525,\r\n    ---\r\n    >   "took" : 611,\r\n\r\nYou see the output of the aggs is not occurring in the diff during the test, and the only diff between the two runs is the time it took to calculate the result.'
5019,'s1monw','add delete cluster setting and throw exception when validation fails\nAdded `DELETE`, `POST` and modified `PUT` method for "/_cluster/settings"\r\n\r\n`DELETE` request will delete transient and persistent settings, there are two parameters `delete_transient` and `delete_persistent` to control delete transient and persistent settings, they are all default to true. It will return the deleted settings\r\n\r\n`POST` will add the settings in request to the cluster settings (same as previous `PUT`)\r\n\r\n`PUT` will clear the existing settings and add the settings in the request\r\n\r\nAlso will throw exception when validation for settings fail\r\n\r\ncloeses #3670 and #5018'
5018,'s1monw',"Update cluster settings doesn't tell when fails\nWhen update cluster settings, if it's not dynamically updateable or fails to validate, it will just been ignored and log on server side, it doesn't tell the client why it's failed"
5017,'rjernst','Setting "max_expansions" to "0" in a fuzzy query causes a "SearchPhaseExecutionException" on version 1.0.0.RC2\nThe query\r\n\r\n```json\r\n{\r\n   "size": 10,\r\n   "query": {\r\n      "fuzzy": {\r\n         "key": {\r\n            "value": "oslo",\r\n            "fuzziness": 1,\r\n            "prefix_length": 1,\r\n            "max_expansions": 0\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nreturns\r\n\r\n```json\r\n{\r\n   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[yef5AB81QRawVqKl0QwuEg][matcher][0]: QueryPhaseExecutionException[[matcher][0]: query[key:oslo~1],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: RuntimeException[java.lang.NullPointerException]; nested: NullPointerException; }]",\r\n   "status": 500\r\n}\r\n```\r\n\r\nwhile\r\n\r\n```json\r\n{\r\n   "size": 10,\r\n   "query": {\r\n      "fuzzy": {\r\n         "key": {\r\n            "value": "asdfasfsdfasd",\r\n            "fuzziness": 1,\r\n            "prefix_length": 1,\r\n            "max_expansions": 0\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nreturns\r\n\r\n```json\r\n{\r\n   "took": 6,\r\n   "timed_out": false,\r\n   "_shards": {\r\n      "total": 1,\r\n      "successful": 1,\r\n      "failed": 0\r\n   },\r\n   "hits": {\r\n      "total": 0,\r\n      "max_score": null,\r\n      "hits": []\r\n   }\r\n}\r\n```\r\n\r\nthe difference is that the value `oslo` exists in the field `key` in one or more documents while `asdfasfsdfasd` does not. Setting `max_expansions` > 0 does not crash. According to documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.x/query-dsl-fuzzy-query.html "0" is the default value of this property. In my opinion passing the default value of a property should not cause the query to crash.'
5011,'dakrone','Simple Query String: Add `lenient` flag to support *value* parse failures\nThis is related to the issue: https://github.com/elasticsearch/elasticsearch/issues/1932'
5008,'dakrone','Simple Query string does not work for prefix query\nThe prefix feature of the simple query string does not work.\r\n<pre>\r\n{\r\n  "query" : {\r\n    "simple_query_string" : {\r\n      "query" : "Professiona*",\r\n      "flags" : 24\r\n      }\r\n    }\r\n}\r\n</pre>\r\n\r\nThe same query works if using \'query_string\' instead of \'simple_query_string\' (and removing the \'flags\' attribute of course).'
5004,'jpountz',"ScriptBytesValues.currentValueHash is wrong\n`ScriptBytesValues.currentValueHash` doesn't return the hash value of the last returned term. The reason is that it has its own `BytesRef` (`ScriptDocValues.scratch`) to store the term while the hash code is computed on the parent's term (`BytesValues.scratch`)."
5003,'javanna','Run REST tests against multiple nodes\nMultiple nodes are now started when running REST tests against the `TestCluster` (default randomized settings are now used instead of the hardcoded `1`)\r\n\r\nAdded also randomized round-robin based on all available nodes, and ability to provide multiple addresses when running tests against an external cluster to have the same behaviour.\r\n\r\nAlso sped up a few warmer tests and fixed get_source realtime test which might fail in some cases due to automatic refresh.'
5002,'javanna','add redis transport plugin to plugins page\nAdd redis transport link to plugins page\r\n\r\nhttps://github.com/kzwang/elasticsearch-transport-redis'
5001,'jpountz','value_count aggregations should support scripts\nSee https://groups.google.com/forum/#!topic/elasticsearch/sZ4l7-pkdTw and #4999 for reference.'
4991,'colings86','Core: Add EnhancedPatternLayout to logging.yml options\n'
4987,'spinscale','Bulk operation throws exception on invalid index name \nWhen doing a bulk operation, if one of the items holds an invalid index name, the operation returns a top level error (HTTP error in the _bulk API or an exception in case of the Java API).\r\nIt\'s expected it will return the error as part of the bulk result, as done for other types of errors. The error should be returned for the specific item and not fail the entire operation. \r\n\r\nExample:   \r\n```\r\n    curl -XPOST "http://localhost:9200/_bulk" -d\'\r\n    { "index" : { "_index" : "INVALID.NAME", "_type" : "type1", "_id" : "1"} }\r\n    { "field1" : "value1" }\r\n    \'\r\n```\r\n\r\nReturns:\r\n```\r\n{\r\n   "error": "InvalidIndexNameException[[INVALID.NAME] Invalid index name [INVALID.NAME], must be lowercase]",\r\n   "status": 400\r\n}\r\n```\r\n'
4986,'dadoonet',"Support externalValue() in mappers\nSome mappers do not support externalValue() to be set. So plugin developers can't use it while building their own mappers.\r\n\r\nSupport added in this PR for:\r\n\r\n* `BinaryFieldMapper`\r\n* `BooleanFieldMapper`\r\n* `GeoPointFieldMapper`\r\n* `GeoShapeFieldMapper`\r\n"
4984,'dadoonet','Check plugin Lucene version\nCheck that a plugin is Lucene compatible with the current running node using `lucene` property in `es-plugin.properties` file.\r\n\r\n* If plugin does not provide `lucene` property, we consider that the plugin is compatible.\r\n* If plugin provides `lucene` property, we try to load related Enum org.apache.lucene.util.Version. If this fails, it means that the node is too "old" comparing to the Lucene version the plugin was built for.\r\n* We compare then two first digits of current node lucene version against two first digits of plugin Lucene version. If not equal, it means that the plugin is too "old" for the current node.\r\n\r\nPlugin developers who wants to launch plugin check only have to add a `lucene` property in `es-plugin.properties` file. If you are using maven to build your plugin, you can do it like this:\r\n\r\nIn `pom.xml`:\r\n\r\n```xml\r\n    <properties>\r\n        <lucene.version>4.6.0</lucene.version>\r\n    </properties>\r\n\r\n    <build>\r\n        <resources>\r\n            <resource>\r\n                <directory>src/main/resources</directory>\r\n                <filtering>true</filtering>\r\n            </resource>\r\n        </resources>\r\n    </build>\r\n```\r\n\r\nIn `es-plugin.properties`, add:\r\n\r\n```properties\r\nlucene=${lucene.version}\r\n```\r\n\r\nBTW, if you don\'t already have it, you can add the plugin version as well:\r\n\r\n```properties\r\nversion=${project.version}\r\n```\r\n\r\nYou can disable that check using `plugins.check_lucene: false`.\r\n\r\nRelated to https://github.com/elasticsearch/elasticsearch-analysis-smartcn/pull/13.'
4983,'jpountz','Aggregations: Add Child Aggregators to limit parent\'s bucket meta-data\nHaving the ability for a sub-aggregate to work off of buckets from the parent aggregate would be helpful in limiting the amount of information that is returned in results.  Currently, the only way to filter a result set by a count of aggregation is to use a has_child query with a score_type of "sum" and apply a custom scoring routine which filters scores of a certain amount.\r\n\r\nI propose adding a child-aggregate, which can be placed anywhere a sub-aggregate can be placed, but the child only has access to the parent aggregate\'s buckets instead of access to the documents of the query.\r\n\r\nExample:  given a flat log of recorded user actions, where each log document is parented to a user document, find users who had 3 or more failed logins (in a given time period).\r\n\r\n```\r\n{\r\n   "min_score": 0,\r\n   "query": {\r\n      "function_score": {\r\n         "boost_mode": "multiply",\r\n         "functions": [\r\n            {\r\n               "script_score": {\r\n                  "params": {\r\n                     "cutoff": 2\r\n                  },\r\n                  "script": "_score < cutoff ? -1 : 1"\r\n               }\r\n            }\r\n         ],\r\n         "query": {\r\n            "has_child": {\r\n               "type": "user_actions",\r\n               "score_type": "sum",\r\n               "query": {\r\n                  "term": {"log_action": "login_failed"  }\r\n                  }\r\n               }\r\n            }\r\n         }\r\n}}\r\n```\r\ngives...\r\n```\r\n{\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 4,\r\n    "hits" : [ {\r\n      "_index" : "test",\r\n      "_type" : "user",\r\n      "_id" : "100",\r\n      "_score" : 3\r\n    }, {\r\n      "_index" : "test",\r\n      "_type" : "user",\r\n      "_id" : "150",\r\n      "_score" : 2\r\n    } ]\r\n  }\r\n}\r\n```\r\nwhereas an aggregate like this\r\n```\r\n{\r\n    "query": { \r\n        "match_all": {}\r\n    },\r\n    "aggs": { \r\n        "unique_users": { \r\n            "terms": {"field":"email"},\r\n            "aggs": { \r\n                "action_totals": { \r\n                    "terms": { \r\n                        "field": "log_action"\r\n                    }\r\n                } \r\n            } \r\n        } \r\n    } \r\n}\r\n```\r\nwould yield too much information in the results...\r\n```\r\n  "aggregations" : {\r\n    "unique_users" : {\r\n      "buckets" : [ {\r\n        "key" : "user1@example.org",\r\n        "doc_count" : 24,\r\n        "action_totals" : {\r\n          "buckets" : [ {\r\n            "key" : "login",\r\n            "doc_count" : 18\r\n          }, {\r\n            "key" : "failed_login",\r\n            "doc_count" : 5\r\n          }, {\r\n            "key" : "change_password",\r\n            "doc_count" : 1\r\n          } ]\r\n        }\r\n      }, {\r\n        "key" : "user2@example.org",\r\n        "doc_count" : 14,\r\n        "action_totals" : {\r\n          "buckets" : [ {\r\n            "key" : "login",\r\n            "doc_count" : 11\r\n          }, {\r\n            "key" : "failed_login",\r\n            "doc_count" : 3\r\n          } ]\r\n        }\r\n      },\r\n```\r\n\r\nAdding the ability for child-aggs to work off their parent meta-data\r\n```\r\n{\r\n    "query": { \r\n        "match_all": {}\r\n    },\r\n    "aggs": { \r\n        "unique_users": { \r\n            "terms": {"field":"email"},\r\n            "aggs": { \r\n                "action_totals": { \r\n                    "terms": { \r\n                        "field": "log_action"\r\n                    },\r\n                    "child-aggs": {\r\n                        "limiting_range": {\r\n                            "range": {\r\n                                "field": "doc_count",\r\n                                "range": [\r\n                                {"from":3}\r\n                                ]\r\n                            }   \r\n                        },\r\n                        "limiting_action": {\r\n                            "term": {\r\n                                "key": "failed_login"\r\n                            }   \r\n                        }\r\n                    }\r\n                } \r\n            } \r\n        } \r\n    } \r\n}\r\n```\r\nWould limit the results to only the interested data\r\n```\r\n  "aggregations" : {\r\n    "unique_users" : {\r\n      "buckets" : [ {\r\n        "key" : "user1@example.org",\r\n        "doc_count" : 5,\r\n        "action_totals" : {\r\n          "buckets" : [ {\r\n            "key" : "failed_login",\r\n            "doc_count" : 5\r\n          } ]\r\n        }\r\n      }, {\r\n        "key" : "user2@example.org",\r\n        "doc_count" : 3,\r\n        "action_totals" : {\r\n          "buckets" : [ {\r\n            "key" : "failed_login",\r\n            "doc_count" : 3\r\n          } ]\r\n        }\r\n      },\r\n```\r\nThis new way of aggregating and limiting does not require a parent-child relationship between the user and the log document types and only requires searching the log type documents to produce the desired results.\r\n\r\nNote that the original top-level aggregations have new doc_count values.  Child-aggregates can be thought to be only limiting actions - never creating new buckets (as that is the role of sub-aggregates already).  Their output can replace the parent aggregate\'s entire result.\r\n\r\nLimitations:  child-aggregates must only be run after all information in the parent aggregate has been collected.  Any aggregate that has a limiting child-aggregate would not be allowed to have sub-aggregates, because changing the parent\'s meta-information would cause discontinuity with the sub-aggregation buckets.'
4980,'javanna','add elasticsearch-osem to integrations page\n'
4965,'javanna',"[TEST] Added ability to skip REST test suites/sections based on their required features\nAs we have different runners for the REST tests we need a mechanism that allows to add features to any of them without breaking all other runners builds.\r\nThe idea is to name features and temporarily use skip sections that mention the required new features, so that runners that don't support it yet will skip the test.\r\n\r\nAdded support for `features` field in skip section.\r\nAdded `Features` class that contains a static list of the features supported by the runner. If a feature mentioned in a skip section is not listed here, the test will be skipped."
4958,'spinscale',"Startup: Add ES_HOME to ES_INCLUDE search path\nWith this change, the elasticsearch script can be linked to another path without having to set ES_INCLUDE to match the installation path. Previously, the elasticsearch would find ES_HOME correctly even if linked but could not find the include script, and finding it would be expected behavior to me based on its current search path.\r\n\r\nI'm going straight to a pull request on this, as it's a minor improvement to the executable and solutions to ES_INCLUDES are generally workarounds listed on closed issues."
4957,'martijnvg',"The binary field shouldn't be stored by default, because it is already available in the _source\n"
4956,'imotov','closes #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"\ncloses #4949 : bug fix : handle "true"/"false" in snapshot api for "include_global_state"\r\n\r\n[see issue #4949](https://github.com/elasticsearch/elasticsearch/issues/4949)'
4954,'spinscale','Cluster State API: Remove index template filtering\nAs the get index template API does a pretty good job of selecting and showing specific index templates to the user, we could potentially remove this functionality, when using the cluster state API - as this functionality is no more needed at that place.\r\n\r\nUp for discussion.'
4949,'imotov','Snapshot API : Why is "include_global_state" boolean handled differently from "ignore_unavailable" in the API?\nWhy is "include_global_state" boolean handled differently from "ignore_unavailable" in the API?\r\n\r\nOne handles string "true"/"false" one does not?\r\n\r\nFrom ```CreateSnapshotRequest```\r\n\r\n```\r\n} else if (name.equals("ignore_unavailable") ||  name.equals("ignoreUnavailable")) {\r\n                ignoreUnavailable = nodeBooleanValue(entry.getValue());\r\n}\r\n```\r\n\r\nvs.\r\n\r\n```\r\nif (!(entry.getValue() instanceof Boolean)) {\r\n                    throw new ElasticsearchIllegalArgumentException("malformed include_global_state, should be boolean");\r\n}\r\n```'
4946,'imotov','Add stats for running snapshot to get snapshot API\n'
4941,'dadoonet','scroll REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in `/_search/scroll` REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\ncurl -XPOST "http://localhost:9200/test/type/1" -d\'\r\n{\r\n    "foo": "bar"\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/_search/scroll" -d "FAKESCROLLID"\r\n\r\n# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}\r\ncurl -XGET "http://localhost:9200/_search/scroll/?source=FAKESCROLLID"\r\n```\r\n'
4940,'martijnvg',"Improve scroll search by using Lucene's IndexSearcher#searchAfter(...)\nImprove the regular scroll search by using Lucene's searchAfter, which allows subsequent scroll request to always have a priority queue size equal to the specified `size` in the first search request. (priority queue is used to collect the competitive hits that match with a query)\r\n\r\nCurrently the priority queue size grows with each subsequent scroll request with what has been specified in `from` of the first search request.\r\n\r\nNote: scan scroll is unaffected by this issue, which already is a highly optimized search to fetch a large part or all docs from a cluster. Scan scroll forcefully sort the hits always by the Lucene docids, while with the regular scroll can now support any sort efficiently."
4936,'dadoonet','Problems upgrading from elasticsearch 0.90.5 to 0.90.10 with elasticsearch-analysis-stempel\nWhen trying to upgrade from elasticsearch 0.90.5 with elasticsearch-analysis-stempel 1.7.0 to elasticsearch 0.90.10 with elasticsearch-analysis-stempel 1.9.0 on a multi node cluster I receive the following error:\r\n\r\norg.elasticsearch.indices.IndexCreationException: [apps] failed to create index [1]\r\n\r\njava version info:\r\njava -version\r\njava version "1.7.0_51"\r\nJava(TM) SE Runtime Environment (build 1.7.0_51-b13)\r\nJava HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode\r\n\r\nLet me know if I can provide you with any additional information.\r\n\r\n[1] https://gist.github.com/jasonthomas/516f17371d32f64d2565'
4930,'martijnvg',"Move parent/child over from id cache to field data\nMove all parent/child queries (has_child, has_parent, top_children) from id cache to field data. This has a number of advantages:\r\n* Parent/child memory footprint will get reduced by using field data, compared to what it now takes with id cache. The id cache use concrete object arrays to store the parent ids which is wasteful in terms of memory usage compared the field data which uses native byte arrays to store the parent ids (via Lucene's PagedBytes). Initial benchmarks have shown that the memory usage can be reduced up to half with parent/child using field data.\r\n* Parent child can use paged data structures because field data uses paged data structures under the hood as well. This will result in a better stability because on the jvm level, because of less garbage collection, which boils down to the fact that the storage behind paged data structures is reused between requests and paged data structures taking less memory in general compared to the concrete object arrays in id cache. \r\n* By reusing the field data parent/child can reuse its infrastructure For example using the CircuitBreaker to fail search requests if too much memory is being spent on parent/child rather then going out of memory.\r\n* The id cache is similar to field data in a sense that represents field values into memory by removing the id cache a lot of duplicate logic / code will be removed.\r\n\r\nThese advantages come at a cost of a small performance loss of up to 10% in query time execution, but the advantages outweigh the performance loss in terms of stability, predictability (less sudden gc collections) and less memory usage. \r\n\r\nThe id cache can be removed, since nothing inside ES is using it. For backward compatibility reason in 1.x releases the id cache statistics will be reported as was before, but it will be based on the `_parent` field in field data and the `_parent` field will not be reported in field data statistics."
4929,'jpountz','Move Aggregations reduce phase to use Paged recycler enabled structures\nWe moved the shard level computation to use paged recycler based data structures, it would be great to also use it in the reduce phase.'
4927,'s1monw','HotThreads fail with AIOOB if busiestThreads > actual threads \nwe currently use the number of hot threads that we are interested in as the value for iterating over the actual hot threads which can lead to AIOOB is the actual number of threads is less than the given number.\r\n\r\nwhich will result in an exception like this:\r\n\r\n```\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 93, Size: 93\r\n\tat java.util.ArrayList.RangeCheck(ArrayList.java:547)\r\n\tat java.util.ArrayList.get(ArrayList.java:322)\r\n\tat org.elasticsearch.monitor.jvm.HotThreads.innerDetect(HotThreads.java:149)\r\n\tat org.elasticsearch.monitor.jvm.HotThreads.detect(HotThreads.java:75)\r\n\tat org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:101)\r\n\t... 5 more\r\n```'
4924,'dadoonet','Fix potential NPE when no source and no body\nIn recent changes, we added missing support for `source` parameter in some REST APIs:\r\n\r\n* #4892 : mget\r\n* #4900 : mpercolate \r\n* #4901 : msearch\r\n* #4902 : mtermvectors\r\n* #4903 : percolate\r\n\r\n```java\r\n        BytesReference content = null;\r\n        if (request.hasContent()) {\r\n            content = request.content();\r\n        } else {\r\n            String source = request.param("source");\r\n            if (source != null) {\r\n                content = new BytesArray(source);\r\n            }\r\n        }\r\n```\r\n\r\nIt\'s definitely better to have:\r\n\r\n```java\r\n        BytesReference content = request.content();\r\n        if (!request.hasContent()) {\r\n            String source = request.param("source");\r\n            if (source != null) {\r\n                content = new BytesArray(source);\r\n            }\r\n        }\r\n```\r\n\r\nThat said, it could be nice to have a single method to manage it for various REST actions.\r\nWhere should we put it in?'
4923,'javanna','Added support for aliases to create index api\nIt is now possible to specify aliases during index creation:\r\n\r\n```\r\ncurl -XPUT \'http://localhost:9200/test\' -d \'\r\n{\r\n    "aliases" : {\r\n        "alias1" : {},\r\n        "alias2" : {\r\n            "filter" : { "term" : {"field":"value"}}\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nCloses #4920'
4920,'javanna','Create index to support aliases\nThe create index api currently supports providing mappings, settings and warmers. It would be nice to be able to provide aliases as well in the same request, during index creation.'
4918,'spinscale','closes #4917 - export ES_MIN_MEM and ES_MAX_MEM in redhat init script\n'
4917,'spinscale','redhat init script does not export ES_MIN_MEM and ES_MAX_MEM\nBy not doing so, setting the variables in /etc/sysconfig/elasticsearch does not actually do what is necessary - override the Xms and Xmx values on startup. '
4912,'spinscale',"[ 0.90.9] Completion - updated properties are not reflected \nI'm using the completion feature on 0.90.9. I've a document which I index with multiple inputs and suggest based search works OK. Then I deleted that doc and re-create it with different inputs, and it still being returned with the first search I did, although I examined the doc (head plugin) and the input field is indeed updated, which does not match that search.\r\nNext thing I re-created the doc with different output field, and again - although seen when I examine the doc (head plugin), the api ( REST and JAVA concreteOption.getText().toString() ) returns the old output."
4911,'dakrone','simple_query_string flags Does not seem to support "PHRASE" flag\nusing Elastic search "0.90.10", "lucene_version" : "4.6"\r\n\r\nRelated to https://groups.google.com/d/topic/elasticsearch/TYVqdYQNZVQ/discussion\r\n\r\nI am trying to use the new "simple_query_string" when I add to the flags "PHRASE" I get "Unknown simple_query_string flag [PHRASE]]"\r\n\r\nHowever If I add the "ALL" flag, it seems to work.\r\n\r\nExample:\r\n```sh\r\ncurl -XPOST localhost:9200/test/_search -d \'{\r\n    "query": {\r\n        "filtered": {\r\n            "query": {\r\n                "simple_query_string": {\r\n                    "query": "horse",\r\n                    "fields": [\r\n                        "sreferenceNumber^20",\r\n                        "sTitle^2",\r\n                        "sDescription"\r\n                    ],\r\n                    "flags":"PHRASE",\r\n                    "default_operator":"AND"\r\n\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nIf I try that same query with the Flags set to all I get results.\r\n\r\nError I am getting, is:\r\n>ElasticSearchIllegalArgumentException[Unknown simple_query_string flag [PHRASE]];\r\n\r\nFrom http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html\r\nIt seems that it should be possible to send in the "PHRASE" flag.'
4910,'dadoonet','mtermvectors REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in msearch REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPOST "http://localhost:9200/test/type/1?refresh" -d\'{\r\n    "foo": "bar"\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/type/_mtermvectors" -d\'\r\n{\r\n    "ids" : ["1"]\r\n}\'\r\n\r\n# This one gives: "ActionRequestValidationException[Validation Failed: 1: multi term vectors: no documents requested;]"\r\ncurl -XGET "http://localhost:9200/test/type/_mtermvectors?source=%7B%22ids%22%3A%5B%221%22%5D%7D"\r\n```\r\n\r\nCloses #4902.'
4908,'dadoonet','mpercolate REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in mpercolate REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPUT "http://localhost:9200/test/.percolator/1" -d\'\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "foo" : "bar"\r\n        }\r\n    }\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/message/_mpercolate" -d \'\r\n{"percolate" : {}}\r\n{"doc" : {"foo" : "bar is in foo"}}\r\n\'\r\n\r\n# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];\r\ncurl -XGET "http://localhost:9200/test/message/_mpercolate?source=%7B%22percolate%22%3A%7B%7D%7D%0A%7B%22doc%22%3A%7B%22foo%22%3A%22bar is in foo%22%7D%7D%0A"\r\n```\r\n\r\nCloses #4900.'
4907,'martijnvg',"Add thread pool cat api\nAdd dedicated thread pool cat api, that can show all thread pool related statistic (size, rejected, queue etc.) for all thread pools (get\r\n    \r\nBy default active, rejected and queue thread statistics should be included for the index, bulk and search thread pool.\r\n\r\n```\r\n$curl 'localhost:9200/_cat/thread_pool?v'\r\nhost      ip            bulk.active bulk.queue bulk.rejected index.active index.queue index.rejected search.active search.queue search.rejected \r\nmvg.local 10.20.100.174           0          0             0            0           0              0             0            0               0\r\n```\r\n    \r\nOther thread statistics of other thread pools can be included via the `h` query string parameter.\r\n\r\n```\r\n$curl 'localhost:9200/_cat/thread_pool?v&h=id,host,index.completed'\r\nid   host      index.completed \r\nSHLd mvg.local               1 \r\n```"
4905,'dadoonet','msearch REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in msearch REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPOST "http://localhost:9200/test/type/1?refresh" -d\'{\r\n    "foo": "bar"\r\n}\'\r\n\r\ncat requests\r\n{}\r\n{"query" : {"match_all" : {}}}\r\n\r\n# This one works\r\ncurl -XGET localhost:9200/_msearch --data-binary @requests\r\n\r\n# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}\r\ncurl -XGET "http://localhost:9200/test/type/_mget?source=%7B%7D%0A%7B%22query%22%3A%7B%22match_all%22%3A%7B%7D%7D%7D%0A"\r\n```\r\n\r\nCloses #4901.'
4904,'javanna','A bit of extra javadoc for updates\nAdding javadoc to UpdateRequestBuilder for a couple of details it took me a while to find.'
4903,'dadoonet','percolate REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in percolate REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPUT "http://localhost:9200/test/.percolator/1" -d\'\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "foo" : "bar"\r\n        }\r\n    }\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/message/_percolate" -d \'{\r\n  "doc" : {\r\n    "foo" : "bar is in foo"\r\n  }\r\n}\'\r\n\r\n# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];\r\ncurl -XGET "http://localhost:9200/test/message/_percolate?source=%7B%22doc%22%3A%7B%22foo%22%3A%22bar%20is%20in%20foo%22%7D%7D"\r\n```\r\n'
4902,'dadoonet','mtermvectors REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in msearch REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPOST "http://localhost:9200/test/type/1?refresh" -d\'{\r\n    "foo": "bar"\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/type/_mtermvectors" -d\'\r\n{\r\n    "ids" : ["1"]\r\n}\'\r\n\r\n# This one gives: "ActionRequestValidationException[Validation Failed: 1: multi term vectors: no documents requested;]"\r\ncurl -XGET "http://localhost:9200/test/type/_mtermvectors?source=%7B%22ids%22%3A%5B%221%22%5D%7D"\r\n```'
4901,'dadoonet','msearch REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in msearch REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPOST "http://localhost:9200/test/type/1?refresh" -d\'{\r\n    "foo": "bar"\r\n}\'\r\n\r\ncat requests\r\n{}\r\n{"query" : {"match_all" : {}}}\r\n\r\n# This one works\r\ncurl -XGET localhost:9200/_msearch --data-binary @requests\r\n\r\n# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}\r\ncurl -XGET "http://localhost:9200/test/type/_mget?source=%7B%7D%0A%7B%22query%22%3A%7B%22match_all%22%3A%7B%7D%7D%7D%0A"\r\n```\r\n'
4900,'dadoonet','mpercolate REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in mpercolate REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPUT "http://localhost:9200/test/.percolator/1" -d\'\r\n{\r\n    "query" : {\r\n        "match" : {\r\n            "foo" : "bar"\r\n        }\r\n    }\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/message/_mpercolate" -d \'\r\n{"percolate" : {}}\r\n{"doc" : {"foo" : "bar is in foo"}}\r\n\'\r\n\r\n# This one gives: BroadcastShardOperationFailedException[[test][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticsearchIllegalArgumentException[Nothing to percolate];\r\ncurl -XGET "http://localhost:9200/test/message/_mpercolate?source=%7B%22percolate%22%3A%7B%7D%7D%0A%7B%22doc%22%3A%7B%22foo%22%3A%22bar is in foo%22%7D%7D%0A"\r\n```\r\n'
4897,'s1monw',"Upgrade to Lucene 4.6.1\nThe Lucene 4.6.1 vote passed so once it's available on the maven mirrors we should upgrade all our branches"
4895,'brwe','Queries with preference local do not respect requested fields\nWhen querying with "preference": "_local", the _source is returned instead of the requested fields.\r\n\r\n```$ curl -XPUT \'http://ks398280.kimsufi.com:9200/test/test/1\' -d \'{"user": "foo", "amount": 35, "data": "some more data"}\'```\r\n\r\n```json\r\n$ curl -XPOST \'http://ks398280.kimsufi.com:9200/test/test/_search?pretty\' -d  \'{\r\n    "query": {\r\n        "match": {\r\n            "data": {\r\n                "query": "some"\r\n             }\r\n        }\r\n    },\r\n    "fields": ["user", "amount"]\r\n}\'\r\n\r\n{\r\n  "took" : 1,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 0.15342641,\r\n    "hits" : [ {\r\n      "_index" : "test",\r\n      "_type" : "test",\r\n      "_id" : "1",\r\n      "_score" : 0.15342641,\r\n      "fields" : {\r\n        "amount" : 35,\r\n        "user" : "foo"\r\n      }\r\n    } ]\r\n  }\r\n}\r\n```\r\n\r\n\r\n```json\r\n$ curl -XPOST \'http://ks398280.kimsufi.com:9200/test/test/_search?pretty\' -d  \'{\r\n    "query": {\r\n        "match": {\r\n            "data": {\r\n                "query": "some"\r\n             }\r\n        },\r\n        "preference": "_local"\r\n    },\r\n    "fields": ["user", "amount"],\r\n}\'\r\n\r\n{\r\n  "took" : 1,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 0.15342641,\r\n    "hits" : [ {\r\n      "_index" : "test",\r\n      "_type" : "test",\r\n      "_id" : "1",\r\n      "_score" : 0.15342641, "_source" : {"user": "foo", "amount": 35, "data": "some more data"}\r\n    } ]\r\n  }\r\n}\r\n```'
4892,'dadoonet','mget REST API should support source parameter\nAs stated in documentation, we should support `?source=` parameter in mget REST operations.\r\n\r\nThis is how to reproduce it:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test"\r\n\r\ncurl -XPOST "http://localhost:9200/test/type/1?refresh" -d\'{\r\n    "foo": "bar"\r\n}\'\r\n\r\n# This one works\r\ncurl -XPOST "http://localhost:9200/test/type/_mget" -d\'{\r\n    "ids": ["1"]\r\n}\'\r\n\r\n# This one gives: {"error":"Failed to derive xcontent from org.elasticsearch.common.bytes.BytesArray@0"}\r\ncurl -XGET "http://localhost:9200/test/type/_mget?source=%7B%22ids%22%3A%20%5B%221%22%5D%7D"\r\n```\r\n\r\n'
4885,'spinscale',"Filtered cluster state still returns stub elements\nThis request:\r\n\r\n    GET /_cluster/state/nodes\r\n\r\nshould only return the `nodes` element, but it still returns `metadata`, `routing_table` etc.  They're empty, but still present.\r\n"
4884,'javanna','Add resetSort() methods to SearchSourceBuilder?\nGreetings,\r\nIn order to use JavaAPI for search request building, it would be really handy to modify or reset contents of `sorts`, `facets` etc. What do you think?'
4879,'MaineC','Allow for executing queries based on pre-defined templates\nIt would be nice to be able to store pre-defined query templates that can be referenced and filled with parameter values at query time. This can be in particular useful to be able to quickly replay queries with slightly different templates but identical parameter values e.g. in order to compare slightly different ways to formulate Elasticsearch queries wrt. runtime performance and actual results returned.\r\n\r\nFor developing and testing the template, both, template_string and template_vars can be submitted as part of the search request:\r\n\r\n\r\n```json\r\nGET _search\r\n{\r\n    "query": {\r\n        "template": {\r\n            "template_string": "{\\"match_{{template}}\\": {}}\\"",\r\n            "template_vars" : {\r\n                "template" : "all"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nYou register a template by storing it in the conf/scripts directory of\r\nelasticsearch. In order to execute the stored template reference it in the query parameters:\r\n\r\n```json\r\nGET _search\r\n{\r\n    "query": {\r\n        "template": {\r\n            "template_string": "storedTemplate",\r\n            "template_vars" : {\r\n                "template" : "all"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n\r\nTemplate language\r\n-----------------\r\n\r\nTemplating is based on Mustache. Substitution of tokens works as follows:\r\n\r\n```json\r\n            "template_string": "{\\"match_{{template}}\\": {}}\\"",\r\n            "template_vars" : {\r\n                "template" : "all"\r\n``` \r\n'
4876,'jpountz',"No index mapper found for field: [<field name>] returning default posting format\ni have index with multiple parent/child related document types\r\nwhen i bulk index parent documents im getting this warning logged for one of the **child documents**\r\nwhy child documents are 'touched' when parent document is indexed ?\r\nalso im getting hundreds of this warnings per minute so any suggestion how to fix it will be really appriciated\r\n\r\nI know that this issue was already raised before (#3088), but it was not exactly fixed (more  like hacked by failing over to default posting format, so i still hope for real fix). \r\n\r\ncontext:\r\n- es version 1.0.0.RC1 (updated from 0.90.10)\r\n- 3 node cluster\r\n- 12 shards per index\r\n- 2 replicas\r\n\r\n   \r\n\r\n"
4875,'jpountz','Geo-hash grid aggregations: make size==0 return all hashes\nLikewise terms aggregations (#4837), setting `size=0` on geo-hash grid aggregations should return all buckets.'
4874,'spinscale','Can\'t set http.max_initial_line_length\nI am getting this error when my Kibana time frame is more than about 6 months:\r\n\r\n"org.elasticsearch.common.netty.handler.codec.frame.TooLongFrameException: An HTTP line is larger than 4096 bytes." \r\n\r\nI have changed the http.max_initial_line_length to 8kb in elasticsearch.yml as described in #3210 but keep getting exactly the same error. \r\n\r\nAm I doing something wrong?\r\n\r\nThanks in advance :)'
4871,'drewr',"RestTable.renderValue() doesn't know about tera and peta\n"
4867,'s1monw','BalancedShardAllocator makes non-deterministic rebalance decisions\nNOTE: this is not a problem in production! It happens that the allocator iterates over the keys of a set which might be different across runs. This makes unittests non-reproducible. We should tie-break on the shard ID in that case.'
4864,'javanna',"Rivers might not get started due to missing _meta document\nWhen a new river is registered by indexing its `_meta` document, its type gets created via dynamic mappings, which triggers a cluster state update task on the master node. The update triggers a cluster state listener (`RiversRouter`) that executes only on the master. The master node looks for the river `_meta` document (get with `preference=_primary`) and schedules a retry in case it is not found (since #4089).\r\n\r\nOnce the master node has found the `_meta` document it decides where to allocate the river and  publishes the new river cluster state containing that information. At that point each node receives the new river cluster state and the node where the river is supposed to be allocated on will start the river locally (`RiversService.ApplyRivers`).\r\n\r\nIn order for the river to be properly allocated, the `_meta` document has to be found through get api. There is a retry mechanism in case the get fails, but not in case the `_meta` document is not found, which can currently happen as this second get doesn't set `preference` to `_primary`, thus the `_meta` document could be found by the master node on the primary shard, but not on the second get call done by the node that is trying to start the river locally. This happens when the document replication hasn't been completed yet.\r\n\r\nLong story short: no retry needed, we just need to add `preference=_primary` to the second get call."
4863,'jpountz',"Improve Aggregations documentation\n* Mostly minor things like typos and grammar stuff\r\n* Some clarifications\r\n* The note on the deprecation was ambiguous. I've removed the problematic part so that it now definitely says it's deprecated"
4857,'martijnvg','Null Pointer Exception updating default mapping to multi_field\nWith 0.90.10, if I index a document with the default mapping, and then try to update the mapping to change a field type to multi_field, the call returns with a 500 error reporting a null pointer exception.\r\n\r\n```javascript\r\n// curl -XPUT \'http://localhost:9200/i/t/1\' -d \'{"version":"1"}\'\r\n{\r\n  "_version": 1,\r\n  "_id": "1",\r\n  "_type": "t",\r\n  "_index": "i",\r\n  "ok": true\r\n}\r\n// curl -XGET \'http://localhost:9200/i/t/_mapping\'\r\n{\r\n  "t": {\r\n    "properties": {\r\n      "version": {\r\n        "type": "string"\r\n      }\r\n    }\r\n  }\r\n}\r\n// curl -XPUT \'http://localhost:9200/i/t/_mapping\' -d \'{"t":{"properties":{"version":{"type":"multi_field"}}}}\'\r\n{\r\n  "status": 500,\r\n  "error": "RemoteTransportException[[hostname.domainname][inet[/192.168.1.1:9300]][indices/mapping/put]]; nested: NullPointerException; "\r\n}\r\n```'
4854,'spinscale','deprecate index status\nWith the addition of the recovery API in issue #4637 the index status API call will be redundant.  We could then deprecate and eventually eliminate the call.'
4852,'drewr','cat headers are inconsistent, incomplete\nThere are some dups and need more aliases...'
4851,'martijnvg','ClearScrollRequest should set a type parameter\nSince ActionRequest requires a bounded type parameter.'
4849,'jpountz','WARNING "failed to prepare/warm" after upgrading from 0.90.3 to 0.90.10\nI have check release note from 0.90.3 to 0.90.10 to see if warmer need some change, but nothing.\r\n\r\nInitially reported on ML : https://groups.google.com/d/msg/elasticsearch/xkX5RVII-Gk/BRd28_zbZUIJ\r\n\r\nTwo samples stack trace :\r\n\r\n```\r\n[2014-01-16 17:27:00,669][WARN ][index.engine.robin       ] [integration] [m112][0] failed to prepare/warm\r\njava.lang.IllegalMonitorStateException\r\n        at java.lang.Object.wait(Native Method)\r\n        at java.lang.Object.wait(Object.java:485)\r\n        at org.elasticsearch.search.SearchService$SearchWarmer$2.awaitTermination(SearchService.java:822)\r\n        at org.elasticsearch.indices.warmer.InternalIndicesWarmer.warm(InternalIndicesWarmer.java:99)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchFactory.newSearcher(RobinEngine.java:1652)\r\n        at org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:155)\r\n        at org.apache.lucene.search.SearcherManager.<init>(SearcherManager.java:89)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1530)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:277)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:660)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:201)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:174)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\n```\r\n```\r\n[2014-01-16 11:17:36,836][WARN ][index.engine.robin       ] [sissor2-pp] [m105][0] failed to prepare/warm\r\njava.lang.IllegalMonitorStateException\r\n        at java.lang.Object.wait(Native Method)\r\n        at java.lang.Object.wait(Object.java:503)\r\n        at org.elasticsearch.search.SearchService$SearchWarmer$2.awaitTermination(SearchService.java:822)\r\n        at org.elasticsearch.indices.warmer.InternalIndicesWarmer.warm(InternalIndicesWarmer.java:99)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine$RobinSearchFactory.newSearcher(RobinEngine.java:1652)\r\n        at org.apache.lucene.search.SearcherManager.getSearcher(SearcherManager.java:155)\r\n        at org.apache.lucene.search.SearcherManager.<init>(SearcherManager.java:89)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1530)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:277)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:660)\r\n        at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:389)\r\n        at org.elasticsearch.indices.recovery.RecoveryTarget$PrepareForTranslogOperationsRequestHandler.messageReceived(RecoveryTarget.java:363)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n```\r\n\r\n'
4846,'martijnvg','Never cache a range filter that uses the now date math expressions\nNever cache a range filter that uses the now date math expressions or compound filters that wrap this kind of filters.'
4845,'dadoonet',"Serving _site plugins do not pick up on index.html for sub directories\nIf one asks for `http://es:9200/_plugin/PLUGIN_NAME/` and the the plugin's _site directory contains an index.html file, it will be correctly served. \r\n\r\nThis is not the case for sub directories: a _site/folder/index.html is not served when requesting  `http://es:9200/_plugin/PLUGIN_NAME/folder/` but one gets a 403 Forbidden response as if trying to browse the folder.\r\n"
4844,'javanna','[DOCS] Various small documentation fixes\nNot sure if this PR should contain multiple commits so that you can cherry-pick or shall I squash the commits?\r\n\r\nMost changes are tiny, but the documented transport.tcp.connect_timeout should be incorrect. It perhaps was the main cause of instability in my cluster since I "raised" it to 5s. :)'
4841,'jpountz',"Remove some abstraction in aggregations collection\nI have been running some basic aggregations benchmarks (the ones in src/test) and it appears that some of the abstraction in aggregations is causing a slight performance hit. The thing is that `collect` is called in a very tight loop so even stuff that usually doesn't matter seems to have an impact here.\r\n\r\nFor instance I tried to change some aggregators to implement ReaderContextAware instead of polling for Bytes/Long/DoubleValues in every call to collect and this improved the response times by a few percents."
4837,'jpountz','Terms aggregations: make size=0 return all terms\nThis is a fork of #1776 for terms aggregations.\r\n\r\nSetting `size: 0` should make the number of terms returned by terms aggregations unlimited.'
4835,'drewr',"_cat/allocation doesn't show disk percentage when 100% of disk is used\nExample:\r\n\r\n```\r\n∴ get 'localhost:9200/_cat/allocation?v'\r\nshards diskUsed diskAvail diskTotal diskPercent host         ip           node\r\n    17   55.4mb   117.4mb   172.8mb          32 Xanadu.local 192.168.0.30 Jonathan Richards\r\n    16     39mb        0b      39mb             Xanadu.local 192.168.0.30 Amiko Kobayashi\r\n    17   53.7mb    43.8mb    97.6mb          55 Xanadu.local 192.168.0.30 Shadow King\r\n```\r\n\r\nThe `diskPercent` field is missing."
4833,'jpountz','Allocation on a non-data node causes a NPE\nI mistakenly tried to allocate a shard on a node which is not a data-node and this resulted in a cryptical `NullPointerException`. We should throw a meaningful exception instead.'
4827,'martijnvg','id_cache stats returns negative values\nafter executing **_cache/clear** action _cluster/stats started returning negative values for id_cache\r\n\r\n```JSON\r\n{\r\n"id_cache": {\r\n         "memory_size": "-1E10b",\r\n         "memory_size_in_bytes": -10313508276\r\n      }\r\n}\r\n```\r\n\r\nalso i started seeing warning below quite often in my logs:\r\n```\r\n[2014-01-21 11:19:28,706][WARN ][transport.netty          ] [<node name>] Message not fully read (response) for [9917139] handler org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$4@37970778, error [false], resetting\r\n\r\n```\r\n\r\ncontext:\r\n- es version 0.90.10\r\n- java version: 1.7.0_51 (oracle)\r\n- os: ubuntu 12.04\r\n- 3 node cluster\r\n- 12 shards per index\r\n- 2 replicas\r\n- around 100,000,000 parent/child related documents (below 1kb each)\r\n\r\n'
4824,'dadoonet','Add _cat/plugins endpoint\nIf we want to have a full picture of plugins running in a cluster, we need to add a `_cat/plugins` endpoint.\r\n\r\nResponse could look like:\r\n\r\n```sh\r\n% curl es2:9200/_cat/plugins?v\r\nnode component                        version   type url                                   desc\r\nes1  mapper-attachments               1.7.0       j                                        Adds the attachment type allowing to parse difference attachment formats\r\nes1  lang-javascript                  1.4.0       j                                        JavaScript plugin allowing to add javascript scripting support\r\nes1  analysis-smartcn                 1.9.0       j                                        Smart Chinese analysis support\r\nes1  marvel                           1.1.0      j/s http://localhost:9200/_plugins/marvel Elasticsearch Management & Monitoring\r\nes1  kopf                             0.5.3       s  http://localhost:9200/_plugins/kopf   kopf - simple web administration tool for ElasticSearch\r\nes2  mapper-attachments               2.0.0.RC1   j                                        Adds the attachment type allowing to parse difference attachment formats\r\nes2  lang-javascript                  2.0.0.RC1   j                                        JavaScript plugin allowing to add javascript scripting support\r\nes2  analysis-smartcn                 2.0.0.RC1   j                                        Smart Chinese analysis support\r\n```\r\n'
4818,'javanna','Deprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean\nDeprecated ToXContent.Params#paramAsBooleanOptional in favour of paramAsBoolean\r\n\r\nCloses #4817'
4817,'javanna','Unify RestRequest paramAsBoolean and paramAsBooleanOptional\nThis is a follow-up of #4808: since `paramAsBoolean` and `paramAsBooleanOptional` do the same, with the only difference being their return type (`boolean` vs `Boolean`), their names should be the same. The fact that their names currently differ can be misleading.\r\n\r\nThe proposal is to rename `paramAsBooleanOptional` to `paramAsBoolean`. The original method will be kept around and deprecated though to keep backwards compatibility.'
4814,'jpountz',"Add tracking of pages to MockPageCacheRecycler\nPage tracking would help make sure that we never forget to release pages when we don't need them anymore."
4813,'jpountz','Norms disabling on existing fields\nWe should allow for disabling norms on existing fields via the update mappings API. Implementation-wise, we would only have to set `omitNorms` to false in the `FieldType` and Lucene would automatically ignore norms on the next fields that would be added to the index and remove data from the index upon merges.\r\n\r\nHowever, the reverse operation cannot be supported, so disabling norms would be a destructive operation.'
4808,'javanna',"RestRequest boolean methods should always accept the same values\nWe currently have two methods that accept boolean values in `RestRequest`: one for the primitive type (`paramAsBoolean`) and one for the `Boolean` object (`paramAsBooleanOptional`).\r\n\r\nThey work pretty much the same way with a small difference: the latter doesn't accept `no` as a boolean value while the the first one does. I think both methods should accept exactly the same values consistently."
4806,'javanna',"Cluster pending tasks always uses default master_timeout parameter\nThe cluster pending tasks doesn't read the input `master_timeout` parameter from the REST layer, although it allows to set the `masterNodeTimeout` through Java API.\r\n\r\nAs a result, when using the REST layer the default `master_timeout` (30 seconds) is always used."
4805,'spinscale','BindException not occuring in elasticserch.log when started as a service\nSteps to reproduce:\r\n1. Fix the transport.port and http.port to fixed values.\r\n2. Start another process that is using port 9300\r\n3. Register ElasticSearch service\r\n4. Try and start ElasticSearch using services\r\n\r\nObserved behavior:\r\nThere should be a bindException in elasticsearch.log file.\r\n\r\nActual behavior:\r\nThere is no error in elasticsearch.log file, but elasticsearch process does not start\r\n\r\nMore notes:\r\nIn Bootstrap class, this seems incorrect check.\r\n            if (foreground) {\r\n                logger.error(errorMessage);\r\n            } else {\r\n                System.err.println(errorMessage);\r\n                System.err.flush();\r\n            }\r\n\r\n'
4803,'s1monw','avoid IndexOutOfBoundsException on all field with no tokens and keywordanalyzer\nshould fix #4771 '
4800,'jpountz','Histogram aggregations: finer-grained rounding\nThe way `HistogramAggregator` works is that for every value, it is going to\r\ncompute a rounded value, that basically looks like `(value / interval) * interval` (using integer arithmetic) and use it as a key in a hash table to aggregate counts.\r\n\r\nHowever, the exact rounded value is not needed yet at that stage, all we need\r\nis a value that uniquely identifies the bucket, such as `(value / interval)`.\r\nWe could only multiply with `interval` again when building the bucket: this way\r\nthe second step is only performed once per bucket instead of once per value.\r\n\r\nAlthough this looks like a micro optimization for the case that was just\r\ndecribed, it makes more sense with the date rounding implementations that we\r\nhave that are more CPU-intensive.'
4799,'javanna',"[DOCS] Various small documentation fixes\nThis pull request is both a code change and a call for help. :) \r\n\r\nI have been stuck in Perforce hell for a few years now and my git skills have been suffering (my private repos are in BitBucket and I only contribute to them, no forks). I rebased my local master branch to avoid having a merge commit, but I might have made things worse. My intent was to squash my small commits into one and send a pull request on that one commit. Can someone provide any pointers to send a better pull request for my last commits? https://github.com/brusic/elasticsearch/commits/docs Please contact me privately (or here if you don't mind the noise).\r\n\r\n"
4794,'s1monw','Add transport.publish_port setting Edit\nAdd transport.publish_port setting to allow users to specify the port\r\nother cluster members should use when connecting to an instance. This\r\nis needed for systems such as OpenShift, where cluster communication\r\nneeds to use a publicly accessibly proxy port, because the normal port\r\n(9300) is bound to a private loopback IP address.\r\n\r\nsee https://github.com/elasticsearch/elasticsearch/pull/4359'
4787,'javanna','Added support for local flag to all cluster state read operations\nAdded base Request class for read operations that usually happen on the master but can be executed locally.\r\n\r\nAdded base TransportAction class for master read operations that execute locally or not depending on the request class (local flag).\r\n\r\nAdded support for local flag where missing, in a backwards compatible manner:\r\n     - IndicesExistsRequest\r\n     - GetAliasesRequest (get alias api, aliases exist api)\r\n     - TypesExistsRequest\r\n     - GetIndexTemplatesRequest (get template, template exists)\r\n     - GetSettingsRequest\r\n     - GetRepositoriesRequest\r\n     - PendingClusterTasks\r\n\r\n Added parsing of the local flag where missing in Rest*Action.\r\n\r\n Updated SPEC adding local flag param where missing and added REST tests that contain use of the local flag where it was just added.\r\n\r\nCloses #3345'
4785,'costin','BulkRequestTests fails on Windows due to line ending differences\nSince windows uses different line endings ``\\r\\n`` vs *nixes ``\\n``, the tests fails as an extra char ``\\r`` is returned on windows.'
4782,'drewr','Timestamp column regression in cat/health and cat/count\nIn #4696 we made the epoch time more Unix-compatible without realizing it was used in the `DateTimeFormatter` which outputs the HMS `timestamp` string.  Now that column shows the wrong time.\r\n\r\nAlso take this opportunity to use `TimeUnit` instead of literal math.'
4778,'dadoonet','Remove the "-f" script argument from the documentation\nIn the document docs/reference/setup.asciidoc, the following example is given :\r\n\r\n    bin/elasticsearch -f -Xmx2g -Xms2g -Des.index.store.type=memory --node.name=my-node\r\n\r\nThe `-f` argument looks to be no longer needed.\r\n'
4775,'s1monw',"Failed to detect hot threads\nI'm not really sure what caused this but I'm pretty sure I didn't get hot threads from the node it mentions:\r\n```\r\n[2014-01-17 01:36:11,648][DEBUG][action.admin.cluster.node.hotthreads] [elastic1008] failed to execute on node [Orfebp5QSN2iIag5IKTrXg]\r\norg.elasticsearch.transport.RemoteTransportException: [elastic1001][inet[/10.64.0.108:9300]][cluster/nodes/hot_threads/n]\r\nCaused by: org.elasticsearch.ElasticSearchException: failed to detect hot threads\r\n        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:103)\r\n        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:43)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:281)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:272)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.monitor.jvm.HotThreads.similarity(HotThreads.java:216)\r\n        at org.elasticsearch.monitor.jvm.HotThreads.innerDetect(HotThreads.java:177)\r\n        at org.elasticsearch.monitor.jvm.HotThreads.detect(HotThreads.java:75)\r\n        at org.elasticsearch.action.admin.cluster.node.hotthreads.TransportNodesHotThreadsAction.nodeOperation(TransportNodesHotThreadsAction.java:101)\r\n        ... 7 more\r\n```"
4772,'bleskes','Updated test for `_source` exclusion changes\nI have updated this test to ensure that we always\r\n\r\n> maintain document structure above the point of exclusion\r\n'
4771,'jpountz','IndexOutOfBoundsException on indexing empty JSON document in 0.90.10\nIn 0.90.10 on indexing an empty json document `{}` into an index with `_all` enabled and a default analyzer of type `keyword` and IndexOutOfBoundsException is thrown.\r\n\r\nThis Bug was introduced with commit 0ef6ed98945d9698f0703283086883554a28dfb7\r\n\r\ncurl-script to reproduce this issue:\r\n\r\n```bash\r\n# delete index\r\ncurl -XDELETE http://localhost:9200/all_field_bug\r\n\r\n# create index with default analyzer for every field\r\n# without analyzer the index query below works fine\r\ncurl -XPUT http://localhost:9200/all_field_bug -d \'{"index.analysis.analyzer.default.type":"keyword"}\'\r\n\r\n# create some fields, type doesn\'t matter here\r\n# index query below works fine with "_all": {"enabled": false}\r\ncurl -XPOST http://localhost:9200/all_field_bug/default -d \'{"properties":{"date":{"type":"date"}}}\'\r\n\r\n# should index this empty json document\r\n# but returns {"error":"IndexOutOfBoundsException[Index: 0, Size: 0]","status":500}\r\ncurl -XPUT http://localhost:9200/all_field_bug/default/1 -d \'{}\'\r\n```\r\n\r\nMore detailed stacktrace:\r\n\r\n```java\r\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\r\n\tat java.util.ArrayList.rangeCheck(ArrayList.java:635)\r\n\tat java.util.ArrayList.get(ArrayList.java:411)\r\n\tat org.elasticsearch.common.lucene.all.AllEntries.boost(AllEntries.java:159)\r\n\tat org.elasticsearch.common.lucene.all.AllTokenStream.incrementToken(AllTokenStream.java:65)\r\n\tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:102)\r\n\tat org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:248)\r\n\tat org.apache.lucene.index.DocumentsWriterPerThread.updateDocument(DocumentsWriterPerThread.java:253)\r\n\tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:453)\r\n\tat org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1520)\r\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1190)\r\n\tat org.elasticsearch.index.engine.robin.RobinEngine.innerIndex(RobinEngine.java:581)\r\n\tat org.elasticsearch.index.engine.robin.RobinEngine.index(RobinEngine.java:492)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.index(InternalIndexShard.java:386)\r\n\tat org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:212)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:556)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:426)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t... 1 more\r\n```\r\n\r\nWill try to come up with a fix, if time is on my side.'
4761,'jpountz',"AbstractFieldMapper.merge doesn't return conflicts when trying to enable or disable norms\nAbstractFieldMapper.merge should return a conflict when trying to enable or disable norms on an existing mapping."
4760,'jpountz','norms.enabled/omit_norms serialization and parsing are inconsistent\nWe compare the field type against the default one in `toXContent` in order to only serialize changes from the default field type, which has `norms.enabled: true`.\r\n\r\nHowever we also have some logic to omit norms in case norms have not been configured and the field is `not_analyzed`.\r\n\r\nThis means that if you configure a field to be `not_analyzed` and have norms enabled:\r\n```javascript\r\n{\r\n  "type": string,\r\n  "index": "not_analyzed",\r\n  "norms": {\r\n    "enabled": true\r\n  }\r\n}\r\n```\r\nit will be parsed correctly, but if you serialize it with toXContent, you will get:\r\n```javascript\r\n{\r\n  "type": "string",\r\n  "index": "not_analyzed"\r\n}\r\n```\r\n`norms.enabled` are missing because they are the same as in the default field type. So parsing it again would return a field which has norms disabled.\r\n\r\nThe same is true for `index_options` (docs, positions and offsets by default, docs_only in case of `not_analyzed` fields) but this is less of an issue given that it doesn\'t make sense to index offsets on a not_analyzed field.'
4757,'martijnvg',"Forcefully never cache any filter that wraps a p/c filter\nAny filter that wraps a p/c filter (has_child & has_parent) either directly or indirectly must never be cached. \r\n\r\nThe reason behind this is that the filter-cache caches per segment reader and the p/c filters rely on executing with a top level reader. The p/c filters execute in a two phase search. The first phase collects the parent ids of any document that matches with the wrapped filter or query. The second phase iterates over all parent or child documents and checks if the parent id of each document (for parent docs this the _uid field value and child docs the _parent field value) is in the set of ids collected in the first phase. The second phase executes per segment, but the first phase executed top level. \r\n\r\nNote: p/c filters on their own can't already be cached, since the cache options are a no-op in the filter parsers."
4754,'jpountz','Faceting using DFS_QUERY_THEN_FETCH fails\nHey,\r\n\r\nthis came via the ML (there are also some prerequisites listed): https://groups.google.com/d/msg/elasticsearch/ySHfbIW4sBQ/8ozOkBcbWTMJ\r\n\r\nWorks on master, fails on 0.90\r\n\r\nTo reproduce (sometimes, not always)\r\n\r\n```\r\npublic class BrokenFacetTest extends ElasticsearchIntegrationTest {\r\n\r\n    @Test\r\n    @TestLogging("_root:DEBUG")\r\n    public void foo() throws Exception {\r\n        client().prepareIndex("test-index", "test-type")\r\n                .setSource("{ \\"id\\": 123, \\"test-value\\": 321 }")\r\n                .setRefresh(true)\r\n                .execute()\r\n                .actionGet();\r\n\r\n        SearchResponse searchResponse = client()\r\n                .prepareSearch("test-index")\r\n                .setQuery(new MatchAllQueryBuilder())\r\n                .addFacet(new TermsFacetBuilder("test-facet").field("test-value"))\r\n                .setSearchType(SearchType.DFS_QUERY_AND_FETCH)\r\n                .execute()\r\n                .actionGet();\r\n\r\n        assertEquals(searchResponse.getFailedShards(), 0); // Failing!\r\n    }\r\n\r\n}\r\n```\r\n\r\nException being logged\r\n\r\n```\r\n[2014-01-16 10:13:16,061][DEBUG][org.elasticsearch.action.search.type] [node_2] [2] Failed to execute query phase\r\norg.elasticsearch.ElasticSearchException\r\n\tat org.elasticsearch.ExceptionsHelper.convertToRuntime(ExceptionsHelper.java:37)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:360)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:338)\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction$AsyncAction.executeSecondPhase(TransportSearchDfsQueryAndFetchAction.java:139)\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryAndFetchAction$AsyncAction$2.run(TransportSearchDfsQueryAndFetchAction.java:123)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.AssertionError\r\n\tat org.elasticsearch.common.recycler.ThreadLocalRecycler$TV.release(ThreadLocalRecycler.java:84)\r\n\tat org.elasticsearch.search.facet.terms.longs.TermsLongFacetExecutor.buildFacet(TermsLongFacetExecutor.java:122)\r\n\tat org.elasticsearch.search.facet.FacetPhase.execute(FacetPhase.java:200)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:129)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:357)\r\n\t... 6 more\r\n```'
4752,'spinscale','Plugins not installed in correct directory in RC1\nIn elasticsearch.yml, I have the following\r\n\r\npath.plugins: ${ES_PLUGINS}\r\n\r\nwhere ES_PLUGINS environmental variable is pointing to a valid directory .../elasticsearch\r\n\r\nand the RC1 is installed under that.  \r\n\r\nIn Beta2, new plugins were installed in the $ES_PLUGINS directory as expected but with RC1, the plugins are being installed in the plugin directory in the RC1 directory instead.\r\n\r\nInstalling river-jdbc with\r\n./bin/plugin -install river-jdbc -url http://bit.ly/1ctvKka\r\n\r\nStarting RC1 with \r\n./bin/elasticsearch --config=$ES_CONFIG/elasticsearch.yml --node.name=node0\r\n\r\nwhere ES_CONFIG points to the ..../elasticsearch directory\r\n\r\nRunning \r\nJava 1.7.045 \r\nOSX 10.9.1'
4749,'jpountz','Support multiple rescores\nSupport multiple rescores\r\n\r\nDetects if rescores arrive as an array instead of a plain object.  If so\r\nthen parse each element of the array as a separate rescore to be executed\r\none after another.  It looks like this:\r\n```js\r\n   "rescore" : [ {\r\n      "window_size" : 100,\r\n      "query" : {\r\n         "rescore_query" : {\r\n            "match" : {\r\n               "field1" : {\r\n                  "query" : "the quick brown",\r\n                  "type" : "phrase",\r\n                  "slop" : 2\r\n               }\r\n            }\r\n         },\r\n         "query_weight" : 0.7,\r\n         "rescore_query_weight" : 1.2\r\n      }\r\n   }, {\r\n      "window_size" : 10,\r\n      "query" : {\r\n         "score_mode": "multiply",\r\n         "rescore_query" : {\r\n            "function_score" : {\r\n               "script_score": {\r\n                  "script": "log10(doc[\'numeric\'].value + 2)"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   } ]\r\n```\r\n\r\nRescores as a single object are still supported.\r\n\r\nAlso add documentation on score_mode when adding documentation about multiple\r\nrescores.\r\n\r\nCloses #4748\r\nCloses #4742\r\n'
4745,'spinscale','Enabling "_timestamp" can cause bulk API to fail entire request instead of single operation\nAs I understand it, the intention of the bulk API is that individual operations may fail, but a failure in an individual operation should generally not cause the failure of all operations in the request.\r\n\r\nFirst let\'s verify that this is generally how it works. I am testing here with a simple case of malformed JSON (though I originally saw the problem with a subtler JSON-parsing issue of unexpected non-printable ASCII characters in JSON data).\r\n\r\n```\r\n$ curl -XPUT http://localhost:9200/testing/\r\n{"ok":true,"acknowledged":true}\r\n\r\n$ curl -XPUT http://localhost:9200/testing/person/_mapping -d \'{"person": {"dynamic": "strict", "properties": {"last_modified": {"type": "date", "format": "dateOptionalTime"},"name": {"type": "string"}}}}\'\r\n{"ok":true,"acknowledged":true}\r\n\r\n$ cat baddata.txt\r\n{"index": {"_id": "1"}}\r\n{"name": "Malformed}\r\n{"index": {"_id": "2"}}\r\n{"name": "Good"}\r\n\r\n$ curl -XPOST http://localhost:9200/testing/person/_bulk --data-binary @baddata.txt\r\n{"took":72,"items":[{"index":{"_index":"testing","_type":"person","_id":"1","error":"MapperParsingException[failed to parse [name]]; nested: JsonParseException[Unexpected end-of-input in VALUE_STRING\\n at [Source: [B@27beb7ec; line: 1, column: 65]]; "}},{"index":{"_index":"testing","_type":"person","_id":"2","_version":1,"ok":true}}]}\r\n```\r\n\r\nThis worked correctly - one item failed with an error, the other succeeded, and we do indeed find one item indexed in a subsequent search.\r\n\r\nNow let\'s re-create that index and enable the magic "_timestamp" field this time:\r\n\r\n```\r\n$ curl -XDELETE http://localhost:9200/testing/\r\n{"ok":true,"acknowledged":true}\r\n\r\n$ curl -XPUT http://localhost:9200/testing/\r\n{"ok":true,"acknowledged":true}\r\n\r\n$ curl -XPUT $LOCAL/testing/person/_mapping -d \'{"person": {"_timestamp": {"enabled": true, "path": "last_modified"}, "dynamic": "strict", "properties": {"last_modified": {"type": "date", "format": "dateOptionalTime"},"name": {"type": "string"}}}}\'\r\n{"ok":true,"acknowledged":true}\r\n\r\n$ curl -XPOST $LOCAL/testing/person/_bulk --data-binary @baddata.txt\r\n{"error":"ElasticSearchParseException[failed to parse doc to extract routing/timestamp]; nested: JsonParseException[Unexpected end-of-input in VALUE_STRING\\n at [Source: [B@68f55ff2; line: 1, column: 65]]; ","status":400}\r\n```\r\n\r\nThis time the entire request errors out and returns a 400 response code, and no items are successfully indexed.\r\n\r\nSince the malformed JSON is limited to a single action in the bulk request, I would expect only that action to fail, regardless of whether the "_timestamp" magic field is enabled or not.\r\n\r\nTested against latest ElasticSearch release:\r\n\r\n```\r\n$ curl http://localhost:9200\r\n{\r\n  "ok" : true,\r\n  "status" : 200,\r\n  "name" : "Pip the Troll",\r\n  "version" : {\r\n    "number" : "0.90.10",\r\n    "build_hash" : "0a5781f44876e8d1c30b6360628d59cb2a7a2bbb",\r\n    "build_timestamp" : "2014-01-10T10:18:37Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.6"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n```'
4738,'spinscale','Consistent REST Get Field Mapping API\nAs of #4071, this commit https://github.com/elasticsearch/elasticsearch/commit/a3abcdc93a004bea53ba2d4cbab585f8820b660d prepared the Get Field Mapping API. However, if index and type exist, but the field does not, it should return an empty JSON response instead of a 404 in order to be consistent.\r\n\r\nRight now, you cannot now, if the index, type or field was the one not being found in in the rest action'
4727,'spinscale','PluginService loads Eclipse\'s plugins jar into classpath\nHi, I\'ve tried to configure an ES client node on a REST project based on Spring but I\'m always getting the following error when I try to query any endpoint:\r\n```\r\njava.lang.LinkageError: loader constraint violation: when resolving method "com.sun.jersey.spi.container.servlet.ServletContainer.getServletConfig()Ljavax/servlet/ServletConfig;" the class loader (instance of org/apache/catalina/loader/WebappClassLoader) of the current class, com/sun/jersey/spi/container/servlet/WebServletConfig, and the class loader (instance of org/apache/catalina/loader/StandardClassLoader) for resolved class, com/sun/jersey/spi/container/servlet/ServletContainer, have different Class objects for the type ainer.getServletConfig()Ljavax/servlet/ServletConfig; used in the signature\r\n```\r\nIt took me so long to find the root cause of this, and it was that org.elasticsearch.plugins.PluginsService.loadPluginsIntoClassLoader() is loading Eclipse\'s pluings jar (in this case Groovy\'s pluing) into the Tomcat\'s context classloader, causing the LinkageError.\r\n\r\nI only need the ES dependency on my project for use it as a client, is there any configuration to avoid this?'
4722,'s1monw',"Deprecated & remove 'omit_term_freq_and_positions' \nWe deprecated `omit_term_freq_and_positions` in `0.20` already and I think for GA we should drop the support entirely for newly created indices."
4718,'dadoonet','generate timestamp when path is null\nShouldn\'t timestamp be generated when value of path is null ?\r\n\r\nMapping definition:\r\n\r\n```bash\r\ncurl -X PUT  http://localhost:9200/twitter/ -d \'{\r\n    "mappings": {\r\n        "_default_": {\r\n            "_timestamp" : {\r\n                "enabled" : "yes",\r\n                "store": "yes",\r\n                "path" : "post_date"\r\n            },\r\n            "properties": {\r\n                "message": {\r\n                    "type": "string"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nGet error when:\r\n\r\n```bash\r\ncurl -X PUT http://127.0.0.1:9200/twitter/tweet/123 -d \'{\r\n  message: "bam bam"\r\n}\'\r\n\r\n=>  {"error":"ElasticSearchParseException[failed to parse doc to extract routing/timestamp]; nested: TimestampParsingException[failed to parse timestamp [null]]; ","status":400}\r\n\r\ncurl -X PUT http://127.0.0.1:9200/twitter/tweet/123 -d \'{\r\n  message: "bam bam",\r\n  post_date: "2009-11-15T14:12:12Z"\r\n}\'\r\n\r\n=> {"ok":true,"_index":"twitter","_type":"tweet","_id":"123","_version":1}\r\n```'
4712,'s1monw','Remove `ElasticsearchInterruptedException` and handle interrupt state  correctly.\nInterruptedExceptions should be handled by either rethrowing or restoring the interrupt state (i.e. calling `Thread.currentThread().interrupt()`). This is important since the caller of the is method or subequent method calls might also be interested in this exception. If we ignore the interrupt state the caller might be left unaware of the exception and blocks again on a subsequent method.'
4711,'drewr','Add _cat/segments\n`/_cat/segments?v`:\r\n\r\n    index   shard prirep ip        state   segment deleted_docs num_docs search size_in_bytes\r\n    twitter     0 p      127.0.0.1 STARTED _6y                0     2272 t              6.2mb\r\n    twitter     0 p      127.0.0.1 STARTED _6z                0       12 t               84kb\r\n\r\nOther columns:\r\n\r\n* committed (comm)\r\n* compound (comp)\r\n* generation (gen)\r\n* memory_in_bytes (mem)\r\n* version (ver)\r\n\r\n'
4703,'martijnvg','Scrolling with has_child filter returns no hits on 2nd request\nWhen using scroll with a has_child filter, the initial request returns the correct total number of hits, but subsequent requests return no hits.\r\n\r\nIt looks like this problem was introduced in 0.90.6, and still occurs in 0.90.10.  0.90.5 works as expected.\r\n\r\nThe number of documents seems to play a part - in my initial test cases with only 2 parent documents, I couldn\'t reproduce the issue.  However, creating 100 parents does reliably reproduce it.  In my testing, 8 parent documents worked fine, but 9 did not.\r\n\r\nIt sounds very similar to the issue mentioned here: http://elasticsearch-users.115913.n3.nabble.com/No-hit-using-scan-scroll-with-has-parent-filter-td4047236.html\r\n\r\nHere\'s a test script (requires jq(1) to grab the scroll ID from the first JSON result):\r\n\r\n```bash\r\n#!/bin/sh\r\n\r\nHOST=\'localhost:9200\'\r\nINDEX=\'test_scroll_jj\'\r\nCURL="curl -q --ipv4 --silent --show-error --fail"\r\n\r\n$CURL -XDELETE "$HOST/${INDEX}?pretty=true" >/dev/null\r\n$CURL -XPOST "$HOST/${INDEX}/?pretty=true" -d \'\r\n{\r\n    "mappings": {\r\n        "homes":{\r\n            "_parent":{\r\n                "type" : "person"\r\n            }\r\n        }\r\n    }\r\n}\' >/dev/null\r\n\r\nfor x in {1..100}; do # in my testing, 8 docs works, 9 fails\r\n    $CURL -XPUT "$HOST/${INDEX}/person/$x/?pretty=true" -d \'{}\' >/dev/null\r\n    $CURL -XPOST "$HOST/${INDEX}/homes?parent=$x&pretty=true" -d \'{}\' >/dev/null\r\ndone\r\n\r\n$CURL -XPOST "$HOST/${INDEX}/_refresh?pretty=true" >/dev/null\r\n\r\necho "REQUEST ONE:"\r\nSCROLL_RESULT=$($CURL -v -XPOST "http://$HOST/${INDEX}/person/_search?pretty=true&scroll=30s" -d\'\r\n{\r\n    "size" : 1,\r\n    "fields" : ["_id"],\r\n    "query" : {\r\n        "filtered" : {\r\n            "filter" : {\r\n                "has_child" : {\r\n                    "type" : "homes",\r\n                    "query" : {\r\n                        "match_all" : {}\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\')\r\necho $SCROLL_RESULT\r\n\r\nscroll_id=$(echo $SCROLL_RESULT | jq -r \'.["_scroll_id"]\')\r\n\r\necho\r\necho "REQUEST TWO:"\r\n$CURL -v "http://$HOST/_search/scroll?scroll=30s&scroll_id=$scroll_id&pretty=true"\r\n```\r\n\r\nThe failing output on 0.90.10:\r\n```\r\n/tmp|⇒  /tmp/scrollbug.sh\r\nREQUEST ONE:\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1...\r\n* Adding handle: conn: 0x7fb832006e00\r\n* Adding handle: send: 0\r\n* Adding handle: recv: 0\r\n* Curl_addHandleToPipeline: length: 1\r\n* - Conn 0 (0x7fb832006e00) send_pipe: 1, recv_pipe: 0\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> POST /test_scroll_jj/person/_search?pretty=true&scroll=30s HTTP/1.1\r\n> User-Agent: curl/7.32.0\r\n> Host: localhost:9200\r\n> Accept: */*\r\n> Content-Length: 321\r\n> Content-Type: application/x-www-form-urlencoded\r\n>\r\n} [data not shown]\r\n* upload completely sent off: 321 out of 321 bytes\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 520\r\n<\r\n{ [data not shown]\r\n* Connection #0 to host localhost left intact\r\n{ "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==", "took" : 5, "timed_out" : false, "_shards" : { "total" : 5, "successful" : 5, "failed" : 0 }, "hits" : { "total" : 100, "max_score" : 1.0, "hits" : [ { "_index" : "test_scroll_jj", "_type" : "person", "_id" : "2", "_score" : 1.0 } ] } }\r\n\r\nREQUEST TWO:\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1...\r\n* Adding handle: conn: 0x7f8589806e00\r\n* Adding handle: send: 0\r\n* Adding handle: recv: 0\r\n* Curl_addHandleToPipeline: length: 1\r\n* - Conn 0 (0x7f8589806e00) send_pipe: 1, recv_pipe: 0\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> GET /_search/scroll?scroll=30s&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==&pretty=true HTTP/1.1\r\n> User-Agent: curl/7.32.0\r\n> Host: localhost:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 410\r\n<\r\n{\r\n  "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs2OkM5SXlBenNyU0lXR21uX3JsN25XcHc7NzpDOUl5QXpzclNJV0dtbl9ybDduV3B3Ozg6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzs5OkM5SXlBenNyU0lXR21uX3JsN25XcHc7MTA6QzlJeUF6c3JTSVdHbW5fcmw3bldwdzswOw==",\r\n  "took" : 0,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 0,\r\n    "max_score" : null,\r\n    "hits" : [ ]\r\n  }\r\n}\r\n```\r\n\r\nAnd the expected output as per 0.90.5:\r\n```\r\n/tmp|⇒  /tmp/scrollbug.sh\r\nREQUEST ONE:\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1...\r\n* Adding handle: conn: 0x7fd04a006e00\r\n* Adding handle: send: 0\r\n* Adding handle: recv: 0\r\n* Curl_addHandleToPipeline: length: 1\r\n* - Conn 0 (0x7fd04a006e00) send_pipe: 1, recv_pipe: 0\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> POST /test_scroll_jj/person/_search?pretty=true&scroll=30s HTTP/1.1\r\n> User-Agent: curl/7.32.0\r\n> Host: localhost:9200\r\n> Accept: */*\r\n> Content-Length: 321\r\n> Content-Type: application/x-www-form-urlencoded\r\n>\r\n} [data not shown]\r\n* upload completely sent off: 321 out of 321 bytes\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 523\r\n<\r\n{ [data not shown]\r\n* Connection #0 to host localhost left intact\r\n{ "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=", "took" : 9, "timed_out" : false, "_shards" : { "total" : 5, "successful" : 5, "failed" : 0 }, "hits" : { "total" : 100, "max_score" : 1.0, "hits" : [ { "_index" : "test_scroll_jj", "_type" : "person", "_id" : "2", "_score" : 1.0 } ] } }\r\n\r\nREQUEST TWO:\r\n* About to connect() to localhost port 9200 (#0)\r\n*   Trying 127.0.0.1...\r\n* Adding handle: conn: 0x7f8a92006e00\r\n* Adding handle: send: 0\r\n* Adding handle: recv: 0\r\n* Curl_addHandleToPipeline: length: 1\r\n* - Conn 0 (0x7f8a92006e00) send_pipe: 1, recv_pipe: 0\r\n* Connected to localhost (127.0.0.1) port 9200 (#0)\r\n> GET /_search/scroll?scroll=30s&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=&pretty=true HTTP/1.1\r\n> User-Agent: curl/7.32.0\r\n> Host: localhost:9200\r\n> Accept: */*\r\n>\r\n< HTTP/1.1 200 OK\r\n< Content-Type: application/json; charset=UTF-8\r\n< Content-Length: 523\r\n<\r\n{\r\n  "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsyMTpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzIzOkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MjI6SC1fSFFlNjJUa1M0MndiUWM2S0t3UTsyNDpILV9IUWU2MlRrUzQyd2JRYzZLS3dROzI1OkgtX0hRZTYyVGtTNDJ3YlFjNktLd1E7MDs=",\r\n  "took" : 2,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 100,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test_scroll_jj",\r\n      "_type" : "person",\r\n      "_id" : "7",\r\n      "_score" : 1.0\r\n    } ]\r\n  }\r\n}\r\n```'
4701,'imotov','Snapshots with missing or failed shards hang on restore operation\nTo reproduce snapshot an index with unavailable primary shards and then try to restore it - the restore operation will hang. Expected result: restore operation should fail with an error. '
4699,'s1monw','Default stopwords list should be `_none_` for all but language-specific analyzers\n/cc @s1monw \r\n\r\nIn #4092 the `standard` analyzer\'s default stopwords list was changed from `english` to `_none_`. The reasons for this are:\r\n\r\n1. Removing stopwords on any string field by default can have surprising results, eg `country_code: "NO"` would be indexed with no value,  `title: "To be or not to be"` would similarly have all words removed.\r\n\r\n2. Stopwords do add value to search, and with tools like the `common` query, we can take stopwords into account while still keeping queries performing well.\r\n\r\n3. Choosing the English stopwords by default can be surprising for users whose primary language isn\'t English. \r\n\r\nHowever, there are two other non-language-specific analyzers which should have a similar treatment, specifically:\r\n\r\n * `pattern` analyzer \r\n * `standard_html` analyzer\r\n\r\nAlso, the change to the `standard` analyzer has not been documented, and the `standard_html` analyzer is not documented at all.'
4697,'spinscale','Use of sigar 1.6.5 library in RHEL6\nIs it possible to make use of default sigar library, present in CentOS/RHEL6?\r\nDoes your sigar-1.6.4.jar file needs to have the .so library present into same directory?\r\n```\r\n$ yum list sigar\r\nLoaded plugins: fastestmirror\r\nLoading mirror speeds from cached hostfile\r\n * base: centos.mirror.rafal.ca\r\n * extras: mirror.netflash.net\r\n * updates: mirror.netflash.net\r\nInstalled Packages\r\nsigar.x86_64    1.6.5-0.4.git58097d9.el6    @base\r\n$ ls -lah /usr/lib64/libsigar.so \r\n-rwxr-xr-x. 1 root root 144K Dec  7  2011 /usr/lib64/libsigar.so\r\n```\r\nThank you.'
4696,'drewr',"Use seconds instead of millis for timestamps in cat api\nOutput of `_cat/health`:\r\n\r\n    time(ms)      timestamp cluster  status nodeTotal nodeData shards pri relo init unassign\r\n    1389481752742 17:09:12  debugger yellow         1        1      7   7    0    0        7\r\n\r\nThe precision of millis isn't necessary here, but more importantly:\r\n\r\n    % date -r 1389481752742\r\n    Sun Dec  3 11:32:22 CST 46000\r\n    % date -r 1389481752\r\n    Sat Jan 11 17:09:12 CST 2014\r\n\r\n"
4694,'dadoonet','add MockPageCacheRecycler in test jar\nMockPageCacheRecycler is missing in test jar which makes failing tests when using\r\ntest jar in plugins:\r\n\r\n```\r\n1> [2014-01-11 10:51:30,531][ERROR][test                     ] FAILURE  : testWikipediaRiver(org.elasticsearch.river.wikipedia.WikipediaRiverTest)\r\n  1> REPRODUCE WITH  : mvn test -Dtests.seed=5DAFD4FBAE587363 -Dtests.class=org.elasticsearch.river.wikipedia.WikipediaRiverTest -Dtests.method=testWikipediaRiver -Dtests.prefix=tests -Dtests.network=true -Dfile.encoding=MacRoman -Duser.timezone=Europe/Paris -Des.logger.level=INFO -Des.node.local=true -Dtests.cluster_seed=134842C2D806FFC0\r\n  1> Throwable:\r\n  1> java.lang.NoClassDefFoundError: org/elasticsearch/cache/recycler/MockPageCacheRecycler\r\n  1>     org.elasticsearch.test.cache.recycler.MockPageCacheRecyclerModule.configure(MockPageCacheRecyclerModule.java:30)\r\n  1>     org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)\r\n```'
4689,'drewr','Make cat/indices primary shard columns enabled with an http param\n'
4685,'martijnvg','NPE in bool filter\ntypo was cause of NPE\r\nsee my comment of commit\r\nhttps://github.com/elasticsearch/elasticsearch/commit/8e0291823ab6e1d638b986d22690b805828c478a'
4681,'bleskes','Expose min/max open file descriptors in Cluster Stats API\nCurrently we only return the average number of open file descriptors of all nodes (under the `avg_open_file_descriptors` key). The min/max values are more interesting to spot problems. Also for consistency, we should use the following format:\r\n\r\n```\r\n "open_file_descriptors": {\r\n      "min": 200,\r\n      "max": 346,\r\n       "avg": 273\r\n }\r\n```'
4679,'martijnvg','NPE while doing haschild query, intermittently\n)],from[0],size[1]: Query Failed [failed to execute context rewrite]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\n^C\r\n\r\n\r\n\r\n\r\n\r\nAnd once a while i see the following\r\n\r\n\r\n\r\n>cache(_type:movie)],from[0],size[1]: Query Failed [failed to execute context rewrite]\r\nat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:99)\r\nat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\r\nat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:529)\r\nat org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:518)\r\nat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:265)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\nat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\nat org.elasticsearch.index.cache.id.simple.SimpleIdCache.refresh(SimpleIdCache.java:210)\r\nat org.elasticsearch.index.search.child.HasChildFilter.contextRewrite(HasChildFilter.java:118)\r\nat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:96)\r\n... 7 more'
4673,'spinscale','Extend defaults for debian / rpm packages\nThe defaults for the deb / rpm packages could contain a setting which controls the startup of the elasticseach node. This may be useful when packages are installed due to library depenencies but no running cluster node is needed.'
4670,'drewr','cat/allocation diskRatio calc wrong\n    shards diskUsed diskAvail diskRatio host       ip           node\r\n         6    257gb     208gb    123.6% iota.local 192.168.56.1 Cat-Man\r\n         6                                                      UNASSIGNED'
4668,'kimchy','Disabling allow_explicit_index breaks bulk\nThe documentation on URL-based access control implies that bulk operations still work if you set `rest.action.multi.allow_explicit_index: false`, as long as you specify the index in the URL. However, it doesn\'t work:\r\n\r\n    POST /foo/bar/_bulk\r\n    { "index": {} }\r\n    { "_id": 1234, "baz": "foobar" }\r\n\r\nreturns \r\n\r\n    explicit index in bulk is not allowed\r\n\r\nSee https://groups.google.com/forum/#!topic/elasticsearch/TLaTkaJjGYg for the discussion.'
4664,'javanna','Deprecate document boost\nThe document boost is a nice feature but since it was removed from lucene 4.0, the way it works in elasticsearch is by adding fields boosts to each field, multiplying it with the original field boost. Here is the interesting commit: https://github.com/elasticsearch/elasticsearch/commit/c60f20413b299e4d9ea0a5fa3e24381e90d914b8#diff-7117c679a1ca0d5002c0c9b9ef8bad16 . That is not exactly how the document boost should work, it has downsides and the same result can be obtained using function_score.\r\n\r\nFor the above reasons we are going to deprecate the document boost.\r\n\r\n'
4662,'jpountz','Aggregations: add a `min_doc_count` option to terms and histogram\nRight now terms aggregations may return terms that match one hit or more. The purpose of the `min_doc_count` option is to make it configurable. For example, if\r\n\r\n```javascript\r\n{\r\n    "aggs" : {\r\n        "tags" : {\r\n            "terms" : {\r\n                "field" : "tag"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nreturns\r\n```javascript\r\n{\r\n    ...\r\n\r\n    "aggregations" : {\r\n        "tags" : {\r\n            "buckets" : [\r\n                {\r\n                    "key" : "search",\r\n                    "doc_count" : 115\r\n                },\r\n                {\r\n                    "key" : "java",\r\n                    "doc_count" : 50\r\n                },\r\n                {\r\n                    "key" : "concurrency",\r\n                    "doc_count" : 12\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nthen\r\n```javascript\r\n{\r\n    "aggs" : {\r\n        "tags" : {\r\n            "terms" : {\r\n                "field" : "tag",\r\n                "min_doc_count": 50\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nwould return\r\n```javascript\r\n{\r\n    ...\r\n\r\n    "aggregations" : {\r\n        "tags" : {\r\n            "buckets" : [\r\n                {\r\n                    "key" : "search",\r\n                    "doc_count" : 115\r\n                },\r\n                {\r\n                    "key" : "java",\r\n                    "doc_count" : 50\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThe special case `min_doc_count: 0` will behave similarly to the [`all_terms` option of facets](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-facets-terms-facet.html#_all_terms) and also return terms that don\'t match any hit. For example, we could have the following response:\r\n```javascript\r\n{\r\n    ...\r\n\r\n    "aggregations" : {\r\n        "tags" : {\r\n            "buckets" : [\r\n                {\r\n                    "key" : "search",\r\n                    "doc_count" : 115\r\n                },\r\n                {\r\n                    "key" : "java",\r\n                    "doc_count" : 50\r\n                },\r\n                {\r\n                    "key" : "concurrency",\r\n                    "doc_count" : 12\r\n                },\r\n                {\r\n                    "key" : "unit testing",\r\n                    "doc_count" : 0\r\n                },\r\n                {\r\n                    "key" : "performance",\r\n                    "doc_count" : 0\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nHistograms are going to support this option as well and the [`empty_bucket` option](http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-aggregations-bucket-histogram-aggregation.html) will be removed in favor of `min_doc_count: 0`.'
4656,'dadoonet','River does not start when using config/templates files\nFrom elasticsearch 0.90.6, when you have templates files defined in `config/templates` dir, rivers don\'t start anymore.\r\n\r\nSteps to reproduce:\r\n\r\nCreate `config/templates/default.json`:\r\n\r\n```javascript\r\n{\r\n  default:\r\n  {\r\n    "template" : "*",\r\n    "mappings" : {\r\n      "_default_" : {\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nStart a dummy river:\r\n\r\n```sh\r\ncurl -XPUT \'localhost:9200/_river/my_river/_meta\' -d \'{ "type" : "dummy" }\'\r\n```\r\n\r\nIt gives:\r\n\r\n```\r\n[2014-01-01 22:08:38,151][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings [_default_]\r\n[2014-01-01 22:08:38,239][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:38,245][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n[2014-01-01 22:08:38,250][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:39,244][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:39,252][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:40,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:40,254][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:41,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:41,255][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:42,249][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts\r\n[2014-01-01 22:08:42,257][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts\r\n```\r\n\r\nWith elasticsearch 0.90.2 or with no template file in `config/templates` dir, it gives:\r\n\r\n```\r\n[2014-01-01 22:22:32,096][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings []\r\n[2014-01-01 22:22:32,221][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] create\r\n[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] start\r\n[2014-01-01 22:22:32,234][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n```\r\n\r\nCloses #4577.'
4647,'jpountz',"Make PageCacheRecycler better reuse memory across threads\nAlthough using a thread-local is nice to avoid contention, we have to divide the available amount of memory by the size of the search thread pool and each thread may end up with not much memory, especially on machines with high numbers of processors and small heaps (eg. if they use doc values). For instance, a machine with 12 processors and 4GB of heap would use 410MB of memory for the recycler but each thread would only have 11MB (given that the default size of the search thread pool is 3 * ${processors}).\r\n\r\nMoreover, I don't think there is a lot of contention on this recycler since it is only used for large objects.\r\n\r\nI think it would be a better trade-off to use the queue recycler by default (which is based on a non-blocking queue)? Or maybe something in-between that would allow for sharing data between several threads (with a configurable concurrency level like ConcurrentHashMap)?\r\n\r\n"
4646,'jpountz',"Use FHV's phraseLimit\nThis prevents poisoning the FVH with documents that contain TONS of matches\r\nwhich take tons of memory and time to highlight.\r\n\r\nCloses #4645"
4644,'martijnvg','Faceting only works on one nested field when multiple nested fields are mapped\nI\'m attempting to create an index where documents have multiple nested fields of the form: ``"parent": [{"k": ..., "v": 1"}]"`` with the mapping seen below (actually a dynamic mapping in my app, but it doesn\'t seem to make a difference).\r\n\r\nI want to facet on the nested fields and filter results down to only specific values of ``k`` which is covered in the [nested field docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-nested-type.html).\r\n\r\nHowever, I\'m running into a problem (bug?) where only 1 nested field seems to work with the facet. All other nested fields always return an empty result set (empty ``terms``).\r\n\r\nTest case to reproduce (using [Sense](https://chrome.google.com/webstore/detail/sense/doinijnbnggojdlcjifpdckfokbbfpbo?hl=en)):\r\n\r\n[Bash/CURL version](https://gist.github.com/schmichael/8303986)\r\n```json\r\nDELETE /test\r\nPOST /test\r\n{\r\n   "mappings" : {\r\n        "testtype" : {\r\n            "_source" : { "enabled" : true },\r\n            "properties" : {\r\n                "mapA": {\r\n                    "type": "nested", \r\n                    "index": "not_analyzed"\r\n                    , "properties": {\r\n                        "k": {\r\n                            "type": "string",\r\n                            "index": "not_analyzed"\r\n                        },\r\n                        "v": {\r\n                            "type": "long",\r\n                            "index": "not_analyzed"\r\n                        }\r\n                    }\r\n                },\r\n                "mapB": {\r\n                    "type": "nested", \r\n                    "index": "not_analyzed"\r\n                    , "properties": {\r\n                        "k": {\r\n                            "type": "string",\r\n                            "index": "not_analyzed"\r\n                        },\r\n                        "v": {\r\n                            "type": "long",\r\n                            "index": "not_analyzed"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPUT /test/testtype/1\r\n{"mapA": [{"k": "K", "v": 1}]}\r\nPUT /test/testtype/2\r\n{"mapB": [{"k": "K", "v": 1}]}\r\n\r\nPOST /test/testtype/_search\r\n{\r\n    "facets": {\r\n        "mapfacet": {\r\n            "facet_filter": {\r\n                "term": {\r\n                    "k": "K"\r\n                }\r\n            },\r\n            "nested": "mapA",\r\n            "terms": {\r\n                "field": "v"\r\n            }\r\n        }\r\n    },\r\n    "size": 0\r\n}\r\nPOST /test/testtype/_search\r\n{\r\n    "facets": {\r\n        "mapfacet": {\r\n            "facet_filter": {\r\n                "term": {\r\n                    "k": "K"\r\n                }\r\n            },\r\n            "nested": "mapB",\r\n            "terms": {\r\n                "field": "v"\r\n            }\r\n        }\r\n    },\r\n    "size": 0\r\n}\r\n```\r\n\r\n**Expected result:**\r\n\r\nBoth facets should return ``"terms": [ { "term": 1, "count": 1 } ]``\r\n\r\n**Actual result:**\r\n\r\nThe facet on nested field ``mapA`` returns no terms.\r\n\r\n**Versions**\r\n\r\nTested on 0.90.3 and 0.90.9.'
4643,'uboness','Sorting by sub-aggregation is broken in terms agg\nTerms aggregations enables sorting by sub metric aggregation, a la:\r\n\r\n```json\r\n{\r\n    "aggregations": {\r\n        "host": {\r\n            "terms": {\r\n                "field": "host",\r\n                "order": {\r\n                    "cpu_avg": "asc"\r\n                }\r\n            },\r\n            "aggs": {\r\n                "cpu_avg": {\r\n                    "avg": {\r\n                        "field": "cpu"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nunfortunately, that got broken along the way.\r\n\r\nThe fix will be mostly based on https://github.com/elasticsearch/elasticsearch/pull/4472\r\n\r\nthx @alexbrasetvik'
4640,'costin','add support for Tera (bytes) and Peta (bytes)\nConsidering the amount of data stored in a ES cluster, the giga byte unit, as a max, seems unsuitable. Tera bytes and potentially peta bytes are more appropriate.'
4637,'aleph-zero','Add recovery API endpoint\nAdd a new API endpoint for retrieving the status of on-going recovery operations. This API will report the status of index recovery for the 1.0 snapshot/restore feature, as well as for gateway recoveries. \r\n\r\nThe API endpoint will live at /_recovery. '
4636,'s1monw','Fix Elasticsearch License Headers \n`s/ElasticSearch and Shay Banon/Elasticsearch`'
4634,'s1monw','Rename `ElasticSearch*` to `Elasticsearch*`\nbefore we release 1.0 we should cleanup naming on the internal classes to use `Elasticsearch` rather than `ElasticSearch`'
4633,'s1monw',"Rename `RobinEngine` to `LuceneEngine` or `DefaultEngine`\nit seems like we settled on Lucene and I don't think we will have another `Engine` in the near future. I think the term `Robin` is misleading or rather confusing so I think we should move to a more appropriate name."
4632,'s1monw',"Add a message about import style\nWe don't have a consistent style and we don't want new contributors to\r\nget hung up on trying to figure out the style."
4630,'s1monw','BalancedShardAllocator might trigger unnecessary relocation under rare circumstances if deltas are very close to the threshold due to rounding issues.\nSometimes if there are very small number of nodes compared to large number of shards and indices deltas between nodes are very close to the default threshold `1.0` but due to the fact that we use floats we might end up with a weight of `1.000000001` which then in-turn triggers a relocation which is unnecessary and is kind of `undone` in the next iteration due to the same issue. \r\n'
4627,'javanna','Allow to open and close all indices without specifying any index\nCurrently, open/close index api require the index parameter, which can be a wildcard expression or even `_all`, but cannot be empty.\r\n\r\nIt is not possible to do `curl -XPOST localhost:9200/_close` as a validation error is thrown, while it is possible to do `curl -XDELETE localhost:9200/`.\r\n\r\nWith #4549 we are adding a settings that allows to control whether to allow or not potentially destructive operation on all indices. Those operations need to have the same behaviour, thus `curl -XPOST localhost:9200/_close` and `curl -XPOST localhost:9200/_open` should be possible as well, although disabled by default. \r\n\r\nThis issue is about accepting the empty index parameter in open/close index api, so that we can either allow it or not depending on the `action.operate_all_indices` settings.'
4625,'martijnvg','Change the `sort` boolean option in percolate api to the sort dsl available in search api\nAt the moment the `sort` boolean option allows to sort via percolate score in descending order. The full sort dsl should be supported, so that in the future if other sort options are available, the percolate api doesn\'t need to break backward compatibility. \r\n\r\nThe following sort options will be the only supported option in percolate api:\r\n```\r\n{\r\n...\r\n"sort" : [\r\n   "_score"\r\n]\r\n....\r\n```'
4624,'martijnvg','Rename `score` to `track_scores` in percolate api\nThe `score` option allows to include the score of a matched percolate query, without sorting by the score. This option should be renamed to `track_scores`, which is consistent with the same functionality in the search api (but then for matches documents).'
4620,'martijnvg','Add internal get index settings api, that the GET /_settings api uses\nThe internal get index settings api can retrieve index settings more efficiently than the cluster state api does, which is what the rest get index settings api now uses.\r\n\r\nOn top of this the get settings api will now also fully support the indices options (`ignore_missing`, `allow_no_indices` and `expand_wildcards`) and the get settings api will also support a new `prefix` option, which allows to only include settings that start with a specific prefix.'
4619,'rmuir',"Cannot forcefully unlock a NativeFSLock which is held by another indexer component\nHello,\r\nI'm running ES as single node cluster, and I'm using logtash. I have ~400 indexes. And now nothing happens :-) only \r\nCaused by: org.apache.lucene.store.LockReleaseFailedException: Cannot forcefully unlock a NativeFSLock which is held by another indexer component: /opt/kullm\r\nann/elasticsearch-0.90.9/data/elasticsearch/nodes/0/indices/ordered-items-2013.11.02/3/index/write.lock\r\n        at org.apache.lucene.store.NativeFSLock.release(NativeFSLockFactory.java:295)\r\n        at org.apache.lucene.index.IndexWriter.unlock(IndexWriter.java:4458)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.createWriter(RobinEngine.java:1415)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:254)\r\n\r\nI already tried sysctl -w vm.max_map_count=262144 described here https://github.com/elasticsearch/elasticsearch/issues/4547 but no effect. "
4616,'jpountz','Add ability to configure circuit breaker with a percentage\nIt would be nice to be able to say "75%" instead of absolute values.'
4615,'spinscale','`cluster.routing.allocation.same_shard.host` should be documented...\nsomething like this:\r\n\r\n```\r\n`cluster.routing.allocation.same_shard.host`::                                                                                                                  \r\n      Allows to enable checks that prevent more than one replica of the                                                                                         \r\n      same shard to be allocated on the same physical host. The checks                                                                                          \r\n      are based on the nodes host name. nodes with the same host name are                                                                                       \r\n      treated as running on the same physical host. Default is `false`. \r\n```'
4610,'imotov','Double wildcards in the the index name can cause a request to hang\nDouble wildcards with non-matching index pattern can cause [Regex#simpleMatch](https://github.com/elasticsearch/elasticsearch/blob/6a04c169326ab99c1e5b4eef6f9fdbed222b5fa0/src/main/java/org/elasticsearch/common/regex/Regex.java#L81) to go into infinite loop. To reproduce, call `Regex.simpleMatch("**ddd", "fff")`.\r\n\r\n'
4609,'s1monw',"[Feature Request] - Expose headers on HttpRequest\nThere is a method available for retrieving the value of a header, but it's not possible to iterate over all headers. \r\n\r\nIs there a reason why this is not exposed? If not, would be nice having that."
4607,'drewr','cat: Add rest of stats\nWe want to add all of the index and node stats.'
4606,'javanna',"Fixed open/close index api when using wildcard only\nNamed wildcards were not always properly replaced with proper values by PathTrie.\r\nDelete index (`curl -XDELETE localhost:9200/*`) worked anyway as the named wildcard is the last path element. \r\nWhen the named wildcard wasn't the last path element (e.g. `curl -XPOST localhost:29200/*/_close`), the variable didn't get replaced with the current '*' value, but with the empty string, which lead to an error as empty index is not allowed by open/close index.\r\n\r\nCloses #4564"
4604,'dakrone','Set default for circuit breaker to 80% of the maximum heap\nThe current default is -1 (no limit).'
4603,'dadoonet','plugin manager: new `timeout` option\nWhen testing plugin manager with real downloads, it could happen that the test run forever. Fortunately, test suite will be interrupted after 20 minutes, but it could be useful not to fail the whole test suite but only warn in that case.\r\n\r\nBy default, plugin manager still wait indefinitely but it can be modified using new `--timeout` option:\r\n\r\n```sh\r\nbin/plugin --install elasticsearch/kibana --timeout 30s\r\n\r\nbin/plugin --install elasticsearch/kibana --timeout 1h\r\n```'
4596,'chilling','Using Haversine for accurate distance measurement\nThe current implementation of an accurate distance calculation is not accurate for distances less than *1m*. Since the *haversine* function is more robust against rounding error of floating point arithmetic, the great circle distance should be replaced by an accurate implementation of the haversine function.\r\n\r\nRelated to #4498'
4595,'martijnvg','Refresh the id_cache if a new child type with _parent field has been introduced\nAlready loaded SimpleIdReaderCache should be reloaded when a new `_parent` has been introduced.\r\n\r\nRelates #4568'
4593,'dakrone','Add /_cat/fielddata to display fielddata on a per-node per-field basis\nThis would be great for figuring out where all your memory is going with regard to field data:\r\n\r\n```\r\n$ curl localhost:9200/_cat/fielddata\r\nnode   body  timestamp   subject  myotherfield  total\r\nnode1  7gb       501mb     1.1gb          17mb   10gb\r\nnode2  1.3gb     100mb     981mb           2mb  3.1gb\r\nnode3  2.7gb      81mb     182mb         512kb  3.2gb\r\n```\r\n\r\nIt would be a nice-to-have for 1.0 RC1, but not required.'
4592,'dakrone','Add field data circuit breaker to stop field data loading from running out of memory\nWe should add a circuit breaker to prevent Elasticsearch from running out of memory when field data is loaded.'
4589,'s1monw','NullPointerException in IndexShardRoutingTable.getActiveAttribute\nim getting error below after killing one node in the cluster \r\n(exception is thrown on remaining nodes)\r\n\r\n\r\n```\r\norg.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution\r\n        at org.elasticsearch.action.support.AdapterActionFuture.rethrowExecutionException(AdapterActionFuture.java:90)\r\n        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:49)\r\n        at org.elasticsearch.action.ActionRequestBuilder.get(ActionRequestBuilder.java:67)\r\n        ...\r\nCaused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:288)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:275)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\r\n        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\r\n        ... 15 more\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.getActiveAttribute(IndexShardRoutingTable.java:441)\r\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferAttributesActiveInitializingShardsIt(IndexShardRoutingTable.java:488)\r\n        at org.elasticsearch.cluster.routing.IndexShardRoutingTable.preferAttributesActiveInitializingShardsIt(IndexShardRoutingTable.java:483)\r\n        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.preferenceActiveShardIterator(PlainOperationRouting.java:169)\r\n        at org.elasticsearch.cluster.routing.operation.plain.PlainOperationRouting.getShards(PlainOperationRouting.java:80)\r\n        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:80)\r\n        at org.elasticsearch.action.get.TransportGetAction.shards(TransportGetAction.java:42)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.<init>(TransportShardSingleOperationAction.java:121)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.<init>(TransportShardSingleOperationAction.java:97)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:74)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:49)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:63)\r\n        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)\r\n        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)\r\n        at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)\r\n        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)\r\n        at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)\r\n\r\n```\r\n\r\ncontext:\r\n- version: 0.90.9\r\n- 3 node cluster\r\n- 2 replicas\r\n- 10 shards per index'
4588,'s1monw',"Elasticsearch shouldn't try to balance shards from nodes with newer version of lucene to nodes with older versions of lucene\nDuring a rolling upgrade I'm constantly seeing IndexFormatTooNewException exceptions as Elasticsearch repeatedly tries to rebalance a shard from an upgraded machine to a non-upgraded machine.  It is causing a large load spike on the non upgraded machine and won't work anyway.  Can Elasticsearch just not try?"
4584,'brwe','Revisit _shard / class names in exposing terms stats for scripts\nThe recent feature, #4161, include support for exposing lucene data / statistics for scripts. The naming though is misleading potentially, both on the class names and on how to access it in scripts.\r\n\r\nThe `_shard` key is used to access it in script. The class names are `ShardTermsLookup`, `ScriptTerms`, `ScriptTerm` for example. The `_shard` name feels too generic, while the `terms` names in the class names is misleading, since one can get data not only for terms (like doc count).\r\n\r\nI think we can try to find a good name for it, and derive the script name and class names from it. Some thoughts include:\r\n\r\n* `_ts` (script) / `TermsXXX` (class): though not all data exposed relates to terms, it encapsulates most of the stats one can get. The down side is, of course, that one can get more info than just terms.\r\n\r\n* `_stats` (script) / `StatsXXX` (class): most of the data that is exposed relates to statistics information. Though again, not all, for example, payload... .\r\n\r\n* `_index` (script) / `IndexXXX` (class): this is an "inverted index" level information, stats and other information that can be derived from the index itself.\r\n\r\nI am personally leaning towards `_index`.'
4582,'s1monw','Term Vector settings should be treated like flags without propergation\n today if a specific feature is disabled for term vectors with something\r\n    like `"store_term_vector_positions" " false` term vectors might be disabled\r\n    altogether even if `"tore_term_vectors" : true` in the mapping. This depends on the\r\n    order of the values in the mapping since the more specific one might override\r\n    the less specific on.'
4581,'s1monw','All field uses wrong setting for `term vectors`\nIn the all field mapper the settings that is used is `store_term_vector`  but it should be `store_term_vectors`.'
4579,'s1monw',"All field might loose configuration on serialization.\nThe all field tries to optimize for default cases and doesn't generate XContent if everything is default. The settings are not tested well enough and there is already differences between master and 0.90. Master is already missing to check `autoboost` and `simiarity` and 0.90 has misses settings if `customFieldDataSettings` is set as the only setting as well."
4577,'dadoonet','River does not start when using config/templates files\nFrom elasticsearch 0.90.6, when you have templates files defined in `config/templates` dir, rivers don\'t start anymore.\r\n\r\nSteps to reproduce:\r\n\r\nCreate `config/templates/default.json`:\r\n\r\n```javascript\r\n{\r\n  default:\r\n  {\r\n    "template" : "*",\r\n    "mappings" : {\r\n      "_default_" : {\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nStart a dummy river:\r\n\r\n```sh\r\ncurl -XPUT \'localhost:9200/_river/my_river/_meta\' -d \'{ "type" : "dummy" }\'\r\n```\r\n\r\nIt gives:\r\n\r\n```\r\n[2014-01-01 22:08:38,151][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings [_default_]\r\n[2014-01-01 22:08:38,239][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:38,245][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n[2014-01-01 22:08:38,250][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:39,244][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:39,252][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:40,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:40,254][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:41,246][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:41,255][INFO ][river.routing            ] [Forge] no river _meta document found, retrying in 1000 ms\r\n[2014-01-01 22:08:42,249][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts\r\n[2014-01-01 22:08:42,257][WARN ][river.routing            ] [Forge] no river _meta document found after 5 attempts\r\n```\r\n\r\nWith elasticsearch 0.90.2 or with no template file in `config/templates` dir, it gives:\r\n\r\n```\r\n[2014-01-01 22:22:32,096][INFO ][cluster.metadata         ] [Forge] [_river] creating index, cause [auto(index api)], shards [1]/[1], mappings []\r\n[2014-01-01 22:22:32,221][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] create\r\n[2014-01-01 22:22:32,228][INFO ][river.dummy              ] [Forge] [dummy][my_river] start\r\n[2014-01-01 22:22:32,234][INFO ][cluster.metadata         ] [Forge] [_river] update_mapping [my_river] (dynamic)\r\n```\r\n\r\n'
4572,'drewr','Errors in _cat API\nexecuting `curl -XGET localhost:9200/_cat/nodes` on a node (no indices) returns:\r\n\r\n```json\r\n{\r\n   "error": "StringIndexOutOfBoundsException[String index out of range: 4]",\r\n   "status": 500\r\n}\r\n```\r\n\r\nAll the following APIs don\'t work with local node:\r\n\r\n```\r\ncurl -XGET localhost:9200/_cat/master\r\ncurl -XGET localhost:9200/_cat/allocation\r\ncurl -XGET localhost:9200/_cat/shards\r\n```\r\nreturn\r\n\r\n```json\r\n{\r\n   "error": "ClassCastException[org.elasticsearch.common.transport.LocalTransportAddress cannot be cast to org.elasticsearch.common.transport.InetSocketTransportAddress]",\r\n   "status": 500\r\n}\r\n```'
4568,'martijnvg','Odd interaction between refresh and parent/child queries \nI\'ve been having some issues with the following sequence of queries:\r\n\r\n    curl -XDELETE \'http://localhost:9201/test__garments\'\r\n    curl -XPOST \'http://localhost:9201/test__garments/garment/1\' -d \'{"id":1, "name":"Some Garment"}\'\r\n    curl -XPOST \'http://localhost:9201/test__garments/_refresh\'\r\n    curl -XPUT \'http://localhost:9201/test__garments/verdict/_mapping\' -d \'{"verdict":{"_parent":{"type":"garment"},"properties":{"id":{"type":"integer"}}}}\'\r\n    curl -XPOST \'http://localhost:9201/test__garments/verdict/1?parent=1\' -d \'{"id":1}\'\r\n    \r\n    curl -XPOST \'http://localhost:9201/test__garments/_refresh\'\r\n    curl -XPOST \'http://localhost:9201/test__garments/verdict/_search\' -d \'\r\n    {\r\n      "query": {\r\n        "filtered": {\r\n          "query": {\r\n            "match_all": {}\r\n          },\r\n          "filter": {\r\n              "has_parent": {\r\n                "type": "garment",\r\n                "query": {\r\n                  "match_all": {}\r\n                }\r\n              }\r\n          }\r\n        }\r\n      }\r\n    }\r\n      \'\r\n\r\nIt should produce 1 result (I\'m indexing a document (garment), adding a child document (verdict) and then querying for child documents that have a parent), but it produces no results (and continues to do so no matter how long I wait). If I remove the first refresh it seems ok or if I do\r\n\r\n    curl -XPOST \'http://localhost:9200/_cache/clear?id_cache=true\'\r\n\r\nbefore the search then I will also get a result (as discussed at https://groups.google.com/forum/#!topic/elasticsearch/oD8EKEYeZuM). I noticed this behavior when moving from 0.20.6 to 0.90.9\r\n\r\nIdeally I would not have to expire the cache by hand - or the circumstances in which one needs to do this should be documented'
4565,'s1monw','Update CountRequest and ValidateRequest to allow querying of types\nCan get all other fields but not types.'
4564,'javanna',"Closing all indices doesn't work when using wildcard only\nClosing all indices doesnt' work when doing:\r\n\r\n```\r\ncurl -XPOST localhost:9200/*/_close\r\n```\r\n\r\nalthough it works using `_all` or using a wildcard expression like `index*`."
4563,'jpountz',"GeoPointFieldMapper.doXContentBody doesn't honor `includeDefaults`\nRelates to #3941"
4560,'jpountz',"Explicit doc values setting\nRight now doc values are enabled on a field if its fielddata format is `doc_values` at creation time of the field mapper.\r\n\r\nSo if you create a field with the `doc_values` fielddata format, then use the update mapping API to change the format to `paged_bytes` instead and restart the node, the field mapper won't know that it needs to index doc values and the `doc_values` format won't be usable anymore.\r\n\r\nI'd like to have a `doc_values` setting on the same level as `index` and `store` in the mappings that would remain true even when the field data format becomes `paged_bytes` so that doc values keep being indexed and the fielddata format can be later set to `doc_values` again."
4559,'jpountz',"Page-based cache recycling.\nHere is an attempt to refactor cache recycling so that it only caches large\r\narrays (pages) that can later be used to build more complex data-structures\r\nsuch as hash tables.\r\n\r\n - QueueRecycler now takes a limit like other non-trivial recyclers.\r\n - New PageCacheRecycler (inspired of CacheRecycler) has the ability to cache\r\n   byte[], int[], long[], double[] or Object[] arrays using a fixed amount of\r\n   memory (either globally or per-thread depending on the Recycler impl, eg.\r\n   queue is global while thread_local is per-thread).\r\n - Paged arrays in o.e.common.util can now optionally take a PageCacheRecycler\r\n   to reuse existing pages.\r\n - All aggregators' data-structures now use PageCacheRecycler:\r\n   - all arrays\r\n   - LongHash can now take a PageCacheRecycler\r\n   - there is a new BytesRefHash (inspired from Lucene but quite different,\r\n     still; for instance it cheats on BytesRef comparisons by using Unsafe)\r\n     that also takes a PageCacheRecycler\r\n\r\nClose #4557"
4557,'jpountz','Improving cache recycling\nJava has a generational collector that relies on the fact that most objects die young. Sometimes, it may happen that there is pressure on the young gen because of the allocation of large objects, and some objects that were actually going to die young are promoted to the old generation in order to make space for new objects. This is bad because collections of the old generation are typically **much** more costly so this is something that we should try to avoid whenever possible.\r\n\r\nSo here comes the cache recycler: by reusing large objects, these objects are promoted to the old generation (because we keep strong refs on them), but on the other hand they help diminish the allocation rate of large objects in the young generation, and this makes short-lived objects more unlikely to be promoted to the old generation.\r\n\r\nAlthough this is nice from a GC point-of-view, this can have bad implications on the application side. Typically today, these cached data-structures grow with time and at some point, they may become oversized for the data that they need to carry. Typically, if you store 10 entries in a hash table of capacity 1M, the cost of iterating over all entries is proportional to 1M, not 10. Moreover, over-sized data-structures also tend to not play nicely with CPU caches.\r\n\r\nIn order to improve it, an idea could be: instead of recycling whole data-structures, we could build paged data-structures and recycle pages individually. This is nice on several levels:\r\n - it would retain the advantage of cache recycling while avoiding over-sized structures\r\n - the recycling logic would be simpler since it would only care about recycling fixed-length arrays\r\n - it is easy to compute the size of Java arrays so we could do byte accounting and make the cache size (in bytes) configurable, eg. "reuse at most 50MB of memory per thread".\r\n\r\nMy plan is to use aggregations to experiment with this idea: they already use paged arrays and hash tables that we could modify to reuse pages.'
4556,'s1monw',"Use `recycler` prefix for CacheRecycler settings\nCurrently the cache recycler uses top level settings for it's\r\nconfiguration. We should prefix them with a `recycler` to make sure\r\nwe don't have any confilicts."
4553,'spinscale','Update frontends.asciidoc\nFix typo'
4550,'jpountz','Better RangeAggregator when used as a sub-aggregator\nRangeAggregator is different from other PER_BUCKET aggregators (such as terms or histogram aggregators) in that it knows the number of buckets it will create in advance. So it could actually be a MULTI_BUCKETS aggregator by just multiplying the bucket ordinals by the number of ranges.\r\n\r\nThis will make RangeAggregator faster and more memory-efficient when used as a sub-aggregator of other PER_BUCKET aggregators.'
4549,'martijnvg','Allow to disable destructive operations on all indices\nAdd `action.destructive_requires_name` in order to control whether wildcards and `_all` are allowed for destructive operations. \r\n\r\nThe following APIs are affected by this option and are considered destructive: delete index, close index, open index, delete_by_query and delete mapping. \r\n\r\nThe `action.destructive_requires_name` will default to `false`, meaning that using wildcards and `_all` in the mentioned APIs is allowed. \r\n\r\nIn the delete api specifying an index will be made required, so `DELETE /` will not be allowed at all times regardless of `action.destructive_requires_name`. (Like is mentioned in #4481) \r\n\r\nAlso the `action.disable_delete_all_indices` option which is applicable for the delete index api and the `action.disable_close_all_indices` which is applicable for the close api will be dropped in favour for `action.destructive_requires_name`'
4544,'drewr','NPE in cat/shards when UNASSIGNED\nThis was introduced with the associative refactoring in #4433.'
4543,'drewr','Cat: Add cache numbers to cat/nodes\nfielddata, filter, maybe bloom as well...'
4542,'martijnvg',"The `fields` option should always return an array\nThe `fields` options allows to extract field values from _source or load specific stored fields. The fields option is supported in various apis (get, search and explain).\r\n\r\nThe behaviour when it comes to array fields with a single value is inconsistent between apis, between source and stored fields. Based on the previous an array field is either serialised as a single value or an array containing a single value.\r\n\r\nDoing the right thing here is difficult because the field option works on both _source and stored fields. The _source contains the meta information (json) to serialise field values correctly, but this information isn't available in stored fields. The plan is to make fields always return an array for both _source and stored fields and in all APIs with the goal to be consistent. Also the `fields` option can only serialise leaf fields, this to be further consistent between stored fields and _source. Metadata (`_id`, `_routing`, `_parent` etc.) fields are always single values, for this reason in the response the metadata fields are never wrapped in a json array.\r\n\r\nIf better serialisation is required for _source, the source filtering feature should be used instead: http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-request-source-filtering.html"
4541,'javanna','Added rest-spec directly to the core repo, removed rest-spec submodule\nAdded rest-spec directly to the core repo, removed rest-spec submodule\r\n\r\nCloses #4540'
4540,'javanna','Merge rest-spec-api into elasticsearch core\nThe REST tests added with #4469 rely on the external https://github.com/elasticsearch/elasticsearch-rest-api-spec repo which is currently pulled in as a git submodule.\r\n\r\nWhen needing to make a non backwards compatible change to the REST layer, we need to update the rest-api-spec submodule as well. This process is not straightforward as we would like it to be.\r\n\r\nTherefore we decided to integrate the rest-api-spec repository into the elasticsearch core itself.\r\n'
4539,'martijnvg','Remove GET `_aliases` api in favour for GET `_alias` api\nCurrently there are two get aliases apis that both have the same functionality, but have a different response structure. The reason for having 2 apis is historic. \r\n\r\nThe GET `_alias` api was added in 0.90.x and is more efficient since it only sends the needed alias data from the cluster state between the master node and the node that received the request.  In the GET `_aliases` api the complete cluster state is send to the node that received the request and then the right information is filtered out and send back to the client.\r\n\r\nThe GET `_aliases` api should be removed in favour for the `alias` api'
4534,'martijnvg',"Named filter and query don't work with parent/child queries\nThe named filter and query support (`_name`) don't work with with the `top_children`, `has_child` and `has_parent` queries and filters.\r\n\r\nOriginates from #4529"
4533,'spinscale',"Get UnsatisfiedResolutionException on Spring-data-elasticsearch with CDI enabled \nI got the below exception when trying to deploy my web application on glassfish4 or JBoss EAP 6.2.\r\nSEVERE:   Exception while loading the app : CDI definition failure:Exception List with 1 exceptions:\r\nException 0 :\r\njavax.enterprise.inject.UnsatisfiedResolutionException: Unable to resolve a bean for 'org.springframework.data.elasticsearch.core.ElasticsearchOperations' with qualifiers [@javax.enterprise.inject.Any(), @javax.enterprise.inject.Default()].\r\n\tat org.springframework.data.elasticsearch.repository.cdi.ElasticsearchRepositoryExtension.createRepositoryBean(ElasticsearchRepositoryExtension.java:71)\r\n\tat org.springframework.data.elasticsearch.repository.cdi.ElasticsearchRepositoryExtension.afterBeanDiscovery(ElasticsearchRepositoryExtension.java:61)"
4532,'spinscale','Update configuration.asciidoc\nI think this should be node.name and not network.host?'
4529,'martijnvg','Named and-filter breaks has_parent-filter\n```bash\r\n#curl -XDELETE \'http://localhost:9200/test/\'\r\ncurl -XPUT \'http://localhost:9200/test/\'\r\n \r\ncurl -XPUT \'http://localhost:9200/test/ancestor/_mapping\' -d \'\r\n{\r\n    "ancestor" : {\r\n        "properties" : {\r\n            "id" : {"type" : "integer", "store" : "yes"},\r\n            "name" : {"type" : "string", "store" : "yes"}\r\n        }\r\n    }\r\n}\r\n\'\r\n \r\ncurl -XPUT \'http://localhost:9200/test/person/_mapping\' -d \'\r\n{\r\n    "person" : {\r\n        "properties" : {\r\n            "id" : {"type" : "integer", "store" : "yes"},\r\n            "name" : {"type" : "string", "store" : "yes"}\r\n        },\r\n        "_parent" : {\r\n            "type" : "ancestor"\r\n        }\r\n    }\r\n}\r\n\'\r\n \r\ncurl -XPUT \'http://localhost:9200/test/ancestor/1\' -d \'\r\n{\r\n        "id" : 1,\r\n        "name" : "mueller"\r\n}\r\n\'\r\ncurl -XPUT \'http://localhost:9200/test/person/2?parent=1\' -d \'\r\n{\r\n        "id" : 2,\r\n        "name" : "mueller"\r\n}\r\n\'\r\n \r\ncurl -XPOST \'http://localhost:9200/test/person/_search\' -d \'\r\n{\r\n    "query": {\r\n        "filtered": {\r\n            "filter": {\r\n                "and": {\r\n                    "filters": [\r\n                        {\r\n                            "has_parent": {\r\n                                "filter": {\r\n                                    "term": {\r\n                                        "id": 1\r\n                                    }\r\n                                },\r\n                                "type": "ancestor"\r\n                            }\r\n                        }\r\n                    ]\r\n                }\r\n            },\r\n            "query": {\r\n                "bool": {\r\n                    "should": [\r\n                        {\r\n                            "match_all": {}\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\'\r\n \r\n#this one does not work\r\ncurl -XPOST \'http://localhost:9200/test/person/_search\' -d \'\r\n{\r\n    "query": {\r\n        "filtered": {\r\n            "filter": {\r\n                "and": {\r\n                        "_name" : "test",\r\n                    "filters": [\r\n                        {\r\n                            "has_parent": {\r\n                                "filter": {\r\n                                    "term": {\r\n                                        "id": 1\r\n                                    }\r\n                                },\r\n                                "type": "ancestor"\r\n                            }\r\n                        }\r\n                    ]\r\n                }\r\n            },\r\n            "query": {\r\n                "bool": {\r\n                    "should": [\r\n                        {\r\n                            "match_all": {}\r\n                        }\r\n                    ]\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\'\r\n```\r\nresults in \r\n{"took":2,"timed_out":false,"_shards":{"total":5,"successful":4,"failed":1,"failures":[{"index":"test","shard":2,"status":500,"reason":"ElasticSearchIllegalStateException[has_parent filter hasn\'t executed properly]"}]},"hits":{"total":1,"max_score":1.0,"hits":[]}}'
4521,'martijnvg','Improve multi_field syntax\nProposed syntax for multi_field\r\n---------------------------------\r\n\r\nAny field type (other than `object`, `nested`) should accept a `fields` parameter which defines any extra fields that should be indexed.  So for example a simple field like:\r\n\r\n    {\r\n        "title": {\r\n            "type": "string"\r\n    }}\r\n\r\ncan be converted into a multi-field by adding a `fields` parameter:\r\n\r\n    {\r\n        "title": {\r\n            "type": "string",\r\n            "fields": {\r\n                "raw": { "type": "string", "index": "not_analyzed" }\r\n           }\r\n    }}\r\n\r\n'
4520,'imotov','Custom _all fields\nIn the quest for a cleaner way of setting up custom `_all` fields, there are two questions that need to be answered:\r\n\r\n1. Does it ever make sense to index tokens from different analyzer chains into a single field?\r\n\r\n2. Can we support per-field boosting on the custom `_all` field (like we do with the `_all` field), and can we only pay the query-time price of per-field boosting if it used?\r\n\r\n\r\nDifferent analyzer chains\r\n-------------------------------------\r\n\r\nI can\'t think of a good use case where it makes sense to combine the output from different analyzer chains into a single field.  The field can only ever be searched via a single analyzer, multiple analyzers can produce tokens which interfere with each other (and so produce wrong results) and the term frequencies for overlapping tokens will be badly messed up.  Also, a clean token stream should never have offsets move "backwards".\r\n\r\nSo I think we can discount multiple analyzers outputting to a single field.\r\n\r\nPer-field boosting\r\n--------------------------\r\n\r\nWhen combining multiple fields into a single field, you lose the effect of field norms (ie title is shorter and thus more important than body).  Field-level boosting at index time is the only way to maintain this distinction.\r\n\r\nThe `_all` field takes field-level boosts into account by storing any boost that is not 1.0 as a payload with each term.  Retrieving these payloads has an impact on query performance, but the `_all` field has an optimization called "auto_boost" which allows you to only pay the price of payloads if any included field has a boost other than 1.0.  \r\n\r\nI think field-level boosts should be supported with custom `_all` fields too.\r\n\r\nProposed syntax\r\n=============\r\n\r\nGiven that we\'re not going to support separate analyzer chains, the current way of implementing custom `_all` fields with multi-fields is verbose and misleading, as it implies that each source field can apply its own analyzer.  \r\n\r\nInstead, we suggest the following:\r\n\r\n    { "title": {\r\n        "type": "string",\r\n        "copy_to": "my_all_field"\r\n    }}\r\n\r\nThe `copy_to` parameter can also support an array of fieldnames:\r\n\r\n    "copy_to": [ "my_all_field_1", "my_all_field_2" ]\r\n\r\nPer-field boosting could be specified in two ways:\r\n\r\n1. With the caret `^` syntax:\r\n\r\n    "copy_to": "my_all_field^2"\r\n\r\n2. As an object:\r\n\r\n    "copy_to": { "field": "my_all_field", "boost": 2 }\r\n\r\nThe destination custom `_all` field can be defined in the mapping:\r\n\r\n    "my_all_field": {\r\n        "type": "string",\r\n        "analyzer": "my_analyzer"\r\n    }\r\n\r\nIf it is not defined in the mapping, then it should be added using dynamic mapping (or fail if dynamic mapping is disabled)\r\n'
4515,'chilling','Geo clean up\nThe default unit for measuring distances is *MILES* in most cases. ES should move over to the *International System of Units* and return *METERS* by default and internally work on a default unit.\r\nIf the internal measurement of explicitly changes to a default unit. `DistanceUnit.DEFAULT` which should relate to *meters* will also effect the **REST API** at the following places:\r\n\r\n  * `ScriptDocValues.factorDistance()` returns *meters* instead of *miles*\r\n  * `ScriptDocValues.factorDistanceWithDefault()` returns *meters* instead of *miles*\r\n  * `ScriptDocValues.arcDistance()` returns *meters* instead of *miles*\r\n        one might use `ScriptDocValues.arcDistanceInMiles()`\r\n  * `ScriptDocValues.arcDistanceWithDefault()` returns *meters* instead of *miles*\r\n  * `ScriptDocValues.distance()` returns *meters* instead of *miles*\r\n        one might use `ScriptDocValues.distanceInMiles()`\r\n  * `ScriptDocValues.distanceWithDefault()` returns *meters* instead of *miles*\r\n        one might use `ScriptDocValues.distanceInMilesWithDefault()`\r\n  * `GeoDistanceFilter` default unit changes from *kilometers* to *meters*\r\n  * `GeoDistanceRangeFilter` default unit changes from *miles* to *meters*\r\n  * `GeoDistanceFacet` default unit changes from *miles* to *meters*\r\n\r\nThe naming of the `GeoBoundingBoxFilter` properties should allow to set the opposite corners (see #4084) namely `top_right` and `bottom_left`. This change also includes the fields `topRight` and `bottomLeft`. Also it should be possible to set the single values by using just `top`, `bottom`, `left` and `right` parameters.\r\n\r\n'
4513,'spinscale','elasticsearch close index error  \nelasticsearch 0.90.5\r\n\r\n\r\n[2013-12-19 16:14:25,278][DEBUG][indices.cluster          ] [Basilisk] [6666][0] removing shard (index is closed)\r\n[2013-12-19 16:14:25,278][WARN ][cluster.service          ] [Basilisk] failed to apply updated cluster state:\r\nversion [7], source [close-index [6666]]\r\nnodes: \r\n   [Basilisk][GzEzIl3dSbOGzxSgKHxwMw][inet[/128.18.110.126:9300]], local, master\r\nrouting_table:\r\nrouting_nodes:\r\n-----node_id[GzEzIl3dSbOGzxSgKHxwMw][V]\r\n---- unassigned\r\n\r\norg.elasticsearch.index.IndexShardMissingException: [6666][0] missing\r\n\tat org.elasticsearch.index.service.InternalIndexService.shardInjectorSafe(InternalIndexService.java:296)\r\n\tat de.spinscale.elasticsearch.service.suggest.SuggestService$2.beforeIndexShardClosed(SuggestService.java:73)\r\n\tat org.elasticsearch.indices.InternalIndicesLifecycle.beforeIndexShardClosed(InternalIndicesLifecycle.java:104)\r\n\tat org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:371)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyDeletedShards(IndicesClusterStateService.java:285)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:395)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)'
4511,'spinscale','template mappings are not loading in 0.90.8\nI have templates that define specific mappings. In testing 0.90.8 today, I noticed that the template mappings are not being used when I load data in to an index that matches the template.  This has worked in all previous versions of ElasticSearch that I have used (0.19x -> 0.90.7).'
4508,'martijnvg','Improve query/filter parsing strictness \nMake the parsing strict or stricter for:\r\n* `geo_shape` filter & query\r\n* `common` query\r\n\r\nParsing must be strict in order to avoid misleading behaviour.'
4506,'martijnvg','Single shards APIs should fail if routing is required.\nIf routing for a specific type is configured to be required, but no routing is specified during a single shard api call a client error should be thrown. The following APIs need this validation: get, mget, explain, termvector, multi term vector and exists.\r\n\r\nNote: this validation already happens in the index and delete APIs, but the error will change to a client error instead of the internal server error.'
4493,'martijnvg',"Made APIs consistently accept a query in the request body's `query` field\nThe following APIs now accept the query in a top level `query` field like:\r\n* delete_by_query\r\n* validate_query\r\n* count\r\n\r\nThese APIs used to accept the query directly in the request body which was inconsistent with the search and explain APIs. For this reason this is a breaking change.\r\n\r\nCount api note: It still has its own piece if code and isn't a shortcut for /_search?search_type=count (so no facets, aggs etc.) The count api code is fine tuned for simple counting and I think it should stay that way. \r\n\r\nRelates to #4074"
4492,'dakrone',"Add _source, _doc and _field support to fields\nWhen requesting `fields` in the search, get, update, etc APIs, we should be able to control where those values come from with these prefixes:\r\n\r\n* `_source`: extract the field value from the `_source` field\r\n* `_fields`: retrieve a `stored` field value\r\n* `_doc`: retrieve the value from fielddata\r\n\r\nThe `_doc` option is particularly useful for retrieving dates as, no matter the format they're specified in in the source, they will be accessible as milliseconds-since-the-epoch from fielddata, without the need to allow dynamic scripts.\r\n\r\nWhile on the subject, perhaps we should support this same syntax in scripting, instead of `docs['foo']`?"
4491,'bleskes','Include/Exclude Filtering Behavior\nCurrently there is a bug in elasticsearch (https://github.com/elasticsearch/elasticsearch/issues/4047) where empty objects are not stored in `_source` when an include/exclude list is present.\r\n\r\nThis is because elasticsearch aggressively removes empty objects from the `_source`.\r\n\r\nFor example, if I have an object\r\n```\r\n{ \'data\': { \'key\': \'value\' } }\r\n```\r\nthe following filters will all result in removing `data` from `_source`: `excludes = [\'data\']`, `excludes = [\'data.*\']`, `excludes = [\'data.key\']`, `excludes = [\'*.key\']`, `includes = [\'data.other\']`\r\n\r\nI believe this behavior is incorrect. I think that we should only remove an object if it is explicitly excluded (`excludes = [\'data\']`, `excludes = [\'data.*\']`) or if no elements are included (`includes = [\'other_data.*\']).  For situations where the object is referenced in an includes list but there is no match, I think the object should remain as an empty object (`includes = [\'data.other\']`).\r\n\r\nThis use case makes more sense if we are talking about some nested object that is indexed...\r\n\r\nExample:\r\n\r\n```\r\n{\r\n  "name": "John Doe",\r\n  "identifiers": {\r\n    "ssn": "987-65-4320",\r\n    "facebook_uid": "12345",\r\n    "twitter_uid": "54321"\r\n  }\r\n```\r\n\r\nand `excludes = [ "*.ssn"]` would drop the entire `identifiers` object if the only key was `ssn` for that object, even if we want the empty identifiers object to remain under all circumstances.  We have the same problem with `includes = ["*.instagram_uid"]`.'
4490,'dakrone','Expose flags in simple_query_string query\nThe `simple_query_string` query allows the user to enable/disable parts of the syntax, eg prefixes, phrases etc.\r\n\r\nI think it makes sense to support the full syntax by default, but to allow the user to disable the parts that they don\'t want, eg:\r\n\r\n    "simple_query_string": {\r\n        "query": "....",\r\n        "disable": "PREFIX|PHRASE"\r\n    }\r\n\r\n... in a similar way that we do with `flags` in the regexp query/filter'
4489,'javanna','[Docs] Fix Typo\nFixes small typo in the geo_distance aggregation docs. ("onceptually" -> "conceptually")\r\n\r\nfixes #4487'
4488,'martijnvg','Combine cluster.routing.allocation.disable.* settings\nCurrently we have three different disable-allocation config options:\r\n\r\n`cluster.routing.allocation.disable_new_allocation`:\r\n\r\nAllows to disable new primary allocations. Note, this will prevent allocations for newly created indices. This setting really make sense when dynamically updating it using the cluster update settings API.\r\n\r\n`cluster.routing.allocation.disable_allocation`:\r\n\r\nAllows to disable either primary or replica allocation (does not apply to newly created primaries, see disable_new_allocation above). Note, a replica will still be promoted to primary if one does not exist. This setting really make sense when dynamically updating it using the cluster update settings API.\r\n\r\n`cluster.routing.allocation.disable_replica_allocation`:\r\n\r\nAllows to disable only replica allocation. Similar to the previous setting, mainly make sense when using it dynamically using the cluster update settings API.\r\n\r\nBesides the fact that `disable: true` reads a bit like a double negative, these settings seem to overlap.  Could we not merge them into a single setting:\r\n\r\n    cluster.routing.allocation.enable: all | primaries | new_primaries | none\r\n\r\nwith the default set to `all`.\r\n\r\nThe existing disable allocation settings will be deprecated in favour for the enable allocation setting.'
4486,'martijnvg','Make doc lookups in queries/filters consistent\nThe `terms` lookup filter uses index, type, id and path while the geoshape filter and query use index, type, id and shape_field_name.\r\n\r\nThe geoshape filter/query should be changed to use `path` instead.'
4484,'dakrone','Don\'t accept type wrapper in index request\nCurrently it is possible to index a document as:\r\n\r\n    POST /myindex/mytype/1\r\n    { "foo"...}\r\n\r\nor as:\r\n\r\n    POST /myindex/mytype/1\r\n    {\r\n        "mytype": {\r\n            "foo"...\r\n        }\r\n    }\r\n\r\nThis makes indexing non-deterministic and fields can be misinterpreted as type names.\r\n\r\nWe should accept only the first form, ie without the type wrapper.\r\n\r\n'
4483,'dakrone','Don\'t repeat type name in update mapping\nCurrently the `update_mapping` API requires the type name to be repeated in the body, eg:\r\n\r\n    POST /my_index/my_type/_mapping\r\n    {\r\n        "my_type": {\r\n            "properties": {...}\r\n        }\r\n    }\r\n\r\nThis repetition is confusing and unnecessary.  Instead should be OK to do:\r\n\r\n     POST /my_index/my_type/_mapping\r\n    {\r\n        "properties": {...}\r\n    }\r\n\r\nBonus points for making the change in a backwards compatible way :)\r\n'
4481,'javanna','Make index required in DELETE index API\nCurrently this request:\r\n\r\n    DELETE /\r\n\r\n... will delete all indices.  We should change this to require an index name or wildcard, to prevent data loss from typos.\r\n\r\nThe delete-all syntax would be either of:\r\n\r\n    DELETE /_all\r\n    DELETE /*\r\n'
4480,'dakrone','Make exists, found, not_found consistent\nThe GET and multi_get APIs return `{"exists": true|false}` while the DELETE and bulk delete APIs return `{"found": true|false}`, and the DeleteResponse class uses `notFound`.\r\n\r\nAll of these should be changed to use `found`, in order to make things more consistent.\r\n\r\n'
4474,'s1monw','XSS vulnerability detected\nHi all. My IT department has detected a cross site scripting vulnerability related to ES - specifically that when the string ```"/<script>cross_site_scripting.nasl</script>.asp"``` gets sent to ES, it doesn\'t get properly escaped. I was surprised that I hadn\'t seen it come up before this when I searched. Is this something reasonably fixable?\r\nThe report is attached, please let me know what other info I can provide. This is present in 0.90.3-0.90.7, at least. I have not tried 1.x.\r\n```\r\nFirst Discovered: Dec 12, 2013 20:18\r\nLast Observed:  Dec 16, 2013 8:52\r\n\r\nDNS Name:  \r\nNetBIOS Name:  \r\n\r\nSynopsis: The remote web server is prone to cross-site scripting attacks.\r\n\r\nDescription\r\nThe remote host is running a web server that fails to adequately sanitize request strings of malicious JavaScript. By leveraging this issue, an attacker may be able to cause arbitrary HTML and script code to be executed in a user\'s browser within the security context of the affected site.\r\n\r\nSolution\r\nContact the vendor for a patch or upgrade.\r\n\r\nSee Also\r\nhttp://en.wikipedia.org/wiki/Cross-site_scripting\r\n\r\nRisk Factor: Medium\r\n\r\nCVSS Base Score\r\n4.3 (CVSS2#AV:N/AC:M/Au:N/C:N/I:P/A:N)\r\n\r\nCVSS Temporal Score\r\n3.6 (CVSS2#E:F/RL:OF/RC:C)\r\n\r\nPlugin Output\r\nThe request string used to detect this flaw was : \r\n\r\n/<script>cross_site_scripting.nasl</script>.asp \r\n\r\nThe output was : \r\n\r\nHTTP/1.1 400 Bad Request \r\nAccess-Control-Allow-Origin: * \r\nContent-Type: text/plain; charset=UTF-8 \r\nContent-Length: 91 \r\n\r\n\r\nNo handler found for uri [/<script>cross_site_scripting.nasl</script>.as \r\np] and method [GET]\r\n\r\nCVE\r\nCVE-2002-1700\r\nCVE-2003-1543\r\nCVE-2005-2453\r\nCVE-2006-1681\r\nCVE-2012-3382\r\n\r\nBID\r\n5011\r\n5305\r\n7344\r\n7353\r\n8037\r\n14473\r\n17408\r\n54344\r\n\r\nCross-References\r\nOSVDB:4989\r\nOSVDB:18525\r\nOSVDB:24469\r\nOSVDB:42314\r\nOSVDB:58976\r\nOSVDB:83683\r\nCWE:79\r\nCWE:80\r\nCWE:81\r\nCWE:83\r\nCWE:20\r\nCWE:74\r\nCWE:442\r\nCWE:712\r\nCWE:722\r\nCWE:725\r\nCWE:811\r\nCWE:751\r\nCWE:801\r\nCWE:116\r\n\r\nVulnerability Publication Date: 2004/04/09\r\n\r\nPlugin Publication Date: 2001/11/30\r\n\r\nPlugin Last Modification Date: 2013/09/04\r\n\r\nPublic Exploit Available: True\r\n```'
4472,'uboness','Support ordering by sub-aggregations for the terms-aggregator\nThe `terms`-aggregator does not currently seem to fully support its documented orderings: it has troubles with ordering by sub-aggregators.\r\n\r\nHere\'s an example that demonstrates the problem:\r\n\r\n    export ELASTICSEARCH_ENDPOINT=http://localhost:9200\r\n    ELASTICSEARCH_ENDPOINT=http://localhost:9200\r\n    curl -XPUT http://localhost:9200/foo -d \'{\r\n        "settings": {"index": {"number_of_shards": 5}},\r\n        "mappings": {\r\n            "type": {\r\n                "properties": {\r\n                    "host": {\r\n                        "type": "string",\r\n                        "index": "not_analyzed"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\'\r\n    {"ok":true,"acknowledged":true}+ curl -XPOST \'http://localhost:9200/_bulk?refresh=true\' -d \'\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app1","cpu":95.1,"timestamp":"2013-12-24T12:00:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app1","cpu":75.1,"timestamp":"2013-12-24T12:01:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app2","cpu":25.1,"timestamp":"2013-12-24T12:00:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app2","cpu":55.1,"timestamp":"2013-12-24T12:01:00Z"}\r\n    \'\r\n    {"took":364,"errors":false,"items":[{"create":{"_index":"foo","_type":"bar","_id":"dic5iqJRX2KimXqVq0tJg","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"Gs1YrXtiSy-GE16bCSblng","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"tfdbxfteSCO_ByUFiNYL6g","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"kWRoKKqlSdOGWaZzEstziw","_version":1,"ok":true,"status":201}}]}\r\n    curl -XPOST \'http://localhost:9200/foo/_search?pretty\' -d \'\r\n    {\r\n        "size": 0,\r\n        "aggregations": {\r\n            "host": {\r\n                "terms": {\r\n                    "field": "host",\r\n                    "order": {\r\n                        "cpu_avg": "asc"\r\n                    }\r\n                },\r\n                "aggs": {\r\n                    "cpu_avg": {\r\n                        "avg": {\r\n                            "field": "cpu"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \'\r\n    {\r\n      "took" : 164,\r\n      "timed_out" : false,\r\n      "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 4,\r\n        "failed" : 1,\r\n        "failures" : [ {\r\n          "index" : "foo",\r\n          "shard" : 0,\r\n          "status" : 500,\r\n          "reason" : "RemoteTransportException[[Piper][inet[/10.0.1.14:9301]][search/phase/query]]; nested: NullPointerException; "\r\n        } ]\r\n      },\r\n      "hits" : {\r\n        "total" : 1,\r\n        "max_score" : 1.0,\r\n        "hits" : [ ]\r\n      },\r\n      "aggregations" : {\r\n        "host" : {\r\n          "buckets" : [ {\r\n            "key" : "app2",\r\n            "doc_count" : 1,\r\n            "cpu_avg" : {\r\n              "value" : 25.1\r\n            }\r\n          } ]\r\n        }\r\n      }\r\n    }\r\n    + curl -XPOST \'http://localhost:9200/foo/_search?pretty\' -d \'\r\n    {\r\n        "size": 0,\r\n        "aggregations": {\r\n            "host": {\r\n                "terms": {\r\n                    "field": "host",\r\n                    "order": {\r\n                        "cpu.avg": "desc"\r\n                    }\r\n                },\r\n                "aggs": {\r\n                    "cpu": {\r\n                        "stats": {\r\n                            "field": "cpu"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \'\r\n    {\r\n      "took" : 12,\r\n      "timed_out" : false,\r\n      "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 4,\r\n        "failed" : 1,\r\n        "failures" : [ {\r\n          "index" : "foo",\r\n          "shard" : 0,\r\n          "status" : 500,\r\n          "reason" : "NullPointerException[null]"\r\n        } ]\r\n      },\r\n      "hits" : {\r\n        "total" : 1,\r\n        "max_score" : 1.0,\r\n        "hits" : [ ]\r\n      },\r\n      "aggregations" : {\r\n        "host" : {\r\n          "buckets" : [ {\r\n            "key" : "app2",\r\n            "doc_count" : 1,\r\n            "cpu" : {\r\n              "count" : 1,\r\n              "min" : 25.1,\r\n              "max" : 25.1,\r\n              "avg" : 25.1,\r\n              "sum" : 25.1\r\n            }\r\n          } ]\r\n        }\r\n      }\r\n    }\r\n\r\nWith the provided changes, I get the expected result:\r\n\r\n    export ELASTICSEARCH_ENDPOINT=http://localhost:9200\r\n    ELASTICSEARCH_ENDPOINT=http://localhost:9200\r\n    curl -XPUT http://localhost:9200/foo -d \'{\r\n        "settings": {"index": {"number_of_shards": 5}},\r\n        "mappings": {\r\n            "type": {\r\n                "properties": {\r\n                    "host": {\r\n                        "type": "string",\r\n                        "index": "not_analyzed"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\'\r\n    {"ok":true,"acknowledged":true}+ curl -XPOST \'http://localhost:9200/_bulk?refresh=true\' -d \'\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app1","cpu":95.1,"timestamp":"2013-12-24T12:00:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app1","cpu":75.1,"timestamp":"2013-12-24T12:01:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app2","cpu":25.1,"timestamp":"2013-12-24T12:00:00Z"}\r\n    {"index":{"_index":"foo","_type":"bar"}}\r\n    {"host":"app2","cpu":55.1,"timestamp":"2013-12-24T12:01:00Z"}\r\n    \'\r\n    {"took":238,"errors":false,"items":[{"create":{"_index":"foo","_type":"bar","_id":"MGBw21a_RReC2_7Aos8g2g","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"40JZSDw0SDevcm4ERm5ReA","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"OZ4h_SQxRpeCWbxdta1dSA","_version":1,"ok":true,"status":201}},{"create":{"_index":"foo","_type":"bar","_id":"Bvrqvfs5T9ak-GEshuIqwQ","_version":1,"ok":true,"status":201}}]}+ curl -XPOST \'http://localhost:9200/foo/_search?pretty\' -d \'\r\n    {\r\n        "size": 0,\r\n        "aggregations": {\r\n            "host": {\r\n                "terms": {\r\n                    "field": "host",\r\n                    "order": {\r\n                        "cpu_avg": "asc"\r\n                    }\r\n                },\r\n                "aggs": {\r\n                    "cpu_avg": {\r\n                        "avg": {\r\n                            "field": "cpu"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \'\r\n    {\r\n      "took" : 2,\r\n      "timed_out" : false,\r\n      "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 5,\r\n        "failed" : 0\r\n      },\r\n      "hits" : {\r\n        "total" : 4,\r\n        "max_score" : 1.0,\r\n        "hits" : [ ]\r\n      },\r\n      "aggregations" : {\r\n        "host" : {\r\n          "buckets" : [ {\r\n            "key" : "app2",\r\n            "doc_count" : 2,\r\n            "cpu_avg" : {\r\n              "value" : 40.1\r\n            }\r\n          }, {\r\n            "key" : "app1",\r\n            "doc_count" : 2,\r\n            "cpu_avg" : {\r\n              "value" : 85.1\r\n            }\r\n          } ]\r\n        }\r\n      }\r\n    }\r\n    curl -XPOST \'http://localhost:9200/foo/_search?pretty\' -d \'\r\n    {\r\n        "size": 0,\r\n        "aggregations": {\r\n            "host": {\r\n                "terms": {\r\n                    "field": "host",\r\n                    "order": {\r\n                        "cpu.avg": "desc"\r\n                    }\r\n                },\r\n                "aggs": {\r\n                    "cpu": {\r\n                        "stats": {\r\n                            "field": "cpu"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n    \'\r\n    {\r\n      "took" : 2,\r\n      "timed_out" : false,\r\n      "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 5,\r\n        "failed" : 0\r\n      },\r\n      "hits" : {\r\n        "total" : 4,\r\n        "max_score" : 1.0,\r\n        "hits" : [ ]\r\n      },\r\n      "aggregations" : {\r\n        "host" : {\r\n          "buckets" : [ {\r\n            "key" : "app1",\r\n            "doc_count" : 2,\r\n            "cpu" : {\r\n              "count" : 2,\r\n              "min" : 75.1,\r\n              "max" : 95.1,\r\n              "avg" : 85.1,\r\n              "sum" : 170.2\r\n            }\r\n          }, {\r\n            "key" : "app2",\r\n            "doc_count" : 2,\r\n            "cpu" : {\r\n              "count" : 2,\r\n              "min" : 25.1,\r\n              "max" : 55.1,\r\n              "avg" : 40.1,\r\n              "sum" : 80.2\r\n            }\r\n          } ]\r\n        }\r\n      }\r\n    }\r\n\r\nI\'m not nearly familiar with the code to claim that these changes are sufficient, but it\'s a start at least. :)'
4469,'javanna','Add tests for REST layer\nThe idea is to test the REST layer by making use of yaml test suites that can be found on the [elasticsearch-rest-api-spec project](https://github.com/elasticsearch/elasticsearch-rest-api-spec), which are already used to test all the official elasticsearch clients.\r\n\r\n'
4464,'jpountz','Stricter parsing for aggregations\nAn `aggregations` object in the JSON document representing the query can only wrap a single aggregation type. The parser logic should be stricter and fail the query if several of them are present instead of only taking one of them into account.'
4461,'martijnvg','Renamed top level `filter` to `post_filter`.\n'
4459,'s1monw',"Shouldn't be necessary to loop over ShardRoutings\nAs several other classes can change the internal state of the RoutingNodes data structure, inefficient looping over nodes and assigned shards was necessary in the AllocationDeciders.\r\n\r\nWith larger clusters, reallocation gets too slow. In our current case, we have 5 years of data with daily indices, 6 shards per index, replication factor 1. Recalculating cluster state can take minutes, with the master sitting at 100% CPU in RoutingNodes.shardsRoutingFor( MutableShardRouting ).\r\n\r\nThe taken approach is\r\na) making RoutingNodes a singleton, since only one active instance should ever exist anyhow,\r\nb) notifying RoutingNodes of changes in MutableShardRouting instances state.\r\n\r\nThis certainly is not the most elegant approach and adds complexity instead of removing it, but is what can be done without a major refactoring of allocation.\r\n\r\nIn the supplied test case execution of the final reallocation is sped up from 22 seconds on my test machine to 4.2 seconds.\r\n\r\nThere is already a PR for this: https://github.com/elasticsearch/elasticsearch/pull/4257"
4458,'s1monw',"Improve RoutingNodes API\nCurrently the `RoutingNodes` API allows modification of it's internal state outside of the class. We already  have a pullrequest (https://github.com/elasticsearch/elasticsearch/pull/4257) that adds 10x performance improvements for shard allocation but it handles most of the state changes manually outside of `RoutingNodes`. We should make sure that the `RoutingNodes` is consistent  at all time and encapsulate it's internal datastructures."
4457,'s1monw','Cancel Allocation fails to reset the state of the source shard when moving\nWhen we move a shard from Node `A` to Node `B` and cancel the relocation the source node still remains in state `RELOCATING`.'
4455,'spinscale','Allow IndicesAdminClient.getAliases() to return all aliases\nIn order to not submit the whole cluster state in the Cat API it makes sense to allow the `GetAliasesRequest` to allow to return all aliases and remove the validation.'
4453,'martijnvg',"Replace `ignore_indices` with `ignore_unavailable`, `expand_wildcards` and `allow_no_indices`\n* `ignore_unavailable` - Controls whether to ignore if any specified indices are unavailable, this includes indices\r\n   that don't exist or closed indices. Either `true` or `false` can be specified.\r\n* `allow_no_indices` - Controls whether to fail if a wildcard indices expressions results into no concrete indices.\r\n   Either `true` or `false` can be specified. For example if the wildcard expression `foo*` is specified and no indices\r\n   are available that start with `foo` then depending on this setting the request will fail. This setting is also applicable\r\n   when `_all`, `*` or no index has been specified.\r\n* `expand_wildcards` - Controls to what kind of concrete indices wildcard indices expression expand to. If `open` is\r\n  specified then the wildcard expression if expanded to only open indices and if `closed` is specified then the wildcard expression if expanded only to closed indices. Also both values (`open,closed`) can be specified to expand to all indices.\r\n\r\nThere're a number of apis that support multiple indices, but didn't support the `ignore_indices` option. I haven't cut these apis over to the new indices options. I'll do that after this PR gets in. The following apis don't support for the new indices options yet: mlt, delete index, delete mapping, delete warmer, index exists, get indices settings, put index settings, put mappings and put warmer apis\r\n\r\nRelates to #4436"
4451,'imotov','Inconsistent treatment of dates without year\nWhen dates are specified without a year, for example: `Dec 15 10:00:00` they are treated as dates in 2000 during indexing and range searches except for the upper included bound `lte`, where they are treated as dates in 1970. Repro: https://gist.github.com/imotov/7978186. Might be related to #2731.'
4445,'martijnvg','has_child hang, could anyone HELP?\nThe following query will hang in our production server, but if I remove\r\nall filters (the term filter and bool filter) in has_child, the query will return \r\nin a few ms, does anyone has any suggestion?\r\n\r\nAny help is greatly appreciated!\r\n\r\n\r\ncurl \'localhost:9200/recruitmentportal/47461a39-1863-4c37-9f98-cfd7e291e6d4/_search?routing=100100&pretty\' -d \'\r\n{\r\n  "from": 0, \r\n  "size": 100, \r\n  "query": {\r\n    "constant_score": {\r\n      "filter": {\r\n        "and": {\r\n          "filters": [\r\n            {\r\n              "term": {\r\n                "_tenantid": "100100"\r\n              }\r\n            }, \r\n            {\r\n              "bool": {\r\n                "must": [\r\n                  {\r\n                    "term": {\r\n                      "RecruitmentPortal.PersonProfile.R_IsDeleted": "0"\r\n                    }\r\n                  }, \r\n                  {\r\n                    "term": {\r\n                      "RecruitmentPortal.PersonProfile.R_CommonState": "0"\r\n                    }\r\n                  }, \r\n                  {\r\n                    "term": {\r\n                      "RecruitmentPortal.PersonProfile.R_IsTms": "1"\r\n                    }\r\n                  }, \r\n                  {\r\n                    "has_child": {\r\n                      "query": {\r\n                        "constant_score": {\r\n                          "filter": {\r\n                            "and": {\r\n                              "filters": [\r\n                                {\r\n                                  "term": {\r\n                                    "_tenantid": "100100"\r\n                                  }\r\n                                }, \r\n                                {\r\n                                  "bool": {\r\n                                    "must": [\r\n                                      {\r\n                                        "query": {\r\n                                          "text": {\r\n                                            "content": {\r\n                                              "query": "abc", \r\n                                              "type": "phrase"\r\n                                            }\r\n                                          }\r\n                                        }\r\n                                      }\r\n                                    ]\r\n                                  }\r\n                                }\r\n                              ]\r\n                            }\r\n                          }, \r\n                          "boost": 1\r\n                        }\r\n                      }, \r\n                      "type": "ResumeFullTextForMetadata"\r\n                    }\r\n                  }\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }, \r\n      "boost": 1\r\n    }\r\n  }\r\n}\'\r\n'
4442,'spinscale','indices.fielddata.cache.expire documentation correction\nAccording to the documentation http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index-modules-fielddata.html\r\n\r\nThe example for absolute value for indices.fielddata.cache.expire is 12GB. However, using GB throws an error:\r\n\r\n{0.90.7}: Initialization Failed ...\r\n1) ElasticSearchParseException[Failed to parse [7GB]]\r\n        NumberFormatException[For input string: "7GB"]\r\n\r\nChanging it to 7G worked properly.'
4440,'bleskes','Expose number of queries in percolator stats\nThe percolator statistics response contains the current number of registered percolation queries. This number is not exposed via the rest endpoint.\r\n'
4439,'javanna','Fixing typo and grammar\n'
4438,'spinscale','there are lots of port ESTABLISHED when i open elasticsearch \n  TCP    127.0.0.1:50075        127.0.0.1:50076        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50076        127.0.0.1:50075        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50077        127.0.0.1:50078        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50078        127.0.0.1:50077        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50079        127.0.0.1:50080        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50080        127.0.0.1:50079        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50081        127.0.0.1:50082        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50082        127.0.0.1:50081        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50083        127.0.0.1:50084        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50084        127.0.0.1:50083        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50085        127.0.0.1:50086        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50086        127.0.0.1:50085        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50087        127.0.0.1:50088        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50088        127.0.0.1:50087        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50089        127.0.0.1:50090        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50090        127.0.0.1:50089        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50091        127.0.0.1:50092        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50092        127.0.0.1:50091        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50093        127.0.0.1:50094        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50094        127.0.0.1:50093        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50095        127.0.0.1:50096        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50096        127.0.0.1:50095        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50097        127.0.0.1:50098        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50098        127.0.0.1:50097        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50122        127.0.0.1:50123        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50123        127.0.0.1:50122        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50124        127.0.0.1:50125        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50125        127.0.0.1:50124        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50126        127.0.0.1:50127        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50127        127.0.0.1:50126        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50128        127.0.0.1:50129        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50129        127.0.0.1:50128        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50130        127.0.0.1:50131        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50131        127.0.0.1:50130        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50132        127.0.0.1:50133        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50133        127.0.0.1:50132        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50134        127.0.0.1:50135        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50135        127.0.0.1:50134        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50136        127.0.0.1:50137        ESTABLISHED     1316\r\n  TCP    127.0.0.1:50137        127.0.0.1:50136        ESTABLISHED     1316\r\n\r\n'
4436,'martijnvg',"Replace `ignore_indices` with 3 other options\nToday the `ignore_indices` option controls what to do when an specified index doesn't exists. \r\n\r\nThe new following options will replace `ignore_indices` to make ignoring indices more flexible:\r\n* `ignore_unavailable` - Controls whether to ignore specified indices that are unavailable. Unavailable refers to indices that don't exist or are closed. \r\n* `expand_wildcards` - Controls what kind of indices should be expanded with wildcard expressions. If `open` is specified a wildcard expression expands only into open indices and if `closed` is specified a wildcard expression only expands into closed indices. The latter only makes sense in the close index api, which it will default to.\r\n* `allow_no_indices` - Controls whether it is allowed that a wildcard expression expands  into zero concrete indices. This option also applies when `_all` or no index has been specified."
4431,'jpountz','Add the ability to disallow field data loading\nField data can be a black hole for memory (eg. on full-text fields), so it should be possible to disallow field data loading on a per-field basis.'
4430,'jpountz','Take into account changes in field data settings on live indices\nAbstractFieldMapper.merge has the logic to handle changes of the field data configuration (in particular the format). However, these changes are not going to be taken into account until the field data cache is cleared. It should be possible to update the field data settings on a live index so that field data which has been loaded for the previous segments remains alive while new segments will be loaded using the new field data configuration. As time goes and merges happen, all segments will eventually be using the new format but the transition would be smoother than by clearing the field data cache and regenerating all entries on the next request.'
4426,'jpountz',"Remove IndexFieldData.getHighestNumberOfSeenUniqueValues()\nAlthough this method may look convenient in order to size data-structures when a request comes in, this value can only grow due to the way it is implemented: when a new field data instance is loaded, what basically happens is something like `highestNumberOfSeenUniqueValues = max(highestNumberOfSeenUniqueValues, atomicFieldData.getNumberUniqueValues());`.\r\n\r\nSo for example if you index lots of data into Elasticsearch and then remove most of it, this value will be highly over-estimated.\r\n\r\nI think a better way to solve this issue would be to compute this information on a per-request basis by iterating over the atomic readers wrapped by the context index searcher. Although this might sound expensive, this is very likely to be very cheap compared to the query execution: there are usually less than 50 segments in an index while queries can easily match thousands or millions of documents. And it also has some advantages compared to the current approach:\r\n - this number will be accurate,\r\n - this number will only take into account segments that are wrapped by the searcher while something implemented at the index field data level could only return a number which is global to all live segments in the index.\r\n\r\nThis method isn't used currently in master so it will be easy to remove. Regarding 0.90, I plan to deprecate it in order not to break plugins that may rely on that method."
4423,'jpountz','Some mappers are missing from MapperBuilders\nMapperBuilders misses the following mappers:\r\n - completion\r\n - geo_point\r\n - geo_shape\r\n - parent\r\n - size\r\n - ttl'
4421,'javanna','Move create index api to new acknowledgement mechanism\nMove create index api to new acknowledgement mechanism introduced in #3786 .'
4419,'javanna','Added new IndicesLifecycle.Listener method that allows to listen for any shard state change\nAdded new `IndicesLifecycle.Listener` method that allows to listen for any shard state change\r\n\r\nCloses #4413'
4417,'dakrone',"Elasticsearch goes into infinite loop with long database names\nThe following command:\r\n\r\n`curl -XPUT http://localhost:9200/test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack`\r\n\r\nput my elasticsearch into an infinite loop, writing the following lines over and over to `elasticsearch.log`:\r\n\r\n\t[2013-12-11 18:36:04,630][WARN ][cluster.action.shard     ] [Payback] [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] sending failed shard for [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1], node[8vmR12rFRp2FI3EA-icfrw], [P], s[INITIALIZING], indexUUID [zQIlZKm3R1C1blwKNxFWJg], reason [Failed to create shard, message [IndexShardCreationException[[test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] failed to create shard]; nested: IOException[File name too long]; ]]\r\n\r\n\t[2013-12-11 18:36:04,630][WARN ][cluster.action.shard     ] [Payback] [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] received shard failed for [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1], node[8vmR12rFRp2FI3EA-icfrw], [P], s[INITIALIZING], indexUUID [zQIlZKm3R1C1blwKNxFWJg], reason [Failed to create shard, message [IndexShardCreationException[[test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][1] failed to create shard]; nested: IOException[File name too long]; ]]\r\n\r\n\t[2013-12-11 18:36:04,638][WARN ][indices.cluster          ] [Payback][test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][2] failed to create shard org.elasticsearch.index.shard.IndexShardCreationException: [test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_test_haystack][2] failed to create shard\r\n\r\n        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:347)\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyInitializingShard(IndicesClusterStateService.java:651)\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewOrUpdatedShards(IndicesClusterStateService.java:569)\r\n        at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:181)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:414)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:679)\r\n    Caused by: java.io.IOException: File name too long\r\n        at java.io.UnixFileSystem.canonicalize0(Native Method)\r\n        at java.io.UnixFileSystem.canonicalize(UnixFileSystem.java:172)\r\n        at java.io.File.getCanonicalPath(File.java:576)\r\n        at org.apache.lucene.store.FSDirectory.getCanonicalPath(FSDirectory.java:129)\r\n        at org.apache.lucene.store.FSDirectory.<init>(FSDirectory.java:143)\r\n        at org.apache.lucene.store.NIOFSDirectory.<init>(NIOFSDirectory.java:64)\r\n        at org.elasticsearch.index.store.fs.NioFsDirectoryService.newFSDirectory(NioFsDirectoryService.java:45)\r\n        at org.elasticsearch.index.store.fs.FsDirectoryService.build(FsDirectoryService.java:129)\r\n        at org.elasticsearch.index.store.distributor.AbstractDistributor.<init>(AbstractDistributor.java:35)\r\n        at org.elasticsearch.index.store.distributor.LeastUsedDistributor.<init>(LeastUsedDistributor.java:36)\r\n        at sun.reflect.GeneratedConstructorAccessor16.newInstance(Unknown Source)\r\n        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(Constructor.java:532)\r\n        at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\r\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\r\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\r\n        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\r\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n        at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\r\n        at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\r\n        at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n        at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n        at org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:837)\r\n        at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n        at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n        at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\r\n        at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\r\n        at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:830)\r\n        at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\r\n        at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\r\n        at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\r\n        at org.elasticsearch.common.inject.InjectorImpl.createChildInjector(InjectorImpl.java:131)\r\n        at org.elasticsearch.common.inject.ModulesBuilder.createChildInjector(ModulesBuilder.java:69)\r\n        at org.elasticsearch.index.service.InternalIndexService.createShard(InternalIndexService.java:345)\r\n        ... 8 more\r\n\r\nNow you might argue that this is a stupid thing to do, and that an ES server should be protected from the public so it's not a Denial of Service attack. Nevertheless, this happened to me by accident while I was developing a test harness for Haystack's ElasticSearch backend. However I think it would be better to respond gracefully to invalid input, instead of trying to fill up the hard disk with infinite useless logs."
4415,'martijnvg',"Distributed percolator incorrect results with auto create type.\nIf the `.percolator` mapping hasn't been created yet and two or more concurrent percolate index requests are processed, the first request would the `.percolator` type mapping, but the real time percolator listener wouldn't be active, this would result in that the subsequent concurrent requests wouldn't be parsed and kept in memory and would never be included in any percolate api result. \r\n\r\nThis issue only occurs when `.percolator` type is created on the fly and *not* when the mapping for `.percolator` has been defined before hand via put mapping or create index api."
4414,'drewr','Add _cat/aliases\nWould be nice to get info about aliases.  Maybe this can go in _cat/indices.'
4413,'javanna','IndicesLifecycle.Listener to support listening for any index shard state change\nAdd new `IndicesLifecycle.Listener` method that allows to listen for any `IndexShardState` internal change.'
4411,'spinscale',"problem picking up templates in the config/templates directory\nI've got it working using the curl query to create a new template.\r\nBut when i set it up in the/templates directory it doesnt seem to get picked up at all.\r\nI've seen a bug raised looking similar to this. What is the work around- can i explicitly set the templates path in the config file? Also should i be able to see anything in the logs telling me this the template files are being picked up or not?\r\n\r\nAlso while looking into this issue i was following the page to see if my templates are created ok : http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-templates.html to get an output of all the loaded up templates, And i've found it doesnt seem to work unless i specify the correct template exact name. So calling curl -XGET localhost:9200/_template/temp* or curl -XGET localhost:9200/_template/ doesnt seem to bring back any output at all.\r\n\r\nThis was using elasticsearch-0.90.3"
4409,'dadoonet','fuzzy query\nCan the fuzzy query be used in the GET request?\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-fuzzy-query.html\r\n\r\n\r\nthe query returns the right results, while fuzzy cannot be parsed.\r\n\r\n\r\n$ curl -XGET \'http://localhost:9200/twitter/tweet/_search?pretty=true\' -d \' \r\n{                              \r\n    "query" : { \r\n        "text" : { "user": "kimchy" }\r\n    }                                                                                     \r\n}\'\r\n{\r\n  "took" : 2,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "2",\r\n      "_score" : 1.0, "_source" : \r\n{ \r\n    "user": "kimchy", \r\n    "postDate": "2009-11-15T14:12:12", \r\n    "message": "Another tweet, will it be indexed?" \r\n}\r\n    }, {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "1",\r\n      "_score" : 0.30685282, "_source" : \r\n{ \r\n    "user": "kimchy", \r\n    "postDate": "2009-11-15T13:12:00", \r\n    "message": "Trying out Elastic Search, so far so good?" \r\n}\r\n    } ]\r\n  }\r\n}\r\n\r\n\r\n\r\n$ curl -XGET \'http://localhost:9200/twitter/tweet/_search?pretty=true\' -d \'{\r\n    "fuzzy" : { "user" : "ki" }\r\n}\'\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[RJw2f6OwT6eA8yXPxkgvtA][twitter][3]: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\n    \\"fuzzy\\" : { \\"user\\" : \\"ki\\" }\\n}]]]; nested: SearchParseException[[twitter][3]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][4]: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\n    \\"fuzzy\\" : { \\"user\\" : \\"ki\\" }\\n}]]]; nested: SearchParseException[[twitter][4]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][0]: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\n    \\"fuzzy\\" : { \\"user\\" : \\"ki\\" }\\n}]]]; nested: SearchParseException[[twitter][0]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][2]: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\n    \\"fuzzy\\" : { \\"user\\" : \\"ki\\" }\\n}]]]; nested: SearchParseException[[twitter][2]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }{[RJw2f6OwT6eA8yXPxkgvtA][twitter][1]: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\n    \\"fuzzy\\" : { \\"user\\" : \\"ki\\" }\\n}]]]; nested: SearchParseException[[twitter][1]: from[-1],size[-1]: Parse Failure [No parser for element [fuzzy]]]; }]",\r\n  "status" : 400\r\n}\r\n'
4407,'spinscale',"Print nice error in bin/elasticsearch if user needs to run Maven\nBefore, people that cloned the repo and expected to be able to run\r\nbin/elasticsearch would be met with an awful shell error: the shell\r\ninterprets Maven variables like ${project.build.finalName} as shell\r\nvariables yet can't handle names of the form ${x.y}. This commit\r\nexplicitly checks to make sure that Maven has done its substitutions\r\nbefore continuing; if Maven hasn't been run, it gives a helpful error\r\nmessage.\r\n\r\nFixes #2954."
4406,'jpountz','"offsets must not go backwards" exception when using index_options=offsets\nto reproduce(on 0.90.8 snapshot):\r\n```\r\ncurl -XPOST http://localhost:9200/foobar -d \'{ "index": { "number_of_shards": "1", "analysis": { "filter": { "wordDelimiter": { "type": "word_delimiter", "split_on_numerics": "false", "generate_word_parts": "true", "generate_number_parts": "true", "catenate_words": "true", "catenate_numbers": "true", "catenate_all": "false" } }, "analyzer": { "content_analyzer": { "tokenizer": "whitespace", "filter": [ "wordDelimiter" ] } } } } }\'\r\n\r\ncurl -XPOST http://localhost:9200/foobar/foobar/_mapping -d \'{ "foobar": { "dynamic": "false", "_all" : { "enabled" : false }, "properties": { "id": { "type": "integer", "index": "not_analyzed", "store": "yes" }, "content": { "type": "string", "analyzer": "content_analyzer", "store": "yes", "term_vector" : "with_positions_offsets", "omit_norms": true, "index_options":"offsets" } } } }\'\r\n\r\ncurl -XPUT http://localhost:9200/foobar/foobar/1 -d \'{"id": 1, "content":"a,a b a/b/c"}\'\r\n```\r\n\r\nyields: \r\n{"error":"IllegalArgumentException[offsets must not go backwards startOffset=0 is < lastStartOffset=2 for field \'content\']","status":500}~\r\n\r\nI guess it\'s a bug on Lucene.'
4404,'colings86','Filtering on results of an aggregation\nThe case here can best be described by referring to the HAVING clause in SQL.  Basically we need to be able do an aggregation but limit the results of the query based on the value of the aggregation.  For example if I wanted to look at what age bands have an average height greater that 5 feet, my query would look something like this:\r\n\r\n```\r\n "aggs": {\r\n        "genders": {\r\n            "terms": {\r\n                "field": "gender"\r\n            },\r\n            "aggs": {\r\n                "age_groups" : {\r\n                    "range" : {\r\n                        "field" : "age",\r\n                        "ranges" : [\r\n                            { "to" : 5 },\r\n                            { "from" : 5, "to" : 10 },\r\n                            { "from" : 10, "to" : 15 },\r\n                            { "from" : 15}\r\n                        ]\r\n                    },\r\n                    "aggs" : {\r\n                        "avg_height" : { \r\n                            "avg" : { "field" : "height" } ,\r\n                            "having" : { "from" : 60 }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n```'
4401,'javanna','Renamed boost name to path\nFirst commit makes the change in a backwards compatible manner, keeping the name around (Part that will be backported).\r\nSecond commit is only for master: removes the boost name and updates the docs accordingly.\r\n\r\nCloses #3315'
4400,'brwe','Fix bug in explain for function_score queries.\nThe explain output for function_score queries with score_mode=max or\r\nscore_mode=min was incorrect, returning instead the value of the last\r\nfunction.  This change fixes this.\r\n\r\nFor example, for the query:\r\n\r\n    {\r\n      "explain": true,\r\n      "query": {\r\n        "function_score": {\r\n          "query": {\r\n            "match_all": {}\r\n          },\r\n          "score_mode": "max",\r\n          "functions": [\r\n            {\r\n              "boost_factor": 1\r\n            },\r\n            {\r\n              "boost_factor": 3\r\n            },\r\n            {\r\n              "boost_factor": 2\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n\r\nResults are returned containing\r\n\r\n    hits: [\r\n      {\r\n       _score: 3,\r\n       ...\r\n       _explanation: {\r\n          value: 2\r\n          ...\r\n      }\r\n    ]\r\n\r\nSimilarly, if "score_mode": "min" is used, the value in the explanation is still 2.\r\n\r\nI would expect the value in _explanation object to be the same as the _score value, and with this patch applied it is.'
4399,'javanna','Made sure that we never throw IndexMissingException in indices query and filter\nIt could happen although we internally use IgnoreIndices.MISSING, due to MetaData#concreteIndices contract, which throws IndexMissingException anyway if all requested indices are missing.\r\nIn case all the indices specified in the query/filter are missing, we just execute the no_match query/filter, no need to throw any error.\r\n\r\nCloses #3428'
4397,'spinscale','Startup scripts: Set vm.max_map_count\nAll the startup scripts in the packages (init and systemd scripts!) should set the above config parameter in order to support the newly default `mmapfs` setting as good as possible.'
4396,'javanna','Unified default ack timeout to 30 seconds\nIncreased also default publish state timeout to 30 seconds (from 5 seconds) and introduced constant for it.\r\nIntroduced AcknowledgedRequest.DEFAULT_ACK_TIMEOUT constant (30 seconds).\r\nRemoved misleading timeout default values coming from the REST layer.\r\nRemoved (in a bw compatible manner) the timeout support in put/delete index template as the timeout parameter was ignored.\r\n\r\nCloses #4395'
4395,'javanna','Unify default ack timeout to 30 seconds\nThe default ack timeout currently is 10 seconds in all apis but delete index (60 seconds). The timeout value is spreaded in different places, and sometimes has different defaults coming from the REST layer.\r\n\r\nThe goal of this issue is to have a unique constant containing the default ack timeout, and change the default to 30 seconds. Also, change the publish state timeout to 30s - which is a better default than 5s to allow for better throttling for slow nodes.\r\n\r\n'
4393,'spinscale','Allow to provide parameters not only through -D\nIt would be much nicer if, on top of `-D`, we could pass parameters to elasticsearch shell using something like `--`, for example, `elasticsearch --path.data=/some/path`'
4392,'spinscale','elasticsearch command to run in foreground by default, -d to daemonize\nFor 1.0, it might be a good chance to change how we run the elasticsearch command by default. Feels like the default should be in the foreground, and we should daemonize only with a `-d`.'
4390,'s1monw','Missing else?\n[I think there should be an else here](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/allocation/allocator/BalancedShardsAllocator.java#L205)\r\n\r\nI think there should be an else here. Otherwise the code inside the if is irrelevant, because the put outside the if will overwrite the results. '
4389,'s1monw','Master build fails\nSuite: org.elasticsearch.update.UpdateTests\r\nCompleted in 14.16s, 11 tests\r\n\r\n\r\nTests with failures (first 3 out of 5):\r\n  - org.elasticsearch.search.aggregations.bucket.DoubleTermsTests.singleValuedField_WithSubAggregation\r\n  - org.elasticsearch.search.aggregations.bucket.DoubleTermsTests.script_MultiValued_WithAggregatorInherited_WithExplicitType\r\n  - org.elasticsearch.indices.mapping.SimpleGetFieldMappingsTests.simpleGetFieldMappingsWithDefaults\r\n'
4388,'markharwood','A GeoHashGrid aggregation. Buckets GeoPoints into cells ...\n...whose dimensions are determined by a choice of GeoHash resolution.\r\n\r\nAdded a long-based representation of GeoHashes to GeoHashUtils for faster evaluation in aggregations.\r\nCreated new GeoHashCellsAggregator based on LongTermsAggregator but which formats bucket results with GeoHash strings rather than longs.\r\n\r\n'
4387,'jpountz','Compress geo-point field data.\nThis commit allows to trade precision for memory when storing geo points.\r\nGeoPointFieldMapper now accepts a `precision` parameter that controls the\r\nmaximum expected error for storing coordinates. This option can be updated on\r\na live index with the PUT mapping API.\r\n\r\nDefault precision is 1cm, which requires 8 bytes per geo-point (50% memory\r\nsaving compared to using 2 doubles). With this default precision,\r\nGeoDistanceSearchBenchmark reports times which are 12 to 23% slower than\r\nthe previous field data implementation.\r\n\r\nClose #4386'
4386,'jpountz','Compress geo-point field data\nToday we use doubles in order to encode latitudes and longitudes when loading field data for geo points into memory. This is 16 bytes per geo point.\r\n\r\nHowever, we could take advantage of the fact that values are in a fixed range, and maybe trade some precision for memory. In particular, I\'ve been thinking about using a fixed-length encoding with configurable precision. This precision could be configurable in mappings:\r\n\r\n```javascript\r\nPUT /test\r\n{\r\n    "mappings": {\r\n        "test": {\r\n            "properties": {\r\n                "pin": {\r\n                    "type": "geo_point",\r\n                    "fielddata": {\r\n                      "format": "compressed",\r\n                      "precision": "1cm"\r\n                   }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nHere are some values of the number of bytes needed per geo point depending on the expected precision:\r\n\r\nPrecision | Bytes per point | Size reduction\r\n-------------|--------------------|-------------------\r\n1km | 4 | 75%\r\n3m | 6 | 62.5%\r\n1cm | 8 | 50%\r\n1mm | 10 | 37.5%\r\n\r\nI plan to use `1cm` has the default, which is good I think since it would be accurate enough for most use-cases and would require 4 bytes per latitude and longitude, which can be efficiently stored in an `int[]` array, for best speed.\r\n\r\nThe same encoding could be used to implement doc values support (#4207).\r\n\r\nFor now, the default format is going to remain exact and based on two double[] arrays, so you need to explicitely opt-in for this format by configuring the field data format in the mappings.'
4385,'martijnvg','Support postings highlighter in percolate api.\n'
4384,'martijnvg','Support postings highlighter in percolate api.\nThe postings hl now uses a searcher that only encapsulate the view of segment the document being highlighted is in, this should be better than using the top level engine searcher.'
4383,'martijnvg',"Adding `force_source` in highlighting to support highlighting in percolator in stored fields.\nAdded the `force_source` option to highlighting that enforces to use of the _source even if there are stored fields. The percolator uses this option to deal with the fact that the MemoryIndex doesn't support stored fields, this is possible b/c the _source of the document being percolated is always present."
4380,'s1monw','cluster reroute returns riddles on failures :)\nWhen issuing a ```_cluster/reroute``` command and it fails, its quite impossible to understand why it failed.\r\n\r\n```\r\nitamar@data001:~$ curl -XPOST \'localhost:9200/_cluster/reroute\' -d \'{ "commands" : [ { "move" : { "index" : "index1", "shard" : 0, "from_node" : "data006", "to_node" : "data019" } }, { "move" : { "index" : "index2", "shard" : 0, "from_node" : "data019", "to_node" : "data006" } } ] }\'\r\n\r\n{"error":"RemoteTransportException[[data014][inet[/10.10.5.14:9300]][cluster/reroute]]; nested: ElasticSearchIllegalArgumentException[[move_allocation] can\'t move [index1][0], from [data006][Lh1y_3zkQWiPjfS7DoSteg][inet[data006/10.10.5.1:9300]]{tag=gen1}, to [data019][12_HJbCyQFGx96iTPEoGxw][inet[data019/10.1.2.2:9300]]{tag=gen1}, since its not allowed, reason: [NO()][YES()][YES()][YES()][YES()][YES()][YES()]]; ","status":400}\r\n```\r\n\r\nThe ```[NO()][YES()][YES()][YES()][YES()][YES()][YES()]]``` part is quite annoying do decrypt and isn\'t documented anywhere. I know each is a decision on allocation, rebalance etc but in case the command can\'t go through we need to know why. Either make this a more descriptive error or have this documented somewhere. I\'d go with the former since it avoids docs becoming stale.\r\n\r\nAs always, I\'ll be happy to do this myself if this makes sense to you.'
4379,'s1monw',"Remove 'term_index_interval' and 'term_index_divisor'\nThese settings are no longer relevant since they are codec /\r\npostingsformat level settings since Lucene 4.0\r\n\r\nCloses #3912"
4378,'dadoonet','Add version to plugins\nPlugin developpers can now add a version number to their es-plugin.properties file:\r\n\r\n```properties\r\nplugin=org.elasticsearch.test.integration.nodesinfo.TestPlugin\r\nversion=0.0.7-SNAPSHOT\r\n```\r\n\r\nAlso, for site plugins, it\'s recommended to add a `es-plugin.properties` file in root site directory with `description` and `version` properties:\r\n\r\n```properties\r\ndescription=This is a description for a dummy test site plugin.\r\nversion=0.0.7-BOND-SITE\r\n```\r\n\r\nWhen running Nodes Info API, you will get information on versions:\r\n\r\n```sh\r\n$ curl \'http://localhost:9200/_nodes?plugin=true&pretty\'\r\n```\r\n\r\n```javascript\r\n{\r\n  "ok" : true,\r\n  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",\r\n  "nodes" : {\r\n    "RHMsToxiRcCXwHiS6mEaFw" : {\r\n      "name" : "node2",\r\n      "transport_address" : "inet[/192.168.0.15:9301]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9201]",\r\n      "plugins" : [ {\r\n        "name" : "dummy",\r\n        "version" : "0.0.7-BOND-SITE",\r\n        "description" : "This is a description for a dummy test site plugin.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : true,\r\n        "jvm" : false\r\n      } ]\r\n    },\r\n    "IKiUOo-LSCq1Km1GUhBwPg" : {\r\n      "name" : "node3",\r\n      "transport_address" : "inet[/192.168.0.15:9302]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9202]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "version" : "0.0.7-SNAPSHOT",\r\n        "description" : "test-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      } ]\r\n    },\r\n    "H64dcSF2R_GNWh6XRCYZJA" : {\r\n      "name" : "node1",\r\n      "transport_address" : "inet[/192.168.0.15:9300]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9200]",\r\n      "plugins" : [ ]\r\n    },\r\n    "mGEZcYl8Tye0Rm5AACBhPA" : {\r\n      "name" : "node4",\r\n      "transport_address" : "inet[/192.168.0.15:9303]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9203]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "version" : "0.0.7-SNAPSHOT",\r\n        "description" : "test-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      }, {\r\n        "name" : "test-no-version-plugin",\r\n        "version" : "NA",\r\n        "description" : "test-no-version-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      }, {\r\n        "name" : "dummy",\r\n        "version" : "NA",\r\n        "description" : "No description found for dummy.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : true,\r\n        "jvm" : false\r\n      } ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nRelative to #2668.\r\nCloses #2784.'
4376,'javanna',"Added REST test suites runner\nThe REST layer can now be tested through tests that are shared between all the elasticsearch official clients.\r\nThe tests are based on REST specification that can be found on the elasticsearch-rest-api-spec project and consist of YAML files that describe the operations to be executed and the obtained results that need to be tested.\r\n\r\nREST tests can be executed through the ElasticsearchRestTests class, which relies on the rest-spec git submodule that contains the rest spec and tests pulled from the elasticsearch-rest-spec-api project.\r\n\r\nThe following are the options supported by the REST tests runner:\r\n\r\n- tests.rest[true|false|host:port]: determines whether the REST tests need to be run and if so whether to rely on an external cluster (providing host and port) or fire a test cluster (default)\r\n- tests.rest.suite: comma separated paths of the test suites to be run (by default loaded from /rest-spec/test). it is possible to run only a subset of the tests providing a sub-folder or even a single yaml file (the default /rest-spec/test prefix is optional when files are loaded from classpath) e.g. -Dtests.rest.suite=index,get,create/10_with_id\r\n- tests.rest.spec: REST spec path (default /rest-spec/api)\r\n- tests.iters: runs multiple iterations\r\n- tests.seed: seed to base the random behaviours on\r\n- tests.appendseed[true|false]: enables adding the seed to each test section's description (default false)\r\n- tests.cluster_seed: seed used to create the test cluster (if enabled)"
4375,'bleskes',"Add an `usage` key to the CPU section of OsStats.toXContent.\nThis is just the sum of existing `sys` and `user`\r\n\r\nAlthough it's a very minor change, doing a PR for sanity check of the name `usage`.\r\n\r\n\r\nCloses #4374"
4374,'imotov',"Add a new `usage` metric to CPU stats\nThe current `os.cpu` section of the `GET _cluster/nodes/stats`, returns `user` and `sys` CPU metrics. For systems that need to show just one number, it's convenient to have one more metric to indicate CPU usage without the distinction."
4373,'kimchy','Optimize dynamic mapping updates on master by processing latest one per index/node\nInstead of processing all the bulk of update mappings we have per index/node, we can only update the last ordered one out of those (cause they are incremented on the node/index level). This will improve the processing time of an index that have large updates of mappings.'
4371,'dadoonet','Query String caching could cause matched_filters not working\nPR for #4361.\r\n\r\nWhen searching with a query containing query_strings inside a bool query, the specified _name is randomly missing from the results due to caching.\r\n'
4370,'jpountz','[Docs] Document which encoding should be used in order to make sense of the offsets returned by the term vectors API\nClose #4363'
4369,'s1monw','Apply slop correctly if phrase query is wrapped in a filtered query.\nIf a phrase query is wrapped in a filtered query due to type filtering\r\nslop was not applied correctly. Also if the default field required a\r\ntype filter the filter was not applied.\r\n\r\nCloses #4356'
4367,'bleskes','Allow field wildcards in the Get Field Mapping API\nThe Get Field Mappings API currently supports retrieving the mapping for specific fields using their explicit full name, index name or relative name. We should add support to resolve fields using wildcards.'
4363,'jpountz',"Terms vector API should document the encoding which is used to compute the offsets\nThe new term vectors API exposes offsets. However, these offsets have been computed for the UTF-16 encoding, so they are going to look buggy if applied to a string which is not UTF-16-encoded. In particular, I'm thinking that if you are using a language such as Python 3 that uses UTF-8 as a default encoding for strings, you shouldn't use these offsets directly to compute sub-strings."
4362,'martijnvg','Percolate API : highlight issue on field with custom analyzer\nThe highlight is empty when field has a custom analyzer. (Using version 1.0.0.Beta2)\r\n\r\nHere are the steps to reproduce the behaviour :\r\nFirst, create an index with a custom analyzer\r\n\r\n     curl -XPUT "http://localhost:9200/" -d\'\r\n     {\r\n        "settings": {\r\n            "index.analysis.analyzer.latin.tokenizer": "whitespace",\r\n            "index.analysis.analyzer.latin.type": "custom"\r\n        }\r\n     }\r\n\r\nThen create a mapping using this custom analyzer :\r\n\r\n     curl -XPUT "http://localhost:9200/idx/message/_mapping" -d\'\r\n     {\r\n       "message": {\r\n          "properties": {\r\n             "message": {\r\n                "type": "string",\r\n                "store": true,\r\n                "analyzer": "latin"\r\n             }\r\n          }\r\n       }\r\n    }\'\r\n\r\nIndex a percolator query :\r\n\r\n     curl -XPUT "http://localhost:9200/idx/.percolator/q1" -d\'\r\n     {\r\n        "query" : {\r\n            "term" : {\r\n                "message" : "bonsai"\r\n            }\r\n        }\r\n     }\'\r\n\r\nFinaly, use _percolate API with highlight on the field that uses a custom analyzer :\r\n\r\n     curl -XPOST "http://localhost:9200/idx/message/_percolate" -d\'\r\n     {\r\n       "highlight": {\r\n          "fields": {\r\n             "message": {}\r\n          }\r\n       },\r\n       "size": 10,\r\n       "doc": {\r\n          "message": "A new bonsai tree in the office"\r\n       }\r\n     }\'\r\n\r\nIt gives the following which do not have any highlight :\r\n\r\n    {\r\n       "took": 19,\r\n       "_shards": {\r\n          "total": 5,\r\n          "successful": 5,\r\n          "failed": 0\r\n       },\r\n       "total": 1,\r\n       "matches": [\r\n          {\r\n             "_index": "idx",\r\n             "_id": "q1",\r\n             "highlight": {}\r\n          }\r\n       ]\r\n    }\r\n\r\n\r\nNote that when using _search API, highlight is done properly :\r\n\r\nIndex a doc :\r\n\r\n     curl -XPUT "http://localhost:9200/idx/message/1" -d\'\r\n     {\r\n         "message" : "A new bonsai tree in the office"\r\n     }\'\r\n\r\nSearch with highlight on the field that uses a custom analyzer :\r\n\r\n     curl -XPOST "http://localhost:9200/idx/message/_search" -d\'\r\n     {\r\n        "query":{\r\n            "term": {"message": "bonsai"}\r\n        },\r\n        "highlight": {\r\n            "fields": {\r\n                "message":{}\r\n            }\r\n        }\r\n     }\'\r\n\r\nGives the following result :\r\n\r\n    {\r\n       "took": 3,\r\n       "timed_out": false,\r\n       "_shards": {\r\n          "total": 5,\r\n          "successful": 5,\r\n          "failed": 0\r\n       },\r\n       "hits": {\r\n          "total": 1,\r\n          "max_score": 0.375,\r\n          "hits": [\r\n             {\r\n                "_index": "idx",\r\n                "_type": "message",\r\n                "_id": "1",\r\n                "_score": 0.375,\r\n                "_source": {\r\n                   "message": "A new bonsai tree in the office"\r\n                },\r\n                "highlight": {\r\n                   "message": [\r\n                      "A new <em>bonsai</em> tree in the office"\r\n                   ]\r\n                }\r\n             }\r\n          ]\r\n       }\r\n    }'
4361,'dadoonet','matched_filters randomly missing\nWhen searching with a query containing query_strings inside a bool query, the specified _name is randomly missing from the results.\r\n\r\nhttps://gist.github.com/laurikari/7824057\r\n\r\nSometimes "matched_filters" is present in one or both of the results, sometimes not.  The behaviour appears to be random.\r\n\r\nThis reproduces both on OS X Mavericks and Ubuntu Linux 12.04.03 (kernel 3.2.0) using elasticsearch-0.90.7.'
4359,'s1monw','Add transport.publish_port setting\nAdd transport.publish_port setting to allow users to specify the port\r\nother cluster members should use when connecting to an instance. This\r\nis needed for systems such as OpenShift, where cluster communication\r\nneeds to use a publicly accessibly proxy port, because the normal port\r\n(9300) is bound to a private loopback IP address.'
4356,'s1monw','Inconsistent search results when running proximity searches\nWhen running a simple query_string search, specifying the search fields (prefixed with the type), the search returns results as expected.\r\nWhen running a proximity search query_string, using the same search fields, the search fails. If I remove the type prefix from the search field name, the search works.\r\n\r\nEXAMPLE:\r\n========\r\n\r\nSetup:\r\n---------\r\ncurl -XPUT \'http://localhost:9201/test/product/1\' -d \'{ \r\n    "desc": "description of product one with something" \r\n}\'\r\ncurl -XPUT \'http://localhost:9200/test/product/2\' -d \'{ \r\n    "desc": "description of product two with something else" \r\n}\'\r\ncurl -XPUT \'http://localhost:9200/test/product/3\' -d \'{ \r\n    "desc": "description of product three with something else again" \r\n}\'\r\ncurl -XPUT \'http://localhost:9200/test/customer/1\' -d \'{ \r\n    "desc": "description of customer one with something" \r\n}\'\r\ncurl -XPUT \'http://localhost:9200/test/customer/2\' -d \'{ \r\n    "desc": "description of customer two with something else" \r\n}\'\r\ncurl -XPUT \'http://localhost:9200/test/customer/3\' -d \'{ \r\n    "desc": "description of customer three with something else again" \r\n}\'\r\n\r\nSimple Search:\r\n----------------------\r\ncurl -XPOST \'http://localhost:9200/test/_search?pretty=true\' -d \'{\r\n  "query" : {\r\n      "query_string" : {\r\n          "query" : "description",\r\n          "fields" : ["customer.desc", "product.desc"]\r\n      }\r\n  }\r\n}\'\r\n\r\nFailing proximity search:\r\n-----------------------------------\r\ncurl -XPOST \'http://localhost:9200/test/_search?pretty=true\' -d \'{\r\n  "query" : {\r\n      "query_string" : {\r\n          "query" : "\\"customer else\\"~5",\r\n          "fields" : ["customer.desc", "product.desc"]\r\n      }\r\n  }\r\n}\'\r\n\r\nSuccessful proximity search:\r\n-----------------------------------------\r\ncurl -XPOST \'http://localhost:9200/test/_search?pretty=true\' -d \'{\r\n  "query" : {\r\n      "query_string" : {\r\n          "query" : "\\"customer else\\"~5",\r\n          "fields" : ["desc"]\r\n      }\r\n  }\r\n}\'\r\n\r\nWe want to be able to allow the user to search specific fields, hence prefixing the \'desc\' field with either \'customer\' or \'product\' (our real ES instance has many \'title\' and \'desc\' fields, so we need to prefix these fields with the type). Is this an incorrect use of fields, or a defect with proximity search?\r\n\r\nJava: 1.7.0_45\r\nES: 90.5\r\nOS: Windows 7'
4355,'javanna','Internal: Make Node and Client interfaces Closeable\norg.elasticsearch.node.Node and org.elasticsearch.client.Client interfaces should extend Closeable. Doing so will allow user to use Client and Node instances with try-with-resources, IOUtils.closeQuietly, Guava Closer and similar utilities.\r\n\r\nThis change is backward compatible and doesn\'t require any changes except adding "extends Closeable" in two places. \r\n'
4352,'jpountz',"Stop FVH from throwing away some query boosts\nThe FVH was throwing away some boosts on queries stopping a number of\r\nways to boost phrase matches to the top of the list of fragments from\r\nworking.\r\n\r\nThe plain highlighter also doesn't work for this but that is because it\r\ndoesn't support the concept of terms at different positions having different\r\nweights.\r\n\r\nCloses #4351"
4351,'jpountz',"A bunch of ways to highlight boost phrase matches over general term matches in score order highlighted fragments don't work\nA bunch of ways to highlight boost phrase matches over general term matches in score order highlighted fragments don't work.  I don't have time to make a curl recreation at the moment but I'll send a pull request with failing tests that I _think_ should all pass.  Note: they all pass for the postings highlighter, but the other two don't have it together."
4350,'jpountz','Ordinal-based string aggregations\nString terms aggregations today work by accumulating counts into a hash table that stores the term values as keys (just like facets when provided `map` as an `execution_hint`).\r\n\r\nSimilarly to facets, we should also have an execution mode that allows to build the buckets based on string ordinals. This proved to be much faster than the `map` execution mode for facets, so hopefully it should help speed-up string terms aggregations as well.'
4348,'martijnvg','Percolate API : Highlight on sub-field issue\nHighlight is not done on sub-field of an object field when using Percolate API (**1.0.0.Beta2**) :\r\n\r\ncreate index :\r\n\r\n     curl -XPUT "http://localhost:9200/test-index"\r\n\r\ncreate a mapping with a field "content" containing two sub-fields (which are stored)\r\n\r\n     curl -XPUT "http://localhost:9200/test-index/test-mapping/_mapping" -d\'\r\n     {"test-mapping":\r\n         {\r\n             "properties": {\r\n                 "content" : {\r\n                     "properties": {\r\n                         "message": {"type": "string","store": "yes"},\r\n                         "lang": {"type": "string","store": "yes"}\r\n                     }\r\n                 }\r\n             }\r\n         }\r\n     }\'\r\n\r\nindex query into `.percolator` :\r\n\r\n     curl -XPUT "http://localhost:9200/test-index/.percolator/q1" -d\'\r\n     {\r\n         "query" : {\r\n             "term" : {\r\n                 "content.message" : "bonsai"\r\n             }\r\n         }\r\n     }\'\r\n\r\nuse Percolate API with highlight on `content.message` field :\r\n\r\n     curl -XPOST "http://localhost:9200/test-index/test-mapping/_percolate" -d\'\r\n     {\r\n         "size":10,\r\n         "highlight": {\r\n             "fields": {"content.message":{}}\r\n         },\r\n         "doc" : {\r\n             "content": {\r\n                 "message": "bonsai tree office",\r\n                 "lang": "en"\r\n             }\r\n         }\r\n     }\'\r\n\r\nresult doesn\'t show highlight :\r\n\r\n     {\r\n        "took": 3,\r\n        "_shards": {\r\n           "total": 5,\r\n           "successful": 5,\r\n           "failed": 0\r\n        },\r\n        "total": 1,\r\n        "matches": [\r\n           {\r\n              "_index": "test-index",\r\n              "_id": "q1",\r\n              "highlight": {}\r\n           }\r\n        ]\r\n     }'
4346,'martijnvg','Percolate API : is size really mandatory for highlight ?\nDocumentation found at http://www.elasticsearch.org/guide/en/elasticsearch/reference/master/search-percolate.html#_percolate_api doesn\'t tell that that "size" parameter is mantadory when using "highlight" parameter. \r\n\r\nHowhever, when using "highlight" parameter :\r\n\r\n     curl -XGET \'localhost:9200/my-index/message/_percolate\' -d \'{\r\n         "highlight":{"fields":{"message":{}}},"doc" : {\r\n             "message" : "A new bonsai tree in the office"\r\n         }\r\n     }\'\r\n\r\nElasticsearch server (1.0.0.Beta2) throws errors telling that "Can\'t highlight if size isn\'t specified" :\r\n\r\n     {\r\n          "took":3,\r\n          "_shards":{\r\n               "total":5,\r\n               "successful":4,\r\n               "failed":1,\r\n               "failures":[\r\n                    {\r\n                         "index":"my-index",\r\n                         "shard":2,\r\n                         "reason":"BroadcastShardOperationFailedException[[my-index][2] ]; nested: PercolateException[failed to percolate]; nested: ElasticSearchIllegalArgumentException[Can\'t highlight if size isn\'t specified]; "\r\n                    }]\r\n               },\r\n          "total":0\r\n     }\r\n\r\n\r\nDocumentation should tell about that. By the way, I can\'t understand why size should be mandatory for highlight to be performed. **Maybe the documentation is right, and code needs to be changed.**'
4344,'bleskes',"Add IO operation stats to the File System statistics\nThe `GET _cluster/nodes/stats/?clear&fs` endpoint currently gives metrics for read and write operations (both number and bytes). It's handy to add another metric which sums writes and reads for a total number of ops."
4343,'bleskes',"Add a total section to file system stats\nThe `GET _cluster/nodes/stats/?clear&fs` endpoint currently gives file system stats for every data path the node is configure for. It's handy to have a `total` section which adds all the per path stats into a global overview.\r\n\r\n"
4342,'kimchy','Allow to disable sending a refresh-mapping to master node\nWhen a node processed an index request, which caused it to update its own mapping, then it sends that mapping to the master. While the master process it, that node receives a state that includes an older version of the mapping. Now, there is a conflict, its not bad (i.e. the cluster state will *eventually* have the correct mapping), but we send for a refresh just in case form that node to the master.\r\n\r\nWith a system that has extreme cases of updates and frequent mapping changes, it might make sense to disable this feature. The `indices.cluster.send_refresh_mapping` setting can be introduced to support that (note, this setting need to be set on the data nodes)\r\n\r\nNote, sending refresh mapping is more important when the reverse happens, and for some reason, the mapping in the master is ahead, or in conflict, with the actual parsing of it in the actual node the index exists on. In this case, the refresh mapping will result in warning being logged on the master node.'
4341,'martijnvg','has_child query can yield in wrong results if parent type has nested inner objects\nIf a parent type has nested inner objects then the `has_child` query (with score_mode) can return wrong results. This is caused by an internal short circuiting mechanism and the fact that hidden nested Lucene document have the same uid as the main document. The internal short circuiting mechanism sees the hidden nested Lucene document as a hit and starts the countdown for short circuiting from the wrong hits. This results in too less hits being returned.\r\n\r\nRelates to #4210 and is the reason why with ES version 1.0 beta2 too less results are being returned.'
4339,'dakrone',"Support 'yaml' as a format for the Analyze API\nFixes #4311.\r\n\r\nCopies the format of the `detailed` format, because putting the `text` format into yaml doesn't make much sense to me."
4338,'dakrone',"Update the Wonderdog repo URL\nI discovered that the Wonderdog repo was relocated; I didn't try the rest of the repo links, so I can't attest to their accuracy."
4335,'kimchy','Fail geohash_cell filter if geohash prefix is not enabled\nWhile we are at it, cleanup also the Java code to conform to the correct naming and conventions we use.'
4334,'imotov','Node deadlock on shutdown\nES Version: 0.90.7, Java version: 1.7 update 45 64 bit Server VM.\r\n\r\nI have a 7 node cluster with 5 master nodes and 2 client nodes. \r\nWhen I was shutting down all nodes to do a full cluster restart, one node did not die and looks there is a deadlock.\r\n\r\n\r\nStack Trace:\r\n\r\n2013-12-03 22:07:50\r\nFull thread dump Java HotSpot(TM) 64-Bit Server VM (24.45-b08 mixed mode):\r\n\r\n"Attach Listener" daemon prio=10 tid=0x00007f8ed4028000 nid=0x5d32 waiting on condition [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"Thread-1" prio=10 tid=0x00007f8e88698000 nid=0x5c6e waiting on condition [0x00007f8e7e861000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)\r\n\tat org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)\r\n\tat org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)\r\n\tat org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)\r\n\r\n"SIGTERM handler" daemon prio=10 tid=0x00007f8ed4042000 nid=0x5c6b in Object.wait() [0x00007f8ee7915000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n\tat java.lang.Object.wait(Native Method)\r\n\t- waiting on <0x00000005fd3d76c8> (a org.elasticsearch.bootstrap.Bootstrap$1)\r\n\tat java.lang.Thread.join(Thread.java:1280)\r\n\t- locked <0x00000005fd3d76c8> (a org.elasticsearch.bootstrap.Bootstrap$1)\r\n\tat java.lang.Thread.join(Thread.java:1354)\r\n\tat java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106)\r\n\tat java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46)\r\n\tat java.lang.Shutdown.runHooks(Shutdown.java:123)\r\n\tat java.lang.Shutdown.sequence(Shutdown.java:167)\r\n\tat java.lang.Shutdown.exit(Shutdown.java:212)\r\n\t- locked <0x00000005fd340058> (a java.lang.Class for java.lang.Shutdown)\r\n\tat java.lang.Terminator$1.handle(Terminator.java:52)\r\n\tat sun.misc.Signal$1.run(Signal.java:212)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#7]" daemon prio=10 tid=0x00007f8e84112800 nid=0x799a waiting on condition [0x00007f8ee7c62000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#12]" daemon prio=10 tid=0x00007f8e8c11f800 nid=0x7999 waiting on condition [0x00007f8ee7ca3000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#3]" daemon prio=10 tid=0x00007f8e8011e000 nid=0x7997 waiting on condition [0x00007f8ee7d25000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#2]" daemon prio=10 tid=0x00007f8e8c11d800 nid=0x7996 waiting on condition [0x00007f8ee7d66000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#4]" daemon prio=10 tid=0x00007f8e84111000 nid=0x7995 waiting on condition [0x00007f8ee7da7000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#8]" daemon prio=10 tid=0x0000000001fa0800 nid=0x7991 waiting for monitor entry [0x00007f8ee7f7c000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)\r\n\t- waiting to lock <0x00000005fdbaaf50> (a java.lang.Object)\r\n\t- locked <0x00000005fae03ef0> (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)\r\n\tat java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#6]" daemon prio=10 tid=0x0000000001f9f000 nid=0x7990 waiting on condition [0x00007f8ee7fbd000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"elasticsearch[AG 8][search][T#1]" daemon prio=10 tid=0x00007f8e8410f000 nid=0x798f waiting on condition [0x00007f8ee7ffe000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:998)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"DestroyJavaVM" prio=10 tid=0x00007f8f1000a800 nid=0x775c waiting on condition [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"elasticsearch[AG 8][clusterService#updateTask][T#1]" daemon prio=10 tid=0x00007f8e84107800 nid=0x7798 waiting on condition [0x00007f8eee056000]\r\n   java.lang.Thread.State: WAITING (parking)\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)\r\n\tat java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)\r\n\tat java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)\r\n\t- locked <0x00000005fdbaaf50> (a java.lang.Object)\r\n\tat org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)\r\n\tat org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)\r\n\tat org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\n"Service Thread" daemon prio=10 tid=0x00007f8f10113800 nid=0x7769 runnable [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"C2 CompilerThread1" daemon prio=10 tid=0x00007f8f10111000 nid=0x7768 waiting on condition [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"C2 CompilerThread0" daemon prio=10 tid=0x00007f8f1010e800 nid=0x7767 waiting on condition [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"Signal Dispatcher" daemon prio=10 tid=0x00007f8f1010c800 nid=0x7766 runnable [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"Surrogate Locker Thread (Concurrent GC)" daemon prio=10 tid=0x00007f8f10102000 nid=0x7765 waiting on condition [0x0000000000000000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\r\n"Finalizer" daemon prio=10 tid=0x00007f8f100eb800 nid=0x7764 in Object.wait() [0x00007f8f0c1bd000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n\tat java.lang.Object.wait(Native Method)\r\n\t- waiting on <0x00000005fce11a08> (a java.lang.ref.ReferenceQueue$Lock)\r\n\tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)\r\n\t- locked <0x00000005fce11a08> (a java.lang.ref.ReferenceQueue$Lock)\r\n\tat java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)\r\n\tat java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:189)\r\n\r\n"Reference Handler" daemon prio=10 tid=0x00007f8f100e7800 nid=0x7763 in Object.wait() [0x00007f8f0c1fe000]\r\n   java.lang.Thread.State: WAITING (on object monitor)\r\n\tat java.lang.Object.wait(Native Method)\r\n\t- waiting on <0x00000005fce13cb0> (a java.lang.ref.Reference$Lock)\r\n\tat java.lang.Object.wait(Object.java:503)\r\n\tat java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)\r\n\t- locked <0x00000005fce13cb0> (a java.lang.ref.Reference$Lock)\r\n\r\n"VM Thread" prio=10 tid=0x00007f8f100e5000 nid=0x7762 runnable \r\n\r\n"Gang worker#0 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001c000 nid=0x775d runnable \r\n\r\n"Gang worker#1 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001e000 nid=0x775e runnable \r\n\r\n"Gang worker#2 (Parallel GC Threads)" prio=10 tid=0x00007f8f1001f800 nid=0x775f runnable \r\n\r\n"Gang worker#3 (Parallel GC Threads)" prio=10 tid=0x00007f8f10021800 nid=0x7760 runnable \r\n\r\n"Concurrent Mark-Sweep GC Thread" prio=10 tid=0x00007f8f100a2000 nid=0x7761 runnable \r\n"VM Periodic Task Thread" prio=10 tid=0x00007f8f1011e800 nid=0x776a waiting on condition \r\n\r\nJNI global references: 284\r\n\r\n\r\nFound one Java-level deadlock:\r\n=============================\r\n"Thread-1":\r\n  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\r\n  which is held by "elasticsearch[AG 8][search][T#8]"\r\n"elasticsearch[AG 8][search][T#8]":\r\n  waiting to lock monitor 0x00007f8e980a33a8 (object 0x00000005fdbaaf50, a java.lang.Object),\r\n  which is held by "elasticsearch[AG 8][clusterService#updateTask][T#1]"\r\n"elasticsearch[AG 8][clusterService#updateTask][T#1]":\r\n  waiting for ownable synchronizer 0x00000005fdba9278, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),\r\n  which is held by "elasticsearch[AG 8][search][T#8]"\r\n\r\nJava stack information for the threads listed above:\r\n===================================================\r\n"Thread-1":\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.awaitTermination(ThreadPoolExecutor.java:1461)\r\n\tat org.elasticsearch.threadpool.ThreadPool.awaitTermination(ThreadPool.java:249)\r\n\tat org.elasticsearch.node.internal.InternalNode.close(InternalNode.java:342)\r\n\tat org.elasticsearch.bootstrap.Bootstrap$1.run(Bootstrap.java:73)\r\n"elasticsearch[AG 8][search][T#8]":\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.terminated(EsThreadPoolExecutor.java:64)\r\n\t- waiting to lock <0x00000005fdbaaf50> (a java.lang.Object)\r\n\t- locked <0x00000005fae03ef0> (a org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor)\r\n\tat java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:704)\r\n\tat java.util.concurrent.ThreadPoolExecutor.processWorkerExit(ThreadPoolExecutor.java:1006)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1163)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n"elasticsearch[AG 8][clusterService#updateTask][T#1]":\r\n\tat sun.misc.Unsafe.park(Native Method)\r\n\t- parking to wait for  <0x00000005fdba9278> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)\r\n\tat java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:867)\r\n\tat java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1197)\r\n\tat java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)\r\n\tat java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)\r\n\tat java.util.concurrent.ThreadPoolExecutor.interruptIdleWorkers(ThreadPoolExecutor.java:781)\r\n\tat java.util.concurrent.ThreadPoolExecutor.tryTerminate(ThreadPoolExecutor.java:695)\r\n\tat java.util.concurrent.ThreadPoolExecutor.shutdown(ThreadPoolExecutor.java:1397)\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.shutdown(EsThreadPoolExecutor.java:56)\r\n\t- locked <0x00000005fdbaaf50> (a java.lang.Object)\r\n\tat org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:395)\r\n\tat org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:656)\r\n\tat org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:417)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:135)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:744)\r\n\r\nFound 1 deadlock.\r\n\r\n'
4331,'jpountz','Fix implementation of currentValueHash in FieldDataSource.Bytes.SortedAndUnique\nClose #4330'
4330,'jpountz','FieldDataSource.Bytes.SortedAndUnique returns wrong hashes\nThe implementation of `currentValueHash` in FieldDataSource.Bytes.SortedAndUnique is not correct and returns hash codes that are always equal to 0, which makes BytesRefHash lookup perform in linear time instead of constant time.'
4326,'jpountz',"Fix _all boosting.\n_all boosting used to rely on the fact that the TokenStream doesn't eagerly\r\nconsume the input java.io.Reader. This fixes the issue by using binary search\r\nin order to find the right boost given a token's start offset.\r\n\r\nClose #4315"
4325,'martijnvg','Percolate API request parsing issue\nI experiment strange behaviour using percolate API using fresh 1.0.0-Beta2 elasticsearch :\r\n\r\nFirst, I index a new percolate query :\r\n\r\n    curl -XPUT \'localhost:9200/my-index/.percolator/1\' -d \'{\r\n        "query" : {\r\n            "match" : {\r\n                "message" : "bonsai tree"\r\n            }\r\n        }\r\n    }\'\r\n\r\nThen when I use Percolate API :\r\n\r\n     curl -XGET \'localhost:9200/my-index/message/_percolate\' -d \'{\r\n         "doc" : {\r\n             "message" : "A new bonsai tree in the office"\r\n         },\r\n         "score":true\r\n     }\'\r\n\r\n**note the usage of `score:true`**\r\n\r\nI get a result without score :\r\n\r\n\t{\r\n\t\t"took": 5,\r\n\t\t"_shards": {\r\n\t\t\t"total": 5,\r\n\t\t\t"successful": 5,\r\n\t\t\t"failed": 0\r\n\t\t},\r\n\t\t"total": 1,\r\n\t\t"matches": [\r\n\t\t\t{\r\n\t\t\t\t"_index": "my-index",\r\n\t\t\t\t"_id": "1"\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n\r\nBut if I change the location of the parameter `score` in the query : \r\n\r\n     curl -XGET \'localhost:9200/my-index/message/_percolate\' -d \'{\r\n         "score":true,\r\n         "doc" : {\r\n             "message" : "A new bonsai tree in the office"\r\n         }\r\n     }\'\r\n\r\nThen, I get the expected result :\r\n\r\n\t{\r\n\t\t"took": 39,\r\n\t\t"_shards": {\r\n\t\t\t"total": 5,\r\n\t\t\t"successful": 5,\r\n\t\t\t"failed": 0\r\n\t\t},\r\n\t\t"total": 1,\r\n\t\t"matches": [\r\n\t\t\t{\r\n\t\t\t\t"_index": "my-index",\r\n\t\t\t\t"_id": "1",\r\n\t\t\t\t"_score": 1\r\n\t\t\t}\r\n\t\t]\r\n\t}\r\n\r\n\r\nI experiment the same behaviour with the `highlight` and `size` parameters.'
4324,'javanna','Fixes #4323 - Debug logging can hide trace logging\nI swapped the order of the `isTraceEnabled` and `isDebugEnabled` checks to ensure that the trace log message is used if trace is enabled.'
4323,'dakrone','In some places trace logging is hidden by isDebugEnabled check\nIn three places the code does something like this:\r\n\r\n    if (logger.isDebugEnabled()) {\r\n        logger.debug(...);\r\n    } else if (logger.isTraceEnabled()) {\r\n        logger.trace(...);\r\n    }\r\nIn most (all?) cases, if trace is enabled, debug is also enabled, meaning that even if trace is enabled, it will still enter the debug block and will therefore log the debug message not the trace message.\r\n\r\nI suggest the blocks should be in the opposite order (check if trace is enabled - if not, check if debug is enabled)\r\n\r\nMinor issue, but annoying if you needed to see the information that that particular trace log would have logged.'
4315,'jpountz',"Per-field boosting of the _all field is broken unless very specific conditions are met\nThe _all field uses payloads in order to be able to store per-field boosts in a single index field. However, the way it is implemented relies on the fact that the token stream doesn't eagerly consume the input `java.io.Reader` (see `AllEntries.read`). So in practice, boost on the _all field doesn't work when under any of these circumstances:\r\n - there is a char filter,\r\n - the tokenizer is not the `standard` tokenizer,\r\n - any token filter has read-ahead logic.\r\n"
4313,'martijnvg','has_parent query can yields inconstant results\nWhen `has_parent` query get wrapped into a `filtered` query, it can yield wrong results.'
4311,'dakrone','Support YAML as output format of the analyze API\nIt would be nice to be able to get the result of an analyzer in YAML instead of JSON. Currently I am not able to get it as YAML. Neither via `format=yaml` nor by setting the accept header to `application\\yaml`.\r\n'
4310,'dakrone','Remove "ok" : true from successful REST responses\nIts pretty meaningless, especially with correct REST codes. Also, we are not consistent in returning it in all APIs. I suggest we simply remove it.'
4307,'s1monw','Shingle filter should expose `filler_token`\nSince Lucene 4.4 release enable_position_increment settings on token filters cannot be set to false which results in underscores appearing for filtered tokens in shingle filters.'
4306,'martijnvg',"has_child filter and query yield inconsistent results\nDocument marked as deleted are not taken into account:\r\n* The deleted docs are not being applied in the has_child query, which they should and were taken into account in has_child filter which isn't necessary.\r\n* The short circuit mechanism needs to know know about deleted docs, but it doesn't and therefor short circuits the has_child query execution too early.\r\n* The above was amplified when the `has_child` query was wrapped into a `filtered` query. \r\n\r\nRelates to #4297 and #4210."
4303,'javanna','minScore is not inclusive\nSee #3934 '
4301,'javanna','Issue with BulkProcessor api and ConcurrentRequests > 1\nI have switched my river to use ```org.elasticsearch.action.bulk.BulkProcessor``` in the last release using the following default values: \r\n```java\r\n  public final static int DEFAULT_CONCURRENT_REQUESTS = 50;\r\n  public final static int DEFAULT_BULK_ACTIONS = 1000;\r\n  public final static TimeValue DEFAULT_FLUSH_INTERVAL = TimeValue.timeValueMillis(10);\r\n  public final static ByteSizeValue DEFAULT_BULK_SIZE = new ByteSizeValue(5, ByteSizeUnit.MB);\r\n``` \r\nUnfortunately it seems there is an issue with this api when a value of ConcurrentRequests different from the default (1) - probably greater than the number of CPUs available. Number of documents send to bulkprocessor does not match the number of documents indexed (all documents are unique).\r\nI have created a small project to reproduce the issue [1].\r\nRun with conccurentRequest = 50 return:\r\njava.lang.RuntimeException: Number of documents indexed 357841 does not match the target 1000000\r\njava.lang.RuntimeException: Number of documents indexed 257381 does not match the target 1000000\r\nRun with conccurentRequest = 8 the results are correct.\r\n\r\nI am using ES 0.90.7\r\n\r\n[1] - https://github.com/richardwilly98/test-elasticsearch-bulk-processor\r\n\r\n'
4300,'bleskes',"External versioning does not persist versions of deletes\nElasticsearch only keeps a record of the version of deletes for a short time (the 'index.gc_deletes' setting, default 60s). I think this can cause issues if you restart a cluster, as the record of deletes will have been lost during the restart. I'm using external versioning.\r\n\r\nIt’s easy to try out:\r\n\r\n1. Create an index with the default of 1 replica per primary shard, spread across two nodes, then shutdown both nodes.\r\n2. Start node 1.\r\n3. Send a document with external version 1.\r\n4. Shutdown node 1.\r\n5. Startup node 2.\r\n6. Startup node 1.\r\n\r\n\r\nIn step 5 node 2 becomes the master, so when node 1 starts in step 6, node 1 removes the document you’ve just sent to it, because the master didn’t have it. If elasticsearch kept records of deletes, it would have known that the reason it didn’t exist on node 2 was because it hasn’t received it yet (it wouldn't have an entry in its 'deletes cache', so would know that it hadn't deleted it).\r\n\r\nAnd there’s a variation on the above which enables a delete to be ‘undone’, despite the fact that the version of the delete is greater than the version of the insert.\r\n\r\n\r\nSo, because it doesn't keep a persistent record of the deletes, when resolving differences between nodes it doesn’t know if a node simply hasn’t received an update yet, or if it has actually deleted it since the most recent update.\r\nMy feeling is that this is wrong. If two nodes disagree about whether a document should exist or not, then I feel like the versioning system should be used, rather than using the master as the version of truth. \r\n\r\n\r\nOne the face of it it seems like a fairly simple change - persist the contents of RobinEngine.versionMap to disk (another lucene index?), and read the contents of this into memory on startup. There's clearly performance issues though if we write/delete a record to some kind of disk store every time a document is index."
4297,'martijnvg',"has_child filter yields inconsistent results when the number of child docs are high\nUsing 0.90.7 we've experienced the following: We have ~100K parent docs and ~1M child docs. We're essentially doing what's described here: http://joelabrahamsson.com/grouping-in-elasticsearch-using-child-documents/\r\n\r\nWhen searching for parent docs and filtering using has_child it works as expected when the has_child filter contains a filter that limits the possible child docs to ~200K. The same goes when the number of child docs in the index is lower. \r\n\r\nWhen we we however use the exact same query/filters with the exception that the has_child filter only limits the possible child docs to  > ~500K we start seeing results that can't possibly be right. Also, when we run the same query again it produces a different result.\r\n\r\nSo, it seems to me that when the number of child docs become large the search results become inconsistent. The result does not however contain any indication that something has gone wrong (10 shards, 10 successful).\r\n\r\nWe're using five small nodes where ES has 1GB memory on each. The relevant index has 10 shards and 1 replica."
4296,'drewr',"_cat/indices waits for cluster health timeout on missing index\nWe're not using `concreteIndices` right after creating it. "
4293,'dadoonet','OOM when building with java6\nThe maven-compiler-plugin upgrade from 2.3.2 to 3.1 (see #4279) could cause out of memory issue when building the project with Maven and JDK6 and default memory settings (no `MAVEN_OPTS`).\r\n\r\nThis issue does not appear with JDK7.\r\n'
4291,'martijnvg','has_child query with score_mode=avg can emit infinity as score\n'
4290,'uboness','Add support for term filtering on terms aggs\nEnable filtering the terms that should be aggregated. This should be based on `include`/`exclude` regexps:\r\n\r\n```\r\n{\r\n    "terms" : {\r\n        "field" : "tags",\r\n        "include" : "[tag1 | tag2 | tag_*]",\r\n        "exclude" : "[tag_10 | tag_2*]"\r\n    }\r\n}\r\n```\r\nThe `exclude` takes precedence over the `include`, meaning, the `include` is evaluated first, and then the `exclude` will do the last filtering'
4289,'spinscale','Clarify de-duplication and optimize/merge\nThis contribution is based on the feedback given in issue 4254 and issue 4255 .'
4288,'spinscale','ES java very high CPU usage\nI am having similar issues as described on this thread. https://github.com/elasticsearch/elasticsearch/issues/1940\r\nAlthough leap second already happened in June but I tried resetting the time anyway. Didn\'t work. \r\nThis is causing a lot of frustration. Any help is much appreciated. \r\n\r\nI have upgraded ES to the latest (elasticsearch-0.90.7-1). Have restarted ES, front end, logstash. no joy at all. \r\n\r\njava -version\r\njava version "1.7.0_45"\r\nOpenJDK Runtime Environment (rhel-2.4.3.2.el6_4-x86_64 u45-b15)\r\nOpenJDK 64-Bit Server VM (build 24.45-b08, mixed mode)\r\n\r\nAfter the upgrade I am seeing these in the logs, which seems like a totally different issues. \r\n\r\n[2013-11-28 03:48:32,796][WARN ][discovery.zen            ] [LA Logger] received a join                    request for an existing node [[Night Thrasher][ecqZvZhDSTGiVkrjj6G_hw][inet[/192.168.1                   28.146:9300]]{client=true, data=false}]\r\n[2013-11-28 03:48:36,006][WARN ][discovery.zen            ] [LA Logger] received a join                    request for an existing node [[Night Thrasher][ecqZvZhDSTGiVkrjj6G_hw][inet[/192.168.1                   28.146:9300]]{client=true, data=false}]\r\n\r\nHot thread output: https://gist.github.com/ydnitin/7687098'
4287,'spinscale','0.19.12: query containing OR & AND ParseException\nHi everyone,\r\n\r\n\r\nI need your thoughts on the following issue.\r\nThe following query failed due to the "OR" in caps, if I search for "or" it works.\r\n\r\n    {\r\n      "from" : 0,\r\n      "size" : 100,\r\n      "query":{\r\n          "bool":{\r\n             "must":[\r\n                {\r\n                   "dis_max":{\r\n                      "tie_breaker":0.7,\r\n                      "queries":[\r\n                         {\r\n                            "field":{\r\n                               "name":"OR"\r\n                            }\r\n                         }\r\n                      ]\r\n                   }\r\n                }\r\n             ]\r\n          }\r\n       }\r\n    }\r\n\r\nI got the following error\r\n\tat org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:211)\r\n\tat org.elasticsearch.index.query.FieldQueryParser.parse(FieldQueryParser.java:168)\r\n\t... 19 more\r\nCaused by: org.apache.lucene.queryParser.ParseException: Encountered " <OR> "OR "" at line 1, column 0.\r\n\r\n\r\nThe same thing happens with the AND.\r\n\r\nis it an issue related to the old version of ES?\r\n\r\nthanks for your help'
4284,'martijnvg','Multi-percolate should respect the rest.action.multi.allow_explicit_index setting\nSee http://github.com/elasticsearch/elasticsearch/issues/3636\r\n'
4282,'spinscale','Upgrade RPM Maven Plugin to 2.1-alpha-3\n'
4281,'dadoonet','Upgrade Maven Jar Plugin to 2.4\n'
4280,'dadoonet','Upgrade Maven Resources Plugin to 2.6\n'
4279,'dadoonet','Upgrade Maven Compiler Plugin to 3.1\n'
4278,'dadoonet','Upgrade Maven Assembly Plugin to 2.4\n'
4277,'dadoonet','Upgrade Maven Eclipse Plugin to 2.9\n'
4276,'dadoonet','Upgrade Maven Source Plugin to 2.2.1\n'
4275,'dadoonet','Upgrade Maven Surefire Plugin to 2.16\n'
4274,'dadoonet','Upgrade Maven Dependency Plugin to 2.8\n'
4273,'s1monw','Fetch / Count might fail if executed on a relocated shard.\nWhen we relocate a shard we might still have pending SearchContext\r\ninstances hanging around that will be used in "in-flight" searches\r\non the already relocated shard. This is a valid operation but if\r\nwe have already closed the underlying directory which happens during\r\ncleanup concurrently the close call on the IndexReader can trigger\r\nan AlreadyClosedException when the NRT reader tries to cleanup files\r\nvia the IndexWriter. This kind of smells like a bug in Lucene, a close should never throw that exception IMO\r\n'
4270,'jpountz','Clean up calls to IndexFieldDataService.getForField\nThis method has two signatures: one that takes a mapper and the other one that\r\ntakes parameters that are normally retrieved from the field mapper. This second\r\none is bad because it allows to potentially discard field data configuration of\r\nthe field mapper.\r\n\r\nFor example, the percolator discards the field mapper configuration and\r\nretrieved field data by forcing the "paged_bytes" format even though the field might have been configured to use doc values.'
4267,'uboness','Terms aggregator should support filtering terms by a regex\nEnable filtering the terms that should be aggregated. This should be based on `include`/`exclude` regexps:\r\n\r\n```\r\n{\r\n    "terms" : {\r\n        "field" : "tags",\r\n        "include" : "[tag1 | tag2 | tag_*]",\r\n        "exclude" : "[tag_10 | tag_2*]"\r\n    }\r\n}\r\n```\r\nThe `exclude` takes precedence over the `include`, meaning, the `include` is evaluated first, and then the `exclude` will do the last filtering'
4266,'dadoonet',"Update to shade plugin 2.2 to shade test artifact as well\nWhen we want to use test artifact in other projects, dependencies\r\nare not shaded as for core artifact.\r\n\r\nIssue opened in maven shade project: [MSHADE-158](http://jira.codehaus.org/browse/MSHADE-158)\r\n\r\n**Don't merge this PR until shade plugin 2.2 is released!**\r\n\r\nVote in progress: http://markmail.org/message/pg565rgqhxwai56u. Released will probably happen on friday november, 29th.\r\n\r\nWhen using it in other projects, you basically need to change your `pom.xml` file:\r\n\r\n```xml\r\n<dependency>\r\n    <groupId>org.elasticsearch</groupId>\r\n    <artifactId>elasticsearch</artifactId>\r\n    <version>${elasticsearch.version}</version>\r\n    <type>test-jar</type>\r\n    <scope>test</scope>\r\n</dependency>\r\n```\r\n\r\nYou can also define some properties:\r\n\r\n```xml\r\n<properties>\r\n    <tests.jvms>1</tests.jvms>\r\n    <tests.shuffle>true</tests.shuffle>\r\n    <tests.output>onerror</tests.output>\r\n    <tests.client.ratio></tests.client.ratio>\r\n    <es.logger.level>INFO</es.logger.level>\r\n</properties>\r\n```"
4265,'dadoonet','BulkProcessor process every n+1 docs instead of n\nWhen you set a BulkProcessor with a bulk actions size of 100, it executes the bulk after 101 documents.\r\n\r\n```java\r\nBulkProcessor.builder(client(), listener).setBulkActions(100).setConcurrentRequests(1).setName("foo").build();\r\n```\r\n\r\nSame for size. If you set the bulk size to 1024 bytes, it will actually execute the bulk after 1025 bytes.\r\n\r\nThis patch fix it.'
4263,'martijnvg','Added aggregation support to the percolate api.\nRelates #4245'
4259,'s1monw','AllocationDeciders should be executed in order, starting at "cheap execution" and "most likely to return no"\nThe different AllocationDeciders are more or less expensive processing wise. They are also more or less likely to return a Decision.NO or Decision.THROTTLE. For large clusters this can result in a 10-15% speedup in recalculating the cluster state (tested with https://github.com/geidies/elasticsearch/blob/optimize_RoutingNodes/src/test/java/org/elasticsearch/cluster/routing/allocation/MassiveClusterRebalanceRoutingTests.java - 3.7 seconds on my test machine compared to 4.2 seconds without the optimization).\r\n\r\nConcurrentRebalanceAllocationDecider - loops over all ShardRoutings, O(n), optimization to O(1) for that in separate PR\r\nDisableAllocationDecider - is O(1) \r\nClusterRebalanceAllocationDecider - is O( #shards_unassigned ) + O( #shards ) or O( 1 ) + O( #shards ) - optimization for that in separate PR, making it O( 2 )\r\nDiskThresholdDecider - O( 2 )\r\nSnapshotInProgressAllocationDecider - O( 1 )\r\nReplicaAfterPrimaryActiveAllocationDecider and RebalanceOnlyWhenActiveAllocationDecider - O( #shards ) with current implementation of RoutingNodes.getShardsRoutingFor( MutableShardRouting ) - optimization in separate PR, making it O( #shards in replica set )\r\nShardsLimitAllocationDecider is O( # shards on node ) + O( 1 ).\r\nAwarenessAllocationDecider is O( # shards in cluster ) * # awareness attributes, making it the most expensive, but least likely to be turned on.\r\nSameShardAllocationDecider is O( # shards on node ) * # nodes on host\r\nThrottlingAllocationDecider, which is O( #shards_per_node ) + O( #shards_per_node )\r\n\r\nIn addition to the re-ordering, instead of applying all AllocationDeciders, skip the rest of one return a Decision.NO. This logic is ported from the Decision.Multi class.'
4257,'s1monw',"Shouldn't be necessary to loop over ShardRoutings\nAs several other classes can change the internal state of the RoutingNodes data structure, inefficient looping over nodes and assigned shards was necessary in the AllocationDeciders.\r\n\r\nWith larger clusters, reallocation gets too slow. In our current case, we have 5 years of data with daily indices, 6 shards per index, replication factor 1. Recalculating cluster state can take minutes, with the master sitting at 100% CPU in RoutingNodes.shardsRoutingFor( MutableShardRouting ).\r\n\r\nThe taken approach is\r\na) making RoutingNodes a singleton, since only one active instance should ever exist anyhow,\r\nb) notifying RoutingNodes of changes in MutableShardRouting instances state.\r\n\r\nThis certainly is not the most elegant approach and adds complexity instead of removing it, but is what can be done without a major refactoring of allocation.\r\n\r\nIn the supplied test case execution of the final reallocation is sped up from 22 seconds on my test machine to 4.2 seconds."
4251,'drewr','Add cat API for pending tasks\nwould be useful to have the pending tasks API also exposed as cat API'
4250,'s1monw',"Deciders shouldn't loop over all shards.\nInstead of using loops over all ShardRoutings, do accounting in RoutingNodes.\r\n\r\nSpeeds up recalculating cluster state on large clusters. This is the background of the patch - we have 20k+ shards in the cluster and the master sits at 100% CPU for minutes trying to apply cluster state changed events.\r\n\r\nrouting.allocation related tests pass. Compilation problem with head pulled into my master to apply changes on top? (org.elasticsearch.common.inject.Inject)\r\nThere is certainly not much elegance with the approach of using a Singleton - but this class is instantiated in one place only anyhow. There is even less elegance of using the notification from MutableShardRouting to RoutingNodes. but i guess this is what can be done without a major refactoring. "
4247,'martijnvg','Added `execution` option to `range` filter and deprecated `numeric_range` filter.\nRelates to #4034'
4245,'martijnvg','Add aggregation support in the percolate api\nAdd aggregation support in the percolate api, just like the facet support.'
4242,'uboness','add support for `shard_size` to terms aggregations\nadd support for `shard_size` parameter (next to `size`) in the terms aggregations - will help increase accuracy on high cardinality fields.\r\n\r\nsimilar to the `shard_size` support in terms facets - #3821 '
4241,'s1monw','Upgrade to Lucene 4.6.0\nThis upgrade would include the following improvements:\r\n\r\n * Remove XIndexWriter in favor of the fixed IndexWriter\r\n * Removes patched XLuceneConstantScoreQuery\r\n * Now uses Lucene passage formatters contributed from Elasticsearch in PostingsHighlighter\r\n * Upgrades to Lucene46 Codec from Lucene45 Codec\r\n * Fixes problem in CommonTermsQueryParser where close was never called.\r\n\r\nhere are the changes in Lucene 4.6:\r\n\r\nhttp://lucene.apache.org/core/4_6_0/changes/Changes.html'
4239,'uboness','Error: "No mapping found for value_field" uses key_field instead of value_field in range facet\nAssume you have a valid key_field - data.field3 and a non existing value_field - data.field5 and you issue a query like:\r\n{"facets":{"facet_result":{"range":{"key_field":"data.field3","value_field":"data.field5","ranges":[{"to":18},{"from":18,"to":19},{"from":19}]}}}}\r\n\r\nThen you will get an error like:\r\n... No mapping found for value_field [data.field3]]; \r\n\r\ninstead of \r\n... No mapping found for value_field [data.field5]]; '
4237,'imotov','During node startup local primaries should be preferred to relocating primaries\nTo reproduce \r\n- set `cluster.routing.allocation.node_initial_primaries_recoveries` to any value lower than `cluster.routing.allocation.node_concurrent_recoveries`\r\n- start cluster with two nodes (node1 and node2)\r\n- create an index with 1000 shards and 0 replicas:\r\n```\r\ncurl -XPUT localhost:9200/test -d \'{"settings": {"number_of_shards":1000, "number_of_replicas":0}}\' \r\n```\r\n- shutdown node2\r\n- enable allocation filtering to exclude node1:\r\n```\r\ncurl -XPUT localhost:9200/_cluster/settings -d \'{"transient": {"cluster.routing.allocation.exclude._id": "...id of node1..."}}\'\r\n```\r\n- at this time half of the shards should be `STARTED` and another half should be `UNASSIGNED`\r\n- start node2\r\n- observe that instead of initializing `node_initial_primaries_recoveries` shards on node2 first, elasticsearch is moving shards from node1 to node2\r\n\r\nExpected behavior: until all local shards are initialized, all `node_initial_primaries_recoveries` shards should be initializing locally and the rest (`node_concurrent_recoveries` - `node_initial_primaries_recoveries`) can be used for relocation.\r\n\r\nImpact: as a result of this bug, sometimes relocating primaries from another node can take over local recovery and a cluster may take very long time to get to green status'
4232,'kimchy','NPE in RobinEngine.acquireSearcher()\nThere appears to be a NullPointerException in the RobinEngine class on version 0.90.7.\r\n\r\n[2013-11-23 01:10:04,745][WARN ][cluster.action.shard     ] [Thing] [indexName][18] sending failed shard for [indexName][18], node[W71dMRgfR3iZuVI77KIsdg], relocating [cV9T2EC9SyaYxhigk4_N6Q], [R], s[RELOCATING], indexUUID [_na_], reason [Failed to perform [deleteByQuery/shard] on replica, message [RemoteTransportException[[Meld][inet[/192.168.1.1:9300]][deleteByQuery/shard/replica]]; nested: NullPointerException; ]]\r\n[2013-11-23 01:10:08,315][WARN ][action.deletebyquery     ] [Thing] Failed to perform deleteByQuery/shard on replica [indexName][18]\r\norg.elasticsearch.transport.RemoteTransportException: [Baron Blood][inet[/192.168.1.1:9300]][deleteByQuery/shard/replica]\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.acquireSearcher(RobinEngine.java:744)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.acquireSearcher(InternalIndexShard.java:601)\r\n        at org.elasticsearch.action.deletebyquery.TransportShardDeleteByQueryAction.shardOperationOnReplica(TransportShardDeleteByQueryAction.java:132)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:245)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicationOperationAction.java:225)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n        at java.lang.Thread.run(Thread.java:722)'
4229,'javanna','Refactored put mapping api to make use of the new recently introduced generic ack mechanism\nRefactored put mapping api to make use of the new recently introduced generic ack mechanism\r\n\r\nCloses #4228'
4228,'javanna','Move put mapping api to new acknowledgement mechanism\nMove put mapping api to new acknowledgement mechanism introduced in #3786 .'
4225,'chilling','SloppyMath\nAdded copy of SloppyMath.java from lucene 4.6+\r\nand setup GeoDistance for new haversin method\r\n\r\ncloses #3862'
4221,'spinscale','Documentation inconsistent on thread pool defaults\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html\r\n\r\nTop of page seems to indicate that the default for queue_size is fixed based on which thread pool, but then below it says it defaults to -1. Am I just missing something here?'
4220,'kimchy',"Bulk should support shard timeout like the index api\nIt'd be nice if the bulk and delete api supported the shard timeout setting like the index api.  This would allow updates to fail quickly if the shard is offline rather than waiting for the client side timeout and leaving the operation hanging around on the server."
4218,'javanna','Refactored delete index api to make use of the new recently introduced generic ack mechanism\nRefactored delete index api to make use of the new recently introduced generic ack mechanism\r\n\r\nSide note: the double notification from each node (one after deleting from metadata, one after deleting from file system) that was in place is not needed anymore, as each node returns the ack in the clusterStateProcessed, which gets called after all the listeners (one of which actually deletes from file system)\r\n\r\nCloses #4217'
4216,'dadoonet','Failed to install fr.pilato.elasticsearch.river/fsriver/0.3.0\nD:\\elasticsearch-0.90.6\\bin>plugin -install fr.pilato.elasticsearch.river/fsrive\r\nr/0.3.0\r\n-> Installing fr.pilato.elasticsearch.river/fsriver/0.3.0...\r\nTrying http://download.elasticsearch.org/fr.pilato.elasticsearch.river/fsriver/f\r\nsriver-0.3.0.zip...\r\nTrying http://search.maven.org/remotecontent?filepath=fr/pilato/elasticsearch/ri\r\nver/fsriver/0.3.0/fsriver-0.3.0.zip...\r\nTrying https://oss.sonatype.org/service/local/repositories/releases/content/fr/p\r\nilato/elasticsearch/river/fsriver/0.3.0/fsriver-0.3.0.zip...\r\nTrying https://github.com/fr.pilato.elasticsearch.river/fsriver/archive/v0.3.0.z\r\nip...\r\nTrying https://github.com/fr.pilato.elasticsearch.river/fsriver/archive/master.z\r\nip...\r\nFailed to install fr.pilato.elasticsearch.river/fsriver/0.3.0, reason: failed to\r\n download out of all possible locations..., use -verbose to get detailed informa\r\ntion\r\n\r\nI am not able to install fsriver plugin. what could be the problem and solution?'
4214,'bleskes','bulkRequest.execute().actionGet() Does not return\nHi\r\n\r\n  I was trying some load tests using the Bulk API.  I created a situation where i am reading from a large file (with individual json objects) and forming separate Bulkrequests.  It so happens that the source file has a large number of json records - 2976800 to be precise.  I increased my heap settings to ensure that the program and also elasticsearch does not run out of memory.  \r\n\r\n On execution, the java program reaches the below line and does not proceed further\r\n                BulkResponse bulkResponse = bulkRequest.execute().actionGet();\r\n\r\nHowever when i queried the _count endpoint for that index, i already see that it has indexed 2967800 records.  Here\'s the output of _count endpoint\r\n\r\n{\r\n  "count" : 2976800,\r\n  "_shards" : {\r\n    "total" : 10,\r\n    "successful" : 10,\r\n    "failed" : 0\r\n  }\r\n\r\nDoes the above mean that the documents have already been indexed?  If so, why does  bulkRequest.execute().actionGet() not return.  I looked into some older issues; looks like it might be related to Issue #1839\r\n\r\n  I understand i can probably chunk my requests using BulkProcessor.  I am going to try that; just wanted to check if there is some issue with the base BulkRequest.execute()'
4213,'bleskes',"Feature external version 'force' option\nMany times it would have been useful to force the update of a document ignoring the version when the version type is external. This is useful when trying to keep the external version in sync with the db optimistic lock version."
4210,'martijnvg','has_child Query scores Adversely Affected by filters\nI am encountering some extremely confusing behavior when employing a has_child query and a filter together on ElasticSearch 0.90.5. Our data involves records of US court cases (type "case") with child documents representing entries in the case\'s docket (type "docket-entry"). I\'ve run a number of experiments to try and wrap my head around what\'s going on here. The TLDR version is this:\r\n\r\nDespite ```filtered``` claiming that filters do not affect the score, scoring of a query including a ```has_child``` clause is adversely affected by filters.\r\n\r\nTo demonstrate the issue, I created and tested a set of filters that match *both* every case and every docket entry in our database. This was to confirm that the problem was not something simple and silly, like the filters clause applying to the child documents and filtering them in some undesirable fashion. I obtained total document counts using:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "match_all": {}\r\n  }\r\n}\r\n```\r\n\r\nThis produced: 150662 cases and 7290248 docket entries.\r\n\r\nI then ran my "match everything" filter:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "or": [\r\n          {\r\n            "range": {\r\n              "last_docket_id": {\r\n                "gte": "1"\r\n              }\r\n            }\r\n          },\r\n          {\r\n            "missing": {\r\n              "field": "last_docket_id"\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhere ```last_docket_id``` is a field that every case should have (as a 1-or-higher) and no docket entry should have. This also produced 150662 cases and 7290248 docket entries.\r\n\r\nUsing this data, I ran some test queries. First, a query involving a ```has_child``` without any filter attached:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "apple samsung",\r\n                "default_operator": "AND"\r\n              }\r\n            },\r\n            {\r\n              "has_child": {\r\n                "type": "docket-entry",\r\n                "query": {\r\n                  "query_string": {\r\n                    "default_operator": "AND",\r\n                    "query": "apple samsung"\r\n                  }\r\n                },\r\n                "score_type": "sum"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis produced the expected results: 505 total documents and a max score of 420.29727, with the first page of results being highly relevant. I then applied my "match everything" filter:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "apple samsung",\r\n                "default_operator": "AND"\r\n              }\r\n            },\r\n            {\r\n              "has_child": {\r\n                "type": "docket-entry",\r\n                "query": {\r\n                  "query_string": {\r\n                    "default_operator": "AND",\r\n                    "query": "apple samsung"\r\n                  }\r\n                },\r\n                "score_type": "sum"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "or": [\r\n          {\r\n            "range": {\r\n              "last_docket_id": {\r\n                "gte": "1"\r\n              }\r\n            }\r\n          },\r\n          {\r\n            "missing": {\r\n              "field": "last_docket_id"\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe degradation in results was immediately apparent. Not only did the ```max_score``` drop to 99.71274 and the quality of the first page of results decline to uselessness, the total documents dropped to 223 and randomly fluctuated with repeated runs of the query.\r\n\r\nTo confirm, I isolated the two segments of the query and ran the same experiments in isolation:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "apple samsung",\r\n                "default_operator": "AND"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nReturned 60 results with a ```max_score``` of 1.5968025\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "query_string": {\r\n                "query": "apple samsung",\r\n                "default_operator": "AND"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "or": [\r\n          {\r\n            "range": {\r\n              "last_docket_id": {\r\n                "gte": "1"\r\n              }\r\n            }\r\n          },\r\n          {\r\n            "missing": {\r\n              "field": "last_docket_id"\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nReturned the same - 60 results, 1.5968025 ```max_score```, which is what I would expect given the documentation. The ```has_child``` half, on the other hand, is disastrous:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "has_child": {\r\n                "type": "docket-entry",\r\n                "query": {\r\n                  "query_string": {\r\n                    "default_operator": "AND",\r\n                    "query": "apple samsung"\r\n                  }\r\n                },\r\n                "score_type": "sum"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nStarts with 498 hits and a ```max_score``` of 419.9289, but adding the filter:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "bool": {\r\n          "should": [\r\n            {\r\n              "has_child": {\r\n                "type": "docket-entry",\r\n                "query": {\r\n                  "query_string": {\r\n                    "default_operator": "AND",\r\n                    "query": "apple samsung"\r\n                  }\r\n                },\r\n                "score_type": "sum"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "or": [\r\n          {\r\n            "range": {\r\n              "last_docket_id": {\r\n                "gte": "1"\r\n              }\r\n            }\r\n          },\r\n          {\r\n            "missing": {\r\n              "field": "last_docket_id"\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nDrops us to 171 hits with a ```max_score``` of 99.37268.\r\n\r\nAt this point, my only conclusion is that either there\'s something about the has_child query that I\'m completely failing to understand or that there\'s a bug in ElasticSearch.'
4209,'martijnvg','Add a field data based TermsFilter\nAdd a terms filter that compares terms out of\r\nthe fielddata cache. When filtering on a large\r\nset of terms this filter can be considerably faster\r\nthan using a standard lucene terms filter.\r\n\r\nAdd the "fielddata" execution mode to the\r\nterms filter parser to enable the use of\r\nthe new FieldDataTermsFilter.\r\n\r\nsee #4181 '
4207,'jpountz',"Doc values support for geo points\nWe should add doc values support for geo points, probably via BinaryDocValues.\r\n\r\nSimilarly to #3993, the challenge is in the mappers, since the mapper needs to know all field values in order to be able to create the field instance (because there can be a single BinaryDocValues instance per document per field).\r\n\r\nAnother open question is about the encoding: storing two doubles (16 bytes) per point is probably wasteful, should we use instead a different encoding that would be more compact while keeping precision good enough? Since the range of possible values is fixed, I'm thinking that a fixed-size encoding that would map [-180,180] into [0, 2^<sup>bits_per_value</sup>] would be rather efficient. For example, if my math is correct, a `bits_per_value` of 24 (62.5% reduction) could give a precision of ~5m, a `bits_per_value` of 32 (50% reduction) would give a precision of ~20mm, and a `bits_per_value` of 40 (37.5% reduction) would give a precision << 1mm."
4205,'spinscale','[doc]add elasticsearch-extended-analyze plugin line to doc\nI release extended-analyze plugin.\r\nPlease merge plugins pages my plugin entry.'
4203,'kimchy',"Running the node stats api while a shard is moving onto the node logs an exception\nRunning the node stats api while a shard is moving onto the node logs this exception:\r\n```\r\n[2013-11-18 01:32:10,422][DEBUG][action.admin.cluster.node.stats] [elastic1002] failed to execute on node [sRNrZmovQIWTMO1WlDDulg]\r\norg.elasticsearch.index.engine.EngineClosedException: [elwiki_content_1384450509][2] CurrentState[CLOSED] \r\n        at org.elasticsearch.index.engine.robin.RobinEngine.ensureOpen(RobinEngine.java:969)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.segmentsStats(RobinEngine.java:1181)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.segmentStats(InternalIndexShard.java:509)\r\n        at org.elasticsearch.action.admin.indices.stats.CommonStats.<init>(CommonStats.java:154)\r\n        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:212)\r\n        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:165)\r\n        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:100)\r\n        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:43)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$AsyncAction$2.run(TransportNodesOperationAction.java:146)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:724)\r\n```\r\n\r\nI see that logging.yml logs actions at debug for easier debugging, but this one floods my logs when nodes move due to my monitoring.  I assume that it also stops stats from being returned which is also unfortunate.  I see it on 0.90.7.\r\n\r\nI don't have a gist recreation right now.  Let me know if one would be useful."
4202,'clintongormley','Problem with parent id containing commas when talking to an index alias\nHi,\r\n\r\nI ran into problems trying to index a child document whose parent id contains commas. This only occurs using an index alias, with the index name it works. This is the resulting exception in 0.90.7:\r\n\r\n```\r\nElasticSearchIllegalArgumentException: index/alias [parentid-testalias] provided with routing value [abc,2] that resolved to several routing values, rejecting operation\r\n\tat org.elasticsearch.cluster.metadata.MetaData.resolveIndexRouting(MetaData.java:338)\r\n\tat org.elasticsearch.action.index.IndexRequest.process(IndexRequest.java:557)\r\n\t...\r\n```\r\n\r\nHere is some code to reproduce the problem:\r\n\r\n```\r\nClient client = new TransportClient().addTransportAddress(new InetSocketTransportAddress("localhost", 9300));\r\n\r\nString indexName = "parentid-test";\r\nString indexAlias = "parentid-testalias";\r\nif (client.admin().indices().exists(new IndicesExistsRequest(indexName)).actionGet().isExists()) {\r\n\tclient.admin().indices().delete(new DeleteIndexRequest(indexName)).actionGet();\r\n}\r\nCreateIndexRequest createIndexRequest = new CreateIndexRequest(indexName);\r\ncreateIndexRequest.mapping("chld", "{\\"chld\\":{\\"_parent\\":{\\"type\\":\\"prnt\\"}}}");\r\nclient.admin().indices().create(createIndexRequest).actionGet();\r\nclient.admin().indices().prepareAliases().addAlias(indexName, indexAlias).execute().actionGet();\r\n\r\nclient.prepareIndex(indexName, "prnt", "abc,1").setSource("{\\"text\\":\\"parent abc,1\\"}").execute().actionGet();\r\nclient.prepareIndex(indexName, "chld", "def,1").setParent("abc,1").setSource("{\\"text\\":\\"child def,1\\"}").execute().actionGet();\r\n\r\nclient.prepareIndex(indexAlias, "prnt", "abc,2").setSource("{\\"text\\":\\"parent abc,2\\"}").execute().actionGet();\r\n// this causes the exception - indexAlias and comma in parent id\r\nclient.prepareIndex(indexAlias, "chld", "def,2").setParent("abc,2").setSource("{\\"text\\":\\"child def,2\\"}").execute().actionGet();\r\n\r\nclient.close();\r\n```\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-aliases.html tells me that this is intended behavior: "...search routing may contain several values separated by comma. Index routing can contain only a single value."\r\n\r\nSo, if this is how it should be, then I would suggest to add some documentation on invalid characters in IDs, as I didn\'t find anything regarding this. Any suggestions?'
4200,'spinscale',"Theadpool defaults in code do not match docs\nThe defaults don't match the reference in several categories.\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/f9ce791578ff103ce9af89fca1394513bc0150f7/src/main/java/org/elasticsearch/threadpool/ThreadPool.java\r\n\r\nvs.\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-threadpool.html"
4199,'martijnvg','[Java client] - FilterBuilder and QueryBuilder should throw ElasticSearchIllegalArgumentException\nQueryBuilderException has been dropped in favor for throwing ElasticSearchIllegalArgumentException. The #buildAsBytes() methods now have `ElasticSearchException` in their signature.\r\n\r\nThis change only applies to the Java client.'
4196,'spinscale','Warmers API breaking when empty Warmup query is created\nRan into this by accident, but basically if you add a warmup query with empty body, ES will accept it(even though I believe it shouldn\'t). \r\nAfter accepting this empty bodied query, it\'s not possible anymore to access warmup queries by regular exp. that would also match this query. Follows how to reproduce(on 0.90.8-SNAPSHOT)\r\n\r\n```\r\ncurl -XPOST http://localhost:9200/foo\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{"id":1,"content":"one"}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/2 -d \'{"id":2,"content":"two"}\'\r\ncurl -XPUT http://localhost:9200/foo/_warmer/warmer_1 -d \'{"query": {"match_all":{}}}\'\r\ncurl -XGET http://localhost:9200/foo/_warmer/w*\r\ncurl -XPUT http://localhost:9200/foo/_warmer/warmer_2\r\ncurl -XGET http://localhost:9200/foo/_warmer/w*\r\n```\r\n\r\nwhich yields:\r\n{"error":"NullPointerException[null]","status":500}'
4192,'martijnvg','Missing filter with nested objects\nHi,\r\n"missing" filter does not work with nested objetcs. Using nested filter with not filter works as a workaround, as you can see in https://gist.github.com/Erni/7484095\r\nHowever what my gut feeling says is "use the missing filter to find the docs without or empty attribute, as it does with normal objects".\r\n\r\nWould it be possible to make "missing" filter work with nested objects as well?'
4188,'dakrone','Handle pretty=false and missing line feed for pretty=true which is the d...\n...efault for main REST action.\r\n\r\nFollowing the debate on #4182 this PR keeps the fact that the main REST action is prettyfied by default but handling the pretty=false parameter and the added line feed in the prettification process.'
4187,'dadoonet','NPE in PluginsService when starting elasticsearch with a wrong user (Fix for 4186)\nWhen starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.\r\n\r\nTo reproduce:\r\n\r\n* create a plugins directory with `rwx` rights for root user only\r\n* launch elasticsearch from another account (elasticsearch for example)\r\n\r\nRelated discussion: https://groups.google.com/forum/#!topic/elasticsearch/_WRW4Qfpo7M\r\n\r\nCloses #4186.'
4186,'dadoonet','NPE in PluginsService when starting elasticsearch with a wrong user\nWhen starting elasticsearch with a wrong linux user, it could generate a `NullPointerException` when `PluginsService` tries to list available plugins in `./plugins` dir.\r\n\r\nTo reproduce:\r\n\r\n* create a plugins directory with `rwx` rights for root user only\r\n* launch elasticsearch from another account (elasticsearch for example)\r\n\r\nRelated discussion: https://groups.google.com/forum/#!topic/elasticsearch/_WRW4Qfpo7M'
4185,'martijnvg','Increase the usage of ImmutableOpenMap for immutable data structures.\nCut over the immutable data structures to ImmutableOpenMap in:\r\n* MetaData\r\n* IndexMetaData\r\n* ClusterState'
4181,'martijnvg','FieldDataTermsFilter\nAdd a terms filter that compares terms out of\r\nthe fielddata cache. When filtering on a large\r\nset of terms this filter can be considerably faster\r\nthan using a standard lucene terms filter.\r\n\r\nAdd the "fielddata" execution mode to the\r\nterms filter parser to enable the use of\r\nthe new FieldDataTermsFilter.\r\n\r\nAdd supporting tests and documentation.'
4180,'javanna','Implementation of blocking close method\nBlocks until all bulk requests have completed.  This fixes #4158'
4172,'bleskes',"java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 130\nCrashes don't seem to be related to any particular query and have been occurring for a number of weeks.\r\n\r\nTwo node cluster running on Debian 7.2\r\n\r\nI can confirm both nodes are running the exact same Elasticsearch and java versions.\r\n\r\n\r\n```Elasticsearch 0.90.5``` \r\n\r\n```openjdk-7-jdk:amd64 7u25-2.3.10-1~deb7u1```\r\n\r\n```Linux int-elastic-01 3.2.0-4-amd64 #1 SMP Debian 3.2.51-1 x86_64 GNU/Linux```\r\n\r\n```\r\n[2013-11-14 14:01:30,311][INFO ][cluster.metadata         ] [int-elastic-01] [test_iss3.dmadeley-django64.dev] update_mapping [suggestions] (dynamic)\r\n[2013-11-14 14:01:31,489][DEBUG][action.admin.cluster.node.info] [int-elastic-01] failed to execute on node [lWRF61bgQ66ip1BmLKygFg]\r\norg.elasticsearch.transport.RemoteTransportException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]\r\nCaused by: org.elasticsearch.transport.TransportSerializationException: Failed to deserialize response of type [org.elasticsearch.action.admin.cluster.node.info.NodeInfo]\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:147)\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124)\r\n    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n    at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n    at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n    at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n    at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)\r\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n    at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)\r\n    at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n    at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n    at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n    at java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.IndexOutOfBoundsException: Readable byte limit exceeded: 130\r\n    at org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.readByte(AbstractChannelBuffer.java:236)\r\n    at org.elasticsearch.transport.netty.ChannelBufferStreamInput.readByte(ChannelBufferStreamInput.java:132)\r\n    at org.elasticsearch.common.io.stream.AdapterStreamInput.readByte(AdapterStreamInput.java:35)\r\n    at org.elasticsearch.common.io.stream.StreamInput.readBoolean(StreamInput.java:267)\r\n    at org.elasticsearch.action.admin.cluster.node.info.NodeInfo.readFrom(NodeInfo.java:231)\r\n    at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:145)\r\n    ... 23 more\r\n[2013-11-14 14:01:32,278][INFO ][cluster.metadata         ] [int-elastic-01] [test_iss3.dmadeley-django64.dev] deleting index\r\n```"
4170,'javanna','Refactored open/close index api to make use of the new recently introduced generic ack mechanism\nRefactored open/close index api to make use of the new recently introduced generic ack mechanism\r\n\r\nCloses #4169'
4169,'javanna','Move open/close index api to new acknowledgement mechanism\nMove open/close index api to new acknowledgement mechanism introduced in #3786 .'
4166,'javanna',"Indices filters doesn't support _name parameter\nIndices filters doesn't support `_name` parameter, useful to get back which filters a document matched using [named filters](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-named-queries-and-filters.html)."
4164,'spinscale','Multi_match fails if fields param is not an array\nThe `fields` param in `multi_match` should accept a string, not just an array of strings.  A single string could contain a wildcard, and thus refer to multiple fields.\r\n\r\n    POST /_validate/query?explain\r\n    {\r\n       "multi_match": {\r\n          "query": "foo bar",\r\n          "fields": "foo"\r\n       }\r\n    }\r\n\r\ngive you:\r\n\r\n    {\r\n       "valid": false,\r\n       "_shards": {\r\n          "total": 1,\r\n          "successful": 1,\r\n          "failed": 0\r\n       },\r\n       "explanations": [\r\n          {\r\n             "index": "test",\r\n             "valid": false,\r\n             "error": "org.elasticsearch.index.query.QueryParsingException: [test] [match] query does not support [fields]"\r\n          }\r\n       ]\r\n    }'
4163,'bleskes','WeightedFilterCache::FilterCacheValueWeigher does not use key size when calculating weight\nYou might consider this not to be a big deal, but this discrepancy may cause significant inaccuracies in cache size calculation, especially in the case of the value being empty (which equates to a size of just 1 byte in the current calculation). For an empty value, the real cost of that entry in the cache is far higher as the key may be hundreds of bytes in size.'
4162,'bleskes',"TransportClient.connectedNodes should contain up to date node info when using SimpleNodeSampler\nThe transport client exposes a list of nodes it's connected to via the connectedNodes property. If sniffing is turned off (which is default), that list is not updated to contain actually node information, like ID, name, attributes etc.\r\n\r\nHaving that information is handy and can avoid confusing people which verify the properties of the nodes they are connected to."
4161,'brwe','make term statistics and term vectors accessible in scripts\nBelow is a minimal example which you can run to see how it works in principle. Documentation lists all options.\r\n\r\nAt this point I would first like to make sure that the general concept is OK. After that I will add complete javadoc etc.\r\n\r\ncloses #3772\r\n\r\nMinimal example:\r\n\r\n```\r\nDELETE paytest\r\n\r\nPUT paytest\r\n{\r\n    "mappings": {\r\n        "test": {\r\n            "properties": {\r\n                "text": {\r\n                    "index_analyzer": "fulltext_analyzer",\r\n                    "type": "string"\r\n                }\r\n            }\r\n        }\r\n    },\r\n    "settings": {\r\n        "analysis": {\r\n            "analyzer": {\r\n                "fulltext_analyzer": {\r\n                    "filter": [\r\n                        "my_delimited_payload_filter"\r\n                    ],\r\n                    "tokenizer": "whitespace",\r\n                    "type": "custom"\r\n                }\r\n            },\r\n            "filter": {\r\n                "my_delimited_payload_filter": {\r\n                    "delimiter": "+",\r\n                    "encoding": "float",\r\n                    "type": "delimited_payload_filter"\r\n                }\r\n            }\r\n        },\r\n        "index": {\r\n            "number_of_replicas": 0,\r\n            "number_of_shards": 1\r\n        }\r\n    }\r\n}\r\n\r\n\r\nPOST paytest/test/1\r\n{\r\n    "text": "the+1 quick+2 brown+3 fox+4 is quick+10"\r\n}\r\n\r\nPOST paytest/test/2\r\n{\r\n    "text": "the+1 quick+2 red+3 fox+4"\r\n}\r\n\r\nPOST paytest/_refresh\r\n\r\n#get the total term frequency for "quick"\r\n\r\nPOST paytest/_search\r\n{\r\n    "script_fields": {\r\n       "ttf": {\r\n          "script": "_index[\\"text\\"][\\"quick\\"].ttf()"\r\n       }\r\n    }\r\n}\r\n\r\n# get the term frequencies\r\nPOST paytest/_search\r\n{\r\n    "script_fields": {\r\n       "freq": {\r\n          "script": "_index[\\"text\\"][\\"quick\\"].freq()"\r\n       }\r\n    }\r\n}\r\nPOST paytest/test/2/_termvector\r\n# get the payloads which are floats in this case\r\nPOST paytest/_search\r\n{\r\n    "script_fields": {\r\n       "payloads": {\r\n          "script": "term = _index[\\"text\\"].get(\\"red\\",_PAYLOADS);payloads = []; for(pos : term){payloads.add(pos.payloadAsFloat(-1));} return payloads;"\r\n       }\r\n    }\r\n}\r\n\r\n#compute very simple score: just use the term frequency\r\nPOST paytest/_search\r\n{\r\n   "script_fields": {\r\n      "tv": {\r\n         "script": "_index[\\"text\\"][\\"quick\\"].freq()"\r\n      }\r\n   },\r\n   "query": {\r\n      "function_score": {\r\n         "functions": [\r\n            {\r\n               "script_score": {\r\n                  "script": "_index[\\"text\\"][\\"quick\\"].freq()"\r\n               }\r\n            }\r\n         ]\r\n      }\r\n   }\r\n}\r\n\r\n```\r\n\r\nand so on. see documentation for all options.\r\n\r\nNote that `_index` was called `_shard` before. See #4584\r\n'
4159,'dakrone',"Add support for Lucene's new SimpleQueryParser\nLucene's new SimpleQueryParser is designed to be able to parse human-entered queries without throwing any exceptions, we should add this and expose it in Elasticsearch.\r\n\r\nSee https://issues.apache.org/jira/browse/LUCENE-5336"
4158,'s1monw',"Add a blocking variant of close() method to BulkProcessor\nUsing BulkProcessor I often want to block when calling BulkProcessor.close() until all  BulkRequests have completed.\r\n\r\nCurrently this isn't possible. If you use concurrentRequests > 0, calling close() will execute a new bulk request asynchronously then return immediately.  \r\n\r\nIt's possible to write a wrapper around BulkProcessor to implement this behaviour, but it's a bit tricky as you have to understand the Exception handling quirks of BulkProcessor and be aware that every call to listener.beforeBulk() will not guarantee a corresponding call to listener.afterBulk().  \r\n\r\nIt would be nice to have this behaviour implemented as part of the core library.  I suggest using this method name:\r\n\r\nboolean awaitClose(long timeout, TimeUnit unit) throws InterruptedException;"
4157,'martijnvg','Size docIdsToLoad appropriately in SearchService\nCloses #4156'
4156,'martijnvg','SearchContext maintains a `size` length `int[]` when there are fewer than `size` results\nReproduce with:\r\n\r\n    Node node = NodeBuilder.nodeBuilder().node();\r\n    Client client = node.client();\r\n\r\n    client.prepareIndex("twitter", "twitter")\r\n        .setSource("{ \\"user\\" : \\"kimchy\\", \\"post_date\\" : \\"2009-11-15T14:12:12\\", \\"message\\" : \\"trying out Elastic Search\\" }")\r\n        .execute()\r\n        .get();\r\n\r\n    WildcardQueryBuilder query = QueryBuilders.wildcardQuery("message", "trying");\r\n    SearchRequestBuilder request = client.prepareSearch("twitter")\r\n            .setSearchType(SearchType.DFS_QUERY_AND_FETCH)\r\n            .setQuery(query)\r\n            .setSize(100000)\r\n            .setScroll("1m");\r\n    SearchResponse response = request.execute().get();\r\n    for (SearchHit hit : response.hits()) {\r\n        System.out.println(hit.getId());\r\n    }\r\n\r\n    client.close();\r\n    node.close();\r\n\r\nObserve by setting a breakpoint in `SearchContext#docIdstoLoad(int[], int, int)` and notice that despite creating a brand new index with only one result the `int[]` is sized at 100,000, to match the specified size.\r\n\r\nThis is leading to unwanted memory pressure, and appears to be fixable by editing `SearchService#shortcutDocIdsToLoad(SearchContext)` to size the `docIdsToLoad` array to the actual number of results.\r\n'
4154,'dadoonet',"Add support for ParseContext.externalValue() to all build-in field mappers\n`ParseContext.externalValue()` is not supported by some build-in field mappers (for example: GeoPointFieldMapper or BooleanFieldMapper)\r\n\r\ni guess this feature was implemented as a 'hack', but it will be great if it could be somehow formalized and propagated to other field mappers\r\n\r\ni need it for my external plugin (i want to be able to inject value for every build-in field mapper)"
4153,'s1monw',"Deadlock in BulkProcessor\nI'm experiencing a deadlock in BulkProcessor during heavy bulk indexing to a single node cluster.\r\n\r\nOccasionally, during heavy indexing, the node becomes unresponsive causing the client to throw NoNodeAvailableException, triggering the issue.\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/bulk/BulkProcessor.java#L279-305\r\n\r\nA semaphore is acquired before the call to client.bulk(request, listener) and only released in the callback.  If an Exception, such as NoNodeAvailableException, is thrown from client.bulk(request, listener), the semaphore is never released.\r\n\r\nWhen the number of Exceptions thrown by client.bulk(request, listener) is greater than the number of concurrent requests supported by BulkProcessor a deadlock occurs and no more documents can be indexed.\r\n\r\nBulkProcessor should be able to handle this by wrapping client.bulk(request, listener) inside a try/catch block and handling the Exception, calling the listener and releasing the semaphore.\r\n\r\n"
4151,'drewr',"Remove heap used from _cat/nodes and add RAM percentage\nIn this view you never care about the actual heap used bytes; you only want to know that your max is set to what you meant and what percentage you're currently using."
4149,'bleskes',"External plugin: stored fields taken from _source (even when _source is disabled)\nif _source is still 'warm' in transaction log and fields query parameter is specified,  requested stored fields are extracted from _source (this happens even when _source is disabled). \r\n\r\nThis behavior is causing issues for me, as im trying to create external plugin that will allow indexing of computed/scripted fields. Computed fields are not present in _source, so when marking them as stored, they still cant be immediately retrieved using fields query parameter. They are available eventually after transaction log gets flushed. \r\n\r\nIt will be great if it will be possible to change this behavior or at least override it on index level."
4147,'bleskes','IndexShardGatewayService should not call post_recovery if shard is in STARTED state\nAt the end recovery, the IndexShardGatewayService will double check the gateway has moved the shard to POST_RECOVERY and if not, do it it self.\r\nThe shard state could have already move to started, causing the post_recovery call to throw an exception and the entire shard recovery to fail.\r\n\r\nThis can happened if, after the gateway moved the shard to POST_RECOVERY:\r\n1) master sent a new cluster state indicating shard is initializing\r\n2) IndicesClusterStateService#applyInitializingShard will send a shard started event\r\n3) Master will mark shard as started and this will be processed quickly and move the shard to STARTED.'
4145,'bleskes','Expose maximum heap settings + heap usage in percent in JVM memory stats\nCurrently the OS section of the node returns memory usage in bytes but also as percent of the total memory. The JVM stats only return heap usage information but not the configured max or the usage in percent.  The max heap settings is handy to have and the usage percent is a useful shortcut.\r\n'
4143,'javanna',"Make sure rivers get started\nWith #3782 we changed the execution order of dynamic mapping updates and index operations. We now first send the mapping update to the master node, and then we index the document. This makes sense but caused issues with rivers as they are started due to the cluster changed event that is triggered on the master node right after the mapping update has been applied, but in order for the river to be started its _meta document needs to be available, which is not the case anymore as the index operation most likely hasn't happened yet. As a result in most of the cases rivers don't get started.\r\nWhat we want to do is retry a few times if the _meta document wasn't found, so that the river gets started anyway.\r\n\r\nCloses #4089, #3840"
4142,'martijnvg',"Bool filter should execute the same as the bool query\nRight now at least one should clause in a bool filter should match with a document, otherwise this document isn't a match regardless if there are any other must or must_not clause. Even though the bool filter doesn't have a `minumum_should_match` open the bool filter behaves as if `minumum_should_match` is set to 1.\r\n\r\nThis behaviour is inconsistent with the bool query and I would like to see that the bool filter works the same as the bool query with regards to the should clauses, so in the case should clauses are defined when there are must_not or must clauses then the should clause are irrelevant. If there are only should clauses defined a document just needs to match with a single should clause.\r\n\r\nI don't think we should support `minumum_should_match` option as is in bool query (makes bool filter execution expensive), but maybe have an option that enables <= 0.90.x behaviour."
4140,'imotov','Settings should be rendered in more readable way when human flag is set to true\nSettings in many REST APIs are hard to read because they are presented in a flattened format. REST APIs that output settings should observe the `human` flag and render settings in a structured format if flag `human` is `true`. \r\n\r\nWithout human flag:\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/test-idx/_settings?pretty\'\r\n{\r\n  "test-idx" : {\r\n    "settings" : {\r\n      "index.analysis.analyzer.default.filter.1" : "lowercase",\r\n      "index.analysis.analyzer.default.filter.2" : "stop",\r\n      "index.analysis.analyzer.default.filter.0" : "standard",\r\n      "index.analysis.analyzer.default.tokenizer" : "uax_url_email",\r\n      "index.analysis.analyzer.default.type" : "custom",\r\n      "index.number_of_shards" : "5",\r\n      "index.number_of_replicas" : "1",\r\n      "index.version.created" : "1000002",\r\n      "index.uuid" : "_VSsOKiwTJ-uq8-3n74nog"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwith human flag:\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/test-idx/_settings?pretty&human\'\r\n{\r\n  "test-idx" : {\r\n    "settings" : {\r\n      "index" : {\r\n        "uuid" : "_VSsOKiwTJ-uq8-3n74nog",\r\n        "analysis" : {\r\n          "analyzer" : {\r\n            "default" : {\r\n              "type" : "custom",\r\n              "filter" : [ "standard", "lowercase", "stop" ],\r\n              "tokenizer" : "uax_url_email"\r\n            }\r\n          }\r\n        },\r\n        "number_of_replicas" : "1",\r\n        "number_of_shards" : "5",\r\n        "version" : {\r\n          "created" : "1000002"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```'
4137,'spinscale',"fixed typos\nI'm going through and finally reading all of the Elasticsearch documentation, so I'll just accumulate any typo/style problems I find here. If it ends up being a lot of commits, I can rebase it into one so it can be applied more easily."
4136,'dakrone','Change default store impl to mmapfs on 64bit Linux.\nFixes #4134'
4135,'martijnvg','NPE for has_child query if number of results exceed certain limit\nIf the number of results exceed a certain limit, has_child query throws a NullPointerException.  \r\n\r\n```\r\ncurl -x \'\' -s -XPOST "http://localhost:9200/myindex/vendor/_search?from=0&size=20&pretty=true&routing=0" -d \'{\r\n   "query" : {\r\n      "has_child" : {\r\n         "query" : {\r\n            "query_string" : { "query" : "signed_date:[now-120d TO now-90d]" }\r\n         },\r\n         "type" : "transaction"\r\n      }\r\n   }\r\n}\r\n\'\r\n\r\n[2013-11-08 15:01:41,825][DEBUG][action.search.type       ] [Phage] [myindex_20131108][1], node[fuBzjMWDQgmr_qm0cVLIog], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@23b13dc6]\r\norg.elasticsearch.transport.RemoteTransportException: [Ch\'od][inet[/192.168.175.128:9301]][search/phase/query+fetch]\r\nCaused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [myindex_20131108][1]: query[filtered(child_filter[transaction/vendor](filtered(signed_date:[1373572895448 TO 1376164895448])->cache(_type:transaction)))->cache(_type:vendor)],from[0],size[20]: Query Failed [Failed to execute main query]\r\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:123)\r\n        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:306)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:686)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:1)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:45)\r\n        at org.elasticsearch.index.search.child.ChildrenConstantScoreQuery$ParentWeight.scorer(ChildrenConstantScoreQuery.java:178)\r\n        at org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:533)\r\n        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)\r\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:167)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:491)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:448)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)\r\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:119)\r\n        ... 7 more\r\n[2013-11-08 15:01:41,827][DEBUG][action.search.type       ] [Phage] All shards failed for phase: [query_fetch]\r\n```\r\n\r\n\r\nI debugged the code and found the issue happens due to class ChildrenConstantScoreQuery.java, line #115 is passing null shortCircuitFilter. This happens when remaining value > 8192 so it\'s not initialized \r\n```\r\n        Filter shortCircuitFilter = null;\r\n        if (remaining == 1) {\r\n            BytesRef id = collectedUids.v().iterator().next().value.toBytesRef();\r\n            shortCircuitFilter = new TermFilter(new Term(UidFieldMapper.NAME, Uid.createUidAsBytes(parentType, id)));\r\n        } else if (remaining <= shortCircuitParentDocSet) {\r\n            shortCircuitFilter = new ParentIdsFilter(parentType, collectedUids.v().keys, collectedUids.v().allocated);\r\n        }\r\n\r\n        ParentWeight parentWeight = new ParentWeight(parentFilter, shortCircuitFilter, searchContext, collectedUids);\r\n```\r\n\r\nThis is bug introduced after 0.90.1 as the query was running fine in past. Thanks!'
4134,'dakrone',"Make mmapfs the default store for 64-bit Linux\nIt appears that ElasticSearch wants to mimic Lucene's defaults in FSDirectory#open (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/store/IndexStoreModule.java#L51)\r\n\r\nLucene's defaults now include Linux in addition to Solaris and Windows when considering MMapDirectory (https://issues.apache.org/jira/browse/LUCENE-3198)\r\n\r\nElasticSearch should follow suit and, according to a lot of discussions on the mailing list, many are already doing this via configuration."
4130,'martijnvg','Query DSL: Wrong result on bool filter with \'must_not\' and \'geo_distance\'\nThe Bool Filter returns the wrong result when combining the \'must_not\' (or \'must\') part with a \'should\' containing a \'geo_distance\' filter.\r\n\r\nConsider following example:\r\n```\r\ncurl -XDELETE localhost:9200/test\r\ncurl -XPUT localhost:9200/test\r\ncurl -XPUT \'localhost:9200/test/test/_mapping\' -d \'{"test" : {"properties" : { "prop1" : {"type":"string"}, "location": {"type":"geo_point"}}}}\'\r\n\r\ncurl -XPUT \'localhost:9200/test/test/1\' -d \'{"prop1": "5", "location":{"lon":"0", "lat":"0"}}\'\r\ncurl -XPUT \'localhost:9200/test/test/2\' -d \'{"prop1": "5", "location":{"lon":"0", "lat":"0"}}\'\r\ncurl -XPUT \'localhost:9200/test/test/3\' -d \'{"prop1": "4", "location":{"lon":"12", "lat":"12"}}\'\r\n```\r\n\r\nExample query 1:\r\n```\r\n{\r\n"filter" : {\r\n    "bool": {\r\n        "must_not": [\r\n            {\r\n                "terms": {\r\n                    "_id": [\r\n                        "1"\r\n                    ]\r\n                }\r\n            }\r\n        ],\r\n        "should": [\r\n            {\r\n                "term": {\r\n                    "_name": "filter_requirements",\r\n                    "prop1": "5"\r\n                }\r\n            },\r\n            {\r\n                "geo_distance": {\r\n                    "_name": "filter_location",\r\n                    "distance": "6km",\r\n                    "location": {\r\n                        "lat": "12", \r\n                        "lon": "12"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}}}\r\n```\r\n\r\nThis returns only the document with the id 3. It should return both, 2 and 3 because 2 also matches the \'should\' part. Document with id 1 is excluded as expected.\r\n\r\nExample query 2:\r\n```\r\n{"filter" : {\r\n    "bool": {\r\n        "should": [\r\n            {\r\n                "term": {\r\n                    "_name": "filter_requirements",\r\n                    "prop1": "5"\r\n                }\r\n            },\r\n            {\r\n                "geo_distance": {\r\n                    "_name": "filter_location",\r\n                    "distance": "6km",\r\n                    "location": {\r\n                        "lat": "12",\r\n                        "lon": "12"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}}}\r\n```\r\n\r\nThis returns all 3 documents as expected. Note that the \'must_not\' part has been removed. Bool filter works as expected.\r\n\r\nExample query 3:\r\n```\r\n{"filter" : {\r\n    "bool": {\r\n        "must_not": [\r\n            {\r\n                "terms": {\r\n                    "_id": [                       \r\n                        "1"     \r\n                    ]\r\n                }\r\n            }\r\n        ],                       \r\n        "should": [                            \r\n            {                         \r\n                "term": {        \r\n                    "_name": "filter_requirements",\r\n                    "prop1": "5"   \r\n                }    \r\n            }    \r\n        ]\r\n    }\r\n}}}\r\n```\r\n\r\nThis will correctly return only the document with id 2. The example above does not show this.. and it\'s kinda late now. You can remove either the \'should not\' or the \'geo_distance\' and the query will execute correctly.'
4127,'costin','using JAVA_OPTS might fail in service.bat\nRelates to #4086 \r\n`JAVA_OPTS` and `ES_JAVA_OPTS` can trip `service.bat` if the former contains multiple parameters or/and the former contains parenthesis.\r\n'
4125,'colings86',"Changed pom to allow import and running from eclipse\nCurrently when importing projects into eclipse you need to run 'mvn\r\neclipse:eclipse' on the command line to generate the poject files. This\r\nmeans that when the pom changes you need to re-run the command on the\r\ncommand line to reflect those changes in the project in eclipse.  This\r\ncommit allows the developer to import the project as an existing maven\r\nproject (can be shared using git after import) and then allows the\r\napplication to be run inside eclipse using the .launch file in\r\n/dev-tools enabling easy debugging of the application within eclipse\r\nwithout requiring a maven build.  Changes are addition of a lauch file and update of the eclipse-lifecycle-plugin so do not affect the normal build or any other IDEs"
4124,'kimchy','_default_ mapping not applied when using separate master/data nodes\nIn 0.90.6 and 0.90.5, \\_default\\_ mappings will not be applied to newly created types if your cluster topology has separate master and data nodes.\r\n\r\nWorking case (all nodes are master/data eligible) with a two node cluster:\r\n\r\n\r\n    # create index \r\n    curl -XPUT http://localhost:9200/test-index\r\n    # create _default_ mapping\r\n    curl -XPUT http://localhost:9200/test-index/_default_/_mapping -d\'{"_default_": {"_timestamp": {"enabled": true}}}\'\r\n    # verify contents\r\n    curl http://localhost:9200/test-index/_default_/_mapping\r\n    # >> {"_default_":{"_timestamp":{"enabled":true},"properties":{}}}\r\n\r\n    # add a new type\r\n    curl -XPUT http://localhost:9200/test-index/new-mapping/_mapping -d\'{"properties": {"foo": "string"}}\'\r\n    #verify contents have _default_ applied\r\n    curl http://localhost:9200/test-index/new-mapping/_mapping\r\n    # >> {"new-mapping":{"_timestamp":{"enabled":true},"properties":{}}}\r\n\r\nBroken case (master/data nodes separate) with a two node cluster:\r\n\r\nSet each node with appropriate settings:\r\n\r\n    # node1\r\n    node.master: true\r\n    node.data: false\r\n    \r\n    # node2\r\n    node.master: false\r\n    node.data: true\r\n\r\nThe same set of commands yield a mapping with defaults applied:\r\n\r\n    curl -XPUT http://localhost:9200/test-index\r\n    curl -XPUT http://localhost:9200/test-index/_default_/_mapping -d\'{"_default_": {"_timestamp": {"enabled": true}}}\'\r\n    curl http://localhost:9200/test-index/_default_/_mapping\r\n    # >> {"_default_":{"_timestamp":{"enabled":true},"properties":{}}}\r\n\r\n    curl -XPUT http://localhost:9200/test-index/new-mapping/_mapping -d\'{"properties": {"foo": "string"}}\'\r\n    curl http://localhost:9200/test-index/new-mapping/_mapping\r\n    # >> {"new-mapping":{"properties":{}}}\r\n\r\n\r\n'
4123,'javanna','Posting highlighter highlight fields with the same name across all document objects when path:just_name is used\nhttps://gist.github.com/roytmana/f34d71bb80f8bd8f9eea'
4122,'javanna','Added support for external query in postings highlighter\nIt is now possible to highlight an external query using the postings highlighter, relates to #3630\r\n\r\nCloses #4121'
4121,'javanna',"Postings highlighter to support query other than the search query\nPlain and fast vector highlighter can highlight based on a query which is not the search query, called `highlight_query`, introduced in #3630. Postings highlighter doesn't support the external query yet and always highlights based on the search query."
4119,'martijnvg','Rename filter param to search api "post_filter"\nUsers tend to use the top-level `filter` param for general filtering, instead of using a `filtered` query.\r\n\r\nIt should be renamed to `post_filter` to dissuade users from using it unless they understand what it is for and `filter` should be deprecated.\r\n\r\nThe top level `filter` is still valid field for backwards compatibility, but it will removed in the future.'
4118,'javanna','Deprecate partial_fields in the search API\nNow that we have `_source`, `_source_include`, `_source_exclude`, we no longer need `partial_fields` http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-fields.html#partial'
4117,'markharwood',"Change ignore_malformed to malformed\nWhen a value is indexed into a numeric field, it can either be:\r\n\r\n* an allowed numeric value\r\n* a string which can be cast to a number\r\n* a float which can be cast to an integer, or rounded down to the correct numeric range\r\n* an illegal value\r\n\r\nCurrently we have `ignore_malformed`.  I suggest changing this to `malformed` which can accept:\r\n\r\n* `cast` - the default - casts when it can but throws an error when it can't\r\n* `ignore` - casts when it can, but silently ignores errors\r\n* `strict` - throws an error unless the value is a valid number which fits the field type\r\n"
4116,'javanna','Postings highlighter doesn\'t return snippets when using "path":"just_name"\nThe postings highlighter doesn\'t return snippets when the field name in the index is different from the one specified in the search request. That happens either using `"path":"just_name"` or using a custom `index_name` in the mapping.'
4115,'javanna','Refactored indices aliases api to make use of the new ack mechanism\nRefactored indices aliases api to make use of the new recently introduced generic ack mechanism\r\n\r\nCloses #4114'
4114,'javanna','Move indices aliases api to new acknowledgement mechanism\nMove indices aliases api to new acknowledgement mechanism introduced in #3786 .'
4113,'kimchy','External method to set rootTypeParsers in DocumentMapperParser incorrect\nlooks like there is small bug in putRootTypeParser method in DocumentMapperParser class:\r\n\r\nmethod:\r\n``` java\r\n    public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {\r\n        synchronized (typeParsersMutex) {\r\n            rootTypeParsers = new MapBuilder<String, Mapper.TypeParser>()\r\n                    .putAll(typeParsers)\r\n                    .put(type, typeParser)\r\n                    .immutableMap();\r\n        }\r\n    }\r\n```\r\n\r\nshould look more like this:\r\n``` java\r\n    public void putRootTypeParser(String type, Mapper.TypeParser typeParser) {\r\n        synchronized (typeParsersMutex) {\r\n            rootTypeParsers = new MapBuilder<String, Mapper.TypeParser>(rootTypeParsers)\r\n                    .put(type, typeParser)\r\n                    .immutableMap();\r\n        }\r\n    }\r\n```\r\n\r\ncurrent version when called will override root type parsers with type parsers\r\n'
4112,'kimchy','Unable to create a nested filtered alias on a dataless master\nWe set-up a two node cluster the first node is a master and contains no data.  The second node has data but is not master eligible.  Both a running 0.90.0. Also, retested on 0.90.6.\r\n\r\nFirst I create an index with an nested document:\r\n\r\ncurl -X POST http://masternode:9200/index_a -d \'{\r\n    "mappings" : {\r\n       "type_a" : {\r\n          "properties" : {\r\n            "table_a" : { "type" : "nested", "properties" : {\r\n              "field_a" : { "type" : "string" },"field_b" : { "type" : "string" }\r\n            }}\r\n          }\r\n       }\r\n    }\r\n}\r\n\'\r\n\r\nReturns: {"ok":true,"acknowledged":true}\r\n\r\nThen I try to create the filter alias:\r\n\r\n\r\ncurl -X POST http://datanode:9200/_aliases -d \'\r\n{\r\n    "actions" : [\r\n        { "add" : { "index" : "index_a", "alias" : "alias_a" , "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "nested": {\r\n                "path": "table_a",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "term": {\r\n                          "table_a.field_a": "y"\r\n                        }\r\n                      }\r\n                    ]\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }} }\r\n    ]\r\n}\'\r\n\r\nReturns: {"error":"RemoteTransportException[[dlesmaster01][inet[/10.13.10.38:9300]][indices/aliases]]; nested: ElasticSearchIllegalArgumentException[failed to parse filter for [alias_a]]; nested: QueryParsingException[[index_a] [nested] failed to find nested object under path [table_a]]; ","status":400}\r\n\r\nIf I submit it to the master node like this:\r\n\r\ncurl -X POST http://masternode:9200/_aliases -d \'\r\n{\r\n    "actions" : [\r\n        { "add" : { "index" : "index_a", "alias" : "alias_a" , "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "nested": {\r\n                "path": "table_a",\r\n                "query": {\r\n                  "bool": {\r\n                    "must": [\r\n                      {\r\n                        "term": {\r\n                          "table_a.field_a": "y"\r\n                        }\r\n                      }\r\n                    ]\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }} }\r\n    ]\r\n}\'\r\n\r\nReturns: {"error":"ElasticSearchIllegalArgumentException[failed to parse filter for [alias_a]]; nested: QueryParsingException[[index_a] [nested] failed to find nested object under path [table_a]]; ","status":400}\r\n\r\nThese all work when the elected master also contains the data.  '
4111,'javanna','Indices query/filter skip parsing altogether for irrelevant indices\nFixes #2416'
4110,'javanna','Postings highlighter does not highlight majority of the results while plain highlighter works as expected\nplease refer to #4103 for some background\r\n\r\nmajority of my results now do not get highlighted with postings even with a simple match query.\r\nEverything gets highlighted with plain highlighter\r\n \r\nI was not able to recreate it but here are some materials:\r\n\r\nrecreation that does not show the issue with few mappings similar to my real ones https://gist.github.com/roytmana/7336502\r\n\r\nmy real mapping https://gist.github.com/roytmana/7330531\r\n\r\nsample query https://gist.github.com/roytmana/7337278\r\nbut it is the same with query_string and match/OR\r\n'
4108,'javanna',"Index time boost in multi_field ignored?\nI use multi_field to index content of multiple properties into a single _all-like multi_field. Each contributing property may define different boost when defining it's instance of the multi_field. these boosts are however ignored when searching against such multi_fied while honored while searching against _all\r\n\r\nfull recreation is here https://gist.github.com/roytmana/7330956\r\n\r\nit compares the same query against my multi_field all and ES _all"
4106,'javanna',"Highlight hit object cache gets reset per hit\n`FetchSubPhase.HitContext` exposes a cache that allows to reuse objects which can be shared between different hits, when executing subfetch phases. \r\n\r\nThis is meant to be used especially in combination with highlighting, as some of the objects are heavy to create and don't need to be recreated per hit.\r\n\r\nThis mechanism is not working properly, as `HitContext` gets recreated per hit, thus the cache is always empty and the objects are never reused."
4105,'bleskes','Custom rewrite methods\nIt would be nice if ES accepts custom rewrite methods. Currently only a limited number of rewrite methods are supported.\r\n\r\nMy suggestion is to register a rewrite method with a name, so that you can use that method later on in, for instance, a match-query.\r\n\r\nBackground: I would like to have a dis_max rewrite method with a tie-breaker'
4103,'javanna',"Postings highlighter wrong highlighting\nAs reported as a comment in #4042, the postings highlighter has a weird behaviour, it seems like it remembers the offsets of the previously highlighted documents. Well, what happens is that it actually highlights the right text against the wrong offsets, because of most likely the silliest mistake one can make working with lucene, that is using doc ids that are relative to the segment they belong as they were unique instead of the doc ids that contain the segment offset too.\r\nSurprising that this wasn't raised by our tests though, as we have a pretty good coverage for the postings highlighter. Will improve that too."
4094,'dakrone',"indices.recovery.concurrent_small_file_streams is not dynamically settable\n`indices.recovery.concurrent_small_file_streams` has all the code to be dynamically settable, but hasn't been added to the `ClusterDynamicSettingsModule`."
4093,'s1monw','0.90.6 memory issue\n0.90.6 needs a lot more direct memory compared to 0.90.5 when using an in memory store for tests.\r\n\r\nSetting needed for 0.90.6\r\n```\r\n-XX:MaxDirectMemorySize=4608m\r\n```\r\n\r\nSetting needed for 0.90.5\r\n```\r\n-XX:MaxDirectMemorySize=512m\r\n```\r\n\r\n'
4091,'javanna','Fix missing affectation in SimpleChildQuerySearchTests.testParentChildQu...\nFix missing affectation in `SimpleChildQuerySearchTests.testParentChildQueriesCanHandleNoRelevantTypesInIndex()`.\r\nThe test continued to assert against the result of the first search request.\r\n\r\nFortunately, this hid no failing assertions.'
4089,'javanna','River does not start\nIn 0.90.6, creating a river seems not to start the river.\r\n\r\nExample debug log (for JDBC river)\r\n\r\nhttps://gist.github.com/jprante/7321028\r\n\r\nWhat am I doing wrong?'
4088,'drewr','Add /_cat/help endpoint\nWould be great to have a unixy usage output, eg:\r\n\r\n    GET /_cat/help  # or GET /_cat\r\n\r\nand, if useful:\r\n\r\n    GET /_cat/help/recovery'
4086,'costin','Fixed two problems with JAVA_OPTS handling\n'
4084,'chilling','Change GeoBBox to be consistent with other implementations\nApparently in the rest of the geo world, people expect to specify `bottom-left` and `top-right`, instead of `top-left` and `bottom-right`.\r\n\r\nShould change to be consistent (while still supporting the old params for bwc)'
4083,'markharwood',"Make 'length' parameters consistent\n * `flt`, `flt_field`, `fuzzy`, and `match` queries use `prefix_length`\r\n * `query_string` query uses `fuzzy_prefix_length`\r\n * `completion` suggested uses `min_length` and `prefix_length`\r\n\r\nBut\r\n\r\n* `mlt_field_query` and `mlt_query` use `min_word_len` and `max_word_len`\r\n* `completion` suggester uses `max_input_len`\r\n* `phrase` and `term` suggesters use `min_word_len` and `prefix_len`\r\n\r\nChange all length parameters to ...`_length`\r\n"
4082,'s1monw','Rename fuzziness/min_similarity to edit_distance\nCurrently we have:\r\n\r\n * `flt` | `flt_field` have `min_similarity`\r\n * `fuzzy` has `min_similarity`\r\n * `query_string` has `fuzzy_min_sim`\r\n * `match` has `fuzziness`\r\n * the completion suggester uses `{ fuzzy: { edit_distance: 2}}`\r\n\r\nFuzziness in Elasticsearch refers to edit-distance, which can be set to 0,1 or 2.  \r\n\r\n`min_similarity` accepts a float value between 0 and 1, but now gets converted to an edit distance based on word length. eg a word of two characters with an edit distance of 2 would match any other word of length 2.\r\n\r\n I would suggest renaming `fuzziness` and `min_similarity` to `edit_distance` everywhere. It should accept 0,1,2 and `auto`, which sets the edit_distance to 1 for words of 1..3 characters, and 2 for words of 4 characters or more.\r\n\r\nThe only fly in the ointment is the `fuzzy` query which also handles fuzzy numbers and dates, which have nothing to do with edit distance.  See proposed deprecation in #4076 \r\n'
4080,'bleskes','Fixes #4047 - Empty objects are not stored in _source when an include/ex...\n...clude list is present\r\n\r\nCloses #4047 \r\n\r\nThis was originally being tracked by pull request #4048.  I closed that pull request because it was sloppy.\r\n\r\nI am uncertain about the behavior for handling includes/excludes with empty objects and wildcards, so I added tests to cover the following cases, but please let me know if you think the results are incorrect...\r\n\r\nGiven\r\n```\r\n{\r\n  "object1": { },\r\n  "object2": { }\r\n}\r\n```\r\n\r\nAnd a mapping\r\n```\r\n{\r\n  ...\r\n  {\r\n    "_source" : {\r\n      "excludes": "object1.*,\r\n      "includes": "object2.*"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nResults in a filtered source of\r\n```\r\n{\r\n  "object1": { },\r\n}\r\n```\r\n\r\nHowever, if given\r\n```\r\n{\r\n  "object1": {\r\n    "key1": "value1"\r\n   },\r\n  "object2": {\r\n    "key2": "value2"\r\n  }\r\n}\r\n```\r\n\r\nand the same mapping, the filtered source would be\r\n```\r\n{\r\n  "object2": {\r\n    "key2": "value2"\r\n  }\r\n}\r\n```'
4079,'jpountz','Warmers: Dedicated Norms/Terms warm options in mappings\nSimilar to the loading part in `fielddata`, where someone can configure if they are loaded eagerly or not, we should have the same option in mappings for `norms`, and for `terms`.\r\n\r\nIdea regarding naming, within a field:\r\n\r\n```\r\n{\r\n   "terms" : {"loading" : "lazy|eager"},\r\n    "norms" : {"enabled" : false|true, "loading" : "lazy|eager"}\r\n}\r\n```\r\n\r\nNote, the above will deprecate the `omit_norms` field mapping, and follows similar structure to the `fileddata` mapping config.'
4074,'martijnvg','Make search APIs consistently accept a query param\nThe `search` and `explain` APIs both accept a `query` parameter in the body, while `count` and `validate_query` expect the body to be the query itself.\r\n\r\nWe should change the `count` and `validate_query` APIs to accept a `query` parameter.  Possibly even change the `count` request to use the same code as for `_search` (with `search_type=count`).'
4071,'brwe','Make indices APIs consistent\nAn index accepts settings, mappings, warmers and aliases.  However, some use singular and some use plural, eg:\r\n\r\nMappings:\r\n\r\n    PUT /index/type/_mapping\r\n    { type: { .... }}\r\n\r\n    GET /index/{type|*|blank}/_mapping\r\n\r\nSettings:\r\n\r\n    PUT /index/_settings\r\n    GET /index/_settings\r\n\r\nWarmers:\r\n\r\n    PUT /index/_warmer/name\r\n    { ... }\r\n    GET /index/_warmer/{name|*|blank}\r\n\r\nAliases:\r\n\r\n    POST /_aliases\r\n    { actions: [....] }\r\n\r\n     PUT /index/_alias/alias\r\n     { ... }\r\n\r\n     GET /{index|*|_all}/_alias/{alias|*}\r\n\r\nThere are a couple of possibilities:\r\n\r\nDistinguish between singular and plural when PUT/POSTing\r\n\r\nCreate/update a mapping, warmer or alias (not settings) with: \r\n\r\n    PUT /index/_mapping|_alias|_warmer/{name}\r\n    { ... }\r\n\r\nThis breaks bwc for mapping (`/index/type/_mapping` vs `/index/_mapping/type`) - possibly support both variants for all actions?\r\n\r\nCreate/update mappings, warmers, aliases, settings with:\r\n\r\n    POST /index/_mappings|_settings|_warmers|_aliases\r\n    { name: {....}, name2: {....} ... }\r\n\r\nRetrieve via singular or plural:\r\n\r\n    GET /index/_mapping|_alias|_warmer|_setting/{name|*|_all|blank}\r\n    GET /index/_mappings|_aliases|_warmers|_settings/{name|*|_all|blank}\r\n\r\nAlternatively, make singular/plural synonyms, so either can be used in any of the above examples\r\n\r\n\r\nOne final thing, all indices can be specified as `_all` or `*` (depending on the API), but eg warmers and aliases only understand wildcards.  I think `_all` should be an acceptable alias for `*`for all of them\r\n\r\n'
4069,'colings86',"Add GET /index API\nWhile we have the `index exists` API:\r\n\r\n    HEAD /index\r\n\r\nand we have:\r\n\r\n    GET /index/_aliases\r\n    GET /index/_mapping\r\n    GET /index/_settings\r\n    GET /index/_warmer\r\n\r\nWe don't have a `GET /index` API which would return all info about the index (ie aliases, settings, mappings, warmers) which could be used to recreate the index with a different name or on a different cluster.\r\n\r\n"
4066,'clintongormley','Rest API: Ensure 503 signals ==  retry on another node\nThe `503` status code is used by the clients to signal "I can\'t handle this for some reason, but you can retry on another node"\r\n\r\nFor instance, if a node can\'t see the minimum master nodes, then it returns a 503, in which case the clients should try another node in the list.  \r\n\r\nAt times, 503 has been used for other types of responses (eg no indices available to search on).  Worth going through the codebase to check that 503s are used consistently.'
4065,'spinscale','Changes to the cluster state API\nThe cluster-state API by default returns all info, and requires the user to specify info that they DON’T want to see. This is inconsistent and difficult to remember.\r\n\r\nThe following filters can be specified:\r\n\r\n* `filter_blocks`\r\n* `filter_index_templates`\r\n* `filter_indices`\r\n* `filter_metadata`\r\n* `filter_nodes`\r\n* `filter_routing_table`\r\n\r\nChange the path to:\r\n\r\n    /_cluster/{metric}/state/{indices}\r\n\r\nWhere:\r\n\r\n* `metric` is blank or a comma separated list of: `blocks`, `index_templates`, `metadata`, `nodes`, `routing_table`\r\n* `indices` is blank or `_all` or  a comma-separated list of indices\r\n'
4062,'imotov','Automatic script reload\nAdd ability to automatically reload scripts in the `config/scripts` directory as they changes.\r\n\r\nThe `config/scripts` directory will be scanned periodically for changes. New and changed scripts will be reloaded and deleted script will be removed from preloaded scripts cache. The reload frequency can be specified using `watcher.interval` setting, which defaults to `60s`. To disable script reloading completely set `script.auto_reload_enabled` to `false`.'
4060,'bleskes','Add some documentation indicating when you would use _source\n'
4059,'dadoonet',"Allow for storage and indexing of computed field\nI'd like to be able to store a computed field. Right now the only option is to query against it is to compute it and explicitly store it in my source document. E.g. I'm syncing my docs from mongodb with a river and would like to boost using the length of an array in the document, but I have to update all my docs in mongodb with the array length in order to do that. I would rather be able to have elasticsearch compute and store the field so that I do not have to update my docs in mongodb.\r\n\r\nI've seen others asking for this as well:\r\nhttps://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/computed/elasticsearch/ro187eatzIo/lTrl4sFSJqoJ"
4057,'spinscale','Changes to the nodes stats API\nRecommendations:\r\n\r\nRemove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.\r\n\r\nChange the path format to:\r\n\r\n    /_nodes/{nodes}/stats/{metric}\r\n\r\nwhere:\r\n\r\n* `{nodes}`: comma-delimited list of nodes or blank\r\n* `{metric}` is blank or comma-delimited list of metrics\r\n\r\nMetrics can be any of:\r\n\r\n* `http`\r\n* `jvm`\r\n* `network`\r\n* `os`\r\n* `process`\r\n* `thread_pool`\r\n* `transport`\r\n* `fs`\r\n* `indices`\r\n\r\nIf `indices` is specified in the metrics list, then the path expands to accept:\r\n\r\n    /_nodes/{nodes}/{metrics}/stats/indices,fs,jvm/{index-metrics}\r\n\r\nwhere:\r\n\r\n* `{index}` is a comma-delimited list of indices, `_all`, or blank\r\n* `{index-metrics}` is a comma-delimited list of the metrics specified in #4054 or blank for all\r\n\r\nand the query-string options accepted by indices statistics in #4054 are also accepted here\r\n\r\n'
4056,'javanna','Add wabisabi to Scala clients.\n'
4055,'spinscale','Changes to the nodes info API\nRecommendations:\r\n\r\nRemove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.\r\n\r\nChange the path format to:\r\n\r\n    /_nodes/{nodes}/{metric}\r\n\r\nwhere:\r\n\r\n* `{nodes}`: comma-delimited list of nodes or blank\r\n* `{metric}` is blank or comma-delimited list of metrics\r\n\r\nMetrics can be any of:\r\n\r\n* http\r\n* jvm\r\n* network\r\n* os\r\n* plugin\r\n* process\r\n* settings\r\n* thread_pool\r\n* timeout\r\n* transport\r\n\r\n\r\nNOTE: Node names and metric names could clash, if you name your nodes eg `process`.  Should probably accept `_all` in the `{nodes}` parameter.\r\n'
4054,'spinscale','Changes to the indices stats API\nRecommendations:\r\n\r\nRemove the `all` and `clear` flags. Default to return `all` by default, unless specific metrics specified.\r\n\r\nChange the path format to:\r\n\r\n    /{index}/_stats/{metric}\r\n\r\nwhere:\r\n\r\n* `{index}`: comma-delimited list of indices, `_all`, or blank\r\n* `{metric}` is blank or comma-delimited list of metrics\r\n\r\nMetrics can be any of:\r\n\r\n* `completion`\r\n* `docs`\r\n* `fielddata`\r\n* `filter_cache`\r\n* `flush`\r\n* `get`\r\n* `id_cache`\r\n* `indexing`\r\n* `merge`\r\n* `refresh`\r\n* `search`\r\n* `store`\r\n* `warmer`\r\n\r\nFour metrics take optional parameters, which can be specified in the query string.  These are:\r\n\r\n* completion: `fields` or `completion_fields`\r\n* fielddata: `fields` or `fielddata_fields`\r\n* indexing: `types`\r\n* search: `groups`\r\n\r\nChange `level` to accept `cluster`, `indices` and `shards`, as per the cluster health API.  Default to `indices`.  If set to `cluster`, only return the `_all` stats.\r\n\r\nAdd the `translog` info from the index status API into the shard-level info of the indices stats API.'
4053,'kimchy','URI routing parameter does not work with Bulk API\nWhen I specify the routing in BULK requests, it does not import all documents to the same shard I specify. It does work when I index 1 document at a time, or when I search. Is this a bug in the Bulk API?\r\n\r\ncurl -XPOST \'http://192.168.1.115:9200/_bulk?routing=a\' -d \'\r\n{ "index" : { "_index" : "articles", "_type" : "article", "_id" : "1" } }\r\n{ "title" : "value1" }\r\n{ "delete" : { "_index" : "articles", "_type" : "article", "_id" : "2" } }\r\n{ "create" : { "_index" : "articles", "_type" : "article", "_id" : "3" } }\r\n{ "title" : "value3" }\r\n{ "update" : {"_id" : "1", "_type" : "article", "_index" : "index1"} }\r\n{ "doc" : {"field2" : "value2"} }\''
4052,'javanna','Added support for highlighting multi term queries using the postings highlighter\nAdded rewrite of the query using the current top level reader instead of an empty one, rewritten all multi term queries using `TopTermsScoringBooleanQueryRewrite(1024)` rewrite mode.\r\n\r\nAs a result, postings highlighter supports now wildcard query, prefix query, fuzzy query, term range query and regexp query.\r\n\r\nCloses #4042'
4051,'javanna',"Finalize acknowledgements work, move over all the api, strenghten ack api and remove boiler plate code\nAll the apis that modify the cluster state and already supported acknowledgement but used the old mechanism with custom notifications have been migrated to the new generic ack mechanism introduced in #3786 : indices aliases, open/close index, delete index, put mapping, create index.\r\n\r\nA MetaDataService that actually submits the cluster state update task has been introduced in cluster reroute, cluster update settings, put/delete warmer (similar to what other apis already do) rather than making changes directly in the `Transport*Action`. The `AcknowledgedRequest` gets converted to an internal `ClusterStateUpdateRequest` (or subclass) that is used for the cluster state update which returns a `ClusterStateUpdateResponse` (or subclass), that needs to be converted back to `AcknowledgedResponse`.\r\n\r\nIntroduced a `TransportClusterStateUpdateAction` base class that allows to share code and avoid repeating boiler plate code in each action. Made `AckedClusterStateUpdateTask` an abstract class that contains the implementation for the methods that are always the same all over the place (similar to #4001). Unified the rest layer too to parse the `timeout` and `master_timeout` parameters in a single place.\r\n\r\nThe `TransportDeleteIndexAction` is the only one that has not been moved over to `TransportClusterStateUpdateAction` as it needs some rework: it currently performs multiple cluster state updates, one per index, and there's a metadata lock involved per each index.\r\n\r\nAlso, I wonder if we should go ahead and add ack support in put/delete index template as they both already support all the needed parameters, which are currently pointless (timeout ignored, acknowledged always true)."
4050,'drewr','Add _cat/health\nNeed health one-liner.'
4047,'bleskes','Empty objects are not stored in _source when an include/exclude list is present\n```\r\ncurl -XDELETE localhost:9200/test\r\ncurl -XPUT localhost:9200/test/test/1 -d \'{ "empty": {}, "not_empty": { "key": "value" } }\'\r\ncurl localhost:9200/test/test/1/_source\r\n```\r\n\r\nReturns ```{ "empty": {}, "not_empty": { "key": "value" } }``` as expected.\r\n\r\n\r\n```\r\ncurl -XDELETE localhost:9200/test\r\ncurl -XPUT localhost:9200/test\r\ncurl -XPUT localhost:9200/test/test/_mapping -d \'{ "test": { "_source" : { "excludes": [ "ignored" ] } } }\'\r\ncurl -XPUT localhost:9200/test/test/1 -d \'{ "empty": {}, "not_empty": { "key": "value" } }\'\r\ncurl localhost:9200/test/test/1/_source\r\n```\r\n\r\nReturns ```{"not_empty":{"key":"value"}}``` which is not expected.'
4045,'imotov',"Error when using aliases on a MoreLikeThisQuery\nthere was a commit that actually fixed that: https://github.com/imotov/elasticsearch/commit/963468cc5c751c49e8fb0494b1578d1382014656 , but  it missed doing that on redirect(that's why this only happens when using a NodeClient to execute the query).\r\n\r\nanyway, doing the same here https://github.com/elasticsearch/elasticsearch/blob/4fa8f6f61f886446a09e3324a74e146ab73b1d0a/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java#L239 fixes the issue. or at least i couldn't reproduce it anymore."
4044,'chilling','ContextSuggester\nThis commit extends the `CompletionSuggester` by context\r\ninformations. In example such a context informations can\r\nbe a simple string representing a category reducing the\r\nsuggestions in order to this category.\r\n\r\nThree base implementations of these context informations\r\nhave been setup in this commit.\r\n\r\n- a Category Context\r\n- a Geo Context\r\n- a Field Context\r\n\r\nAll the mapping for these context informations are\r\nspecified within a context field in the completion\r\nfield that should use this kind of information.\r\n\r\n## Mapping Example\r\n\r\nThe following example shows the mapping for a GeoContext.\r\n\r\n    {\r\n        "testType":{\r\n            "properties":{\r\n                "testField":{\r\n                    "type":"completion",\r\n                    "index_analyzer":"simple",\r\n                    "search_analyzer":"simple",\r\n                    "payloads":true,\r\n                    "preserve_separators":false,\r\n                    "preserve_position_increments":true,\r\n                    "context":{\r\n                        "geo":{\r\n                            "separator":"|",\r\n                            "precision":8,\r\n                            "neighbors":true\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n## Indexing\r\n\r\nDuring indexing a document the subfield `context` of the\r\ncompletion field contains the data to be used in order\r\nto provide suggestions.\r\n\r\n    {\r\n        "testField":{\r\n            "input":[["pizza - berlin","pizza","food"]],\r\n            "context":"u33dc1v0xupz"\r\n        }\r\n    }\r\n\r\n## Suggestion Example\r\n\r\nThe Suggestion request is extended by a context value. The\r\nsuggest request for a geolocation looks like\r\n\r\n    {\r\n        "suggest":{\r\n            "text":"pizza",\r\n            "completion":{\r\n                "field":"testField",\r\n                "size":10,\r\n                "context":{\r\n                    "geo":"u33dc0cpke4q"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nThe context objects contains a field with the same name as\r\ndefined in the mapping. According to the type of the context\r\nthis field contains the data associated with the suggestion\r\nrequest. In this example the geohash of a location.\r\n\r\n## Category Context\r\n\r\nThe simplest way to use this feature is a category context. It\r\nsupports a arbitrary name of a category to use with the completion\r\nsuggest API.\r\n\r\nTo set the context support to the category type this option must be\r\nset to `true`:\r\n\r\n    "testField":{\r\n        "type":"completion",\r\n        "context":{\r\n            "category": true\r\n        }\r\n    }\r\n\r\nThe name of the context category then needs to be set within the\r\nsuggestion context during indexing:\r\n\r\n    {\r\n        "testField":{\r\n            "input":[["pizza - berlin","pizza","food"]],\r\n            "context":"delivery"\r\n        }\r\n    }\r\n\r\nand can be used by setting the category value:\r\n\r\n    {\r\n        "suggest":{\r\n            "text":"pizza",\r\n            "completion":{\r\n                "field":"testField",\r\n                "size":10,\r\n                "context":{\r\n                    "category":"delivery"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n\r\n## Field Context\r\n\r\nThe Field Context works like the category context but the value of this\r\nwill context will not explicitly be set. It refers to another field in\r\nthe document. In example a `category` field.\r\n\r\n    {\r\n        "category":{\r\n            "type": "string"\r\n        },\r\n        "testField":{\r\n            "type":"completion",\r\n            "context":{\r\n                "field": "category"\r\n            }\r\n        }\r\n    }\r\n\r\nfor indexing the field context must be set to true:\r\n\r\n    {\r\n        "category":"delivery",\r\n        "testField":{\r\n            "input":[["pizza - berlin","pizza","food"]],\r\n            "context":true\r\n        }\r\n    }\r\n\r\nand suggestions use the `context.field` value\r\n\r\n    {\r\n        "suggest":{\r\n            "text":"pizza",\r\n            "completion":{\r\n                "field":"testField",\r\n                "size":10,\r\n                "context": {\r\n                    "field": "delivery"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\n\r\n## Geo Context\r\n\r\nThe last context feature is the GeoContext. It take a location into account.\r\nFor example if one searches for delivery services it might be use full to find\r\nresults around the location the query was sent. This context internally works\r\non geohashes only but the REST API allows any form defined for `geo_points`\r\n\r\nIn the mapping this kind of context is configured by two parameters:\r\n\r\n- precision\r\n- neighbors\r\n\r\nThe `precision` option is used to configure the range of result. If the\r\n`neighbors` option is enabled not only the given geohash cell will be used\r\nbut also all it\'s neighbors. \r\n\r\n    "context":{\r\n        "geo":{\r\n            "separator":"|",\r\n            "precision":8,\r\n            "neighbors":true\r\n        }\r\n    }\r\n\r\nThe context during indexing is set to the location of the input:\r\n\r\n    {\r\n        "testField":{\r\n            "input":[["pizza - berlin","pizza","food"]],\r\n            "context": "u33dc1v0xupz"\r\n        }\r\n    }\r\n\r\nTo get a ist of suggestions around a specific area the `context.geo` field\r\nmust contain the position of this area:\r\n\r\n    {\r\n        "suggest":{\r\n            "text":"pizza",\r\n            "completion":{\r\n                "field":"testField",\r\n                "size":10,\r\n                "context": {\r\n                    "geo": "u33dc0cpke4q"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nCloses #3959'
4042,'javanna','Postings highlighter does not support wildcards\nCurrently postings highlighter does not support wildcards and I suspect prefix queries as well - it would only highlight complete terms. \r\n\r\nWould it be possible to make it support at least trailing wildcards and prefix queries such as photo* \r\nif not possible, the documentation should state that'
4040,'martijnvg',"Fixed bug that document with a suggest field can't be percolated.\nThe CompletionTokenStream doesn't properly forward the call to its attributes, so when the percolator needs to access terms of this stream, null was returned and this isn't expected in the MemoryIndex.\r\n\r\nRelates to #4028"
4037,'bleskes','[DOC] Improve documentation on search stats groups\nDocument the ability to return all search statistics groups and provide examples of returning search statistics for groups.'
4035,'aleph-zero','Add recovery stats to index stats API\nToday, the index stats API is cleaner and can replace almost fully the index status API, what its missing is the recovery stats part. I suggest adding the recovery stats API to the index stats API as an option.'
4034,'martijnvg',"Deprecate `numeric_range` filter, add a field data execution option to range filter\n`numeric_range` is very confusing, at the end, the question is if range filter should be done using the index itself, or using field data (in memory). I suggest deprecating `numeric_range`, and adding `execution` field to `range` filter, with possible values of `index` and `fielddata`. The `numeric_range` filter will be removed in the next major version after 1.0.\r\n\r\nIf its set to `fielddata`, we simply try and execute it using it (and if a mapper doesn't implement it, we simply fail the execution with unsupported exception)."
4033,'dakrone','Remove text and field queries\n`text` query has been replaced by `match` query and been deprecate for some time.\r\n\r\n`field` query is very confusing, its effectively the Lucene query string syntax on a specific field, but its not evident from the name (can be confuse with `match`). It would be much better to remove it, and ask people to use `query_string` with a properly set default field. This is much more explicit regarding what it does.'
4032,'s1monw',"Add suggest stats\nWe have the suggest API, but we don't have specific statistics for the suggest API (how long it took to execute it on each shard, similar to query phase in search, or percolation).\r\n\r\nPotentially, this can be safely backported to 0.90 as well."
4030,'aleph-zero',"_cat/recovery API should also show relocations\nIt'd be super handy to also show the progress of relocation operations in the `_cat/recovery` output."
4029,'spinscale',"Move systemd files from /etc to /usr/lib\nAs documented in systemd's manual pages tmpfiles.d(5) and systemd.unit(5),\r\na package should install its default configuration in /usr/lib, which can\r\nbe overridden by system administrators in /etc.\r\n\r\nNew locations in the rpm:\r\n/usr/lib/systemd/system/elasticsearch.service\r\n/usr/lib/tmpfiles.d/elasticsearch.conf\r\n\r\nThe man pages are available online:\r\nhttp://www.freedesktop.org/software/systemd/man/tmpfiles.d.html\r\nhttp://www.freedesktop.org/software/systemd/man/systemd.unit.html#Unit%20Load%20Path"
4028,'martijnvg','NPE when percolating a document that contains a completion field\nAlthough it might not make much sense to percolate a document containing a completion field, that\'s what you end up doing if you percolate while indexing, and your mapping contains a completion field. When adding the completion field to the memory index a NullPointerException is thrown. \r\n\r\nThis happens with the 0.90 branch. Happens also with master, at least percolating an existing document.\r\n\r\nHere is the recreation:\r\n```\r\ncurl -XPUT localhost:9200/hotels -d \'\r\n{\r\n  "mappings": {\r\n    "hotel" : {\r\n      "properties" : {\r\n        "name" : { "type" : "string" },\r\n        "city" : { "type" : "string" },\r\n        "name_suggest" : {\r\n          "type" :     "completion"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XGET localhost:9200/hotels/hotel/_percolate -d \'{\r\n    "doc" : {\r\n        "name" :         "Mercure Hotel Munich",\r\n        "city" :         "Munich",\r\n        "name_suggest" : "Mercure Hotel Munich"\r\n    }\r\n}\r\n\'\r\n```\r\n\r\nHere is the stacktrace:\r\n\r\n```\r\njava.lang.RuntimeException: java.lang.NullPointerException\r\n\tat org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:463)\r\n\tat org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:370)\r\n\tat org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:450)\r\n\tat org.elasticsearch.index.percolator.PercolatorExecutor.percolate(PercolatorExecutor.java:422)\r\n\tat org.elasticsearch.index.percolator.PercolatorService.percolate(PercolatorService.java:111)\r\n\tat org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:93)\r\n\tat org.elasticsearch.action.percolate.TransportPercolateAction.shardOperation(TransportPercolateAction.java:41)\r\n\tat org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.lucene.util.BytesRefHash.add(BytesRefHash.java:274)\r\n\tat org.apache.lucene.index.memory.MemoryIndex.addField(MemoryIndex.java:437)\r\n\t... 10 more\r\n```'
4020,'drewr',"Add heap used percentage to _cat/nodes\nWhile we're printing mem information, include the heap ratio for quick trouble-spotting. Let's do this for now instead of a separate `heap` endpoint like with es2unix."
4016,'drewr','Add ip and node name to _cat/recovery\nNeed these as it greatly improves readability.'
4014,'drewr','Create _cat/allocation\nAdd endpoint to visualize shard distribution across cluster.'
4009,'bleskes','Not resending shard started messages when shard state is POST_RECOVERY and master died before processing the previous one\nWhen a node completes recovering a shard, it changes the shard status to `POST_RECOVERY` and sends a shard started message to the master. The master processes it and send a new cluster state which cause the shard to be moved `STARTED`.\r\n\r\nIf the master dies before processing that message, the message needs to be resent to the new master. We already have a mechanism in place for that but it needs to be extended to cover the `POST_RECOVERY` state.'
4006,'clintongormley','mget API - document the support for fields parameter in URL, add support for fields parameter in request body\nThe documentation for mget API only specifies that the fields parameter can be specified per document to get. But I tried and am using the un-documented feature of specifying the fields parameter in the URL. It would be good to make it a documented feature. It would also be good to support the fields parameter in the request body, as a top-level element. The motivation is that there is a limit on the URL size (4K as far as I remember) and a big field list may hit the limit. I had this problem in practice. It is also not best to repeat the same big field list for each document to get, and actually impossible at all if the ids element is used. Please see the reproduction below:\r\n```\r\ncurl -XDELETE "http://localhost:9200/test/"; echo\r\ncurl -XPUT "http://localhost:9200/test/" -d\'{\r\n   "settings": {\r\n      "number_of_replicas": 0,\r\n      "number_of_shards": 5\r\n   }\r\n}\'; echo\r\n\r\ncurl -XPUT \'http://localhost:9200/test/order/1-1\' -d \'{\r\n   "productName":"doc 1",\r\n   "quantity":1\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-2\' -d \'{\r\n   "productName":"doc 2",\r\n   "quantity":20\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-3?refresh=true\' -d \'{\r\n   "productName":"doc 3",\r\n   "quantity":5\r\n}\'; echo\r\n\r\n# This works, returns 3 docs with productName field only\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty=true&fields=productName\' -d \'{\r\n   "ids" : [ "1-1", "1-2", "1-3" ]\r\n}\'; echo\r\n\r\n# Fields parameter is ignored. This returns 3 docs with _source field\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty=true\' -d \'{\r\n   "fields" : [ "productName" ],\r\n   "ids" : [ "1-1", "1-2", "1-3" ]\r\n}\'; echo\r\n```'
4004,'javanna','Path, dev and mount cannot be retrieved using the Java API\nThe nodes stats api allows to get back information about the file system storage where the data is located.\r\n\r\nAlthough the rest api returns `fs.path`, `fs.mount` and `fs.dev` (as the `FsStats#toXContent` method prints them out), those cannot be retrieved using the Java API as their getter methods are missing.'
4002,'kimchy','bulk response has errors indication + status per item\nWhen using the bulk API, in case of an error for one of the actions specified, the client needs to parse the entire response to figure out whether all of them succeeded or not.\r\n\r\nHaving a simple boolean field (`"errors" : true/false`) indicating whether all requests worked or at least one failed would help a lot as the response does not have to be parsed any more.'
3999,'spinscale',"Allow setting the node.name and other options in the Debian/RPM packages\nWhile you can set extra java options with the Debian/RPM packages, you can't pass other Elasticsearch options like node name, node.rack_id etc.\r\n\r\nWould be useful to support these"
3997,'dadoonet','PR for mget API should support global routing parameter\nmget API support `_routing` field but not `routing` parameter.\r\n\r\nReproduction here:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test/"; echo\r\ncurl -XPUT "http://localhost:9200/test/" -d\'{\r\n   "settings": {\r\n      "number_of_replicas": 0,\r\n      "number_of_shards": 5\r\n   }\r\n}\'; echo\r\n\r\ncurl -XPUT \'http://localhost:9200/test/order/1-1?routing=key1\' -d \'{\r\n   "productName":"doc 1"\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-2?routing=key1\' -d \'{\r\n   "productName":"doc 2"\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-3?routing=key1&refresh=true\' -d \'{\r\n   "productName":"doc 3"\r\n}\'; echo\r\n\r\n# This works and gives 3 documents\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty\' -d \'{\r\n    "docs" : [\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-1",\r\n            "_routing" : "key1"\r\n        },\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-2",\r\n            "_routing" : "key1"\r\n        },\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-3",\r\n            "_routing" : "key1"\r\n        }\r\n    ]\r\n}\'; echo\r\n\r\n# This does not work and returns "exists" : false for each doc\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty&routing=key1\' -d \'{\r\n\t"ids": [\r\n\t\t"1-1",\r\n\t\t"1-2",\r\n\t\t"1-3"\r\n\t]\r\n}\'; echo\r\n```\r\n\r\nCloses #3996.'
3996,'dadoonet','mget API should support global routing parameter\nmget API support `_routing` field but not `routing` parameter.\r\n\r\nReproduction here:\r\n\r\n```sh\r\ncurl -XDELETE "http://localhost:9200/test/"; echo\r\ncurl -XPUT "http://localhost:9200/test/" -d\'{\r\n   "settings": {\r\n      "number_of_replicas": 0,\r\n      "number_of_shards": 5\r\n   }\r\n}\'; echo\r\n\r\ncurl -XPUT \'http://localhost:9200/test/order/1-1?routing=key1\' -d \'{\r\n   "productName":"doc 1"\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-2?routing=key1\' -d \'{\r\n   "productName":"doc 2"\r\n}\'; echo\r\ncurl -XPUT \'http://localhost:9200/test/order/1-3?routing=key1&refresh=true\' -d \'{\r\n   "productName":"doc 3"\r\n}\'; echo\r\n\r\n# This works and gives 3 documents\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty\' -d \'{\r\n    "docs" : [\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-1",\r\n            "_routing" : "key1"\r\n        },\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-2",\r\n            "_routing" : "key1"\r\n        },\r\n        {\r\n            "_index" : "test",\r\n            "_type" : "order",\r\n            "_id" : "1-3",\r\n            "_routing" : "key1"\r\n        }\r\n    ]\r\n}\'; echo\r\n\r\n# This does not work and returns "exists" : false for each doc\r\ncurl -XPOST \'http://localhost:9200/test/order/_mget?pretty&routing=key1\' -d \'{\r\n\t"ids": [\r\n\t\t"1-1",\r\n\t\t"1-2",\r\n\t\t"1-3"\r\n\t]\r\n}\'; echo\r\n```'
3995,'javanna','Update cluster settings api to support acknowledgements\nAdd support for acknowledgements from the other nodes to the update cluster settings api. \r\nAdd support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).\r\nAdd also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.'
3993,'jpountz',"Numeric field mappers should encode doc values in binary doc values fields (?)\nToday we are using sorted-set doc values to encode doc values for numeric fields. However, numeric data differs a bit from string data in that values are usually more useful (and not much larger) than ordinals and sorted-set is quite slow at getting the values from a doc ID (because of the levels of indirection and that getting the values will require random disk I/O).\r\n\r\nSo I was thinking that it would probably a better trade-off to store numeric values in a binary doc values field. Since all per-doc values would be stored sequentially on disk, we wouldn't have the random I/O issue anymore.\r\n\r\nThe two main challenges if we go this way would be to allow a field mapper to buffer all field values before creating a single Lucene field instance, and to pick a space and CPU efficient encoding for multi-valued numeric fields.\r\n\r\n"
3992,'martijnvg',"Fixed concurrency issue in simple id cache.\nThe lget() of a map can only be used if the map isn't shared. This issue concurrency issue occurs in none of the released versions and only exists in the master and 0.90 branch. Also only the `top_children` query is affected."
3987,'jpountz','ContextIndexSearcher discards Engine.searcher()\nThis is a problem for our tests since we mock RobinEngine so that it exposes an AssertingIndexSearcher in order to catch bugs. For instance we would have caught #3955 earlier if we had used this AssertingIndexSearcher.'
3986,'javanna','Added support for acknowledgements where missing\n- Update index settings api\r\n- Delete mapping api\r\n- Cluster reroute api\r\n- Update cluster settings api\r\n\r\n'
3985,'javanna','Cluster reroute api to support acknowledgements\nAdd support for acknowledgements from the other nodes to the cluster reroute api. \r\nAdd support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).\r\nAdd also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.'
3984,'javanna','Delete mapping api to support acknowledgements\nAdd support for acknowledgements from the other nodes to the delete mapping api. \r\nAdd support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).\r\nAdd also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.'
3983,'javanna','Update index settings api to support acknowledgements\nAdd support for acknowledgements from the other nodes to the update index settings api. \r\nAdd support for the timeout parameter in its requests, which refers to the maximum wait for acknowledgements (default 10s as in the other apis).\r\nAdd also support for the acknowledged flag in the response, which tells whether the request was acknowledged or not.'
3981,'bleskes','Source filtering in mget doesn\'t understand 0/1 as false/true\nIn most places:\r\n\r\n    _source: true | false\r\n\r\ncan also be written as:\r\n\r\n    _source: 1 | 0\r\n\r\nBut this isn\'t supported in the `mget` api:\r\n\r\n\r\n    curl -XPUT \'http://localhost:9200/test_1/test/1?pretty=1\' -d \'\r\n    {\r\n       "count" : "1",\r\n       "include" : {\r\n          "field1" : "v1",\r\n          "field2" : "v2"\r\n       }\r\n    }\r\n    \'\r\n\r\n    curl -XPUT \'http://localhost:9200/test_1/test/2?pretty=1\' -d \'\r\n    {\r\n       "count" : "1",\r\n       "include" : {\r\n          "field1" : "v1",\r\n          "field2" : "v2"\r\n       }\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'http://localhost:9200/_mget?pretty=1\' -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "_source" : 0,\r\n             "_index" : "test_1",\r\n             "_id" : "1",\r\n             "_type" : "test"\r\n          },\r\n          {\r\n             "_source" : 1,\r\n             "_index" : "test_1",\r\n             "_id" : "2",\r\n             "_type" : "test"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "docs" : [\r\n    #       {\r\n    #          "_source" : {\r\n    #             "count" : "1",\r\n    #             "include" : {\r\n    #                "field1" : "v1",\r\n    #                "field2" : "v2"\r\n    #             }\r\n    #          },\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : true,\r\n    #          "_version" : 1\r\n    #       },\r\n    #       {\r\n    #          "_source" : {\r\n    #             "count" : "1",\r\n    #             "include" : {\r\n    #                "field1" : "v1",\r\n    #                "field2" : "v2"\r\n    #             }\r\n    #          },\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "2",\r\n    #          "_type" : "test",\r\n    #          "exists" : true,\r\n    #          "_version" : 1\r\n    #       }\r\n    #    ]\r\n    # }\r\n\r\n'
3979,'imotov','The +index pattern without a wildcard in the index list is handled inconsistently \nIf a cluster has three indices `foo`, `bar` and `baz`, the index list `b*,+foo` in search request is resolved into all three indices, while `+foo,b*` throws `IndexMissingException[[+foo] missing]`\r\n\r\nRepro:\r\n```\r\ncurl -XPUT "http://localhost:9200/foo/doc/1?pretty" -d \'{"f": "v"}\'\r\ncurl -XPUT "http://localhost:9200/bar/doc/1?pretty" -d \'{"f": "v"}\'\r\ncurl -XPUT "http://localhost:9200/baz/doc/1?pretty" -d \'{"f": "v"}\'\r\ncurl -XPOST "http://localhost:9200/_refresh?pretty"\r\necho "Searching indices b*,+foo - works"\r\ncurl "http://localhost:9200/b*,%2bfoo/_search?pretty"\r\necho "Searching indices +foo,b* - doesn\'t work"\r\ncurl "http://localhost:9200/%2bfoo,b*/_search?pretty"\r\n```'
3971,'javanna','add reference for ember-data-elasticsearch-kit to integrations page\n'
3969,'dakrone','Add `_cat/recovery` API endpoint\nWe should add the `_cat/recovery` API endpoint to display information at the command line about recovering replica shards.'
3968,'javanna','Resolved issue #3797\nI changed the class MultiMatchQuery for support the issue #3797'
3965,'s1monw','NullPointerException using "has_child" filter after upgrade to v0.90.5\nAfter upgrading from v0.90.1 -> v0.90.5, we noticed that some of our has_child filter queries started to fail on some shards. Following is the request - \r\n```\r\ncurl -x \'\' -s -XPOST \'http://localhost:9201/data_index_20131011/vendor/_search?from=0&size=2\' -d \'\r\n{\r\n  "filter" : {\r\n    "has_child" : {\r\n      "query" : {\r\n        "match" : {\r\n          "set_aside_descriptions" : {\r\n            "query" : "No set aside used."\r\n          }\r\n        }\r\n      },\r\n      "type" : "transaction"\r\n    }\r\n  }\r\n}\r\n\'\r\n```\r\nGives failure in response \r\n```\r\n  "_shards" : {\r\n    "total" : 4,\r\n    "successful" : 2,\r\n    "failed" : 2,\r\n    "failures" : [ {\r\n      "index" : "data_index_20131011",\r\n      "shard" : 0,\r\n      "status" : 500,\r\n      "reason" : "RemoteTransportException[[mach2.node][inet[/<internal ip>:9301]][search/phase/query]]; nested: QueryPhaseExecutionException[[data_index_20131011][0]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "\r\n    }, {\r\n      "index" : "data_index_20131011",\r\n      "shard" : 1,\r\n      "status" : 500,\r\n      "reason" : "QueryPhaseExecutionException[[data_index_20131011][1]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]]; nested: NullPointerException; "\r\n    } ]\r\n  },\r\n\r\n```\r\n\r\nAnd the error trace from logs - \r\n```\r\n[2013-10-24 08:54:07,330][TRACE][search                   ] [mach2.node] Query phase failed\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [data_index_20131011][0]: query[ConstantScore(cache(_type:vendor))],from[0],size[2]: Query Failed [Failed to execute main query]\r\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:219)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:623)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:612)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n        at java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.shortCircuit(MatchDocIdSet.java:82)\r\n        at org.elasticsearch.index.search.child.HasChildFilter$ParentDocSet.matchDoc(HasChildFilter.java:174)\r\n        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.get(MatchDocIdSet.java:69)\r\n        at org.elasticsearch.common.lucene.search.FilteredCollector.collect(FilteredCollector.java:61)\r\n        at org.apache.lucene.search.Scorer.score(Scorer.java:65)\r\n        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.score(ConstantScoreQuery.java:245)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:624)\r\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:162)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:488)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:444)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)\r\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:134)\r\n        ... 7 more\r\n```\r\n\r\nThe same query works fine in v0.90.1. It\'s tough to provide a gist to replicate because the error is data dependent and index size is more than a GB, it only happens for some queries. For e.g. \r\n```"query" : "blah blah"``` works fine but \r\n```"query" : "No set aside used."``` fails. \r\n\r\n'
3960,'brwe','function_score should throw exception if decay function is used with multi valued field\nThe decay functions for function_score do not handle fields with multiple values. Decay functions silently compute the distance from the first value. This can lead to confusion, as seen in issue #3926. Instead of using the first value in the field, an exception should be thrown if a field has multiple values.'
3959,'chilling','Context extension of the Suggester\nThe suggester API is a good choice to get simple suggestions. But these suggestions may take additional informations into account. Typing in something like `restaurant` will result in restaurants all over the world. But in this case the geolocation of the request can be used to get better results, namely those which are close to point of the request. Another usecase could be a simple `type` or `group` information which allows the suggester to return results fitting this context.\r\nIn general the suggester should be able to handle generic information and create result with this information taken into account.'
3955,'jpountz','has_child can cause an infinite loop (100% CPU) when used in bool query\nThe defect is in \r\nChildrenQuery.ParentScorer#nextDoc()\r\n```java\r\n          @Override\r\n            public int nextDoc() throws IOException {\r\n                if (remaining == 0) {\r\n                    return NO_MORE_DOCS;\r\n                }\r\n...\r\n``` \r\nThis fails to update `currentDocId`. This causes `ChildrenQuery.ParentScorer#docID()` to continue returning the last doc id forever.\r\n\r\nThe BooleanScorer calls\r\n  ```java \r\npublic boolean score(Collector collector, int max, int firstDocID)\r\n```\r\npassing the `firstDocID` it pulled from the `docID()`. When the ParentScorer finishes with a low id document but continues to get processed and added to the BucketTable it opens the possibility of a document from the other boolean sub queries overlapping and causing get an infinite loop.\r\n\r\nThe fix is to set \r\n```currentDocId = NO_MORE_DOCS;```\r\nin both `int nextDoc()` and `int advance(int target)`'
3952,'s1monw','Empty query_string generates SearchParseException\nWhen executing a query with an empty query_string an org.elasticsearch.search.SearchParseException is raised.\r\n\r\nHow to reproduce:\r\n```\r\ncurl -XPUT localhost:9200/test/test/1 -d \'{}\'\r\n\r\ncurl -XPOST http://localhost:9200/test/test/_search -d \'{ \r\n    "query" : {"query_string":{ query: ""}},\r\n}\'\r\n```\r\n\r\nTested with 0.90.5.\r\n\r\nResponse:\r\n{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[MkT5D2yYRgq6g73gx9141A][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\" : {\\"query_string\\":{\\"query\\": \\"\\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse \'\': Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; nested: ParseException[Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][3]: SearchParseException[[test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\" : {\\"query_string\\":{\\"query\\": \\"\\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse \'\': Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; nested: ParseException[Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\" : {\\"query_string\\":{\\"query\\": \\"\\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse \'\': Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; nested: ParseException[Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][0]: SearchParseException[[test][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\" : {\\"query_string\\":{\\"query\\": \\"\\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse \'\': Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; nested: ParseException[Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; }{[MkT5D2yYRgq6g73gx9141A][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\" : {\\"query_string\\":{\\"query\\": \\"\\"}}}]]]; nested: QueryParsingException[[test] Failed to parse query []]; nested: ParseException[Cannot parse \'\': Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; nested: ParseException[Encountered \\"<EOF>\\" at line 1, column 0.\\nWas expecting one of:\\n    <NOT> ...\\n    \\"+\\" ...\\n    \\"-\\" ...\\n    <BAREOPER> ...\\n    \\"(\\" ...\\n    \\"*\\" ...\\n    <QUOTED> ...\\n    <TERM> ...\\n    <PREFIXTERM> ...\\n    <WILDTERM> ...\\n    <REGEXPTERM> ...\\n    \\"[\\" ...\\n    \\"{\\" ...\\n    <NUMBER> ...\\n    <TERM> ...\\n    \\"*\\" ...\\n    ]; }]","status":400}\r\n\r\nIt will generate the following exception on the server:\r\n[2013-10-23 11:31:10,295][DEBUG][action.search.type       ] [Tomazooma] [test][3], node[MkT5D2yYRgq6g73gx9141A], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@acda796]\r\norg.elasticsearch.search.SearchParseException: [test][3]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query" : {"query_string":{"query": ""}}}]]\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:561)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:464)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:449)\r\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:442)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:214)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:202)\r\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:679)\r\nCaused by: org.elasticsearch.index.query.QueryParsingException: [test] Failed to parse query []\r\n        at org.elasticsearch.index.query.QueryStringQueryParser.parse(QueryStringQueryParser.java:231)\r\n        at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:207)\r\n        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:281)\r\n        at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:255)\r\n        at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:549)\r\n        ... 12 more\r\nCaused by: org.apache.lucene.queryparser.classic.ParseException: Cannot parse \'\': Encountered "<EOF>" at line 1, column 0.\r\nWas expecting one of:\r\n    <NOT> ...\r\n    "+" ...\r\n    "-" ...\r\n    <BAREOPER> ...\r\n    "(" ...\r\n    "*" ...\r\n    <QUOTED> ...\r\n    <TERM> ...\r\n    <PREFIXTERM> ...\r\n\r\n'
3945,'jpountz',"Allow string fields to store term counts\nTo use this one you send a string to a field of type 'token_count'.  This\r\nmakes the most sense with a multi-field."
3941,'bleskes','GetFieldMapping API\nThis new API allows retrieving the mappings for specific fields. This is offered as an alternative to traversing the results of the standard Get Mapping API, which returns the entire mapping for indices and types.\r\n\r\nThe new API is available under the following REST endpoints, allowing to resolve fields for multiple indices and types in one `GET` request:\r\n\r\n```\r\n/_mapping/field/{fields}\r\n/{index}/_mapping/field/{fields}\r\n/{index}/{type}/_mapping/field/{fields}\r\n```\r\n\r\nFor example, the following call will return the mapping information for the `text` and `user.name` of an index of tweets:\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/twitter/tweet/_mapping/field/text,user.name"\r\n```\r\n\r\nReturns:\r\n\r\n```\r\n{\r\n   "twitter": {\r\n      "tweet": {\r\n         "text": {\r\n            "full_name": "text",\r\n            "mapping": {\r\n               "text": { "type": "string" }\r\n            }\r\n         },\r\n         "user.name": {\r\n            "full_name": "user.name",\r\n            "mapping": { \r\n               "name": { "type": "string" }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nSimilar to other APIs, fields names can be one of the following (in resolution order):\r\n* Full names\r\n* Index names\r\n* Field name (excluding complete path)\r\n\r\nThe response will use the same naming specified in the url:\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/_mapping/field/name"\r\n```\r\n\r\nreturns\r\n\r\n```\r\n{\r\n   "twitter": {\r\n      "tweet": {\r\n         "name": {\r\n            "full_name": "user.name",\r\n            "mapping": {\r\n               "name": { "type": "string" }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nLast, you request outputting default values (suppressed by default):\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/_mapping/field/name?include_defaults=true" \r\n```\r\n\r\n```\r\n{\r\n   "twitter": {\r\n      "tweet": {\r\n         "name": {\r\n            "full_name": "user.name",\r\n            "mapping": {\r\n               "name": {\r\n                  "type": "string",\r\n                  "index_name": "name",\r\n                  "boost": 1,\r\n                  "index": "analyzed",\r\n                  "store": false,\r\n                  "term_vector": "no",\r\n                  "omit_norms": false,\r\n                  "index_options": "positions",\r\n                  "analyzer": "default",\r\n                  "postings_format": "default",\r\n                  "doc_values_format": "default",\r\n                  "similariry": "default",\r\n                  "fielddata": {},\r\n                  "null_value": null,\r\n                  "include_in_all": false,\r\n                  "position_offset_gap": 0,\r\n                  "search_quote_analyzer": "default",\r\n                  "ignore_above": -1\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n'
3938,'costin','Introduce ES_STARTUP_MODE variable to define procrun --Startup mode, give possibility to override\nThis makes it possible to override the startup mode\r\nwhen installing the windows service (e.g. if auto startup mode is\r\nrequired). default is still manual'
3937,'javanna','Fix small typo in terms lookup tests mapping.\nterms -> term and terms -> arr.term as used in the actual tests.  The tests had a mapping defined but were actually using dynamic mapping since docs were indexing with a field name other than what was defined in the mapping.'
3931,'s1monw','Awareness attributes can\'t be reset once they are set.\nonce you set `"cluster.routing.allocation.awareness.attributes"` via the API you can\'t reset it to an empty list which would disable the awareness entirely. This happens since the `Settings.getAsArray` returns the default if the list is null or empty which essentially prevents an update to an empty list.\r\n\r\n'
3930,'javanna','add reference for ember-data elasticsearch kit client\nAdd reference for client https://github.com/roundscope/ember-data-elasticsearch-kit  - an ember-data kit for both pushing and querying objects to Elasticsearch cluster'
3928,'costin','service.bat incorrectly assumes JRE will have a \'client\' directory\n\r\n```bat\r\nrem Check JVM server dll first\r\nset JVM_DLL=%JAVA_HOME%\\jre\\bin\\server\\jvm.dll\r\n\r\nif exist "%JVM_DLL%" goto foundJVM\r\n\r\nset JVM_DLL=%JAVA_HOME%\\bin\\client\\jvm.dll\r\n\r\nif exist "%JVM_DLL%" (\r\necho Warning: JAVA_HOME points to a JRE and not JDK installation; a client (not a server^) JVM will be used...\r\n) else (\r\necho JAVA_HOME points to an invalid Java installation (no jvm.dll found in "%JAVA_HOME%"^). Existing...\r\ngoto:eof\r\n)\r\n```\r\n\r\nI have a JRE installation on a Windows Server system. It has no `client` directory, but it does have a `server` directory: `C:\\Program Files\\Java\\jre7\\bin\\server`'
3927,'javanna','Fix markup\nJust a tiny tiny typo'
3926,'brwe','_geo_distance sort: Support for many-to-many geo distance sort\nWhat we\'d like to see:\r\n\r\nAbility to specify many geo points in geo distance sort, like this:\r\n\r\n```json\r\n{\r\n    "sort": [\r\n        {\r\n            "_geo_distance": {\r\n                "geo_points.point": [\r\n                    {\r\n                        "lat": 59.959946,\r\n                        "lon": 30.313819\r\n                    },\r\n                    {\r\n                        "lat": 59.979788,\r\n                        "lon": 30.304513\r\n                    }\r\n                ]\r\n            }\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nUse-case: each user has several points of interest and wants to find other users who are close to these points. For example, I have work, home, and favourite breakfast cafe. I\'d like to find people who hang out near any of those places. Right now I should fire as many queries, as many points of interest I have, and then I need to merge results outside of elasticsearch. Having ability to use single query for this.\r\n\r\nLooks like it could be quite trivial change, an extra loop to iterate requested points. I might be wrong about this.\r\n\r\nI understand, that it could be very cpu intensive for querying with many points, but the point here is to use small amount of points.'
3925,'spinscale',"NewRelic intigration\nI'm using @NewRelic to monitor most of my stack (PHP) and since I use ElasticSearch quite a lot in my stack, it would be nice to have ES monitored too\r\n\r\nI've added `-javaagent:/usr/share/elasticsearch/lib/newrelic.jar` to my JAVA_OPTS and it does show up in NewRelic - but only as a JVM, there is no data shown for http traffic - even though NewRelic should support Netty \r\n\r\nI've followed [this guide](https://docs.newrelic.com/docs/java/java-agent-manual-installation) - but my practical experience with Java is quite limited :)"
3924,'brwe',"Feature Request: pre-select terms in TermVector request\nDear All\r\n\r\n  I have a feature request regarding the TermVector API. I was really happy to see this commit, which I had fledgingly written as a plugin before. Thanks @brwe ! Though, would it be possible to submit a list of terms and only have the TermVector returned for those? My hope is that it's considerably faster than the full request. I have a use case, where I know the terms for which I need the information before making the request.\r\n\r\n   Some pointers of how to do it myself are appreciated, too, though I am afraid my solution won't be as efficient.\r\n\r\nBest and many, many thanks for the great work.\r\nMax."
3921,'dadoonet',"Java API Documentation (0.90+) needs update for accessors in Facets docs\nJust noticed that the doc for the Facets (e.g. http://www.elasticsearch.org/guide/en/elasticsearch/client/java-api/current/java-facets.html#_use_facet_response_7) contains lots of .facets() calls when it's been changed to getFacets()\r\n\r\nI noticed this issue before with the MultiSearchResponse example. Either a lot of people don't use the latest API, or the docs with Java ;-)\r\n\r\n"
3920,'jpountz','Term Count on Search Results\nWould anyone else be interesting in getting elasticsearch to return a count of the terms in a field in the search results?  If you (like me) need to return a word count of a field then this could be useful to you.  I also could get a count of distinct terms but I\'m not super sure who\'d use it.  I was thinking the api could be something like this:\r\n\r\n```bash\r\ncurl -XPOST "http://localhost:9200/test/test/_search?pretty" -d \'{\r\n  "fields": [ "foo._term_count" ],\r\n  "query": {\r\n    "query_string": {\r\n      "query": "findme"\r\n    }\r\n  }\r\n}\'\r\n```\r\nAnd it\'d return ```"foo._term_count" : 6,``` in the results.\r\n\r\nIt\'d require ```term_vector```s to be stored but not offsets or positions.  Since it\'d count the terms on each search result it\'d be similar to highlighting using the FVH but faster because it does essentially no work other than the term vector scanning.\r\n\r\nI don\'t imagine you\'d be able to sort by them.'
3919,'spinscale','Errors with carrot2 clustering plugin - lingo3g module\nHello everybody, here are some log excerpt from ES:\r\n\r\n[2013-10-16 10:36:43,867][INFO ][lingo3g.cc               ] Lexical resources reloaded.\r\n[2013-10-16 10:38:27,315][DEBUG][action.search.type       ] [Blevins, Sally] [108] Failed to execute fetch phase\r\njava.lang.RuntimeException: cannot invoke method: substring\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessor.getValue(MapAccessor.java:39)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\r\n\tat org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)\r\n\tat org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:75)\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:197)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:414)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.RuntimeException: cannot invoke method: min\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.StaticReferenceAccessor.getValue(StaticReferenceAccessor.java:31)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.executeAll(MethodAccessor.java:140)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)\r\n\t... 17 more\r\nCaused by: java.lang.RuntimeException: cannot invoke method: length\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:63)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MapAccessor.getValue(MapAccessor.java:39)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.VariableAccessor.getValue(VariableAccessor.java:37)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.executeAll(MethodAccessor.java:140)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)\r\n\t... 23 more\r\nCaused by: java.lang.NullPointerException\r\n\tat sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:601)\r\n\tat org.elasticsearch.common.mvel2.optimizers.impl.refl.nodes.MethodAccessor.getValue(MethodAccessor.java:48)\r\n\t... 30 more\r\n\r\n\r\nwhich appears a couple of times, and is then followed by:\r\n\r\n\r\n[2013-10-16 10:38:37,725][INFO ][lingo3g.cc               ] Lexical resources reloaded.\r\n[2013-10-16 10:38:39,028][DEBUG][action.search.type       ] [Blevins, Sally] [161] Failed to execute fetch phase\r\njava.lang.NullPointerException\r\n\tat ASMAccessorImpl_10516961681381912712750.getValue(Unknown Source)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)\r\n\tat ASMAccessorImpl_18419208811381912707400.getValue(Unknown Source)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.compiler.ExecutableAccessor.getValue(ExecutableAccessor.java:42)\r\n\tat ASMAccessorImpl_13890246471381912707370.getValue(Unknown Source)\r\n\tat org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n\tat org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n\tat org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:86)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\r\n\tat org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\r\n\tat org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)\r\n\tat org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:75)\r\n\tat org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:197)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:414)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:406)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n\r\nand there are loads of pages with this previous message'
3918,'bleskes','Parse Exception\nHi,\r\nIf I execute the follow query its shows a exception,\r\n{\r\n  "from" : 0,\r\n  "size" : 20,\r\n  "query" : {\r\n    "multi_match" : {\r\n      "fields" : [ "Description", "jobStatus","rxCustomerID" ],\r\n"query" : "no"\r\n    }\r\n  },\r\n  "sort" : [ {\r\n    "_uid" : {\r\n      "order" : "desc"\r\n    }\r\n  } ],\r\n  "highlight" : {\r\n    "pre_tags" : [ "<b>" ],\r\n    "post_tags" : [ "</b>" ],\r\n    "fields" : {\r\n      "*" : { }\r\n    }\r\n  }\r\n}\r\n\r\nNote: The field rxCustomerID is long\r\n\r\nException: NumberFormatException[For input string: "no"]; }]\r\n\r\nCan anyone explain how to resolve that?\r\n\r\nI need the query for multi_match only'
3917,'martijnvg','Reject index requests that contain metadata fields in the request body.\nMetadata fields (_routing, _parent, _ttl) can also be specified in the request body and in certain scenarios they don\'t get indexed (just stored in the source). \r\n\r\nExample (ttl is not configured):\r\n```json\r\n{\r\n"_ttl" : 1\r\n}\r\n```\r\n\r\nA term query on `_ttl` field would fail in the case, this can be confusing, especially when the ttl feature isn\'t used at all.\r\n\r\nIn the case that metadata fields are in the request body and these fields don\'t get indexed then this index request should fail.'
3916,'dadoonet','`ignore_indices=missing` option should be supported on closed indices\nWhen running a search on an alias which redirect queries to a closed index, search fails:\r\n\r\n```\r\n{"error":"ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]","status":403}\r\n```\r\n\r\nAdding `ignore_indices=missing` has no effect here.\r\n\r\n```sh\r\ncurl -XDELETE \'http://localhost:9200/toto1\';echo\r\ncurl -XDELETE \'http://localhost:9200/toto2\';echo\r\ncurl -XPOST \'http://localhost:9200/toto1/test/1?refresh\' -d \'{ "user" : "kimchy" }\';echo\r\ncurl -XPOST \'http://localhost:9200/toto2/test/1?refresh\' -d \'{ "user" : "kimchy" }\';echo\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'{ "actions" : [ \r\n\t{ "add" : { "index" : "toto1", "alias" : "toto" } }, \r\n\t{ "add" : { "index" : "toto2", "alias" : "toto" } } \r\n] }\';echo\r\ncurl -XPOST \'http://localhost:9200/toto2/_close\'; echo\r\n\r\n# This one should not fail\r\ncurl -XPOST \'http://localhost:9200/toto/_search?ignore_indices=missing\' -d \'{ "query" : { "match_all" : {} } }\';echo\r\n\r\n# This one should fail\r\ncurl -XPOST \'http://localhost:9200/toto/_search\' -d \'{ "query" : { "match_all" : {} } }\';echo\r\n```\r\n\r\nWe should support that option and ignore all closed indices when `ignore_indices=missing` is set.'
3913,'spinscale','Add support for Lucene SuggestStopFilter\nThe suggest stop filter is an improved version of the stop filter, which\r\ntakes stopwords only into account if the last char of a query is a\r\nwhitespace. This allows you to keep stopwords, but to allow suggesting for\r\n"a".\r\n\r\nExample: Index document content "a word". You are now able to suggest for\r\n"a" and get back results in the completion suggester, if the suggest stop\r\nfilter is used on the query side, but will not get back any results for\r\n"a " as this is identified as a stopword.\r\n\r\nThe implementation allows to set the `remove_trailing` parameter for a\r\ncustom stop filter and thus use the suggest stop filter instead of the\r\nstandard stop filter.'
3912,'s1monw','Remove term_index_interval/divisor\nThe `term_index_interval` and `term_index_divisor` settings are no longer relevant and should be removed\r\n'
3910,'javanna','Add generic count down mechanism\nIn our code we often need to wait for multiple events to happen up to a timeout (mainly in the node acknowledgement logic), and we then notify some listener. We usually do it using an `AtomicInteger` to count down and an `AtomicBoolean` to check whether the notification has already been sent.\r\n\r\nIt would be nice to have a single way to achieve this, having a class (e.g. CountDown) that exposes this behaviour, which can be thoroughly tested and used whenever we need to.'
3907,'costin','Fixes problems with whitespaces and other nasty chars in service.bat (Issue #3906)\nFix for https://github.com/elasticsearch/elasticsearch/issues/3906\r\nNow you can install the service even into "C:\\Program Files (x86)"'
3906,'costin','service.bat fails if ES_HOME contains whitespaces and parentheses\nExecuting service.bat on windows 7 x64 in a directory like \r\nC:\\Program Files (x86) \r\nfails due to "syntaktisch an dieser Stelle nicht verarbeitbar" (in englisch somewhat like: bad syntax or syntactically not processible). Seems to be a problem with double quote handling/escaping.'
3903,'javanna','search_analyzer does not seem to kick in as expected\nI realized this issue while working on recreation script for #3881\r\n\r\nIt seems to me that the `search_analyzer` does not kick in in some cases, the following two recreation scripts are provided.\r\n\r\nThe [first scripts](https://gist.github.com/lukas-vlcek/6972058) shows a simple use case where the `search_analyzer` kicks in as expected. Everything looks ok to me here.\r\n\r\nThe [second script](https://gist.github.com/lukas-vlcek/6972064) (which is based on the original recreation scripts from #3881) shows that the `search_analyzer` does NOT kick in and yield correct results ONLY if stated explicitly. Not sure if I understand what it is fundamental difference between the use case in first script and use case in the second script. Both scripts have very similar structure and unless I made some typo there I do not understand why the search analyzer haven\'t been used automatically.\r\n\r\nBoth scripts were tested against the latest master:\r\n```\r\nElasticsearch version and build:\r\n{\r\n  "ok" : true,\r\n  "status" : 200,\r\n  "name" : "Anelle",\r\n  "version" : {\r\n    "number" : "1.0.0.Beta1",\r\n    "build_hash" : "9a062e465ce86aa9a72dd6dd5337be6447a83506",\r\n    "build_timestamp" : "2013-10-13T15:08:48Z",\r\n    "build_snapshot" : true,\r\n    "lucene_version" : "4.5"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n```'
3900,'martijnvg','Join option on nested facet filters\nAccording to the documentation for the nested filter join option, it is enabled by default and would execute a block join between the parent and nested documents. When disabled, it returns the nested documents as stated. It is the default enabled state that I find confusing. I cannot come up with a test case where the block join would return any documents. The integration tests in SimpleNestedTests disable the join option.\r\n\r\nI have not yet profiled the code, but I suspect that the code in NestedQueryParser is constructing a ToParentBlockJoinQuery with an incorrect parentFilter, probably the default NonNestedDocsFilter.INSTANCE.\r\n\r\nIf this is not a bug, can the tests be updated to include an example where the default join is enabled and returns documents? There are a few assertions that are commented out in SimpleNestedTests related to join queries.'
3899,'javanna','Replaced hardcoded boolean value with correct parameter.\nCloses #3898\r\n\r\nI\'ve tracked the problem down to the MapperQueryParser class. As you can see from the commit the "true" value was hard coded for one of the code branches instead of using the \'quoted\' parameter sent to the method.\r\n'
3898,'javanna','No results are found with specific use case when using a custom word_delimiter filter\nThe problem only manifests when all of the following conditions are met:\r\n\r\n - a custom word delimiter is used when analyzing a string field\r\n - the query string query is used\r\n - use_dis_max parameter is set to false\r\n - more than one field is used for the query\r\n - query string contains a special character (like "." or "/")\r\n\r\nThis is how to reproduce the issue:\r\n\r\n```sh\r\n# create the index\r\ncurl -XPOST http://localhost:9200/test -d \'{\r\n  "settings": {\r\n    "index": {\r\n      "number_of_shards": 1,\r\n      "number_of_replicas": 0\r\n    },\r\n    "analysis": {\r\n      "analyzer": {\r\n        "text_ascii": {\r\n          "type": "custom",\r\n          "tokenizer": "whitespace",\r\n          "filter": ["asciifolding",\r\n          "lowercase",\r\n          "custom_word_delimiter"]\r\n        }\r\n      },\r\n      "filter": {\r\n        "custom_word_delimiter": {\r\n          "type": "word_delimiter",\r\n          "generate_word_parts": true,\r\n          "generate_number_parts": false,\r\n          "catenate_numbers": true,\r\n          "catenate_words": false,\r\n          "split_on_case_change": false,\r\n          "split_on_numerics": false,\r\n          "stem_english_possessive": false\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "mappings": {\r\n    "person": {\r\n      "properties": {\r\n        "name": {\r\n          "type": "string",\r\n          "analyzer": "text_ascii"\r\n        },\r\n        "address": {\r\n          "type": "string",\r\n          "analyzer": "text_ascii"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\'\r\n\r\n# index the document\r\ncurl -XPUT \'http://localhost:9200/test/person/1\' -d \'{\r\n    "name" : "bogdan mihai dumitrescu",\r\n    "address" : "amsterdam"\r\n}\'\r\n\r\n# search for the document with a dot in the query \r\n# expect one result to be returned, but this returns 0 hits\r\ncurl -XPOST \'http://localhost:9200/test/_search\' -d \'{\r\n  "query": {\r\n    "query_string": {\r\n      "query": "bogdan.dumitrescu",\r\n      "fields": ["name", "address"],\r\n      "use_dis_max": false,\r\n      "default_operator": "and",\r\n      "analyzer": "text_ascii"\r\n    }\r\n  }\r\n}\'\r\n```'
3894,'dadoonet','Add integration test for PluginManager\nWe want to make sure that Plugin Manager still downloading plugins from internet.\r\nNew tests requires internet access (`@Network` annotation has been added).\r\n\r\nBy default, tests annotated with `@Network` are not launched.\r\n\r\nIf you need to run these tests, use `-Dtests.network=true` option.'
3892,'s1monw','function_score (FunctionScoreQuery) decay functions do not allow date math\nFor "linear", "gauss", "exp" ... functions, the "origin" function should allow parameters and date math.  "now" as origin fails to parse.  Just as "now-7d" would as well.  Obviously offset can replace part of the date math, the origin still would be useful to be set in that way.'
3886,'bleskes','Make source filtering parameters of the Get Source API consistent with the rest\nWith #3301, we added _source_* query parameters to many APIs allowing to filter the `_source` those APIs return. The get source api currently uses other naming for those parameters, not using the _source prefix. We should make it consistent with the ones, like this:\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/index/type/1/_source/?_source_include=field1,field2" ```'
3884,'costin','elasticsearch-service-x64 unable to reflect ES_HEAP_SIZE\nHi, \r\nI am running elasticsearch  0.90.5 on windows 2008 R2. In service.bat I updated ES_HEAP_SIZE as 2g I checked script with Echo ON and it showed my 2g .\r\nHowever when I run the service and check heap committed it comes as 111 mb . \r\n\r\nI check running the es with elasticsearch.bat with ES_HEAP_SIZE=2g and it shows heap committed as 1.9 g. \r\nBy the way I am using server jre 1.7 u40 . \r\nHowever I earlier tried client jre 1.7 u 25 but the result is same . \r\n\r\nThanks'
3883,'martijnvg','Add more percolate statistics\nAdd the following statistics to indices and node stats apis:\r\n* `num_queries` - The number of loaded percolator queries.\r\n* `memory_size` - The amount of memory the percolator queries take in memory.'
3881,'s1monw','Multiple tokens at the same position not working correctly with match query if AND operator is used\nIf multiple tokens are output at the same position then `match` queries are not working correctly if `AND` operator is used.\r\n\r\nFirst I noticed this issue when using Hunspell token filter (something similar has been reported in [LUCENE-5057](https://issues.apache.org/jira/browse/LUCENE-5057) but it is not really a Lucene issue). With Hunspell it is possible to get multiple output tokens from a single input token, all at the same position. However, client query usually contains only one of those tokens or token that can output different set of tokens. When using `match` query and `AND` operator the document is not matching (while it should be).\r\n\r\n_I also think that this can impact other linguistics packages (like Basis`s RBL?)_\r\n\r\nSimilar situation can be simulated using synonym filter. Imagine that we are using query time synonyms.\r\n\r\nLet\'s say we index simple document:\r\n```\r\n{ text : "Quick brown fox" }\r\n```\r\nand we define query time synonym "quick, fast". Now let\'s see what we can do with this in the following [recreation script](https://gist.github.com/lukas-vlcek/6923179) (using ES 0.90.5), output commented below:\r\n```\r\n#!/bin/sh\r\n \r\necho "Elasticsearch version"\r\ncurl localhost:9200; echo; echo;\r\n \r\necho "Delete index"; curl -X DELETE \'localhost:9200/i\'; echo; echo;\r\n \r\necho "Create index with analysis and mappings"; curl -X PUT \'localhost:9200/i\' -d \'{\r\n  "settings" : {\r\n    "analysis" : {\r\n      "analyzer" : {\r\n        "index" : {\r\n          "type" : "custom",\r\n          "tokenizer" : "standard",\r\n          "filter" : ["lowercase"]\r\n        },\r\n        "search" : {\r\n          "type" : "custom",\r\n          "tokenizer" : "standard",\r\n          "filter" : ["lowercase","synonym"]\r\n        }\r\n      },\r\n      "filter" : {\r\n        "synonym" : {\r\n          "type" : "synonym",\r\n          "synonyms" : [\r\n            "fast, quick"\r\n          ]\r\n  }}},\r\n  "mappings" : {\r\n    "t" : {\r\n      "properties" : {\r\n        "text" : {\r\n          "type" : "string",\r\n          "index_analyzer" : "index",\r\n          "search_analyzer" : "search"\r\n}}}}}}\'; echo; echo;\r\n \r\n# Wait for all the index shards to be allocated\r\ncurl -s -X GET \'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=5s\' > /dev/null\r\n \r\necho "Test synonyms for \'fast\': should output two tokens"; curl -X POST \'localhost:9200/i/_analyze?analyzer=search&format=text&text=fast\'; echo; echo;\r\n \r\necho "Index data: \'Quick brown fox\'"; curl -X POST \'localhost:9200/i/t\' -d \'{\r\n  "text" : "Quick brown fox"\r\n}\'; echo; echo;\r\n \r\necho "Refresh Lucene reader"; curl -X POST \'localhost:9200/i/_refresh\'; echo; echo;\r\n \r\necho "Testing search";\r\necho ===========================\r\necho "1) query_string: quick";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"query_string":{"query":"quick","default_field":"text"}}}\'; echo; echo;\r\n \r\necho "2) query_string: fast - is search_analyzer used?";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"query_string":{"query":"fast","default_field":"text"}}}\'; echo; echo;\r\n \r\necho "2.5) query_string: fast - forcing search_analyzer";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"query_string":{"query":"fast","default_field":"text","analyzer":"search"}}}\'; echo; echo;\r\n \r\necho "3) query_string: fast - forcing search_analyzer, forcing AND operator";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"query_string":{"query":"fast","default_field":"text","analyzer":"search","default_operator":"AND"}}}\'; echo; echo;\r\n \r\necho "4) match query: quick";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"match":{"text":{"query":"quick","analyzer":"search"}}}}\'; echo; echo;\r\n \r\necho "5) match query: fast";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"match":{"text":{"query":"fast","analyzer":"search"}}}}\'; echo; echo;\r\n \r\necho "6) match query: fast - forcing AND operator";\r\ncurl -X GET \'localhost:9200/_search\' -d \'{"query":{"match":{"text":{"query":"fast","analyzer":"search","operator":"AND"}}}}\'; echo; echo;\r\n```\r\nOutput of queries:\r\n```\r\n1) query_string: quick\r\n{"took":4,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.15342641,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.15342641, "_source" : {\r\n  "text" : "Quick brown fox"\r\n}}]}}\r\n\r\n2) query_string: fast - is search_analyzer used?\r\n{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}\r\n\r\n2.5) query_string: fast - forcing search_analyzer\r\n{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {\r\n  "text" : "Quick brown fox"\r\n}}]}}\r\n\r\n3) query_string: fast - forcing search_analyzer, forcing AND operator\r\n{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {\r\n  "text" : "Quick brown fox"\r\n}}]}}\r\n\r\n4) match query: quick\r\n{"took":2,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {\r\n  "text" : "Quick brown fox"\r\n}}]}}\r\n\r\n5) match query: fast\r\n{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":0.04500804,"hits":[{"_index":"i","_type":"t","_id":"0N2FX_vxR5qsMTYczFPl1w","_score":0.04500804, "_source" : {\r\n  "text" : "Quick brown fox"\r\n}}]}}\r\n\r\n6) match query: fast - forcing AND operator\r\n{"took":4,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}\r\n```\r\nMy comments on results:\r\n\r\n_(note that comment no.2 may contain question regarding other non related issue)_\r\n\r\n1) `query_string` for query "quick" works as expected.\r\n\r\n2) `query_string` for query "fast" does not seem to work. According to the [documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-core-types.html#string) I was expecting that `search_analyzer` defined in `string` type mapping would be used. But anyway, this should not be the topic of this issue... :smile:\r\n\r\n2.5) `query_string` for query "fast" works (if I explicitly force `search` analyzer) so we can say query time synonym works fine.\r\n\r\n3) The same situation as in 2.5) except we are forcing `AND` operator. It should work and it is working.\r\n\r\n4) Now, let\'s use `match` query and query for "quick". It works fine.\r\n\r\n5) Again, `match` query but query for "fast". It works, so far so good.\r\n\r\n6) The same as in 5) except we are forcing `AND` operator. It should work (I hope) but it is not.\r\n\r\nIf I could speculate about why this is happening:\r\n\r\na) MatchQueryParser does something like:\r\n```\r\n... if ("and".equalsIgnoreCase(op)) {\r\n    matchQuery.setOccur(BooleanClause.Occur.MUST);\r\n} ...\r\n``` \r\n\r\nb) and MatchQuery does not take account on the position of tokens. It simply stacks all incoming tokens into BooleanQuery. It contains patterns similar to the following excerpt:\r\n```\r\nBooleanQuery q = new BooleanQuery(positionCount == 1);\r\nfor (int i = 0; i < numTokens; i++) {\r\n    boolean hasNext = buffer.incrementToken();\r\n    assert hasNext == true;\r\n    final Query currentQuery = newTermQuery(mapper, new Term(field, termToByteRef(termAtt)));\r\n    q.add(currentQuery, occur);\r\n}\r\n```\r\nThe position of tokens is not taken into account which would explain why this is not working as expected in combination with `AND` operator in situations described above.\r\nI think if incoming tokens share the same position it should generate Boolean subquery with `OR` operator (?).'
3879,'javanna','Document strict dynamic type mapping.\nDocument how to disable dynamic field mapping in the dynamic mapping section.  For #3877'
3878,'spinscale','Search using BooleanQueryBuilder and GeoShapeQueryBuilder results in "Current context not an ARRAY but OBJECT"\nFirst of all, thanks for building such an amazing service. I am loving my experience with ElasticSearch so far.\r\n\r\nWhat I\'ve run into is that trying to use the BooleanQueryBuilder in conjunction with the GeoShapeQueryBuilder is resulting in the following exception.\r\n\r\n    org.elasticsearch.search.builder.SearchSourceBuilderException: Failed to build search source\r\n    \tat org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:579)\r\n    \tat org.elasticsearch.action.search.SearchRequest.source(SearchRequest.java:258)\r\n    \tat org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:839)\r\n    \tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)\r\n    \tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)\r\n    \tat com.tomtom.lbs.vectordb.data.services.ElasticSearchService.search(ElasticSearchService.java:119)\r\n    \t... 38 more\r\n    Caused by: org.elasticsearch.common.jackson.core.JsonGenerationException: Current context not an ARRAY but OBJECT\r\n    \tat org.elasticsearch.common.jackson.core.base.GeneratorBase._reportError(GeneratorBase.java:444)\r\n    \tat org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator.writeEndArray(SmileGenerator.java:553)\r\n    \tat org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeEndArray(JsonXContentGenerator.java:59)\r\n    \tat org.elasticsearch.common.xcontent.XContentBuilder.endArray(XContentBuilder.java:227)\r\n    \tat org.elasticsearch.index.query.BoolQueryBuilder.doXArrayContent(BoolQueryBuilder.java:182)\r\n    \tat org.elasticsearch.index.query.BoolQueryBuilder.doXContent(BoolQueryBuilder.java:149)\r\n    \tat org.elasticsearch.index.query.BaseQueryBuilder.toXContent(BaseQueryBuilder.java:65)\r\n    \tat org.elasticsearch.search.builder.SearchSourceBuilder.toXContent(SearchSourceBuilder.java:601)\r\n    \tat org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:576)\r\n    \t... 43 more\r\n\r\nHere is my code to build and execute the query:\r\n\r\n```java\r\nGeoShapeQueryBuilder geoQuery = QueryBuilders.geoShapeQuery("searchGeometry", new RectangleImpl(y1, y2, x1, x2, SpatialContext.GEO));\r\nBoolQueryBuilder query = QueryBuilders.boolQuery();\r\n\r\nif (featureIds.length > 0)\r\n\t\tquery.must(QueryBuilders.termQuery("featureType", featureIds[0]));\r\n\r\nquery.must(geoQuery);\r\n\r\nSearchRequestBuilder request = esClient.prepareSearch("index_name")\r\n\t\t.setSearchType(SearchType.QUERY_THEN_FETCH)\r\n\t\t.setQuery(query)\r\n\t\t.setFrom(0)\r\n\t\t.setSize(maxResults)\r\n\t\t.setExplain(false);\r\n\r\nSearchResponse response = request.execute().actionGet();\r\n```\r\n\r\nAm I doing something wrong?  I have a feeling I am but it looks right to me.\r\n\r\nThis isn\'t a big deal for me, because I can just build a JSON query manually and that works perfectly fine.  However it\'d be nice to be able to use the QueryBuilders as make the code much easier to read.\r\n\r\nThanks!'
3874,'dadoonet','Support year in date math - #3828\nTests and fix for #3828'
3873,'brwe','Get term vector api broken for missing payloads\nIf not all tokens in a field have a payload, the java api for term vectors will return the payload of the previous token at that position if there was one.\r\n\r\nFor example, suppose a field only contains two tokens each occurring once, the first having a payload and the second not, then for the second token, the payload of the first would be returned.\r\n'
3872,'brwe','inconsistent behavior and documentation(?) errors with function_score\nTrying to use the new `function_score` query in 0.90.5 I stumbled across some errors. In a couple of cases the code does not work as documented.\r\n\r\n## boost_mode `multiply`\r\n\r\nUsing `"boost_mode": "multiply"` fails:\r\n\r\n````\r\n{\r\n    "query": {\r\n        "function_score": {\r\n            "boost_mode": "multiply",\r\n            "query": {\r\n                "match": {\r\n                    "title": "elasticsearch"\r\n                }\r\n            },\r\n            "functions": [\r\n                {\r\n                    "gauss": {\r\n                        "date": {\r\n                            "scale": "4w"\r\n                        }\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n````\r\n\r\nresult:\r\n\r\n````\r\nQueryParsingException[[psb-1.1] function_score illegal boost_mode [multiply]]\r\n````\r\n\r\n`"boost_mode": "mult"` works. However, `score_mode` just behaves the other way round:\r\n\r\n````\r\nQueryParsingException[[psb-1.1] function_score illegal score_mode [mult]];\r\n````\r\n\r\n## `boost` as decay_function\r\n\r\nBoth the blog post introducing the feature and the documentation propose a query along the lines of\r\n\r\n````\r\n"query": {\r\n  "function_score": {\r\n    "query": {\r\n      "match": { "title": "elasticsearch"}\r\n    },\r\n    "functions": [\r\n      { "boost":  1 },\r\n      {\r\n        "gauss": {\r\n          "timestamp": {\r\n            "scale": "4w"\r\n          }\r\n        }\r\n      }\r\n    ],\r\n    "score_mode": "sum"\r\n  }\r\n}\r\n````\r\n\r\nrespectively\r\n\r\n````\r\n"function_score": {\r\n    "functions": [\r\n        {\r\n            "boost": "3",\r\n            "filter": {...}\r\n        },\r\n        {\r\n            "filter": {...},\r\n            "script_score": {\r\n                "params": {\r\n                    "param1": 2,\r\n                    "param2": 3.1\r\n                },\r\n                "script": "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)"\r\n            }\r\n        }\r\n    ],\r\n    "query": {...},\r\n    "score_mode": "first"\r\n}\r\n````\r\n\r\nThe second sample is intended to provide a replacement for the `custom_filters_score_query`. However, the first example fails against 0.90.5 with the following trace:\r\n\r\n````\r\nParse Failure [Failed to parse source [{ "query": { "function_score": { "query": { "match": { "title": "elasticsearch" } }, "functions": [ { "boost": 1 }, { "gauss": { "timestamp": { "scale": "4w" } } } ], "score_mode": "sum" } } }]]]; nested: QueryParsingException[[psb-1.1] No function with the name [boost] is registered.]; }\r\n````\r\n\r\nI failed to figure out what\'s the right incantation to invoke `boost`.\r\n'
3870,'javanna',"Fix small doc mistakes\nIt's not much :)"
3869,'martijnvg','Fix for "has_child ignores type" - #3818\n`HasChildFilter` ignores the `childType` provided. The fix is to include this in a filter while searching for the children to fix the context.\r\n\r\nTest cases verify that we do not qualify children whose types conflict with the provided type in `has_child`.\r\n\r\nThe test scenario is simpler than the description in #3818. Simply, create a parent with two children of different types and issue a `has_child` query with fields from one child and the type of another. This returns the parent, whereas with the fix, it won\'t'
3867,'javanna','Error while updating index settings when adding url parameters to the URL\nto reproduce(0.90.3):\r\n```\r\ncurl -XPOST http://localhost:9200/foo\r\ncurl -XPUT http://localhost:9200/foo/_settings -d \'{"index.number_of_replicas":"0"}\'\r\ncurl -XPUT http://localhost:9200/foo/_settings?foo=bar -d \'{"index.number_of_replicas":"0"}\'\r\n```\r\nso, the first update settings work ok as expected, but for the second one(with param foo=bar)\r\nI get:\r\n\r\n{"error":"ElasticSearchIllegalArgumentException[Can\'t update non dynamic settings[[index.foo]] for open indices[[foo]]]","status":400}\r\n\r\nnot sure if that is the way its supposed to be, since its not mentioned on the docs\r\n(http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-update-settings.html)\r\n\r\nanyway, if that is the way its supposed to be, is there anyway to go around this issue? this is kind of a problem for me when executing cross domain requests using jquery(it adds a callback parameter to the url).\r\ni couldnt spot this in other places(for the cluster update settings api, if a setting is not known its just ignored).'
3866,'polyfractal','PHP client quick start instructions are wrong\nOn the page: \r\n  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_quickstart.html \r\nit states the require part of composer.json should be\r\n  "elasticsearch/elasticsearch-php": "~0.4"\r\nThis doesn\'t work. Further investigation led me to the client pages on GitHub, where on\r\nhttps://github.com/elasticsearch/elasticsearch-php\r\nit says:\r\n  "elasticsearch/elasticsearch": "~0.4"\r\nwhich does work. I then noticed this is also what it says on \r\n  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_installation_2.html\r\nso the quick start page is wrong with the extra \'-php\' bit.\r\n\r\nAlso, incidentally, the link "Composer can be found at their website." at the bottom of \r\n  http://www.elasticsearch.org/guide/en/elasticsearch/client/php-api/current/_installation_2.html\r\ndoesn\'t work: it has an extraneous colon (the one at the top of the page is fine)'
3864,'jpountz','Eager loading of field data\nField data takes a long time to load and can make the first queries following a refresh very slow. The current way to solve this issue is to register warmers based on queries, but we could also have a configuration option for this in the mappings.'
3862,'chilling','Consider using SloppyMathUtils\nSloppyMathUtils functions trade (little) precision for speed, this would probably be a better trade-off for our geo stuff?'
3860,'jpountz','Add support for combining fields to the FVH\nCloses #3750'
3859,'brwe','Enable delimited payload token filter\nThe [DelimitedPayloadTokenFilter](http://lucene.apache.org/core/4_1_0/analyzers-common/org/apache/lucene/analysis/payloads/DelimitedPayloadTokenFilter.html) would be useful to have in elasticsearch for testing functionality using payloads, for example the term vector api or scoring based on payloads as requested in issue #3772.'
3858,'martijnvg','Migrate from Trove to Hppc\n'
3856,'martijnvg','Many percolators kills server with java.lang.OutOfMemoryError: Java heap space\nES version 0.90.5\r\nJava version: 1.7.0_40\r\nLinux CentOS 64bit (2.6.32-358.14.1.el6.x86_64)\r\nDefault config with some changes:\r\n```\r\nindex.number_of_shards: 1\r\nindex.number_of_replicas: 0\r\nbootstrap.mlockall: true\r\n```\r\n```\r\n# testing environment set memory to 512MB\r\nES_HEAP_SIZE=512m\r\n```\r\nI wanted to test how many percolator documents can handle one ElasticSearch (ES) instance and how fast it works. First idea is to create 15000 percolator documents.\r\nTried 1GB HEAP size, then 2GB heap size but did not succeeded (on Windows). So moved to linux box. There is no much memory. enabled mlockall (BTW if it\'s enabled or disabled - same result), defined to create only one shard without replicas.\r\n\r\nCreated script that generates percolator and puts one by one to ES. Query is simple:\r\n```\r\n{\r\n  "query":{\r\n    "bool":{\r\n      "should":[\r\n        {"match":{"headline":"3BPV i3ZBGQgK mc72 5EhW"}},\r\n        {"match":{"bodytext":"NGgfMl XOIb nEgbLoXQ eL1g ItP DvE OE7eMj OJawh3"}},\r\n        {"wildcard":{"bodytext":"0vRa* Cf* 47b* hz735* cQJXyo* qMyL*"}}\r\n      ]\r\n    }\r\n  }\r\n}\r\n```\r\nevery time text is random strings with different length but number of words is same.\r\nI create index\r\n```\r\ncurl -XPOST http://192.168.1.144:9200/demo\r\n```\r\nand then iterate 15000 times generating percolators and add one by one to ES. \r\n```\r\ncurl -XPUT http://192.168.1.144:9200/_percolator/demo/[name] -d \'{\r\n  "query":{\r\n    "bool":{\r\n      "should":[\r\n        {"match":{"headline":"3BPV i3ZBGQgK mc72 5EhW"}},\r\n        {"match":{"bodytext":"NGgfMl XOIb nEgbLoXQ eL1g ItP DvE OE7eMj OJawh3"}},\r\n        {"wildcard":{"bodytext":"0vRa* Cf* 47b* hz735* cQJXyo* qMyL*"}}\r\n      ]\r\n    }\r\n  }\r\n}\'\r\n```\r\nAfter ~2000 documents HEAP memory becomes full and ES becomes nonfunctional.\r\n\r\nIf i do not create index and add percolator documents then HEAP memory usage is low. I can create 15000 percolator documents easy and fast. After creating all percolator documents I create index - then HEAP memory fills up quickly and bye bye ES - nonfunctional again.'
3852,'jpountz',"SimpleDateMappingTests.testLocale assumes that the default locale is english\nI got a failure running SimpleDateMappingTests.testLocale on a computer with a french locale because it couldn't figure out how to parse the date which is written in english."
3851,'martijnvg','Add facet support to percolate api.\nA percolate query is just a document with a query embedded into it and besides the query it can also contain arbitrary metadata. The facet support will allow that metadata of matched queries will be facetable. \r\n\r\n(note: this is different then just executing a search request on an index with _percolator type)'
3849,'martijnvg',"Prohibit adding a parent mapping at runtime\nProhibit adding a parent mapping to a type at runtime by raising a MergeMappingException.\r\n\r\nThe breaking part here, is that if a document uses a top level `_parent` field, that it won't be indexed. This is because `_parent` is actually reserved metadata field and this also consistent with other metadata fields (like ttl, routing)"
3848,'martijnvg','Reject indexing requests which specify a parent, if no parent type is defined\nIf you submit an indexing request and specify a parent but the mapping doesn\'t define a parent type, we currently silently ignore the parent value: \r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/test/type/1?parent=1" -d\'\r\n{\r\n   "field": "value" \r\n}\'\r\n```\r\n\r\nWe should reject the request.\r\n\r\nThe breaking part: Index requests with the parent flag will be rejected if there is no `_parent` metadata field in the mapping.'
3847,'dadoonet','plugin -remove deletes bin directory\nI tried to install a plugin directly from disk, but used the wrong arguments. Totally my fault for not RTFM properly.\r\n\r\nHowever, by following the instructions printed out by `bin/plugin`, I ended up deleting my $ES/bin directory.\r\n\r\n```\r\n$ pwd\r\n/tmp/elasticsearch-0.90.5\r\n$ ls\r\nLICENSE.txt    NOTICE.txt     README.textile bin            config         lib\r\n$ bin/plugin --install file:///tmp/foo.zip\r\n-> Installing file:///tmp/foo.zip...\r\nFailed to install file:///tmp/foo.zip, reason: plugin directory /tmp/elasticsearch-0.90.5/plugins already exists. To update the plugin, uninstall it first using -remove file:///tmp/foo.zip command\r\n$ bin/plugin -remove file:///tmp/foo.zip\r\n-> Removing file:///tmp/foo.zip\r\nRemoved file:///tmp/foo.zip\r\n$ ls\r\nLICENSE.txt    NOTICE.txt     README.textile config         lib\r\n```\r\n\r\nI reproduced the problem in 0.90.5 and the latest master.'
3840,'javanna','river instance not created after successful creation of _meta document\nriver instance is not created after successfully issuing PUT _meta request \r\n\r\nthis problem happens when creating river on completely fresh cluster \r\nand only for first river (following updates to _meta document for this river still dont start it)\r\ncreating second river works fine (but first one is still down)\r\n\r\nim using latest elasticsearch sources from master branch\r\nand running it from eclipse (one node cluster, no replication)\r\nto replicate delete all cluster data files, then start single node cluster and create river as first request to the cluster\r\n\r\noverall it looks like racing condition, after debugging for while i see that when checking if river cluster state changed (RiverClusterStateUpdateTask), event.state().metaData().index(riverIndexName) is not returning newly created river (probably slowed down by initial index creation)\r\n\r\ndebug logs:\r\n\r\n[2013-10-07 15:35:38,937][DEBUG][cluster.service          ] [Peepers] processing [create-index [_river], cause [auto(index api)]]: execute\r\n[2013-10-07 15:35:38,950][DEBUG][indices                  ] [Peepers] creating Index [_river], shards [1]/[0]\r\n[2013-10-07 15:35:43,713][DEBUG][index.mapper             ] [Peepers] [_river] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[file:/mnt/data/projects/git/elasticsearch/elasticsearch/target/classes/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]\r\n[2013-10-07 15:35:43,722][DEBUG][index.cache.query.parser.resident] [Peepers] [_river] using [resident] query cache with max_size [100], expire [null]\r\n[2013-10-07 15:35:43,965][DEBUG][index.store.fs           ] [Peepers] [_river] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]\r\n[2013-10-07 15:35:45,458][INFO ][cluster.metadata         ] [Peepers] [_river] creating index, cause [auto(index api)], shards [1]/[0], mappings []\r\n[2013-10-07 15:35:45,560][DEBUG][index.cache.filter.weighted] [Peepers] [_river] full cache clear, reason [close]\r\n[2013-10-07 15:35:45,570][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [3], source [create-index [_river], cause [auto(index api)]]\r\n[2013-10-07 15:35:45,572][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 3\r\n[2013-10-07 15:35:45,574][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 3\r\n[2013-10-07 15:35:45,579][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute\r\n[2013-10-07 15:35:45,584][DEBUG][indices.cluster          ] [Peepers] [_river] creating index\r\n[2013-10-07 15:35:45,587][DEBUG][indices                  ] [Peepers] creating Index [_river], shards [1]/[0]\r\n[2013-10-07 15:35:45,615][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-10-07 15:35:47,464][DEBUG][index.mapper             ] [Peepers] [_river] using dynamic[true], default mapping: default_mapping_location[null], loaded_from[file:/mnt/data/projects/git/elasticsearch/elasticsearch/target/classes/org/elasticsearch/index/mapper/default-mapping.json], default percolator mapping: location[null], loaded_from[null]\r\n[2013-10-07 15:35:47,472][DEBUG][index.cache.query.parser.resident] [Peepers] [_river] using [resident] query cache with max_size [100], expire [null]\r\n[2013-10-07 15:35:47,519][DEBUG][index.store.fs           ] [Peepers] [_river] using index.store.throttle.type [node], with index.store.throttle.max_bytes_per_sec [0b]\r\n[2013-10-07 15:35:47,593][DEBUG][indices.cluster          ] [Peepers] [_river][0] creating shard\r\n[2013-10-07 15:35:47,595][DEBUG][index.service            ] [Peepers] [_river] creating shard_id [0]\r\n[2013-10-07 15:35:49,258][DEBUG][index.deletionpolicy     ] [Peepers] [_river][0] Using [keep_only_last] deletion policy\r\n[2013-10-07 15:35:49,287][DEBUG][index.merge.policy       ] [Peepers] [_river][0] using [tiered] merge policy with expunge_deletes_allowed[10.0], floor_segment[2mb], max_merge_at_once[10], max_merge_at_once_explicit[30], max_merged_segment[5gb], segments_per_tier[10.0], reclaim_deletes_weight[2.0], async_merge[true]\r\n[2013-10-07 15:35:49,293][DEBUG][index.merge.scheduler    ] [Peepers] [_river][0] using [concurrent] merge scheduler with max_thread_count[1]\r\n[2013-10-07 15:35:49,377][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [CREATED]\r\n[2013-10-07 15:35:49,389][DEBUG][index.translog           ] [Peepers] [_river][0] interval [5s], flush_threshold_ops [5000], flush_threshold_size [200mb], flush_threshold_period [30m]\r\n[2013-10-07 15:35:49,467][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [CREATED]->[RECOVERING], reason [from gateway]\r\n[2013-10-07 15:35:49,474][DEBUG][index.gateway            ] [Peepers] [_river][0] starting recovery from local ...\r\n[2013-10-07 15:35:49,620][DEBUG][index.engine.robin       ] [Peepers] [_river][0] starting engine\r\n[2013-10-07 15:35:49,708][DEBUG][cluster.service          ] [Peepers] processing [create-index [_river], cause [auto(index api)]]: done applying updated cluster_state (version: 3)\r\n[2013-10-07 15:35:50,875][DEBUG][index.shard.service      ] [Peepers] [_river][0] scheduling refresher every 1s\r\n[2013-10-07 15:35:50,882][DEBUG][index.shard.service      ] [Peepers] [_river][0] scheduling optimizer / merger every 1s\r\n[2013-10-07 15:35:50,884][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [RECOVERING]->[POST_RECOVERY], reason [post recovery from gateway, no translog]\r\n[2013-10-07 15:35:50,888][DEBUG][index.gateway            ] [Peepers] [_river][0] recovery completed from [local], took [1.4s]\r\n[2013-10-07 15:35:50,890][DEBUG][cluster.action.shard     ] [Peepers] sending shard started for [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]\r\n[2013-10-07 15:35:50,892][DEBUG][cluster.action.shard     ] [Peepers] received shard started for [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]\r\n[2013-10-07 15:35:50,900][DEBUG][cluster.service          ] [Peepers] processing [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]: execute\r\n[2013-10-07 15:35:50,905][DEBUG][cluster.action.shard     ] [Peepers] [_river][0] will apply shard started [_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING], indexUUID [hhVydPb3QvuD110I9V4XyA], reason [after recovery from gateway]\r\n[2013-10-07 15:35:50,915][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [4], source [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]\r\n[2013-10-07 15:35:50,917][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 4\r\n[2013-10-07 15:35:50,920][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 4\r\n[2013-10-07 15:35:50,924][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute\r\n[2013-10-07 15:35:50,936][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-10-07 15:35:50,937][DEBUG][index.shard.service      ] [Peepers] [_river][0] state: [POST_RECOVERY]->[STARTED], reason [global state moved to started]\r\n[2013-10-07 15:35:51,155][DEBUG][cluster.service          ] [Peepers] processing [shard-started ([_river][0], node[FDoGx4I-T8SLGc1IYsBkzw], [P], s[INITIALIZING]), reason [after recovery from gateway]]: done applying updated cluster_state (version: 4)\r\n[2013-10-07 15:35:51,762][DEBUG][cluster.service          ] [Peepers] processing [update-mapping [_river][test]]: execute\r\n[2013-10-07 15:35:51,962][DEBUG][cluster.metadata         ] [Peepers] [_river] update_mapping [test] (dynamic) with source [{"test":{"index_analyzer":"default_index","search_analyzer":"default_search","properties":{"type":{"type":"string"}}}}]\r\n[2013-10-07 15:35:52,022][DEBUG][cluster.service          ] [Peepers] cluster state updated, version [5], source [update-mapping [_river][test]]\r\n[2013-10-07 15:35:52,032][DEBUG][cluster.service          ] [Peepers] publishing cluster state version 5\r\n[2013-10-07 15:35:52,042][DEBUG][cluster.service          ] [Peepers] set local cluster state to version 5\r\n[2013-10-07 15:35:52,055][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: execute\r\n[2013-10-07 15:35:52,233][DEBUG][river.cluster            ] [Peepers] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-10-07 15:35:52,382][DEBUG][cluster.service          ] [Peepers] processing [update-mapping [_river][test]]: done applying updated cluster_state (version: 5)\r\n'
3836,'spinscale','NPE in facet filter parser\nES version 0.90.5, class `FacetParseElement`, method `parse`, line #86:\r\n\r\n    filter = context.queryParserService().parseInnerFilter(parser).filter();\r\n\r\nMethod `IndexQueryParserService.parseInnerFilter` is annotated as @Nullable and in fact it does return `null` in certain cases and it results in NPE in the aforementioned `FacetParseElement.parse` method.\r\n\r\nExamples of query that throws NPE:\r\n\r\n1) Empty \'facet_filter\':\r\n\r\n    {\r\n      "facets": {\r\n        "foo": {\r\n          "filter": {\r\n            "match_all": {}\r\n          },\r\n          "facet_filter": {}\r\n        }\r\n      }\r\n    }\r\n\r\n2) Empty \'filters\' array in \'and\' filter:\r\n\r\n    {\r\n      "facets": {\r\n        "foo": {\r\n          "filter": {\r\n            "match_all": {}\r\n          },\r\n          "facet_filter": {\r\n            "and": {\r\n              "filters": []\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nWhile syntactically or semantically those queries might not be correct ElasticSearch should return a meaningful error rather than NPE.'
3835,'spinscale','Java Client TransportSerializationException\nGreetings, I\'ve been having trouble with the latest releases of elasticsearch and its java client.\r\nI\'m getting this exception for various things I\'m trying to do:\r\n\r\n<code>\r\n[elasticsearch[Shrunken Bones][transport_client_worker][T#4]{New I/O worker #4}] WARN  org.elasticsearch.transport.netty - [Shrunken Bones] Message not fully read (response) for [1] handler org.elasticsearch.action.TransportActionNodeProxy$1@15eda628, error [true], resetting\r\nException in thread "main" org.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:171)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:125)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.io.StreamCorruptedException: unexpected end of block data\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1369)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)\r\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:498)\r\n\tat java.net.InetSocketAddress.readObject(InetSocketAddress.java:187)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:601)\r\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1004)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1866)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1964)\r\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1888)\r\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\r\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:369)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:169)\r\n\t... 23 more\r\n</code>\r\n\r\nHere are some notes:\r\n1. I\'m using the same jvm for the client and elasticsearch. Tested with java7_40, java7_26 and java6_something\r\n2. I\'m using matched versions of elasticsearch and its java client\r\n3. I\'m getting this exception for all versions of elasticsearch after 0.90.0.RC1 (yep, tested them all)\r\n4. I\'m using out-of-the-box configuration\r\n\r\nHere\'s some example code that I\'m running when I get the errors:\r\n<code>\r\ngetClient().admin().indices().prepareDeleteMapping(INDEX_TAGS).setType(TYPE_TAG).execute().actionGet();\r\n</code>\r\n\r\nDo you think it\'s an issue or am I messing up something?'
3833,'javanna','Delete warmer api to support acknowledgements\nThe delete warmer api always returns `"acknowledged":true`, as it doesn\'t support any acknowledgement mechanism when it comes to updating the cluster state. It currently returns after the master has applied the change locally, which doesn\'t necessarily mean that all the other nodes already know about the new cluster state version.\r\n\r\nWe then need to add support for acknowledgements from the other nodes and the timeout parameter, which refers to the maximum wait for acknowledgements (default 10s as in the other apis) in a backward compatible manner.'
3832,'s1monw','Deleting documents while scrolling deletes everything\nI have found rather a nasty bug (?) that I am not sure if it is due to my fault or ES. The code is intertwined with a lot of my own code so I am going to paste snippets. \r\n\r\n### What\'s happening?\r\n\r\nWhen deleting __only__ some documents while scrolling through a search query, then instead __all__ the documents are deleted. \r\n\r\nThis is how I delete:\r\n\r\n```java\r\nclient().prepareDeleteByQuery().setQuery(QueryBuilders.fieldQuery("_id", id))\r\n```\r\nNote: My routing is `uri` field so I have to delete by query. \r\n\r\n\r\n### More Details\r\n\r\nThis is how I do my search.\r\n\r\n```java\r\nSearchResponse searchResponse = client()\r\n                .prepareSearch(indicesForInterval(...))\r\n                .setSize(2000)\r\n                .setSearchType(SearchType.SCAN)\r\n                .setScroll(new TimeValue(5, TimeUnit.MINUTES))\r\n                .setFilter(...)\r\n                .execute().actionGet();\r\n```\r\n\r\n__However__, if I use `termQuery` instead of `fieldQuery` then the correct behavior happens. \r\n\r\nI can provide more detail if needed. But I have over 300 lines of code that has all this stuff baked in to my own classes. \r\n\r\nIs this a bug? Is it supposed to delete all documents when I am doing a scroll?'
3831,'javanna','Put warmer api to support acknowledgements\nThe put warmer api always returns `"acknowledged":true`, as it doesn\'t support any acknowledgement mechanism when it comes to updating the cluster state. It currently returns after the master has applied the change locally, which doesn\'t necessarily mean that all the other nodes already know about the new cluster state version.\r\n\r\nWe then need to add support for acknowledgements from the other nodes and the `timeout` parameter, which refers to the maximum wait for acknowledgements (default 10s as in the other apis) in a backward compatible manner. '
3828,'dadoonet','Support year units in date math expressions\nAccording to http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-date-format.html, the date math expressions support M (month), w (week), h (hour), m (minute), and s (second) units. Why years are not supported? Please add support for year units.'
3826,'imotov','Snapshot/Restore API - Phase I\n## Snapshot And Restore\r\n\r\nThe snapshot and restore module will allow to create snapshots of individual indices or an entire cluster into a remote repository and restore these indices back to the same or a different cluster afterwards. The phase I will only support shared file system repository and S3 repository.\r\n\r\n### Repositories\r\n\r\nBefore any snapshot or restore operation can be performed a snapshot repository should be registered in Elasticsearch. The following command registers a shared file system repository with the name `my_backup` that will use location `/mount/backups/my_backup` to store snapshots.\r\n\r\n```\r\n$ curl -XPUT \'http://localhost:9200/_snapshot/my_backup\' -d \'{\r\n    "type": "fs",\r\n    "settings": {\r\n        "location": "/mount/backups/my_backup",\r\n        "compress": true\r\n    }\r\n}\'\r\n```\r\n\r\nOnce repository is registered, its information can be obtained using the following command:\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/_snapshot/my_backup?pretty\'\r\n```\r\n```\r\n{\r\n  "my_backup" : {\r\n    "type" : "fs",\r\n    "settings" : {\r\n      "compress" : "false",\r\n      "location" : "/mount/backups/my_backup"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIf a repository name is not specified, or `_all` is used as repository name Elasticsearch will return information about all repositories currently registered in the cluster:\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/_snapshot\'\r\n```\r\n\r\nor\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/_snapshot/all\'\r\n```\r\n\r\n##### Shared File System Repository\r\n\r\nThe shared file system repository (`"type": "fs"`) is using shared file system to store snapshot. The path specified in the `location` parameter should point to the same location in the shared filesystem and be accessible on all data and master nodes. The following settings are supported:\r\n\r\n`location` - Location of the snapshots. Mandatory\r\n`compress` - Turns on compression of the snapshot files. Defaults to `true\r\n`concurrent_streams` - Throttles the number of streams (per node) preforming snapshot operation. Defaults to 5\r\n`chunk_size` - Big files can be broken down into chunks during snapshotting if needed. Defaults to unlimited.\r\n\r\n\r\n### Snapshot\r\n\r\nA repository can contain multiple snapshots of the same cluster. Snapshot are identified by unique names within the cluster. A snapshot with the name `snapshot_1` in the repository `my_backup` can be created by executing the following command:\r\n\r\n```\r\n$ curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true"\r\n```\r\n\r\nThe `wait_for_completion` parameter specifies whether or not the request should return immediately or wait for snapshot completion. By default snapshot of all open and started indices in the cluster is created. This behavior can be changed by specifying the list of indices in the body of the snapshot request.\r\n\r\n```\r\n$ curl -XPUT "localhost:9200/_snapshot/my_backup/snapshot_1" -d \'{\r\n    "indices": "index_1,index_2",\r\n    "ignore_indices": "missing"\r\n}\'\r\n```\r\n\r\nThe list of indices that should be included into the snapshot can be specified using the `indices` parameter that supports multi index syntax. The snapshot request also supports the `ignore_indices` option. Setting it to `missing` will cause indices that do not exists to be ignored during snapshot creation. By default, when `ignore_indices` option is not set and an index is missing the snapshot request will fail.\r\n\r\nThe index snapshot process is incremental. In the process of making the index snapshot Elasticsearch analyses the list of the index files that are already stored in the repository and copies only files that were created or changed since the last snapshot. That allows multiple snapshots to be preserved in the repository in a compact form. Snapshotting process is executed in non-blocking fashion. All indexing and searching operation can continue to be executed against the index that is being snapshotted. However, a snapshot represents the point-in-time view of the index at the moment when snapshot was created, so no records that were added to the index after snapshot process had started will be present in the snapshot.\r\n\r\nBesides creating a copy of each index the snapshot process can also store global cluster metadata, which includes persistent cluster settings and templates. The transient settings and registered snapshot repositories are not stored as part of the snapshot.\r\n\r\nOnly one snapshot process can be executed in the cluster at any time. While snapshot of a particular shard is being created this shard cannot be moved to another node, which can interfere with rebalancing process and allocation filtering. Once snapshot of the shard is finished Elasticsearch will be able to move shard to another node according to the current allocation filtering settings and rebalancing algorithm.\r\n\r\nOnce a snapshot is created information about this snapshot can be obtained using the following command:\r\n\r\n```\r\n$ curl -XGET "localhost:9200/_snapshot/my_backup/snapshot_1"\r\n```\r\n\r\nAll snapshots currently stored in the repository can be listed using the following command:\r\n\r\n```\r\n$ curl -XGET "localhost:9200/_snapshot/my_backup/_all"\r\n```\r\n\r\nA snapshot can be deleted from the repository using the following command:\r\n\r\n```\r\n$ curl -XDELETE "localhost:9200/_snapshot/my_backup/snapshot_1"\r\n```\r\n\r\nWhen a snapshot is deleted from a repository, Elasticsearch deletes all files that are associated with the deleted snapshot and not used by any other snapshots. If the deleted snapshot operation is executed while the snapshot is being created the snapshotting process will be aborted and all files created as part of the snapshotting process will be cleaned. Therefore, the delete snapshot operation can be used to cancel long running snapshot operations that were started by mistake.\r\n\r\n### Restore\r\n\r\nA snapshot can be restored using this following command:\r\n\r\n```\r\n$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore"\r\n```\r\n\r\nBy default, all indices in the snapshot as well as cluster state are restored. It\'s possible to select indices that should be restored as well as prevent global cluster state from being restored by using `indices` and `restore_global_state` options in the restore request body. The list of indices supports multi index syntax. The `rename_pattern` and `rename_replacement` options can be also used to rename index on restore using regular expression that supports referencing the original text as explained [here](http://docs.oracle.com/javase/6/docs/api/java/util/regex/Matcher.html#appendReplacement(java.lang.StringBuffer,%20java.lang.String)).\r\n\r\n```\r\n$ curl -XPOST "localhost:9200/_snapshot/my_backup/snapshot_1/_restore" -d \'{\r\n    "indices": "index_1,index_2",\r\n    "ignore_indices": "missing",\r\n    "restore_global_state": false,\r\n    "rename_pattern": "index_(.)+",\r\n    "rename_replacement": "restored_index_$1"\r\n}\'\r\n```\r\n\r\nThe restore operation can be performed on a functioning cluster. However, an existing index can be only restored if it\'s closed. The restore operation automatically opens restored indices if they were closed and creates new indices if they didn\'t exist in the cluster. If cluster state is restored, the restored templates that don\'t currently exist in the cluster are added and existing templates with the same name are replaced by the restored templates. The restored persistent settings are added to the existing persistent settings.\r\n\r\n'
3825,'drewr','Threadpool defaults assume single node per machine\nThe [threadpool defaults](https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/threadpool/ThreadPool.java#L103) are too aggressive if you run more than one ES node on a single machine.  Would like to add a `threadpool.nodes_per_machine` setting (default `1`) which is used to calculate the defaults. Could be that influences the value of `availableProcessors` that is then used in the settings.\r\n'
3824,'costin','Checking for jvm.dll fails when %JAVA_HOME% contains spaces\n'
3822,'martijnvg','Parent / child queries should also work apis other than search api\nThe `has_child`, `has_parent` and `top_children` now *only* work in the search api due to how this api executed these queries. These queries should also work in other apis, like count, explain and delete by query apis.'
3820,'spinscale',"Debian Package sets /etc/elasticsearch/* to 0644\nDEBIAN/postinst:37\r\n    chmod 644 /etc/elasticsearch/*\r\n\r\nthis is only a good idea as long as there are no subdirectories in /etc/elasticsearch/ because after updating the elasticsearch package files in /etc/elasticsearch/synonyms (for example) can't be read anymore.\r\n"
3819,'jpountz',"Better warm-up of merged segments\nQuery-based warm-up is convenient as it doesn't require to know how things work under the hood to warm-up a segment: people just have to put there queries that are similar to their users' queries.\r\n\r\nHowever, warm-up could be made more efficient through `IndexWriterConfig.setMergedSegmentWarmer`/`IndexWriter.IndexReaderWarmer` which doesn't require running a query at all and is run during the merging phase, before the new segment goes live. I think we should expose a way to register such warmers and maybe use something like `SimpleMergedSegmentWarmer` by default."
3818,'martijnvg',"has_child ignores type field for Grandparent/Parent/Child query\nI've got an issue where the type field seems to be ignored for the child document, ES (0.90.2) seems to return matches as long as any of it's child documents (regardless of type) has a match in a field with the same name. Please see the following gist: https://gist.github.com/erlingwl/6779401\r\n\r\nThis was confirmed by Martijn as well, see this thread: https://groups.google.com/forum/#!topic/elasticsearch/V-bKL9QHftc"
3814,'bleskes','Accept dynamic templates with only a match type criteria\nIf some wants all strings in an index to default to `not_analyzed`, you can do that via the dynamic template settings:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/index" -d\'\r\n{\r\n   "mappings": {\r\n      "_default_": {\r\n         "dynamic_templates": [\r\n            {\r\n               "test": {\r\n                  "match_mapping_type": "string",\r\n                  "match": "*", // <- not needed when we have \'match_mapping_type\'\r\n                  "mapping": {\r\n                      "index": "not_analyzed"\r\n                  }\r\n               }\r\n            }\r\n         ]\r\n      }\r\n   }\r\n}\'\r\n```\r\n\r\nHowever, if you remove the `"match":"*"`, you get the following error:\r\n```\r\n{\r\n   "error": "MapperParsingException[mapping [_default_]]; nested: MapperParsingException[template must have match or path_match set]; ",\r\n   "status": 400\r\n}\r\n```\r\n\r\nA minor issue but if one specifies `match_mapping_type`, we can default `match` to `*`\r\n\r\n'
3812,'bleskes','Get index templates API returns and error if no template is supplied\nCall this:\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/_template"\r\n````\r\n\r\nAnd get a status code of 500.'
3809,'kimchy','Empty filters returns no result\nto reproduce:\r\n```\r\ncurl -XPOST http://localhost:9200/foo -d \'{"settings":{"number_of_shards":1,"number_of_replicas":0}}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{"id":1,"content":"foobar"}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d \'{"filter":{"and":{"filters":[]}}}\'\r\n```\r\nthis returns results for 0.90.0 until 0.90.2, and starting with 0.90.3 it returns no results.\r\ni looked around a little bit and this seems to be due to https://github.com/elasticsearch/elasticsearch/commit/74a7c46b0e0e4c921e30574214da6b77ef354b18\r\njust want to confirm this is and will be the correct behavior for this.\r\nthanks'
3808,'s1monw','pattern_capture token filter does not throw error with patterns missing.\nJust a simple note. When creating a custom_filter, the pattern_capture token filter does not throw an error with patterns missing. Instead, it must default to some behavior. I had mistakenly used "pattern" with a single string rather than "patterns" with an array.\r\n\r\nhttps://gist.github.com/jtreher/6766911\r\n\r\n{\r\n\t"settings": {\r\n\t\t "_default_" : {},\r\n\t\t"analysis": {\r\n\t\t\t"filter": {\r\n        \t\t"sample":{\r\n        \t\t\t"type":"pattern_capture",\r\n        \t\t\t"pattern":"(([a-z]+)(\\\\d*))"\r\n        \t\t\r\n        \t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}'
3807,'dakrone',"Low-disk space awareness\nWe are seeing some large indices being optimized and occasionally servers running very low on disk space because of that.\r\n\r\nMaybe if an index doesn't have enough room to optimize a rebalancing should kick in?\r\n\r\nAt least be smart enough not to get to a point of running out of hard-disk space while optimizing, and then crashing. Which is something that we've seen happen."
3806,'jpountz','Disk-based field data\nLucene 4.0 introduced *doc values*, which are very similar to field data except that they are computed and persisted to disk (per segment) at index time. At search time, these data-structures are either loaded into memory or directly read from disk  depending on the doc values format. Starting with Lucene 4.5, the default is to load the small data-structures that matter for performance into memory (ordinals) and to keep the large data-structures on disk (values). It would be interesting to have a new field data implementation that would be backed by doc values.\r\n\r\nIntegration into Elasticsearch would allow for having disk-based field data and for configuring smaller heaps, which would be less subject to garbage collection issues. On the other hand, this will require additional disk space and since doc values are disk-based by default, they will probably be slower for field-data-intensive workloads.'
3804,'bleskes','[Feature Request] Add node version to NodeStats\nI know I can already get the version through a different endpoint, but it would be great if this could also be available through "_cluster/nodes/stats".\r\nSounds reasonable?'
3803,'martijnvg','Facet on nested documents field\nHello,\r\nI have to perform facets on a huge quantity of documents, but the query must be something like:\r\n\r\nI am searching for documents having field name TheField containing XXXXXX and their children, then perform a facets on all of them on the field TheStatField.\r\n\r\nI am unable to do it using nested documents.'
3801,'jpountz','ClassCastException when terms facet produces high counts\nLooks like every terms facet that has to produce high term counts fails with this ClassCastException. This is 100% reproducible for me with this field.\r\n\r\nIf i narrow the query to a smaller result set it works because it seems like it does not have to cast.\r\n\r\nElasticSearch v0.90.5 on OSX 10.8.4.\r\n\r\n    POST http://localhost:9200/graylog2_*/_search\r\n\r\n    {\r\n        "query" : {\r\n            "match_all" : {  }\r\n        },\r\n        "facets" : {\r\n            "tag" : {\r\n                "terms" : {\r\n                    "field" : "min",\r\n                    "size" : 1    \r\n                }    \r\n            }\r\n        }\r\n    }\r\n\r\nThe query returns:\r\n\r\n    {\r\n        error: ReduceSearchPhaseException[Failed to execute phase [fetch], [reduce] ]; nested: ClassCastException[org.elasticsearch.search.facet.terms.longs.InternalLongTermsFacet$LongEntry cannot be cast to org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet$DoubleEntry]; \r\n        status: 503\r\n    }    \r\n\r\nFrom the ElasticSearch log:\r\n\r\n\torg.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [fetch], [reduce] \r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:182)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:156)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchQueryThenFetchAction.java:150)\r\n\t\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:407)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:150)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:134)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tat java.lang.Thread.run(Thread.java:722)\r\n\tCaused by: java.lang.ClassCastException: org.elasticsearch.search.facet.terms.longs.InternalLongTermsFacet$LongEntry cannot be cast to org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet$DoubleEntry\r\n\t\tat org.elasticsearch.search.facet.terms.doubles.InternalDoubleTermsFacet.reduce(InternalDoubleTermsFacet.java:182)\r\n\t\tat org.elasticsearch.search.controller.SearchPhaseController.merge(SearchPhaseController.java:340)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.innerFinishHim(TransportSearchQueryThenFetchAction.java:193)\r\n\t\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.finishHim(TransportSearchQueryThenFetchAction.java:180)\r\n\t\t... 8 more'
3800,'javanna',"Added generic cluster state update ack mechanism\nAdded new AckedClusterStateUpdateTask interface that can be used to submit cluster state update tasks and allows actions to be notified back when a set of (configurable) nodes have acknowledged the cluster state update. Supports a configurable timeout, so that we wait for acknowledgement for a limited amount of time (will be provided in the request as it curently happens, default 10s).\r\n\r\nInternally, a low level AckListener is created (InternalClusterService) and passed to the publish method, so that it can be notified whenever each node responds to the publish request. Once all the expected nodes have responded or the timeoeout has expired, the AckListener notifies the action which will return adding the proper acknowledged flag to the response.\r\n\r\nIdeally, this new mechanism will gradually replace the existing ones based on custom endpoints and notifications (per api).\r\n\r\nOnce we get this in I'll start adding this new mechanism to the apis that don't currently support acknowledgements when changing the cluster state (e.g. put and delete warmer etc.).\r\n\r\nCloses #3786"
3797,'javanna','multi_match lenient query with boosted field crashes with NullPointerException\nA lenient multi_match query with a boosted field with type mismatch crashes. Simple example:\r\n\r\n`curl -XPUT http://localhost:9200/blog/post/1?pretty=1 -d \'{"foo":123, "bar":"xyzzy"}\'`\r\n`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d \'{"multi_match": {"fields": ["foo^2", "bar"], "lenient": true, "query": "xyzzy"}}\' # crashes with NullPointerException`\r\n\r\nInterestingly, it works for internal _id field:\r\n\r\n`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d \'{"multi_match": {"fields": ["_id^2", "bar"], "lenient": true, "query": "xyzzy"}}\' # works`\r\n\r\nAnd it doesn\'t crash when there\'s no type mismatch:\r\n\r\n`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d \'{"multi_match": {"fields": ["foo^2", "bar"], "lenient": true, "query": "123"}}\' # works`\r\n\r\nOther queries for reference:\r\n\r\n`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d \'{"multi_match": {"fields": ["foo", "bar"], "lenient": true, "query": "xyzzy"}}\' # works`\r\n`curl -XGET http://localhost:9200/blog/post/_count?pretty=1 -d \'{"multi_match": {"fields": ["foo", "bar"], "query": "xyzzy"}}\' # crashes with NumberFormatException - expected`'
3796,'javanna','Fix comment grammar\n'
3793,'martijnvg','Order of parameters in suggest should not be important\nThe following works:\r\n\r\n    curl -XPUT \'http://localhost:9200/test/test/1?pretty=1\' -d \'\r\n    {\r\n       "color" : "green"\r\n    }\r\n    \'\r\n\r\n\r\n    curl -XGET \'http://localhost:9200/_search?pretty=1&size=0\' -d \'\r\n    {\r\n       "suggest" : {\r\n          "my_suggest" : {\r\n             "text" : "green",\r\n             "term" : {\r\n                "field" : "color"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [],\r\n    #       "max_score" : 1,\r\n    #       "total" : 1\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "suggest" : {\r\n    #       "my_suggest" : [\r\n    #          {\r\n    #             "length" : 5,\r\n    #             "options" : [],\r\n    #             "text" : "green",\r\n    #             "offset" : 0\r\n    #          }\r\n    #       ]\r\n    #    },\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 2\r\n    # }\r\n\r\nIf you switch the `term` and `text` order, then you get this instead:\r\n\r\n    curl -XGET \'http://localhost:9200/_search?pretty=1&size=0\' -d \'\r\n    {\r\n       "suggest" : {\r\n          "my_suggest" : {\r\n             "term" : {\r\n                "field" : "color"\r\n             },\r\n             "text" : "green"\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[u9ErnmcARw2y3uLywGfydQ][test][1]: SearchParseException[[test][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }{[u9ErnmcARw2y3uLywGfydQ][test][4]: SearchParseException[[test][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }{[u9ErnmcARw2y3uLywGfydQ][test][2]: SearchParseException[[test][2]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"suggest":{"my_suggest":{"term":{"field":"color"},"text":"green"}}}]]]; nested: ElasticSearchIllegalArgumentException[The required text option is missing]; }]\r\n'
3792,'spinscale','Allow starting multiple ES instance via init scripts\nif you have really powerful machines, it is advised to start up several elasticsearch nodes inside of that machine.\r\n\r\nRight now neither the deb nor the RPM init script support this. An idea is to configure a `NUM_OF_INSTANCES` parameter, which would allow to fire up and shut down more than one elasticsearch instance.'
3790,'spinscale',"Add version to Analyzers\nThis relates somewhat to #3775 since we want to change the default analyzer in 1.0 we somehow need to decide which version of an analyzer we need to load for a name in a mapping. For instance if an index was created with `0.90.x` or earlier the *old* `default` analyzer must be used in order to maintain index backwards compatibility for searches etc.\r\n\r\nWe don't necessarily need this in `0.90` but for now I flagged it as `0.90` unless it's breaking anything"
3786,'javanna','Generic cluster state update ack mechanism\nMost of the apis that allow to make changes to the cluster state (e.g. delete index, put mapping, open/close index) currently support an acknowledgement mechanism.\r\n\r\nWhen it comes to updating the cluster state, the update request (e.g. open/close index, put mapping etc.) is processed on the master, then the updated cluster state is pushed to all the other nodes. The json response contains a boolean `acknowledged` flag that tells whether the cluster state change has already been applied by all nodes. The master node waits (maximum 10 seconds, configurable per request) for an ack message from each node. Those ack notifications are api specific, although similar given their same purpose.\r\n\r\nThe goal of this issue is to add support for a generic ack mechanism that can be reused (and added where missing) in all apis  that update the cluster state. Ideally, the new mechanism should work at a lower level and consists of a listener that is called whenever we get a response directly to the publish request, instead of having an additional endpoint per api and waiting for custom notifications asynchronously.\r\n\r\nThis relates to the work done in #3736, as the master publish request gets now a response from each node when the new cluster state has already been processed (not only when it was received and the update was locally submitted, but not necessarily processed yet). \r\n\r\nNote that the ack mechanism needs to be completely detached from the 5 seconds wait introduced with #3736, whose goal was to try and wait for replies before processing another cluster state update on the master. Those same replies would become our generic ack messages, which determine the value of the `acknowledged` flag in the response: `true` if all nodes acked the cluster state update, `false` if the configurable timeout expired.\r\n'
3785,'costin',"service.bat doesn't properly set the memory limits for installed services\nCurrently the memory options are passed on as java options but these are ignored by the jvm.dll. They need to be extracted and passed through different arguments so they can be used before starting the java process.\r\nUnfortunately this also means doing some conversion (from GB to MB and MB to KB)."
3783,'bleskes','Use the new index UUID to ensure mapping update events from older indices are not applied to new indices with the same name\nThis can currently happen if an index is repeatedly created by an indexing operation and deleted quickly afterwards. '
3782,'bleskes','Update-mapping event is not sent if the subsequent indexing operation run into trouble\nIf an index request updated the mapping but failed when actually indexing into a shard, the mapping update was not sent to master.'
3781,'spinscale',"Random startup / connection failures\nHello,\r\n\r\nElasticsearch has been part of [Semaphore](https://semaphoreapp.com)'s CI build platform since January 2013. Over time and with all versions (we're currently [running](http://docs.semaphoreapp.com/version-information) 0.90.2 inside Ubuntu 12.04 LXC containers) our users randomly get connection errors such as:\r\n\r\n```\r\nSkipping index creation, cannot connect to Elasticsearch\r\n(The original exception was: #<Errno::ECONNREFUSED: Connection refused - connect(2)>)\r\n```\r\n\r\nUser who's log I'm getting this from is running Tire 0.5.8.\r\n\r\nThis is how we set up Elasticsearch:\r\n\r\n```\r\ncd /tmp\r\nwget http://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.2.tar.gz -O elasticsearch.tar.gz\r\ntar -xf elasticsearch.tar.gz\r\nrm elasticsearch.tar.gz\r\nmv elasticsearch-* elasticsearch\r\nmv elasticsearch /usr/local/share\r\ncurl -L https://github.com/elasticsearch/elasticsearch-servicewrapper/tarball/master | tar -xz\r\nmv *servicewrapper*/service /usr/local/share/elasticsearch/bin/\r\nrm -Rf *servicewrapper*\r\n/usr/local/share/elasticsearch/bin/service/elasticsearch install\r\nln -s `readlink -f /usr/local/share/elasticsearch/bin/service/elasticsearch` /usr/local/bin/rcelasticsearch\r\nservice elasticsearch start\r\n```\r\n\r\nIn some cases we've even told users to run another `sudo service elasticsearch stop/start` themselves before the build, and even though the output of that looks good, the problem may still happen.\r\n\r\nI'd really love to get some clue into why this is happenning. I would appreciate any help."
3775,'s1monw',"Change default Analyzer in next major version\nThe default analyzer (StandardAnalyzer in Lucene terms) is not a really good default since it really aims to be applied on English full-text. I think it would be wise to only use `StandardTokenizer` which is based on `Unicode Standard Annex #29` and a `LowercaseFilter`. I could think of using `ASCIIFoldingFilter` as well since most of the users will expect folding to work out of the box. \r\n\r\nThis is really a basis for discussion but I think we should really get rid of stopwords in the default analyzer since it's really trappy."
3774,'s1monw','do not silently fail on plugin install with source files\nWhen trying to install a plugin from source files only a debug error is issued, making the plugin cli exit without an error code. This is creates problems for automated installs because the error will not be detected.\r\n\r\nCurrent behavior:\r\n```bash\r\nubuntu@es5:~$ sudo plugin -i elasticsearch/elasticsearch-river-wikipedia\r\n-> Installing elasticsearch/elasticsearch-river-wikipedia...\r\nTrying https://github.com/elasticsearch/elasticsearch-river-wikipedia/archive/master.zip...\r\nDownloading ...........DONE\r\nInstalled elasticsearch/elasticsearch-river-wikipedia into /usr/local/elasticsearch-0.90.5/plugins/river-wikipedia\r\nubuntu@es5:~$ echo $?\r\n0\r\nubuntu@es5:~$ plugin -l\r\nInstalled plugins:\r\n    - HQ\r\n    - head\r\n    - bigdesk\r\n    - paramedic\r\n    - browser\r\n```\r\n\r\nNew behavior:\r\n```bash\r\nbkw@Aeronaut ★ ~/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT\r\n> ./bin/plugin -i elasticsearch/elasticsearch-river-wikipedia\r\n-> Installing elasticsearch/elasticsearch-river-wikipedia...\r\nTrying https://github.com/elasticsearch/elasticsearch-river-wikipedia/archive/master.zip...\r\nDownloading ......DONE\r\nInstalled elasticsearch/elasticsearch-river-wikipedia into /Users/bkw/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT/plugins/river-wikipedia\r\nFailed to install elasticsearch/elasticsearch-river-wikipedia, reason: Plugin installation assumed to be site plugin, but contains source code, aborting installation.\r\nbkw@Aeronaut ☄ 1 ~/tmp/es/elasticsearch-1.0.0.Beta1-SNAPSHOT\r\n> echo $?\r\n74\r\n```\r\n\r\nPR includes a dummy plugin zipfile with only one java file in it for testing.\r\n\r\nCLA: XYVCQP8743E5W4R'
3773,'javanna','Issue #3768 Changing log level and not reporting IndexOutOfBoundExceptio...\nLog level changed to warn and the exception is not reported..As it is not relevant'
3772,'brwe',"Add support for using payloads to boost terms\nIt would be great to be able so have a mapping field which stores payloads with terms and be able to use the payloads to boost the score of the document.\r\n\r\nIn my particular use case, I have documents which are tagged by users and after running through filters and algorithms we can determine which tags are most likely useful and which are likely spam. We'd like to pass that information on to the search index so that we can boost the documents we think are most appropriate to the search terms. In this case the boost is known at indexing time and applies to the terms themselves and not to the field or the documents.\r\n\r\nThis is something that's been possible with Lucene for quite awhile and which Solr had partial support for, but never fully implemented out of the box. (See for example http://wiki.apache.org/solr/Payloads, http://searchhub.org/2009/08/05/getting-started-with-payloads/, http://hnagtech.wordpress.com/2013/04/19/using-payloads-with-solr-4-x/).\r\n\r\nIdeally, it would be best to pass the payload in as a separate JSON field value in the document. The Solr tokenizer for payloads (DelimitedPayloadTokenFilterFactory) uses a delimiter, but I've found this to be problematic when dealing with user generated terms. In addition, it would be best to have the payload value somehow available in scripting so the payloads can be indexed once and then the scoring algorithms tweaked as necessary to get the right scores."
3763,'martijnvg','Refresh index (and specific shard if routing=True) before doing search:\nBasically add an option (i searched the docs) to refresh an entire index or that specific shard before searching if it is possible?\r\n\r\nThanks'
3761,'javanna','Throwing exception from BulkProcessor.Listener.afterBulk results in closing down transport\nThrowing exception from the BulkProcessor.Listener.afterBulk results in transport getting closed. This results in all subsequent calls getting NoNodeAvailableException.\r\nIdeally it should throw ExecutionException on the caller thread or if the bulk action is executed by scheduled background thread, just log and eat up the exception probably.\r\n\r\nAdded unit test.\r\n\r\n```java\r\nimport org.elasticsearch.action.bulk.BulkProcessor;\r\nimport org.elasticsearch.action.bulk.BulkRequest;\r\nimport org.elasticsearch.action.bulk.BulkResponse;\r\nimport org.elasticsearch.action.index.IndexRequest;\r\nimport org.elasticsearch.action.index.IndexRequestBuilder;\r\nimport org.elasticsearch.client.transport.TransportClient;\r\nimport org.elasticsearch.common.settings.ImmutableSettings;\r\nimport org.elasticsearch.common.transport.InetSocketTransportAddress;\r\n\r\nimport java.util.ArrayList;\r\nimport java.util.List;\r\n\r\npublic class BulkProcessorTest {\r\n\r\n    private static final int NUM_DOCS = 10;\r\n    private static final String CONSTANT_ID = "abcdef";\r\n\r\n    @org.junit.Test\r\n    public void test() throws InterruptedException {\r\n        TransportClient client = createClient();\r\n        BulkProcessor bulkProcessor = BulkProcessor.builder(client, new BulkProcessorProblematicListener()).setBulkActions(NUM_DOCS).build();\r\n\r\n        List<Document> docs = createDocsWithSameId(NUM_DOCS);\r\n\r\n        int iter = 3;\r\n        for (int i = 0; i < iter; i++) {\r\n            System.out.println("Iteration #" + i);\r\n            for (Document doc : docs) {\r\n                IndexRequestBuilder indexRequestBuilder = client.prepareIndex("test", "bulk", doc.id);\r\n                indexRequestBuilder.setSource(doc.toJson());\r\n                // set op type to create for put-if-absent\r\n                indexRequestBuilder.setOpType(IndexRequest.OpType.CREATE);\r\n                bulkProcessor.add(indexRequestBuilder.request());\r\n            }\r\n            Thread.sleep(1000);\r\n        }\r\n    }\r\n\r\n    private List<Document> createDocsWithSameId(int n) {\r\n        List<Document> docs = new ArrayList<Document>();\r\n        for (int i = 0; i < n; i++) {\r\n            docs.add(createConstantIdDocument(i));\r\n        }\r\n        return docs;\r\n    }\r\n\r\n    private Document createConstantIdDocument(int id) {\r\n        Document doc = new Document();\r\n        doc.id = CONSTANT_ID;\r\n        doc.value = id;\r\n        return doc;\r\n    }\r\n\r\n    private static class Document {\r\n\r\n        private String id;\r\n        private int value;\r\n\r\n        public String toJson() {\r\n            return "{\\"value\\":\\"" + value + "\\"}";\r\n        }\r\n    }\r\n\r\n    private static class BulkProcessorProblematicListener implements BulkProcessor.Listener {\r\n        @Override\r\n        public void beforeBulk(long executionId, BulkRequest request) {\r\n\r\n        }\r\n\r\n        @Override\r\n        public void afterBulk(long executionId, BulkRequest request, BulkResponse response) {\r\n            if (response.hasFailures()) {\r\n                throw new RuntimeException("Failure in response - " + response.buildFailureMessage());\r\n            }\r\n        }\r\n\r\n        @Override\r\n        public void afterBulk(long executionId, BulkRequest request, Throwable failure) {\r\n            // throwing exception here makes the transport close resulting in NoNodeAvailableException\r\n            throw new RuntimeException("Caught exception in bulk: " + request + ", failure: " + failure, failure);\r\n        }\r\n    }\r\n\r\n    private TransportClient createClient() {\r\n        ImmutableSettings.Builder settings = ImmutableSettings.settingsBuilder().put("cluster.name", "test-es-cluster");\r\n        settings.put("nodes_sampler_interval", "10s");\r\n        TransportClient client = new TransportClient(settings);\r\n        client.addTransportAddress(new InetSocketTransportAddress("localhost", 9300));\r\n        return client;\r\n    }\r\n\r\n}\r\n\r\n'
3760,'costin',"Incorrect JVM_DLL environment variable definition in service.bat\nI am running Windows 8 64-bit. First, I uninstalled all previous Java-related installations. I confirmed that I had no ```C:\\Program Files\\Java``` folders. I then downloaded and installed JRE 7 from the last download link on this page:\r\n\r\nhttp://www.oracle.com/technetwork/java/javase/downloads/java-se-jre-7-download-432155.html\r\n\r\nI confirmed that Java was installed properly. Here is the installed path to ```jvm.dll```:\r\n\r\n```C:\\Program Files\\Java\\jre7\\bin\\server\\jvm.dll```\r\n\r\nMy JAVA_HOME environment variable is set to ```C:\\Program Files\\Java\\jre7```.\r\n\r\nUnfortunately, service.bat sets the JVM_DLL environment variable to this:\r\n\r\n```\r\n%JAVA_HOME%\\jre\\bin\\server\\jvm.dll\r\n```\r\n\r\nNote the extra ```jre``` path that does not exist with the JRE I installed. Perhaps this path changed in later versions of the JRE or in Oracle's installer? This problem causes the service to fail to start with this error:\r\n\r\n```\r\n[2013-09-22 14:23:27] [info]  [19408] Starting service...\r\n[2013-09-22 14:23:27] [error] [19408] Failed creating java C:\\Progra~1\\Java\\jre7\\jre\\bin\\server\\jvm.dll\r\n[2013-09-22 14:23:27] [error] [19408] The system cannot find the path specified.\r\n[2013-09-22 14:23:27] [error] [19408] ServiceStart returned 1\r\n```\r\n\r\nI fixed this easily by simply removing ```jre\\``` from the ```set JVM_DLL``` line. The batch file should probably use a different mechanism for determining the location of ```jvm.dll```."
3758,'drewr',"Cat shards/indices don't properly handle index parameter\n`/_cat/indices` doesn't handle it right, and `/_cat/shards` doesn't accept any indices."
3754,'s1monw',"Boost doesn't seem to work for prefix queries\nBoosts don't seem to work for prefix queries.  See https://gist.github.com/nik9000/6643155\r\n\r\nIt looks like the queryNorm value is always the inverse of the boost value, cancelling out the boost.\r\n\r\nThat seems wrong."
3753,'javanna','Allow slop = -1 in span queries (#3673)\nFinally. Fix #3673.'
3752,'martijnvg',"Allow _boost field to be indexed and stored in mapping\nThe idea of the `_boost` field is that it provides an index time boost for all fields a document has, because of this the `_boost` field isn't stored or indexed separately and only saved as part of the `_source`.\r\n\r\nHowever the `_boost` field is also used to as just a sort field, but because it can't be indexed separately, this doesn't work. By allowing to make the `_boost` indexable separately, sorting by the `_boost` field does work. By default the `_boost` field remains not indexed and not stored.\r\n\r\nThis issue originates from PR #2913"
3750,'jpountz',"It'd be cool if the highlighter could combine fields\nSo maybe I'm doing this wrong, but when I want to search across the same field with different analyzers run against it I use a multifield with the different analyzers setup in the mapping and then query the field who's analyzer I want to use.  This ends up highlighting the field twice - once per subfield.  It'd be cool if I could combine those highlight operations into one so the snippets would be combined together and sorted together.\r\n\r\nThis is an example of how I build the query: https://gist.github.com/nik9000/6638883\r\n"
3747,'spinscale','NPE when sorting on a completion field\n    curl -XPUT \'http://localhost:9200/test?pretty=1\' -d \'\r\n    {\r\n       "mappings" : {\r\n          "test" : {\r\n             "properties" : {\r\n                "name" : {\r\n                   "type" : "completion"\r\n                }\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n    curl -XGET \'http://localhost:9200/_search?pretty=1\' -d \'\r\n    {\r\n       "sort" : "name"\r\n    }\r\n    \'\r\n\r\n     SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; \r\n     shardFailures {[PDCUCebERaCutbxDWeAORg][test][4]:\r\n     SearchParseException[[test][4]:\r\n     from[-1],size[-1]:\r\n     Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:\r\n     NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][3]:\r\n     SearchParseException[[test][3]:\r\n     from[-1],size[-1]:\r\n     Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:\r\n     NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][2]:\r\n     SearchParseException[[test][2]:\r\n     from[-1],size[-1]:\r\n     Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:\r\n     NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][0]:\r\n     SearchParseException[[test][0]:\r\n     from[-1],size[-1]:\r\n     Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:\r\n     NullPointerException; }{[PDCUCebERaCutbxDWeAORg][test][1]:\r\n     SearchParseException[[test][1]:\r\n     from[-1],size[-1]:\r\n     Parse Failure [Failed to parse source [{"sort":"name"}]]]; nested:\r\n     NullPointerException; }]\r\n'
3746,'spinscale','Missing completion fields in nodes stats\nThe indices stats API supports `fields`/`completion_fields`/`fielddata_fields`:\r\n\r\n    GET /_stats/completion?fields=name\r\n    ....\r\n         "completion": {\r\n            "size": "20b",\r\n            "size_in_bytes": 20,\r\n            "fields": {\r\n               "name": {\r\n                  "size": "20b",\r\n                  "size_in_bytes": 20\r\n               }\r\n            }\r\n         }\r\n\r\nBut the nodes/indices stats only supports `fields` for fielddata, not for completion stats:\r\n\r\n    GET /_nodes/stats/indices/completion?fields=name\r\n    ....\r\n            "completion": {\r\n               "size": "20b",\r\n               "size_in_bytes": 20\r\n            }\r\n'
3742,'s1monw','Rename Engine#seacher() into Engine#acquireSearcher()\nThe name should reflect that the caller is responsible for\r\nreleaseing the searcher again.'
3739,'costin','service.bat should handle JRE not just JDK for starting Elasticsearch\nCurrently service.bat assumes a JDK is used but there are plenty of cases where a JRE Is used instead (the default Java installation).\r\nInstead of failing, service.bat should use that instead and give a proper warning.'
3738,'martijnvg',"Add documentation for nested filter's join parameter\nThe documentation for `nested` filter:\r\n\r\nhttp://www.elasticsearch.org/guide/reference/query-dsl/nested-filter/\r\n\r\nDoes not explain the `join` option added in https://github.com/elasticsearch/elasticsearch/issues/2606.\r\n\r\n> The nested filters will now support the a join option. Which controls whether to perform the block join. By default this enabled, but when disabled it returns the nested documents as hits instead of the joined root documen"
3734,'s1monw','Allow omit_norms for _all field\nPlease add support for settings omit_norms on the _all field.'
3729,'dadoonet','add time zone setting for relative date math in range filter/query\nHi,\r\n\r\nWhen using a range filter/query and using relative date math to round, it\'s not always preferable to have the resulting date be in UTC. Adding some way to specify the time zone of the relative date would remedy this. Ideally this could either be an explicit offset or the name of a time zone recognized by Joda-Time time.\r\n\r\nFor example, say I am in the America/New_York time zone and I want to schedule a query to run every day at 2 AM that gets back all documents from yesterday, without hard coding any dates or time zones. In it\'s current form, you can define a range filter/query like this:\r\n\r\n```\r\n"range" : {\r\n  "_timestamp" : {\r\n    "gte" : "now/d-1d",\r\n    "lt" : "now/d"\r\n  }\r\n}\r\n```\r\n\r\nUnfortunately since ElasticSearch defaults to the UTC timezone, this would only find documents from yesterday in the UTC time zone, and not America/New_York like we want.\r\n\r\nThis can be worked around by either adding an absolute timestamp that\'s calculated at the application layer or adding the current time zone offset to the range in the query. However, it would be much easier to specify the intended time zone as part of the range definition, similar to the way the date histogram facet works.\r\n\r\nPerhaps a syntax similar to `now/d-1d||<tz>` would work. The `||<tz>` would be optional and `<tz>` would follow the same rules as the pre_zone and post_zone settings for the date histogram facet.'
3727,'spinscale','Adding date format to an existing mapping silently fails\nWe have an existing index with data and mapping with a date field with two existing formats, e.g.:\r\n\r\nexisting mapping:\r\n    "tzoffset_timestamp": {\r\n        "type": "date",\r\n        "store": true,\r\n        "format": "EEE MMM dd HH:mm:ss.S Z yyyy||EEE MMM dd HH:mm:ss.SSS Z yyyy",\r\n        "include_in_all": true\r\n    }\r\n\r\nwhen executing curl put mapping command with an additional format:\r\n\r\n\r\n    "tzoffset_timestamp": {\r\n        "type": "date",\r\n        "store": true,\r\n        "format": "EEE MMM dd HH:mm:ss.S Z yyyy||EEE MMM dd HH:mm:ss.SSS Z yyyy||yyyy-MM-dd\'T\'HH:mm:ss.SSSZZ",\r\n        "include_in_all": true\r\n    }\r\n\r\nsucceeds with "ok / acknowledged", yet getting the mapping back provides original two formats.'
3725,'costin','service.bat fails unexpectedly if JAVA_HOME contains spaces\nThe detection of Java version fails if ``JAVA_HOME`` contains spaces. In turn, this causes the `service.bat` to exit without any message.'
3724,'javanna','NullPointerException on facet_filter when no actual filter is added\nto reproduce(0.90.5):\r\n```\r\ncurl -XPOST http://localhost:9200/foo\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{"id":1,"content":1}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/2 -d \'{"id":2,"content":2}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d \'{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{"and":{"filters":[]}}}}}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d \'{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{ }}}}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d \'{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{"and":{"filters":[{ "term" : { "id" : 1 } }]}}}}}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d \'{"query":{"match_all":{}},"facets":{"content":{"terms":{"field":"content"},"facet_filter":{ "term" : { "id" : 1 } }}}}\'\r\n```\r\n\r\nfirst two searches fail with:\r\n{"error":"SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[l8GT-BrMQYqwmTWkppI5tw][foo][3]: SearchParseException[[foo][3]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\":{\\"match_all\\":{}},\\"facets\\":{\\"content\\":{\\"terms\\":{\\"field\\":\\"content\\"},\\"facet_filter\\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][2]: SearchParseException[[foo][2]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\":{\\"match_all\\":{}},\\"facets\\":{\\"content\\":{\\"terms\\":{\\"field\\":\\"content\\"},\\"facet_filter\\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][4]: SearchParseException[[foo][4]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\":{\\"match_all\\":{}},\\"facets\\":{\\"content\\":{\\"terms\\":{\\"field\\":\\"content\\"},\\"facet_filter\\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][1]: SearchParseException[[foo][1]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\":{\\"match_all\\":{}},\\"facets\\":{\\"content\\":{\\"terms\\":{\\"field\\":\\"content\\"},\\"facet_filter\\":{ }}}}]]]; nested: NullPointerException; }{[l8GT-BrMQYqwmTWkppI5tw][foo][0]: SearchParseException[[foo][0]: query[ConstantScore(*:*)],from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"query\\":{\\"match_all\\":{}},\\"facets\\":{\\"content\\":{\\"terms\\":{\\"field\\":\\"content\\"},\\"facet_filter\\":{ }}}}]]]; nested: NullPointerException; }]","status":400}'
3722,'spinscale','RPM should not be starting service on installation?\nHi all\r\n\r\nCurrently we are looking at deploying elasticsearch on our aws cloud. We have an automated deployment process that can grab an RPM and put it onto an instance and then bake the image for deployment to multiple instances. However this gets broken by the RPM due to it automatically switching on the elasticsearch service on installation.\r\n\r\nFedora suggest that you should not automatically switch on the service as this is insecure, instead you should rely on the init.d script via chkconfig to switch on the service:\r\n\r\nhttp://fedoraproject.org/wiki/Packaging:SysVInitScript#Why_don.27t_we....\r\n\r\nQuote:\r\n> start the service after installation? \r\n> Installations can be in changeroots, in an installer context, or in other situations \r\n> where you don\'t want the services started. "\r\n\r\n\r\nI\'m happy to submit a patch if you think this is valid.\r\n\r\nDip\r\n'
3719,'spinscale','Enable a configurable delay for shutdown when calling killproc.\nUseful as soem nodes may take longer than others.'
3718,'spinscale',"Completion suggest API does not support size parameter\nThe new completion suggester is great, but it doesn't support a size parameter and instead always returns just 5 results. I'd like to be able to get all results. "
3715,'s1monw','Simplify NestedFieldComparators for numerics\nThe average and sum comparators basically share the same code which is\r\ncopy-past today. We can simplify this into a base class which reduces\r\ncode duplication and prevents copy-paste bugs.'
3712,'spinscale','Elasticsearch startup script doesn\'t work from directory with spaces in path\n1. Unzip the elasticsearch archive into a directory containing spaces in the one of the path components, e.g., /Volumes/Storage/test stuff.\r\n2. Run /Volumes/Storage/test stuff/elasticsearch/elasticsearch-0.90.4/bin/elasticsearch.\r\n\r\nIt reports that ES_CLASSPATH isn\'t set.\r\n\r\nThe culprit is at line 78. Needs to be "`dirname "$0"`"/elasticsearch.in.sh; do\r\n\r\nYes, the doubly nested double quotes work.'
3710,'kimchy','Better handling of /_all/_search when no indices exist\nAt the moment, a search against a cluster which has no indices will return a 503 Service Unavailable.  This is a "big" error code which in other places means that a node or cluster is down.\r\n\r\nRather throw an IndexMissing exception with a 404.  Similarly, the same should apply to count, mlt etc etc'
3708,'martijnvg','Analyzer caching problem with QueryParserService? (0.90.4)\nI don\'t have a good explanation for the following issue we are seeing, but it\'s definitely there. I dug as deep as I could and it seems like an analyzer caching issue with the QueryParserService.\r\n\r\nHere\'s the setup:\r\n\r\n1. Index mapping defines several full-text fields, and then some non-analyzed fields The full-text fields don\'t have an analyzer set - we index them using the _analyzer field.\r\n\r\n2. We have a custom analyzer plugged in; it is a PerFieldAnalyzerWrapper where the default analyzer is KeywordAnalyzer and then a custom stemming analyzer is configured for the full-text fields. The analyzer is properly configured as verified to work correctly.\r\n\r\n3. We use QueryString query to executed searches (yes, I know). The following query works as expected - the full-text fields get analyzed using the correct analyzer, and the non full-text fields are just kept untouched and are used as a whole:\r\n\r\n```\r\n{"filtered":{"query":{"bool":{"must":[{"query_string":{"query":"url:\\"https://www.facebook.com/111111111111/\\" url:\\"https://www.facebook.com/111111111112/\\"","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}},{"query_string":{"query":"foo title:bar","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}],"must_not":{"query_string":{"query":"test","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}}},"filter":{"terms":{"lang":["english"]}}}}\r\n```\r\n\r\n4. But when I switch the order of the clauses, the url field (which is the not_analyzed one) is being tokenized and I can verify it is going through the stemmer defined in custom_analyzer for only 4 fields, by name:\r\n\r\n```\r\n{"filtered":{"query":{"bool":{"must":[{"query_string":{"query":"foo title:bar","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}, {"query_string":{"query":"url:\\"https://www.facebook.com/111111111111/\\" url:\\"https://www.facebook.com/111111111112/\\"","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}],"must_not":{"query_string":{"query":"test","fields":["title","topic","replies.text"],"use_dis_max":false,"analyzer":"custom_analyzer"}}}},"filter":{"terms":{"lang":["english"]}}}}\r\n```\r\n\r\nThe must_not clause doesn\'t seem to have any effect.\r\n\r\n5. I debugged this as deep as Lucene\'s QueryParser, which apparently calls the correct analyzer object but still gets a tokenized stream.\r\n\r\nNot sure how to go about this any further - will be happy to help with nailing this down.'
3706,'s1monw',"NestedFieldComparator misses to copy slot if root doc has docID==0 and 'Avg' is used \nThe average `NestedFieldComparator` doesn't copy the underlying Comparator slot if `rootDoc == 0`. This was triggered by some of our tests lately. "
3705,'javanna','Added third highlighter type based on lucene postings highlighter\nAdded third highlighter type based on lucene postings highlighter\r\n\r\nRequires field index_options set to "offsets" in order to store positions and offsets in the postings list.\r\nConsiderably faster than the plain highlighter since it doesn\'t require to reanalyze the text to be highlighted: the larger the documents the better the performance gain should be.\r\nRequires less disk space than term_vectors, needed for the fast_vector_highlighter.\r\nBreaks the text into sentences and highlights them. Uses a BreakIterator to find sentences in the text. Plays really well with natural text, not quite the same if the text contains html markup for instance.\r\nTreats the document as the whole corpus, and scores individual sentences as if they were documents in this corpus, using the BM25 algorithm.\r\n\r\nUses forked version of lucene postings highlighter to support:\r\n- per value discrete highlighting for fields that have multiple values, needed when number_of_fragments=0 since we want to return a snippet per value\r\n- manually passing in query terms to avoid calling extract terms multiple times, since we use a different highlighter instance per doc/field, but the query is always the same\r\n\r\nThe lucene postings highlighter api is  quite different compared to the existing highlighters api, the main difference being that it allows to highlight multiple fields in multiple docs with a single call, using sequential IO.\r\nThe way it is introduced in elasticsearch in this first round is a compromise trying not to change the current highlight api, which works per document, per field. The main disadvantage is that we lose the sequential IO, but we can always refactor the highlight api to work with multiple documents later on.\r\n\r\nSupports pre_tag, post_tag, number_of_fragments (0 highlights the whole field), require_field_match, order by score and html encoding.\r\n\r\nCloses #3704'
3704,'javanna',"New highlighter based on lucene postings highlighter\nLucene includes the [postings highlighter](http://blog.mikemccandless.com/2012/12/a-new-lucene-highlighter-is-born.html) for some time now. The following are its main advantages:\r\n\r\n- faster than plain highlighter and requires less disk space than `term_vectors`, needed for the fast vector highlighter\r\n- outputs nice sentences as snippets\r\n- scores snippets based on BM25 algorithm\r\n\r\nIt's worth to try and add it as our third highlighter implementation."
3702,'costin','add elasticsearch as a service for Windows platforms\nbased on Apace Commons Daemon\r\nsupports both x64 and x86\r\nintroduces service.bat for installing/removing the service'
3701,'s1monw',"Make TestCluster based integration tests more repoducible\nWhile testing an async system providing reproducible tests that\r\nuse randomized components is a hard task we should at least try to\r\nreestablish the enviroment of a failing test as much as possible.\r\nThis commit allows to re-establish the shared 'TestCluster' by\r\nresetting the cluster to a predefined shared state before each test.\r\n\r\nBefore this commit a tests that is executed in isolation was likely\r\nusing a entirely different node enviroment as the failing test since\r\nthe 'TestCluster' kept intermediate nodes started by other tests around."
3700,'spinscale','NoShardAvailableActionException in ES 0.90.3 on startup\nWhen a river [1] has been registered and ES is restarted.\r\nI keep getting the following exception in the log:\r\n[2013-09-10 10:22:55,066][INFO ][node                     ] [Box IV] version[0.90.3], pid[26144], build[5c38d60/2013-08-06T13:18:31Z]\r\n[2013-09-10 10:22:55,067][INFO ][node                     ] [Box IV] initializing ...\r\n[2013-09-10 10:22:55,115][INFO ][plugins                  ] [Box IV] loaded [mongodb-river, mapper-attachments, lang-groovy, lang-javascript], sites [river-mongodb, head]\r\n[2013-09-10 10:22:58,456][INFO ][node                     ] [Box IV] initialized\r\n[2013-09-10 10:22:58,456][INFO ][node                     ] [Box IV] starting ...\r\n[2013-09-10 10:22:59,639][INFO ][transport                ] [Box IV] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.1.117:9300]}\r\n[2013-09-10 10:23:03,145][INFO ][cluster.service          ] [Box IV] new_master [Box IV][AllhKDYIScKfW8Ue1YcmQw][inet[/192.168.1.117:9300]], reason: zen-disco-join (elected_as_master)\r\n[2013-09-10 10:23:03,169][INFO ][discovery                ] [Box IV] elasticsearch/AllhKDYIScKfW8Ue1YcmQw\r\n[2013-09-10 10:23:03,216][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]\r\norg.elasticsearch.action.NoShardAvailableActionException: [_river][0] null\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)\r\n\tat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)\r\n\tat org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)\r\n\tat org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)\r\n\tat org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)\r\n\tat org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)\r\n\tat org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-09-10 10:23:03,683][INFO ][gateway                  ] [Box IV] recovered [7] indices into cluster_state\r\n[2013-09-10 10:23:03,702][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]\r\norg.elasticsearch.action.NoShardAvailableActionException: [_river][0] null\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)\r\n\tat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)\r\n\tat org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)\r\n\tat org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)\r\n\tat org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)\r\n\tat org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)\r\n\tat org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-09-10 10:23:03,849][DEBUG][action.get               ] [Box IV] [_river][0]: failed to execute [[_river][river101][_meta]: routing [null]]\r\norg.elasticsearch.action.NoShardAvailableActionException: [_river][0] null\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:123)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)\r\n\tat org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)\r\n\tat org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)\r\n\tat org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:92)\r\n\tat org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:179)\r\n\tat org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:112)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:85)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:59)\r\n\tat org.elasticsearch.river.routing.RiversRouter$1.execute(RiversRouter.java:109)\r\n\tat org.elasticsearch.river.cluster.RiverClusterService$1.run(RiverClusterService.java:103)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n\r\nThe same plugin version works without this exception in ES 0.90.2. Even with the exception data are correctly imported in ES by the river.\r\n\r\n[1] - https://github.com/richardwilly98/elasticsearch-river-mongodb'
3696,'spinscale',"Feature Request: Don't reindex the document when updating non-indexed fields\nRead title."
3695,'javanna','Cannot make Path Hierarchy filter when paired with another filter - ClassNotFoundException\nUsing ES 0.90.3\r\n\r\nCheck the following command which tries to make some analyzers with two filters:\r\n```\r\ncurl -XPUT "localhost:9200/text5" -d\'\r\n{\r\n    "analysis": {\r\n        "analyzer": {\r\n            "str_search_analyzer": {\r\n                "tokenizer": "keyword",\r\n                "filter": ["lowercase"]\r\n            },\r\n\r\n            "str_index_analyzer": {\r\n                "tokenizer": "keyword",\r\n                "filter": ["lowercase", "substring"]\r\n            }\r\n        },\r\n        "filter": {\r\n            "substring2": {\r\n                "type": "path_hierarchy",\r\n                "delimiter": "."\r\n            },\r\n            "substring": {\r\n                "type": "nGram",\r\n                "min_gram": 1,\r\n                "max_gram": 10\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\nAnswer: \r\n```\r\n{"error":"IndexCreationException[[text2] failed to create index]; nested: ElasticSearchIllegalArgumentException[failed to find token filter type [path_hierarchy] for [substring2]]; nested: NoClassSettingsException[Failed to load class setting [type] with value [path_hierarchy]]; nested: ClassNotFoundException[org.elasticsearch.index.analysis.pathhierarchy.PathHierarchyTokenFilterFactory]; ","status":400}\r\n```\r\n\r\nAnd it is true because in the github repo it is under ```org.elasticsearch.index.analysis.PathHierarchyTokenFilterFactory``` instead of ```org.elasticsearch.index.analysis.pathhierarchy.PathHierarchyTokenFilterFactory```\r\n\r\nBut this request is OK:\r\n```\r\ncurl -XPUT "localhost:9200/text3" -d\'\r\n {\r\n     "analysis": {\r\n             "substring": {\r\n                 "type": "path_hierarchy",\r\n                 "delimiter": "."\r\n             }\r\n        }\r\n     }\r\n }\'\r\n\r\n{"ok":true,"acknowledged":false}\r\n```\r\n'
3694,'s1monw','fix SearchWithRandomExceptionsTests.testRandomExceptions\nThe test is trapped in an infinite loop for some seeds. This causes the test log to fill and finally an oom when running the full suite. The following seeds reproduce this: \r\n6D1FE72CC0B92909 \r\n843B52FFCF8E5B33\r\n6F9CC4362D6D82CB'
3693,'spinscale','Confusing exception raised by create_index with index-alias collision\nElasticsearch raises an InvalidIndexNameException when you call create_index for a "name", which already exists as an Alias. In all other circumstances, an InvalidIndexNameException is raised from use of illegal characters in an index name. \r\n\r\nIt would make more sense to raise the IndexAlreadyExistsException. Because in most cases one is trying to ensure the name is present, whether as an alias or as an index.\r\n\r\nWhen it is an index-alias collision the suffix "an alias already exists with that name" would be added to the exception.\r\n\r\nIndexAlreadyExistsException: [same-name-index] Already exists, an alias already exists with that name.'
3692,'spinscale','index analysis module not working correctly v0.90.3\nI am trying to define an analyzer in the elasticsearch.yml base on the [doc](http://www.elasticsearch.org/guide/reference/index-modules/analysis/\r\n)\r\n\r\n    index :\r\n        analysis :\r\n            analyzer : \r\n                standard : \r\n                    type : standard\r\n                    stopwords : [stop1, stop2]\r\n                myAnalyzer1 :\r\n                    type : standard\r\n                    stopwords : [stop1, stop2, stop3]\r\n                    max_token_length : 500\r\n               ...\r\n\r\nHowever, this is not working and it seems like that I have to put it like this to get it working:\r\n\r\n    index.analysis.analyzer.standard:        \r\n        type : standard\r\n        stopwords : [stop1, stop2]\r\n    index.analysis.analyzer.myAnalyzer1:        \r\n        type : standard\r\n        stopwords : [stop1, stop2, stop3]\r\n        max_token_length : 500\r\n    ...\r\n\r\nIs this a bug in the code or the doc?'
3691,'spinscale','ElasticSearch shell script fails due to unsupported syntax on non-Bash shells\nThe first line of the elasticsearch script specifies `/bin/sh`, but this is generally defined as pointing to a Bourne-compatible shell. It cannot be assumed to point to Bash shell.\r\n\r\nLine 92 contains the followin:\r\n\r\n `JAVA=$(which java)`\r\n\r\n1. The first line should be changed to /bin/bash if that is intended.\r\n2. Or line 92 must be changed so that `which java` is surrounded within backtics, which is the only common (though not able to be nested) syntax across all Bourne-derived shells (Bourne, Dash, Korn, Bash).\r\n\r\nThank you.\r\n\r\nBrian Yoder'
3688,'spinscale','search_analyzer reset in completion suggester mappings\nHi all,\r\n\r\nI\'ve created a mapping with custom index_analyzer and search_analyzers specified in the mapping, and pulled the mappings to verify that things were set correctly. I then index a few large batches of data, and check the mappings again to find that search_analzyer is now "simple" in the mappings instead of my custom analyzer.\r\n\r\nI\'m working on narrowing down a more exact test case, but thought maybe this would be enough to start taking a look.\r\n\r\nOn a related note, it seems that the completion suggester doesn\'t support taking an analyzer option anywhere at query time.'
3687,'spinscale','Added armhf versions of openjdk 6 and 7\nThe debian init script does not include support for arm version of java. \r\n\r\nAdded java-6-openjdk-armhf and java-7-openjdk-armhf to JDK_DIRS in debian init.d script\r\n\r\nFixes elasticsearch/elasticsearch#3659'
3686,'javanna','Set nowInMillis to search context created by the count, validate query and explain api\nSet nowInMillis to search context created by the count api, validate query api and explain api so that "NOW" can be used within queries\r\n\r\nAdded nowInMillis to ShardCountRequest, ShardValidateRequest and ExplainRequest in a backwards compatible manner\r\n\r\nFixes #3625, #3626 & #3629 '
3685,'spinscale','Deb/RPM: Disable immediate restart on package upgrade\nIn order to have the possibility for unattended upgrades and to make sure, that these automatic upgrades do not start rebalancing the shards of a cluster around, the restart of elasticsearch after a package upgrade is now disabled by default. This is also important when providing repositories.\r\n\r\nYou can reenable it by setting `RESTART_ON_UPGRADE=true` in `/etc/default/elasticsearch` for debian packages or `/etc/sysconfig/elasticsearch` for redhat packages.\r\n\r\n**Note**: This is a breaking change, as the debian package used to restart the elasticsearch service on every update, which now needs to be done by hand.'
3684,'martijnvg','Feature request: support ?version=&version_type= params in update API\nIt would be great if the `update` API supported the same versioning parameters as the `index` API. The failure semantics is a bit more complex; what I would expect would be:\r\n\r\n* `version_type=internal`: fail if version on getting the doc does not match the version specified; ignore `retry_on_conflict` and fail immediately on any conflict (a conflict would mean the version after indexing no longer matches)\r\n* `version_type=external`: fail if version on getting the doc is >= specified version; obey `retry_on_conflict` while the version in conflict is < the specified version; fail if a retry sees a version >= the version specified.'
3683,'javanna','pull request for #3673 (span_near query should accept slop = -1 (bis))\nmaster branch'
3682,'bleskes',"Fixed inability to set keep_words_path in KeepWordFilter\nIt seems `keep_words_path` can't be set, because `keep_words` (`arrayKeepWords`) is not `null` when not defined in config. So `ElasticSearchIllegalArgumentException` is thrown."
3681,'dadoonet','Remove get index templates deprecated methods\nIn 0.90.4, we deprecated some code:\r\n\r\n* `GetIndexTemplatesRequest#GetIndexTemplatesRequest(String)` moved to `GetIndexTemplatesRequest#GetIndexTemplatesRequest(String...)`\r\n* `GetIndexTemplatesRequest#name(String)` moved to `GetIndexTemplatesRequest#names(String...)`\r\n* `GetIndexTemplatesRequest#name()` moved to `GetIndexTemplatesRequest#names()`\r\n\r\n* `GetIndexTemplatesRequestBuilder#GetIndexTemplatesRequestBuilder(IndicesAdminClient, String)` moved to  `GetIndexTemplatesRequestBuilder#GetIndexTemplatesRequestBuilder(IndicesAdminClient, String...)`\r\n\r\n* `IndicesAdminClient#prepareGetTemplates(String)` moved to `IndicesAdminClient#prepareGetTemplates(String...)`\r\n\r\n* `AbstractIndicesAdminClient#prepareGetTemplates(String)` moved to `AbstractIndicesAdminClient#prepareGetTemplates(String...)`\r\n\r\nWe can now remove that old methods in 1.0.\r\n\r\n**Note**: it breaks the Java API\r\n\r\nRelative to #2532.\r\n'
3680,'dadoonet','Remove RestActions#splitXXX(String) methods\nWe want to remove RestActions#splitXXX(String) methods:\r\n\r\n* `RestActions#splitIndices(String)`\r\n* `RestActions#splitTypes(String)`\r\n* `RestActions#splitNodes(String)`\r\n\r\nAnd replace with `Strings.splitStringByCommaToArray(String)`\r\n\r\nIt breaks the JAVA API.'
3679,'dakrone',"Add documentation heading and section links\nIt is a frequent usecase that one wants to deep link into the ES documentation, but it is made difficult due to the lack of fragment identifiers in the html.\r\n\r\nCompare MongoDB's installation document:\r\n\r\nhttp://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian/\r\n\r\nto that of ES's:\r\n\r\nhttp://www.elasticsearch.org/guide/reference/setup/installation/\r\n\r\nOf note is their use of heading and section links. I can easily express to a fellow developer or colleague where to find information on apt:\r\n\r\nhttp://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian/#configure-package-management-system-apt\r\n\r\nAnother popular example is github's documentation:\r\n\r\nhttps://help.github.com/articles/github-flavored-markdown#differences-from-traditional-markdown\r\n\r\nI personally believe this would help aid communication in forum posts, stackoverflow questions / answers, github issues, irc, etc.\r\n\r\n\r\n"
3677,'drewr','Alias filter not applied when using \'multi-index\' syntax with wild card in URL\nIf I try to use a multi-index search with a wildcard on filter aliases the filter does not get applied. However it does work with a multi-index comma separated list. See example below - "someone_else" is included in the results. Only tested with version 0.90.2.\r\n\r\n```\r\ncurl -XPUT \'http://localhost:9200/twitter/tweet/1\' -d \'{\r\n    "user" : "kimchy"\r\n}\'\r\n\r\ncurl -XPUT \'http://localhost:9200/twitter/tweet/2\' -d \'{\r\n    "user" : "jbrook"\r\n}\'\r\n\r\ncurl -XPUT \'http://localhost:9200/twitter/tweet/3\' -d \'{\r\n    "user" : "someone_else"\r\n}\'\r\n\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'\r\n{\r\n    "actions" : [\r\n        {\r\n            "add" : {\r\n                 "index" : "twitter",\r\n                 "alias" : "tweets_by_kimchy",\r\n                 "filter" : { "term" : { "user" : "kimchy" } }\r\n            }\r\n        },\r\n        {\r\n            "add" : {\r\n                 "index" : "twitter",\r\n                 "alias" : "tweets_by_jbrook",\r\n                 "filter" : { "term" : { "user" : "jbrook" } }\r\n            }\r\n        }\r\n    ]\r\n}\'\r\n\r\ncurl -XPOST \'http://localhost:9200/tweets_by_kimchy,tweets_by_jbrook/_search?pretty=1\'\r\n\r\n{\r\n  "took" : 1,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "1",\r\n      "_score" : 1.0, "_source" : {\r\n    "user" : "kimchy"\r\n}\r\n    }, {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "2",\r\n      "_score" : 1.0, "_source" : {\r\n    "user" : "jbrook"\r\n}\r\n    } ]\r\n  }\r\n\r\n\r\ncurl -XPOST \'http://localhost:9200/tweets_by_*/_search?pretty=1\'\r\n\r\n{\r\n  "took" : 2,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 3,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "1",\r\n      "_score" : 1.0, "_source" : {\r\n    "user" : "kimchy"\r\n}\r\n    }, {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "2",\r\n      "_score" : 1.0, "_source" : {\r\n    "user" : "jbrook"\r\n}\r\n    }, {\r\n      "_index" : "twitter",\r\n      "_type" : "tweet",\r\n      "_id" : "3",\r\n      "_score" : 1.0, "_source" : {\r\n   ** "user" : "someone_else" **\r\n}\r\n    } ]\r\n  }\r\n```'
3676,'imotov','term lookup query with no results should evaluate to no match\nWhen a term lookup query returns no results it is considered a no filter rather than a no match which causes other filters to ignore the results.\r\n\r\nFor example:\r\n\r\n```js\r\n {\r\n   "query": {\r\n       "filtered": {\r\n           "filter": {\r\n               "and": [\r\n                   {\r\n                       "term": {\r\n                           "name": "value1"\r\n                       }\r\n                   },\r\n                   {\r\n                       "terms": {\r\n                           "_id": {\r\n                               "index": "users",\r\n                               "type": "user",\r\n                               "id": "2",\r\n                               "path": "followers"\r\n                           }\r\n                       }\r\n                   }\r\n               ]\r\n           }\r\n       }\r\n   } \r\n}\r\n```\r\n\r\nIn this case if the terms lookup returned no results it would ignore the terms lookup filter altogether when it should return no results because of the overall evaluation of the and should be false.'
3675,'imotov','term lookup query with no results should evaluate to no match\nWhen a term lookup query returns no results it is considered a no filter rather than a no match which causes other filters to ignore the results.\r\n\r\nFor example:\r\n\r\n```js\r\n {\r\n   "query": {\r\n       "filtered": {\r\n           "filter": {\r\n               "and": [\r\n                   {\r\n                       "term": {\r\n                           "name": "value1"\r\n                       }\r\n                   },\r\n                   {\r\n                       "terms": {\r\n                           "_id": {\r\n                               "index": "users",\r\n                               "type": "user",\r\n                               "id": "2",\r\n                               "path": "followers"\r\n                           }\r\n                       }\r\n                   }\r\n               ]\r\n           }\r\n       }\r\n   } \r\n}\r\n```\r\n\r\nIn this case if the terms lookup returned no results it would ignore the terms lookup filter altogether when it should return no results because of the overall evaluation of the and should be false.'
3673,'javanna','span_near query should accept slop = -1 (bis)\nHi,\r\n\r\nPrevious fix (#3122) for issue #3079 (span_near query should accept slop = -1) is incomplete. Similar fix (e.g change int slop = -1;  into  Integer slop = null;) should also be applied in src/main/java/org/elasticsearch/index/query/SpanNearQueryBuilder.java.\r\n\r\nCould you please also make the fix available in the 0.90.x branch ?\r\n\r\nThanks'
3670,'s1monw','No obvious way to delete persistent cluster admin settings\nThere\'s no delete API for these, if you want to set them back to defaults on next startup.\r\n\r\nkarmi on irc suggested setting them to the empty string, e.g.:\r\n\r\n```\r\ncurl -XPUT \'localhost:9200/_cluster/settings\' -d \'{"persistent":{"threadpool.search.type":""}}\'\r\n```\r\n\r\nBut then on next restart I got warnings like this:\r\n\r\n```\r\n[2013-09-11 20:08:41,548][WARN ][node.settings            ] [lo3uppaldbs009.palomino.pearson.com] failed to refresh settings for [org.elasticsearch.threadpool.ThreadPool$ApplySettings@49e3f731]\r\norg.elasticsearch.ElasticSearchIllegalArgumentException: No type found [], for [search]\r\n        at org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:452)\r\n        at org.elasticsearch.threadpool.ThreadPool.updateSettings(ThreadPool.java:468)\r\n        at org.elasticsearch.threadpool.ThreadPool$ApplySettings.onRefreshSettings(ThreadPool.java:809)\r\n        at org.elasticsearch.node.settings.NodeSettingsService.clusterChanged(NodeSettingsService.java:84)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:321)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:95)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:724)\r\n```\r\n\r\nSo now I have no way to tell what kind of thread pool I have.\r\n\r\nVersion: 0.90.3\r\n'
3669,'imotov','CompletionFieldMapper ignores path: just_name\n`CompletionFieldMapper` is using full name of the field instead of index name during indexing and as a result completion doesn\'t work on `multi_field` fields with `"path": "just_name"`\r\n\r\nRepro: https://gist.github.com/imotov/b11f866c48b090158d7e '
3664,'dadoonet','date_histogram facet missing periods\nThe date histogram facet only returns data where documents exist, it would be good to have a parameter to autofill missing periods with 0 values eg if I have data for years 2009, 2011 & 2012 there will only be 3 entries in the facet, I would like elasticsearch to create the missing entry for 2010 with 0 values so there are 4 entries in the facet.'
3661,'javanna','JsonGenerationException thrown in SuggestResponse#toString method\nThe following exception is consistently thrown (and caught) by the SuggestResponse#toString method:\r\n\r\n```\r\ncom.fasterxml.jackson.core.JsonGenerationException: Can not write a field name, expecting a value\r\n\tat com.fasterxml.jackson.core.base.GeneratorBase._reportError(GeneratorBase.java:444)\r\n\tat com.fasterxml.jackson.core.json.UTF8JsonGenerator.writeFieldName(UTF8JsonGenerator.java:167)\r\n\tat org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeFieldName(JsonXContentGenerator.java:74)\r\n\tat org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:265)\r\n\tat org.elasticsearch.common.xcontent.XContentBuilder.startArray(XContentBuilder.java:210)\r\n\tat org.elasticsearch.search.suggest.Suggest$Suggestion.toXContent(Suggest.java:317)\r\n\tat org.elasticsearch.search.suggest.Suggest.toXContent(Suggest.java:148)\r\n\tat org.elasticsearch.action.suggest.SuggestResponse.toString(SuggestResponse.java:74)\r\n\tat java.lang.String.valueOf(String.java:2854)\r\n\tat java.io.PrintStream.println(PrintStream.java:821)\r\n```\r\n\r\nThere seems to be a missing startObject/endObject. Even though it is not a blocking problem, it would be nice to be able to print out the string representation of a suggest response when needed.'
3660,'jpountz',"Make bounds on RangeFacet optionally inclusive/exclusive\nThe default behaviour of range facets in ES is that the 'from' bound is inclusive and the 'to' bound is exclusive. This makes sense for many use cases, but it would be useful to be able to change this, for example by making the to bound inclusive.\r\n\r\nI have the changes ready (+tests :), I would like to commit to 0.90.X and trunk. The changed files are RangeFacetBuilder, RangeFacet, RangeFacetExecutor and RangeFacetParser.\r\n\r\nPlease can you let me know if I can go ahead and commit?\r\n\r\nThanks,\r\nShaun"
3659,'spinscale',"Include ARMHF Version of Java in JDK_DIRS\nWe're working on deploying Elasticsearch in an arm environment and have noticed that the armhf version of openjdk-7 is not included in the JDK-DIRS for the Elasticsearch service. \r\n\r\nNow while Java should theoretically set its Java_Home, doing the default apt-get install openjdk-7-jre does not do this (on X86 or ARM) which is why I'm assuming the JDK_DIRs list was created. \r\n\r\nThe path would be /usr/lib/jvm/java-7-openjdk-armhf/ and /usr/lib/jvm/java-6-openjdk-armhf/"
3658,'dadoonet','Replace RestActions#splitXXX(String) methods with splitValues(String)\nWe have duplicated code in:\r\n\r\n* RestActions#splitIndices(String)\r\n* RestActions#splitTypes(String)\r\n* RestActions#splitNodes(String)\r\n\r\nThe idea is to have one single RestActions#splitValues(String) method.\r\n'
3657,'martijnvg',"Add clear scroll api\nAdd an api that allows to clear all the resources associated with a search scroll id. Example usage:\r\n```\r\ncurl -XDELETE 'localhost:9200/_search/scroll/{scroll_id}'\r\n```"
3654,'s1monw','Add support for MockDirectoryWrapper in tests\nThe ultimate weapon against stuff like  #3652  is to use `MockDirectoryWrapper` we should use it in our tests by default.'
3653,'s1monw','Rename `IndexShard#searcher()` to `IndexShard#acquireSearcher()`\nto prevent issues like in #3652  we should make sure the name tells that it needs to be released'
3652,'s1monw','CompletionStats can cause resource leak since requested searchers are not closed\nI found this very sneaky problem while hunting a couple of other problems that I thought are related to potentially not closed index readers etc. I added support for MockDirectoryWrapper and the completion stats blew up immediately. While I might need more time to fully integrate the MockDirectoryWrapper from Lucene I want to fix this obvious bug first...'
3651,'dadoonet','enable GET /_template to show all templates\nPR for #2532\r\n\r\n/_template shows:\r\nNo handler found for uri [/_template] and method [GET]\r\n\r\nIt would make sense to list the templates as they are listed in the /_cluster/state call.\r\n\r\nSee also:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wnGOnT-JTQo\r\n'
3650,'javanna',"Elasticsearch using huge amount of processes\nHello,\r\n\r\nwe send logfiles (catalina.out) with logstash (version 1.1.13-flatjar) using multiline{} to our elasticsearch server (version 0.90.2).\r\n\r\nAfter several hours elasticsearch crashes because of not having enough memory, more precisely, the limit of 10240 open processes for the elasticsearch user has been reached. No more threads can be created.\r\n\r\nThe actual setting of ulimit is as follows:\r\n\r\nelasticsearch soft nofile 65535\r\nelasticsearch hard nofile 65535\r\nelasticsearch soft nproc 10240\r\nelasticsearch hard nproc 10240\r\n\r\nI don't have any clue at the moment, how we can fix this issues. Maybe someone can help.\r\n\r\nRegards\r\nOliver"
3648,'s1monw','CompletionSuggest should throw an exception if input string contains a reserved character.\nCurrently the CompletionSuggester reserves 0x00 and 0xFF for internal use. We should throw an exception if those characters are used in an input string.'
3647,'spinscale','Clients crashing with Bad file descriptor / Cannot assign requested address\nWhen doing a lot of small/fast requests (in this case head requests to document IDs, most of them resulting in a hit), Elasicsearch seems to crash in a way that makes all other programs die for a few seconds. After that, connections can seemingly be reestablished.\r\nSeeing as in this case, running a ruby script will kill a python script running in the background, I\'d assume it\'s not a problem in the client implementations\r\n\r\n```\r\n# cat /etc/elasticsearch/elasticsearch.yml \r\nnetwork.host: 127.0.0.1\r\ncluster:\r\n  name: some-thing\r\npath:\r\n  logs: /var/log/elasticsearch\r\n  data: /home/somthing/elasticsearch\r\nindex:\r\n  number_of_shards: 1\r\n  number_of_replicas: 0\r\n  refresh_interval: 20s\r\n  cache:\r\n    field:\r\n      type: soft\r\nboostrap:\r\n  mlockall: true\r\nindices:\r\n  fielddata:\r\n    cache:\r\n      size: 30%\r\n```\r\n\r\n```\r\nVM name: Java HotSpot(TM) 64-Bit Server VM\r\nVM vendor: Oracle Corporation\r\nVM version: 23.25-b01\r\nUptime: 189 hours, 28 minutes, 4 seconds and 77 milliseconds\r\nJava version: 1.7.0_25\r\n```\r\n\r\n```\r\nHTTP & Transport\r\nHTTP address: inet[/127.0.0.1:9200]\r\nBound address: inet[/127.0.0.1:9200]\r\nPublish address: inet[/127.0.0.1:9200]\r\nTransport address: inet[/127.0.0.1:9300]\r\nBound address: inet[/127.0.0.1:9300]\r\nPublish address: inet[/127.0.0.1:9300]\r\n```\r\n\r\n```\r\nES version: 0.90.3\r\n```\r\n\r\nThese are the limits of ES:\r\n```\r\n# cat /proc/18426/limits \r\nLimit                     Soft Limit           Hard Limit           Units     \r\nMax cpu time              unlimited            unlimited            seconds   \r\nMax file size             unlimited            unlimited            bytes     \r\nMax data size             unlimited            unlimited            bytes     \r\nMax stack size            8388608              unlimited            bytes     \r\nMax core file size        0                    unlimited            bytes     \r\nMax resident set          unlimited            unlimited            bytes     \r\nMax processes             256606               256606               processes \r\nMax open files            65535                65535                files     \r\nMax locked memory         65536                65536                bytes     \r\nMax address space         unlimited            unlimited            bytes     \r\nMax file locks            unlimited            unlimited            locks     \r\nMax pending signals       256606               256606               signals   \r\nMax msgqueue size         819200               819200               bytes     \r\nMax nice priority         0                    0                    \r\nMax realtime priority     0                    0                    \r\nMax realtime timeout      unlimited            unlimited            us        \r\n```\r\n\r\nThis is the stacktrace from stretcher (ruby client):\r\n\r\n```\r\nFaraday::Error::ConnectionFailed: Bad file descriptor - Bad file descriptor (Errno::EBADF)\r\norg/jruby/RubyIO.java:2025 • close\r\ngems/excon-0.25.3/lib/excon/socket.rb:192 • connect\r\ngems/excon-0.25.3/lib/excon/socket.rb:174 • connect\r\norg/jruby/RubyArray.java:1617 • each\r\ngems/excon-0.25.3/lib/excon/socket.rb:152 • connect\r\ngems/excon-0.25.3/lib/excon/socket.rb:32 • initialize\r\ngems/excon-0.25.3/lib/excon/connection.rb:363 • socket\r\ngems/excon-0.25.3/lib/excon/connection.rb:105 • request_call\r\ngems/excon-0.25.3/lib/excon/middlewares/mock.rb:42 • request_call\r\ngems/excon-0.25.3/lib/excon/middlewares/instrumentor.rb:22 • request_call\r\ngems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call\r\ngems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call\r\ngems/excon-0.25.3/lib/excon/middlewares/base.rb:15 • request_call\r\ngems/excon-0.25.3/lib/excon/connection.rb:244 • request\r\ngems/faraday-0.8.8/lib/faraday/adapter/excon.rb:45 • call\r\ngems/faraday_middleware-0.9.0/lib/faraday_middleware/request/encode_json.rb:23 • call\r\ngems/faraday_middleware-0.9.0/lib/faraday_middleware/response_middleware.rb:30 • call\r\ngems/faraday-0.8.8/lib/faraday/response.rb:8 • call\r\ngems/stretcher-1.19.0/lib/stretcher/server.rb:219 • request\r\norg/jruby/ext/thread/Mutex.java:149 • synchronize\r\ngems/stretcher-1.19.0/lib/stretcher/server.rb:217 • request\r\ngems/stretcher-1.19.0/lib/stretcher/server.rb:155 • mget\r\ngems/stretcher-1.19.0/lib/stretcher/index.rb:74 • mget\r\n```\r\n\r\nThis is what esclient will look like:\r\n```\r\nTraceback (most recent call last):\r\n  File "/usr/local/bin/esdump", line 71, in <module>\r\n    scrollres = es.scroll(scroll_id)\r\n  File "/usr/local/lib/python2.7/dist-packages/esclient.py", line 251, in scroll\r\n    query_string_args=query_string_args, encode_json=False)\r\n  File "/usr/local/lib/python2.7/dist-packages/esclient.py", line 122, in send_request\r\n    self.last_response = requests.request(method.lower(), url, **kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/requests/api.py", line 44, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/requests/sessions.py", line 335, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/requests/sessions.py", line 438, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File "/usr/local/lib/python2.7/dist-packages/requests/adapters.py", line 327, in send\r\n    raise ConnectionError(e)\r\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host=\'localhost\', port=9200): Max retries exceeded with url: /_search/scroll?scroll=10m (Caused by <class \'socket.error\'>: [Errno 99] Cannot assign requested address)\r\n```'
3645,'jpountz','Big number support for statistical facets\nI frequently run into issues trying to run statistical facets on my data sets. When trying to total a few hours worth of data we can easily go over the 64bit long value.  It would make things much easier if ES implemented <http://docs.oracle.com/javase/6/docs/api/java/math/BigInteger.html> or something similar. Which would allow people to sum large amounts of data.\r\n\r\n'
3644,'martijnvg','Remove matched_filters in favor for matched_queries\nBecause of #3581 `matchedFilters` response part can now also contain named queries as results, the name `matchedFilters` only suggests that named filters may up end as result. Changing the name to `matchedQueries` will suggest that named elements from the whole query dsl can be matched (both named queries and filters).'
3642,'dadoonet','Plugin Manager should support -remove group/artifact/version naming\nWhen installing a plugin, we use:\r\n\r\n```sh\r\nbin/plugin --install groupid/artifactid/version\r\n```\r\n\r\nBut when removing the plugin, we only support:\r\n\r\n```sh\r\nbin/plugin --remove dirname\r\n```\r\n\r\nwhere `dirname` is the directory name of the plugin under `/plugins` dir.\r\n\r\nCloses #3421.'
3641,'dadoonet','Plugin Manager: add silent mode.\nNow with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.\r\n\r\n```sh\r\nbin/plugin --install karmi/elasticsearch-paramedic --silent\r\n```\r\n\r\nCloses #3628.\r\n\r\nWhen OK, I will push it to master as well.'
3639,'jpountz','Huge GC load with populated field cache\nThis is followup for [my email](https://groups.google.com/forum/#!searchin/elasticsearch/CPU$20consumption$20after$20long$20period$20of$20time/elasticsearch/_ihDrmVExLA/HaOgH3SE2RsJ).\r\n\r\nHere you may see what happens if you fill the whole heap with field cache (big query with faceting for 50gb of data). Indexing happens every 10 minutes and with full cache it becomes very painful because of GC.\r\n\r\n![cache full](http://puu.sh/4kios.png)\r\n\r\nAnd this is what happens after `/_cache/clear`:\r\n\r\n![cache clear](http://puu.sh/4kiri.png)\r\n\r\nWe run elasticsearch 0.90.2 like that:\r\n\r\n`/usr/lib/jvm/oracle-jdk-bin-1.7/bin/java -Xms5g -Xmx5g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.foreground=yes -Des.path.home=/opt/elasticsearch -cp :/opt/elasticsearch/lib/elasticsearch-0.90.2.jar:/opt/elasticsearch/lib/*:/opt/elasticsearch/lib/sigar/* -Des.config=/etc/elasticsearch/elasticsearch.yml org.elasticsearch.bootstrap.ElasticSearch`\r\n\r\nSettings in yml file only affect data location and network settings.\r\n\r\nIs this expected behaviour? Elasticsearch becomes very slow even for simple "get doc by id" (like 10-20 seconds instead of 10-500ms).'
3636,'kimchy','Not allowing index names in request body for multi-get/search/bulk when indices are already given in url\nRational: Many users currently use URL-based access control to secure access to ES. For multi-search/get queries, currently the user can put the indices in the request body, which poses a challenge to the URL-based security approach. \r\n\r\nThis request is to add a feature for multi-search/get such that when index(or indices) is given in the URL, prohibiting the request body to contain the index (indices). \r\n\r\nIn this way, all request to ES can be secured via URL.\r\n\r\nAdd a flag, called `rest.action.multi.allow_explicit_index` that can be set in the settings/config (default to `true`). If set to `false`, will reject requests that have explicit index specified in their body.'
3631,'javanna','FlushNotAllowedEngineException during optimize\nI have encountered the following exception a few times recently on 0.90.3 while I was during some testing.  Seems to happen sporadically on a random shard during an optimize, maybe I am hitting some weird race condition?\r\n\r\n```\r\n[2013-09-05 05:42:23,925][DEBUG][action.admin.indices.optimize] [samcro] [stackoverflow][0], node[i45bAah_SUa9zPPLmFYw4g], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.admin.indices.optimize.OptimizeRequest@5ef574db]\r\norg.elasticsearch.index.engine.FlushNotAllowedEngineException: [stackoverflow][0] already flushing...\r\n\tat org.elasticsearch.index.engine.robin.RobinEngine.flush(RobinEngine.java:818)\r\n\tat org.elasticsearch.index.engine.robin.RobinEngine.optimize(RobinEngine.java:1014)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.optimize(InternalIndexShard.java:512)\r\n\tat org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:115)\r\n\tat org.elasticsearch.action.admin.indices.optimize.TransportOptimizeAction.shardOperation(TransportOptimizeAction.java:49)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:228)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:205)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:179)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n```'
3630,'s1monw','Support highlighting against queries other than the search query\nThis issue was originally about rescore queryies and I\'ve left it intact below so the discussion makes sense.  I\'ve changed the issue to be more general.  Now this is a feature request to let the user specify a query against which highlighting will run in the highlight element.  Like this:\r\n```bash\r\ncurl -XPOST http://localhost:9200/test/test/_search?pretty -d\'{\r\n  "fields": [\r\n    "_id"\r\n  ],\r\n  "highlight": {\r\n    "order": "score",\r\n    "fields": {\r\n      "text": {\r\n        "number_of_fragments": 1,\r\n        "score_query": {\r\n          "bool": {\r\n            "must": {\r\n              "match": {\r\n                "text": {\r\n                  "query": "central iraq"\r\n                }\r\n              }\r\n            },\r\n            "should": {\r\n              "match_phrase": {\r\n                "text": {\r\n                  "query": "central iraq",\r\n                  "phrase_slop": 3,\r\n                  "boost": 10.0\r\n                }\r\n              }\r\n            },\r\n            "minimum_should_match": 0\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "query": {\r\n    "match": {\r\n      "text": {\r\n        "query": "central iraq"\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\n\r\nThis is the old issue:\r\nWhen sorting highlights by score the rescore query should be used if there is one.  At least, I think it should be.\r\n\r\nThis gist: https://gist.github.com/nik9000/6451307\r\nShould spit out: ```"Mesopotamia, while Mithna ibn Haris withdraw from <em>central Iraq</em> to the region near the Arabian desert to delay"```\r\nRather than: ```"of <em>Iraq</em> fell to the Muslims after initial resistance in the Battle of Hira.  File:Ctesiphon, <em>Iraq</em> (2117465493)"```\r\n\r\nI have a pretty simplistic solution for this I\'ll submit once I\'m done running it through tests.'
3629,'javanna','Validate query api parses wrong date range query when using "now"\nSame problem as #3625 and #3626, but with the validate query api. \r\nWhen using the validate query API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to a valid query anyway, but the explain shows that something went wrong while parsing the date range. The following curl reproduction shows the different result obtained using the search API and the validate query API.\r\n\r\n```\r\ncurl -XPUT localhost:9200/index1/type1/1 -d \'{\r\n  "date": "2013-09-03T15:07:47.000Z"\r\n}\r\n\'\r\n\r\ncurl -XPOST localhost:9200/index1/_refresh\r\n\r\n#one hit gets returned (id 1) using search api\r\ncurl -XGET localhost:9200/index1/_search -d \'\r\n{ \r\n\t"query" : {\r\n\t    "query_string": {\r\n            "query": "date:[* TO now-1d]"\r\n        }\t\r\n\t}\r\n}\r\n\'\r\n#validate query api returns a weird query with a negative time from epoch (`date:[* TO -86400000]`")\r\ncurl -XGET localhost:9200/index1/type1/_validate/query?explain -d \'\r\n{ \r\n    "query_string": {\r\n      "query": "date:[* TO now-1d]"\r\n    }\r\n}\r\n\'\r\n```'
3628,'dadoonet','Plugin Manager: add silent mode\nNow with have proper exit codes for elasticsearch plugin manager (see #3463), we can add a silent mode to plugin manager.\r\n\r\n```sh\r\nbin/plugin --install karmi/elasticsearch-paramedic --silent\r\n```\r\n\r\n'
3627,'javanna','Query timeout ignored\nHi again,\r\n\r\nI have a timeout specified in all of my queries to prevent some slow queries from backing up the queue, but it doesn\'t seem to be working at the moment. I have many, many entries in my slowlog anywhere from 2s to 22s for queries with a timeout of 1500ms specified. Here\'s a snippet of one.\r\n\r\n[2013-09-05 13:35:32,967][WARN ][index.search.slowlog.query] [qdave] [quizlet][0] took[6.3s], took_millis[6311], types[], stats[], search_type[QUERY_THEN_FETCH], total_shards[10], source[{"fields":[],"from":0,"size":"50","timeout":"1500ms", ...\r\n\r\nAs always, anything else I can provide which will help debug this?'
3626,'javanna',' Explain api parses wrong date range query when using "now"\nSame problem as #3625, but with the explain api. \r\nWhen using the explain API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to a no match being returned. The following curl reproduction shows the different result obtained using the search API and the explain API.\r\n\r\n```\r\ncurl -XPUT localhost:9200/index1/type1/1 -d \'{\r\n  "date": "2013-09-03T15:07:47.000Z"\r\n}\r\n\'\r\n\r\ncurl -XPOST localhost:9200/index1/_refresh\r\n\r\n#one hit gets returned (id 1) using search api\r\ncurl -XGET localhost:9200/index1/_search -d \'\r\n{ \r\n\t"query" : {\r\n\t    "query_string": {\r\n            "query": "date:[* TO now-1d]"\r\n        }\t\r\n\t}\r\n}\r\n\'\r\n#explain api says the document doesn\'t match\r\ncurl -XGET localhost:9200/index1/type1/1/_explain -d \'\r\n{ \r\n    "query_string": {\r\n      "query": "date:[* TO now-1d]"\r\n    }\r\n}\r\n\'\r\n```'
3625,'javanna','Count api parses wrong date range query when using "now"\nWhen using the count API with date range queries that contain `now` (e.g. `[* TO now-1d]`), the parsed query contains a negative number that leads to count 0 being returned. The following curl reproduction shows the different result obtained using the search API and the count API.\r\n\r\n```\r\ncurl -XPUT localhost:9200/index1/type1/1 -d \'{\r\n  "date": "2013-09-03T15:07:47.000Z"\r\n}\r\n\'\r\n\r\ncurl -XPOST localhost:9200/index1/_refresh\r\n\r\n#one hit gets returned using search api\r\ncurl -XGET localhost:9200/index1/_search -d \'\r\n{ \r\n\t"query" : {\r\n\t    "query_string": {\r\n            "query": "date:[* TO now-1d]"\r\n        }\t\r\n\t}\r\n}\r\n\'\r\n#count 0 using count api\r\ncurl -XGET localhost:9200/index1/_count -d \'\r\n{ \r\n    "query_string": {\r\n      "query": "date:[* TO now-1d]"\r\n    }\r\n}\r\n\'\r\n```'
3624,'kimchy','Memory increase from 0.90.2 to 0.90.3 on java client API\n## Context\r\nOur Java batch indexes 13 millions of document with the ElasticSearch Java API.\r\nAnd our ElasticSearch cluster contains a single data node and a single shard.\r\n\r\n## Problem\r\nBy upgrading the ElasticSearch server and client from version 0.90.2 to 0.90.3, the batch stops with an OutOfMemoryError. This memory error only occurs on client side (ie. the batch).\r\nUntil now, 512 Mb will be enough to run the batch without memory problem (as well as in production environment). With the old 0.19.2 version, we neither had this problem.\r\nBy updating the Xmx value to 1g, the batch falls again. The Xms has to be set to 1300Mb in order the batch finish with success.\r\nBy downgrading client version from 0.90.3 to 0.90.2 (the ES cluster still runs with the 0.90.3 version), our batch problem goes away and 512 Mb of memory are enough.\r\nSo I believe a change between the 0.90.2 and 0.90.3 versions causes ES to require more memory (example: increase byte[] array buffer default size ?)\r\n\r\n## More informations\r\nThe batch uses the BulkRequestBuilder API. Bulk request contains 5 000 request. At most 20 threads could be running in parallel. But they are not writing to ES at the same time.\r\nAs you can see on the below screenshot, a OutOfMemory hprof dump indicates that 800Mb of byte[] is coming from the org.elasticsearch.common.bytes.BytesArray structure.\r\nWe have 25 000 org.elasticSearch.action.index.IndexRequest in memory.\r\n\r\n![batch_memory_live_objects](https://f.cloud.github.com/assets/838318/1088059/d7c7c4ea-1629-11e3-896e-23074c99be4e.jpg)\r\n\r\nDo other ElasticSearch users have this kind of memory problem with the 0.90.3 version?\r\n\r\nTo solve our issue, I see many possibilities: \r\n* Increase batch Xmx to 1300Mb. Our production environnement as more than 40 millions of documents so I fears this value will be not enough. \r\n* Use the 0.90.2 version of ElasticSearch for our batch\r\n* Wait a new version of ElasticSearch that fix this problem.\r\n* Decrease the number of request in bulk\r\n* Decrease the number of threads\r\n\r\nThe last two solutions have a flaw: the batch will run more longer. \r\n'
3623,'bleskes','Make the acceptable compression overhead used by MultiOrdinals configurable and default to FASTEST\nWe currently store ordinal in a compressed fashion using Lucene\'s PackedInt structures. Currently we use the COMPACT compression setting which tries to save on memory as much as possible at the expense of CPU and execution time. To reduce the CPU cost, we will default to FASTEST compression but make it configurable so people can change it if needed, via the mapping API:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/index/type/_mapping" -d\'\r\n{\r\n   "type": {\r\n      "properties": {\r\n         "field": {\r\n            "type": "string",\r\n            "fielddata": {\r\n               "acceptable_overhead_ratio": 0.2 \r\n            }\r\n         }\r\n      }\r\n   }\r\n}\'\r\n```\r\n'
3621,'areek',"Elasticsearch stops responding to all network requests\nHi all,\r\n\r\nI'm having an issue where ES seems to die while the JVM continues running. I can run JVM monitoring tools and see that GCs happen and get stack traces, but elasticsearch is completely nonresponsive.\r\n\r\nI have the following output from jstack -F which hopefully helps out.\r\n\r\nhttps://gist.github.com/rdeaton/6444525"
3620,'s1monw',"NullPointerException in ConcurrentMergeScheduler\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException\r\n        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:99)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$1$1.finish(AnalyzingCompletionLookupProvider.java:134)\r\n        at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$SuggestFieldsConsumer$1.finish(Completion090PostingsFormat.java:152)\r\n        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:204)\r\n        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)\r\n        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:365)\r\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:98)\r\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3772)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3376)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\r\n\r\nAs always, let me know if there's anything more I can provide to help debug."
3619,'s1monw','NullPointerException in CompletionStats\n[2013-09-04 00:46:46,113][DEBUG][action.admin.cluster.node.stats] [qtau] failed to execute on node [3MfX8Mx6Rmmp0_fukOT4lQ]\r\norg.elasticsearch.transport.RemoteTransportException: [qantiup][inet[/192.168.72.143:9300]][cluster/nodes/stats/n]\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.searcher(RobinEngine.java:682)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.completionStats(InternalIndexShard.java:536)\r\n        at org.elasticsearch.indices.InternalIndicesService.stats(InternalIndicesService.java:299)\r\n        at org.elasticsearch.node.service.NodeService.stats(NodeService.java:165)\r\n        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:100)\r\n        at org.elasticsearch.action.admin.cluster.node.stats.TransportNodesStatsAction.nodeOperation(TransportNodesStatsAction.java:43)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:280)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:271)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)'
3618,'s1monw','data loss in minimalistic test project\nHello,\r\n\r\nI\'ve set up a minimalistic Test-project using elasticsearch. Via a Java-bean, data is being created (four times via a loop -> four documents should be created) and via another bean my whole existing documents are shown on a webpage (with jsp)\r\n(The code for the create-Bean and the showAll-Bean is attached).\r\n\r\nThe problem is, that my loop creates in average 3 documents and not the wanted 4.\r\nMy documents always store the following two parameters (just for testing purpose): Time-ID (that is the counter of the loop, so it is always a number between 1 and 4) and the actual timestamp (down to milli-secs).\r\n\r\nIm getting a "true" by creating on the web page and the created id for the documents four times for creating but in my sense-plugin in chrome browser i see the data only created 3 times.\r\n\r\n(I\'m also still having the problem, that my array gets out of bounds, if there are more than 10 files, like it is descriped in issue 3051 (https://github.com/elasticsearch/elasticsearch/issues/3051). I\'m using elasticsearch 0.90.3)\r\n\r\nCode:\r\n-------------------------------------------------------------------------------------------------------------\r\npublic String createESData()\r\n\t{\r\n\t\tString result = "";\r\n\t\tString data = "";\r\n\t\tNode node = nodeBuilder().clusterName("testcluster").node();\r\n\t\tClient client = node.client();\r\n\t\t\r\n\t\ttry {\r\n\t\t\t\r\n\t\t\tfor (int i=0; i<4; i++)\r\n\t\t\t{\r\n\t\t\t\tdata = "{" +\r\n\t\t\t\t    "\\"Time-ID\\": \\"" + (i+1) + "\\"," +\r\n\t\t\t\t    "\\"Timestamp\\": \\"" + Calendar.getInstance().getTimeInMillis() + "\\"" +\r\n\t\t\t\t    "}";\r\n\t\t\t\t\r\n\t\t\t\tIndexResponse response = client.prepareIndex("times", "time")\r\n\t\t\t\t        .setSource(data)\r\n\t\t\t\t        .execute()\r\n\t\t\t\t        .actionGet();\r\n\t\t\t\t\r\n\t\t\t\tresult += (i+1) + ")\\t\\t" + response.getId() + "<br />" + "\\t" + "TRUE" + "<br />";\r\n\t\t\t}\r\n\t\t}\r\n\t\tcatch (Exception ex) {\r\n\t\t\tresult = "FEHLER: <br />" + ex.toString();\r\n\t\t}\r\n\r\n\t\tnode.close();\r\n\t\treturn result;\r\n\t}\r\n\r\n\r\n\r\npublic String showAll()\r\n\t{\r\n\t\tString result = "";\r\n\t\t\r\n\t\tNode node = nodeBuilder().clusterName("testcluster").node();\r\n\t\tClient client = node.client();\r\n\t\t\r\n\t\tSearchHits sh = client.prepareSearch("times").execute().actionGet().getHits();\r\n\t\t\r\n\t\tresult += "Anzahl der Dateien: " + sh.getTotalHits() + "<br />";\r\n\t\t\r\n\t\tfor (int i = 0; i < sh.getTotalHits(); i++)\r\n\t\t{\r\n\t\t\tresult += "ID " + sh.getAt(i).getSource().get("Time-ID") \r\n\t\t\t\t\t+ ": \\t\\t" + sh.getAt(i).getSource().get("Timestamp")\r\n\t\t\t\t\t+ "\\t\\t" + "(" + sh.getAt(i).getId() + ")"\r\n\t\t\t\t\t+ "<br />";\r\n\t\t}\r\n\r\n\t\tnode.close();\r\n\t\treturn result;\r\n\t}'
3614,'drewr','Add "remove *" support to _alias \nSometimes it would be very handy to remove an aliases from all indexes. This is the case when implementing "index symlink" behaviour. For example, when creating an index per month, it would be very convenient to perform the following:\r\n\r\n```bash\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'\r\n{\r\n    "actions" : [\r\n        { "remove" : { "index" : "*", "alias" : "current" } },\r\n        { "add" : { "index" : "august-2013", "alias" : "current" } }\r\n    ]\r\n}\'\r\n```\r\n\r\nThis prevents the application from having to find the previous current index, get the associated aliases, and perform the remove operation.\r\n\r\nAdditionally, it would be nice to remove all aliases from an index as well:\r\n\r\n```bash\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'\r\n{\r\n    "actions" : [\r\n        { "remove" : { "index" : "august-2013", "alias" : "*" } }\r\n    ]\r\n}\'\r\n```\r\n\r\nAnd of course, you should be able to combine them as well:\r\n\r\n```bash\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'\r\n{\r\n    "actions" : [\r\n        { "remove" : { "index" : "*", "alias" : "*" } }\r\n    ]\r\n}\'\r\n```'
3613,'javanna',"Highlighting can return excerpt with no highlights\nYou can configure the highlighting api to return an excerpt of a field\r\neven if there wasn't a match on the field.\r\n\r\nThe FVH makes excerpts from the beginning of the string to the first\r\nboundary character after the requested length or the boundary_max_scan,\r\nwhichever comes first.  The Plain highlighter makes excerpts from the\r\nbeginning of the string to the end of the last token before the requested\r\nlength.\r\n\r\nCloses #1171"
3610,'s1monw','BalancedShardsAllocator prematurely modifies `unassigned` shards list\nBalancedShardsAllocator modifies the unassigned shards list during initialization which is used by AllocationDeciders. Yet, premature modification can cause unintended throtteling of primary allocation which causes cluster to go in a red state until recovery / relocation has caught up to not throttle the allocation anymore. This is a very rare scenario that will only have a prominent effect on full cluster restarts or similar heavy weight operations.'
3602,'s1monw','Separate parsing impl from setter in SearchParseElement\nCurrently all SearchParseElement classes are implemented like this:\r\n\r\npublic class HighlighterParseElement implements SearchParseElement {\r\n   @Override\r\n       public void parse(XContentParser parser, SearchContext context) throws Exception {\r\n      // actual parsing logic\r\n      context.highlight(new SearchContextHighlight(fields));\r\n   }\r\n}\r\n\r\nFor some (advanced) uses I want to be able to parse individual elements myself by relying on the actual core implementation, and to do that I need to use a hack like this:\r\n\r\n                            HighlighterParseElement tmp = new HighlighterParseElement();\r\n                            SearchContext ctx = new SearchContext(0, null, null, null, null, null, null, null);\r\n                            tmp.parse(parser, ctx);\r\n                            searchContextHighlight = ctx.highlight();\r\n\r\nJust for the sake of parsing. It would be nice if the actual parsing logic could be separated from the call to the setter, something like this:\r\n\r\n    @Override\r\n    public void parse(XContentParser parser, SearchContext context) throws Exception {\r\n        try {\r\n            context.highlight(parseImpl(parser));\r\n        }\r\n        catch (IllegalArgumentException ex)\r\n        {\r\n            throw new SearchParseException(context, "Highlighter global preTags are set, but global postTags are not set");\r\n        }\r\n    }\r\n\r\n    public static SearchContextHighlight parseImpl(XContentParser parser) throws Exception {\r\n      // actual impl\r\n   }'
3599,'jpountz','Error while sorting on fields which are not present for some of the documents\nThis issue is experienced on version 0.90.3\r\nSearching in two indexes with sort on two fields - _subtype and fileSize. _subtype field is present for all indices, fileSize - is present only in the mapping of one index. \'ignore_unmapped\' is set to true for both of the fields.  \r\n\r\n```\r\n{\r\n    "sort": [\r\n        {\r\n            "_subtype": {\r\n                "order": "desc",\r\n                "ignore_unmapped": true\r\n            }\r\n        },\r\n        {\r\n            "fileSize": {\r\n                "order": "asc",\r\n                "ignore_unmapped": true\r\n            }\r\n        }\r\n    ],\r\n\r\n    "query": {\r\n        "query_string": {\r\n            "query": "_parents.$id:5223195f93f46463974aa1f4"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThe result for JSON search is:\r\n\r\n```\r\n{\r\n    error: ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ArrayIndexOutOfBoundsException[1];\r\n    status: 500\r\n}\r\n```\r\n\r\nWhen searchhing with Java API:\r\n\r\n```\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [query], [reduce] \r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:172) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:150) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:144) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:176) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:144) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$2.run(TransportSearchDfsQueryThenFetchAction.java:131) ~[elasticsearch-0.90.3.jar:na]\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]\r\n\tat java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 1\r\n\tat org.elasticsearch.search.controller.ShardFieldDocSortedHitQueue.lessThan(ShardFieldDocSortedHitQueue.java:118) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.search.controller.ShardFieldDocSortedHitQueue.lessThan(ShardFieldDocSortedHitQueue.java:35) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.apache.lucene.util.PriorityQueue.downHeap(PriorityQueue.java:247) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]\r\n\tat org.apache.lucene.util.PriorityQueue.pop(PriorityQueue.java:184) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]\r\n\tat org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:281) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerExecuteFetchPhase(TransportSearchDfsQueryThenFetchAction.java:177) ~[elasticsearch-0.90.3.jar:na]\r\n\tat org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:170) ~[elasticsearch-0.90.3.jar:na]\r\n\t... 8 common frames omitted\r\n```\r\n\r\nInteresting thing is that if I change fileSize field into some totally diffrerent field, which is not present in any index, for example fileSizeAAA, I do not get this error. If I omit one the _subtype field, and leave only fileSize field, then I get another error:\r\n\r\n```\r\n{\r\n\r\n    error: ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[org.apache.lucene.search.ScoreDoc cannot be cast to org.apache.lucene.search.FieldDoc];\r\n    status: 503\r\n\r\n}\r\n```\r\n'
3595,'s1monw','NullPointerException on empty filter with 0.90.3\nProblem discussed on mailing list here:\r\nhttps://groups.google.com/forum/#!topic/elasticsearch/Y1ewHtYxExs\r\n\r\nGist with log here:\r\nhttps://gist.github.com/benoit-intrw/6374962'
3594,'bleskes','The light Finnish stemmer is specified as "light_finish"\nThis is mostly a typo as it does look like it uses the correct stemmer in Java, but would it be worthwhile to correct the spelling from "finish" to "finnish"?\r\n\r\nSee:\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/analysis/StemmerTokenFilterFactory.java#L136'
3592,'dadoonet','Improved logic that removes top-level folder from archives when needed\nWhether we remove the top-level folder from the archive depends now on the zip itself and not on where it was downloaded from. That makes it work installing local files too.'
3591,'s1monw','ArrayIndexOutOfBoundsException when using empty preference parameter\nto reproduce(tested on 0.90.3):\r\n```\r\ncurl -XGET http://localhost:9200/_search?preference=\r\n```\r\n\r\nthis returns:\r\n```\r\n{\r\nerror: "StringIndexOutOfBoundsException[String index out of range: 0]",\r\nstatus: 500\r\n}\r\n```\r\n\r\nI\'m not really sure what is the desired behavior in this case, because it could be one of:\r\n    1 - ignore the parameter and treat as if nothing was sent\r\n    2 -  treat the empty string as the preference parameter\r\n    3 - return a friendly error message for it\r\n\r\nanyway, the error happens here (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java#L172)\r\nand it could be easily fixed by either making a check for an empty string before comparing its first character, or changing this line https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/cluster/routing/operation/plain/PlainOperationRouting.java#L164 for something like preference.isEmpty(), which also covers the empty string case.'
3587,'s1monw','SimpleHTMLEncoder to not encode non-ASCII chars\nCurrently Lucene\'s SimpleHTMLEncoder has this:\r\n\r\n      default:\r\n           if (ch < 128)\r\n           {\r\n                 result.append(ch);\r\n             }\r\n           else\r\n             {\r\n                 result.append("&#").append((int)ch).append(";");\r\n             }\r\n\r\nThis produces a lot of garbage especially when asking for fragments when using the fvh/html encoder combo, where the encoded string may get cut in the middle of an encoded entity\r\n\r\nThe obvious solution is to account for length changes caused by the encoder, but an even simpler solution will be to remove the above code portion and just use default: result.append(ch). Nowadays when its all UTF8 it doesn\'t make much sense to keep it anyway.\r\n\r\nI\'m posting this here and not on Lucene\'s because a) it will get fixed faster here and b) its more of an ES issue, because of the fragment size DSL etc'
3586,'polyfractal','Throw exception when content _id is an object\nAn exception is thrown if the provided id does not match the content id, but only if the content id is a string field.  If the content id is a complex object, no exception is thrown and the document is indexed anyway, leading to problems with search later.\r\n\r\nThis fix adds an additional check for _id fields that are objects and throws an exception if one is encountered.\r\n\r\nFixes #3517'
3585,'bleskes','Count and Search API status codes are inconsistent (count does not return 400)\nI\'ve got a query that produces a 400 when performing a search. However, when I use the same query to perform a count, a 200 status is returned.\r\n\r\nhttps://gist.github.com/garron/6346888\r\n\r\nThe output from this gist is:\r\n\r\n{"ok":true,"_index":"myindex","_type":"mydoc","_id":"5kngTmpeR5GtJg6Vu46tUw","_version":1}\\n\r\nCount using a good query\r\n200\r\nSearch using a good query\r\n200\r\nCount using a bad query\r\n200\r\nSearch using a bad query\r\n400\r\n\r\nIf you take out the "-o /dev/null", you can see that both the search and count produce errors in the "bad query" case, but the count API still returns a 200.'
3582,'javanna',"Install site plugin with custom url doesn't filter directory\n1. Download  https://github.com/mobz/elasticsearch-head/zipball/master\r\n2. Copy downloaded file to target server /tmp/mobz-elasticsearch-head-0c2ac0b.zip\r\n3. Install plugin with command:\r\n```\r\n bin/plugin --url file:/tmp/mobz-elasticsearch-head-0c2ac0b.zip --install head\r\n```\r\n4. The plugins is installed in plugins/head/_site/mobz-elasticsearch-head-0c2ac0b/... instead of plugins/head/_site/...\r\n\r\nThe folder could be stripped because it's same as file name."
3581,'martijnvg','Extend named filter support to queries\nAt moment the `_name` option is supported on all filters, and the search responses then indicates which filters that have the `_name` option have matched.\r\n\r\nThis name tagging should also be supported for queries. The matchedFilters will now also contain named queries and  because of this the method naming in the Java apis have been changed to `matchedQueries(...)` instead of `matchedFilters(...)` in the SearchHit class. The `matchedFilters(...)` methods are still supported, but have been deprecated and will be removed in from version 1.0.Beta1.'
3580,'s1monw',"Forced awareness fails to balance shards\nI built a cluster for amazon zone awareness as follows:\r\n\r\nBackground: I've setup the number of shards to 3, and I have allocation awareness set to match the awszone property which I'm setting in a config. \r\n\r\nThere's 6 nodes total:\r\n\r\n  2 in us-west-2a\r\n  2 in us-west-2b\r\n  1 in us-east-2a\r\n  1 in us-east-2b\r\n\r\nSo in theory, I should be able to lose either an entire side of the country. I would also expect the nodes in us-west to balance the shards, not just allocate a single shard to the machine. \r\n\r\nHowever, the western zone shards aren't balancing except to move 1 shard onto each of the other hosts in the zone. \r\n\r\nS1monw is on the case. \r\n\r\nDiscussion here: https://groups.google.com/forum/m/#!topic/elasticsearch/9yZw7sryFb4"
3579,'electrical','0.90.3 [RPM]: Initscript does not work on SLES11SP3\nHi folks,\r\n\r\nfirst and foremost thank you for such a great project. I am starting using it in conjunction with logstash and so far, it works great :)\r\n\r\nI am testing the deployment on SLES11SP3 x86_64, and there is a small issue with the initscript. The service will not start, because daemon is neither a funciton nor an executable on SUSE. I adjusted the initscript (patch below) to fix that. Also, the initscript should be installed in /etc/init.d/ and ideally there would be a symlink /usr/sbin/rcelasticsearch which points to /etc/init.d/elasticsearch (SUSE specific though and not really necessary).\r\n\r\n<pre>\r\n--- elasticsearch.orig\t2013-08-27 11:08:30.000000000 +0200\r\n+++ elasticsearch\t2013-08-27 12:00:03.000000000 +0200\r\n@@ -10,7 +10,7 @@\r\n # Provides: Elasticsearch\r\n # Required-Start: $network $named\r\n # Required-Stop: $network $named\r\n-# Default-Start: 2 3 4 5\r\n+# Default-Start: 2 3 5\r\n # Default-Stop: 0 1 6\r\n # Short-Description: This service manages the elasticsearch daemon\r\n # Description: Elasticsearch is a very scalable, schema-free and high-performance search solution supporting multi-tenancy and near realtime search.\r\n@@ -63,7 +63,6 @@\r\n }\r\n \r\n start() {\r\n-    checkJava\r\n     [ -x $exec ] || exit 5\r\n     [ -f $CONF_FILE ] || exit 6\r\n     if [ -n "$MAX_LOCKED_MEMORY" -a -z "$ES_HEAP_SIZE" ]; then\r\n@@ -82,7 +81,12 @@\r\n     fi\r\n     echo -n $"Starting $prog: "\r\n     # if not running, start it up here, usually something like "daemon $exec"\r\n-    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\r\n+    # SUSE uses startproc to daemonize processes\r\n+    if [ -x /sbin/start_daemon ]; then\r\n+\t/sbin/start_daemon -u $ES_USER -p $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\r\n+    else\r\n+\tdaemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -Des.default.path.home=$ES_HOME -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR\r\n+    fi\r\n     retval=$?\r\n     echo\r\n     [ $retval -eq 0 ] && touch $lockfile\r\n@@ -92,7 +96,7 @@\r\n stop() {\r\n     echo -n $"Stopping $prog: "\r\n     # stop it here, often "killproc $prog"\r\n-    killproc -p $pidfile $prog\r\n+    killproc -p $pidfile $JAVA\r\n     retval=$?\r\n     echo\r\n     [ $retval -eq 0 ] && rm -f $lockfile\r\n@@ -114,7 +118,12 @@\r\n \r\n rh_status() {\r\n     # run checks to determine if the service is running or use generic status\r\n-    status -p $pidfile $prog\r\n+    if [ -r /etc/rc.status ]; then\r\n+\tcheckproc -p $pidfile $JAVA\r\n+\t# rc_status -v -r\r\n+    else\r\n+    \tstatus -p $pidfile $JAVA\r\n+    fi\r\n }\r\n \r\n rh_status_q() {\r\n@@ -122,6 +131,8 @@\r\n }\r\n \r\n \r\n+checkJava\r\n+\r\n case "$1" in\r\n     start)\r\n         rh_status_q && exit 0\r\n\r\n</pre>\r\n\r\nApologies for not simply filing a pull request. I am still learning my way around git/github :-/'
3576,'dakrone',"Prevent large file recovery from blocking small file recovery\nIt's currently easy to block the concurrent streams pipeline for recovering files with large files when transferring large segments across the network.\r\n\r\nWe should keep these large transfers from blocking small file recovery (like the very small files created when a new index is created)."
3575,'martijnvg','Faceting on field with different type (across indices or document types) lead to unexpected behavior\nWhen you have a simple situation with two types, one field with two different types (string and long here) then faceting on that field leads to unexpected behavior:\r\n\r\n    curl -X PUT localhost:9200/i/t/1 -d \'{"answer": "321"}\'\r\n    curl -X PUT localhost:9200/i/t2/1 -d \'{"answer": 321}\'  \r\n    curl -X POST localhost:9200/_refresh\r\n    curl -X GET localhost:9200/i/_search -d \'{"facets": {"test": {"terms": {"field": "answer"}}}}\'\r\n\r\nImho ES should fail to run this facet instead. Related to #3106'
3574,'martijnvg','Add document highlighter to percolate api \nAdd document highlighting to percolate api, that highlights for each matching percolator query snippets inside the document being percolated.\r\n\r\nAll highlight options that are supported via the search api are also supported in the percolate api. The `size` option is a required option if highlighting is specified in the percolate api.\r\n\r\n### Percolate highlight example\r\n\r\n#### Index document:\r\n```bash\r\ncurl -XPUT \'localhost:9200/my-index/_percolator/1\' -d \'{ \r\n    "query": { \r\n        "match" : { \r\n            "body" : "brown fox"  \r\n        }   \r\n    } \r\n}\'\r\n```\r\n\r\n#### Index second document:\r\n```bash\r\ncurl -XPUT \'localhost:9200/my-index/_percolator/2\' -d \'{ \r\n    "query": { \r\n        "match" : { \r\n            "body" : "lazy dog"  \r\n        }   \r\n    } \r\n}\'\r\n```\r\n\r\n#### Percolate request:\r\n```bash\r\ncurl -XGET \'localhost:9200/my-index/my-type/percolate\' -d \'{\r\n    "doc" : {\r\n        "body" : "The quick brown fox jumps over the lazy dog"\r\n    },\r\n    "highlight" : {\r\n        "fields" : {\r\n            "body" : {}\r\n        }\r\n    },\r\n    "size" : 5\r\n}\'\r\n```\r\n\r\n#### Percolate response:\r\n```json\r\n{\r\n   "took": 18,\r\n   "_shards": {\r\n      "total": 5,\r\n      "successful": 5,\r\n      "failed": 0\r\n   },\r\n   "total": 2,\r\n   "matches": [\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "1",\r\n         "highlight": {\r\n            "body": [\r\n               "The quick <em>brown</em> <em>fox</em> jumps over the lazy dog"\r\n            ]\r\n         }\r\n      },\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "2",\r\n         "highlight": {\r\n            "body": [\r\n               "The quick brown fox jumps over the <em>lazy</em> <em>dog</em>"\r\n            ]\r\n         }\r\n      }\r\n   ]\r\n}\r\n```'
3571,'spinscale','debian package init-script: start-stop-daemon needs -u argument\nOn Ubuntu 12.04.3, the init-script provided in the debian package fails to detect that the server is no longer running if a pid-file is still in place. Typically this happens if the server crashes for some reason.\r\n\r\nTo fix the issue, append «-u elasticsearch» to the start-stop-daemon - tests in the start) and status) blocks of the init-script.\r\n\r\nPatch:\r\n\r\n    diff -u init.d-old init.d-new \r\n    --- init.d-old\t2013-08-26 10:35:57.282790191 +0200\r\n    +++ init.d-new\t2013-08-26 10:34:19.510794716 +0200\r\n    @@ -139,7 +139,7 @@\r\n     \tlog_daemon_msg "Starting $DESC"\r\n     \r\n     \tset +e\r\n    -\tstart-stop-daemon --status --pidfile "$PID_FILE" >/dev/null\r\n    +   start-stop-daemon --status --pidfile "$PID_FILE" -u elasticsearch >/dev/null\r\n     \tif [ "$?" != "0" ]; then\r\n     \t\t# Prepare environment\r\n     \t\tmkdir -p "$LOG_DIR" "$DATA_DIR" "$WORK_DIR" && chown "$ES_USER":"$ES_GROUP" "$LOG_DIR" "$DATA_DIR" "$WORK_DIR"\r\n    @@ -195,7 +195,7 @@\r\n     \t;;\r\n       status)\r\n     \tset +e\r\n    -\tstart-stop-daemon --status --pidfile "$PID_FILE" >/dev/null 2>&1\r\n    +\tstart-stop-daemon --status --pidfile "$PID_FILE" -u elasticsearch >/dev/null 2>&1\r\n     \tif [ "$?" != "0" ]; then\r\n     \t\tif [ -f "$PID_FILE" ]; then\r\n     \t\t    log_success_msg "$DESC is not running, but pid file exists."\r\n\r\n'
3563,'martijnvg',"Id cache statistics not updated when id_cache is being cleared\nIssue found reported via pr #3561. \r\n\r\nThe id cache statistics aren't updated in the case the clear cache api is invoked and id cache entries are evicted when a segment is merged away."
3561,'martijnvg','Call onRemoval of shard IdCache during clear.\nThis looks like a copy/paste issue where onCached was being called\r\nrather than onRemoval. This should fix the ID cache stats not being\r\ncorrect after a call to /_cache/clear?id_cache=true'
3560,'bleskes','Cluster Setting update can hang if gets settings which are not dynamically updatable\nAlso holds for settings that do not exist:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/_cluster/settings" -d\'\r\n{\r\n    "transient": {\r\n        "cluster.routing.allocation.same_shard.host": true\r\n    }\r\n}\'\r\n```\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/_cluster/settings" -d\'\r\n{\r\n    "transient": {\r\n        "cluster": true\r\n    }\r\n}\'\r\n```'
3559,'uboness','Random scoring should take into account index name and shard id\nRelative to #1170 \r\n\r\nRandom scoring should take into account index name and shard id.\r\n'
3558,'martijnvg','Avoid shading of org.joda.convert package, fixes #3557\nAnother fix for shading issue.'
3555,'s1monw',"NullPointerException during concurrent merges\nLooks like it may be related to the new suggestion stuff. I'm on 0.90.3, Java 7. Will be trying to reproduce on master shortly, but let me know if there's other stuff you need to debug.\r\n\r\n[2013-08-21 22:02:05,166][WARN ][index.engine.robin       ] [qanticharm] [quizlet][9] failed engine\r\norg.apache.lucene.index.MergePolicy$MergeException: java.lang.NullPointerException\r\n        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:99)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:518)\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.search.suggest.completion.AnalyzingCompletionLookupProvider$1$1.finish(AnalyzingCompletionLookupProvider.java:134)\r\n        at org.elasticsearch.search.suggest.completion.Completion090PostingsFormat$SuggestFieldsConsumer$1.finish(Completion090PostingsFormat.java:152)\r\n        at org.apache.lucene.codecs.TermsConsumer.merge(TermsConsumer.java:204)\r\n        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:72)\r\n        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:365)\r\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:98)\r\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3772)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3376)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:405)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:482)\r\n[2013-08-21 22:02:05,180][WARN ][cluster.action.shard     ] [qanticharm] sending failed shard for [quizlet][9], node[27LWb8rFQiafhZhgRH4vjg], [R], s[STARTED], reason [engine failure, message [MergeException[java.lang.NullPointerException]; nested: NullPointerException; ]]"
3554,'clintongormley','Removed ambiguously-worded ES_HEAP_SIZE comment\nMultiple people have misread the original comment "(defaults to 256m min, 1g max)" as meaning "you must set this to a value between 256m and 1g" - probably partly due to the wording, partly due to it being slightly odd to ["[recommend setting] the min and max memory to the same value"](set the min and max memory to the same value) yet not defaulting to doing so.\r\n\r\nIn the absence of being able to word it better, I\'ve just stripped it out to minimise confusion (and none of the other settings have their defaults included in the comments)'
3553,'javanna',"Updates and inserts are silently failing.\nAfter adding new field in elastic search index updates and inserts started failing.\r\nStatus of cluster is green and there is no information in log.\r\nWe tried restarting elastic search.\r\nIndex status seems fine\r\nAlso tried to close and re-open the index but nothing helped.\r\nWe are using tire gem to interact with elastic search.\r\nWe created new index and same code works fine with new index.\r\nI didn't find any help on internet regarding this issue.\r\nCan some please help us to troubleshoot this issue."
3551,'dadoonet','Plugin Manager can not download _site plugins from github\nSounds like github changes a bit download url for master zip file.\r\n\r\nFrom `https://github.com/username/reponame/zipball/master` to `https://github.com/username/reponame/archive/master.zip`.\r\n\r\nWe need to update plugin manager to reflect that change.'
3548,'s1monw','Got only one Node but Status is "Green with 4 Nodes"\nconfiguration (One Node only)\r\n\r\nLogstash_1.1.13\r\nElasticsearch_0.20.6\r\nApache_2.2 + passenger_module\r\nKibana_2\r\n\r\nfor some reason the cluster state show i have 4 nodes and the status is green:\r\n\r\n{\r\n  "cluster_name" : "startapp",\r\n  "status" : "green",\r\n  "timed_out" : false,\r\n  "number_of_nodes" : 4,\r\n  "number_of_data_nodes" : 1,\r\n  "active_primary_shards" : 30,\r\n  "active_shards" : 30,\r\n  "relocating_shards" : 0,\r\n  "initializing_shards" : 0,\r\n  "unassigned_shards" : 0\r\n}\r\n\r\nas far as i can see from: \r\nhttp://chrissimpson.co.uk/elasticsearch-yellow-cluster-status-explained.html\r\n\r\nSince i only have one Node, the status should be Yellow with one node\r\nI tried to check Network, hostname, PTR + A record And checked Elasticsearch configuration and found no reason for this to happen\r\n\r\nWhat should i check next ?\r\n \r\n'
3547,'s1monw',"Phrase suggest can make bad suggestions when searching for an uncommon phrase and confidence > 1\nIt looks like sometimes documents get sharded in such a way the phrase suggest confidence stops working from time to time.  I have a gist (https://gist.github.com/nik9000/6287620) that normally produces a phrase response with no suggestions (like it should) but if you run it a bunch of times (5-20) it'll eventually return one with suggestions.  I've determined that if you set the number of shards to 1 this won't happen."
3544,'brwe','Wrong analyzer used when indexing dynamic property\nHi,\r\n\r\nI\'m seeing some unusual behaviour when indexing documents in Elasticsearch with a dynamic template. I have a unit test that\'s failing intermittently. The flow of the test is as follows:\r\n\r\n    1) Initialise in-memory Elasticsearch cluster (one local node, no replicas)\r\n    2) Create new index\r\n    3) Create new type mapping\r\n    4) Index some documents\r\n    5) Refresh index and wait for all documents to be processed\r\n    6) Query Elasticsearch for documents\r\n\r\nThe type mapping I\'m using includes the following dynamic template definition:\r\n\r\n    {\r\n        "participants": {\r\n            "path_match": "participants.*",\r\n            "mapping": {\r\n                "type": "string",\r\n                "store": "yes",\r\n                "index": "analyzed",\r\n                "analyzer": "whitespace"\r\n            }\r\n        }\r\n    }\r\n\r\nThis template is intended to produce fields of the form:\r\n\r\n    participants.new = [ \'user-1\', \'user-2\' ]\r\n    participants.removed = [ \'user-3\' ]\r\n\r\nThe problem I have is that occasionally (perhaps once in every ten runs) the test will fail because step 6 does not return all the expected documents. When I check the indexed terms for the missing documents I see that values in the \'participants\' field have been split into separate tokens on the \'-\' character. This seems to suggest that the default analyzer is being used for indexing instead of the whitespace one.\r\n\r\nSo far I haven\'t been able to detect any pattern to the failures. The unexpected tokenisation only affects a portion of the indexed documents and can occur at any point in the indexing process (i.e. it isn\'t always the first or last document that has problems).\r\n\r\nHere is a gist with a simplified version of my original test: https://gist.github.com/pmclellan/64b192537c97529ec2e4 This version fails much more consistently, usually on the first run. However, I have noticed that the problem does not occur if I index documents in a synchronous manner (i.e. waiting for a response to each indexing request before issuing the next).\r\n\r\nCheers,\r\nPaul\r\n'
3543,'s1monw',"FVH can end in very very long running recursion on phrase highlight\nThis is a followup from [LUCENE-5182](https://issues.apache.org/jira/browse/LUCENE-5182) ...due to the nature of FVH extract logic a simple phrase query can put a FHV into a super long running recursion. I had documents taking literally days to return form the extract phrases logic. I have a test that reproduces the problem and a possible fix. The reason for this is that the FVH never tries to early terminate if a phrase is already way beyond the slop coming from the phrase query. If there is a document with lot of occurrences or two or more terms in the phrase this literally tries to match all possible combinations of the terms in the doc. I don't think we can fix this FVH without rewriting it since this alg is freaking crazy and somehow `n!` of all the positions etc. I am not even sure what the Big-O of this is but I have a patch that tires to prevent this thing from going totally nuts."
3541,'javanna',"Rename readable_format flag to human\nGiven that most unix tools support the human readable (-h) flag to turn the output into something that humans can easily understand, we should rename the readable_format flag (added with #3432) to something along those lines. Let's call it `human`."
3538,'bleskes','Dynamic templates from an index template are skipped if a new type already have dynamic templates\nThe two dynamic templates lists should be merged. \r\n\r\nExample:\r\n\r\nAdd a template:\r\n```\r\ncurl -XPUT "http://localhost:9200/_template/template_1" -d\'\r\n{\r\n   "template": "te*",\r\n   "mappings": {\r\n      "type1": {\r\n         "dynamic_templates": [\r\n            {\r\n               "t1": {\r\n                  "match": "*",\r\n                  "mapping": {\r\n                     "store": "yes"\r\n                  }\r\n               }\r\n            }\r\n         ]\r\n      }\r\n   }\r\n}\'\r\n```\r\n\r\ncreate a type:\r\n```\r\ncurl -XPUT "http://localhost:9200/test1" -d\'\r\n{\r\n   "mappings": {\r\n      "type1": {\r\n         "dynamic_templates": [\r\n            {\r\n               "t2": {\r\n                  "match": "t*",\r\n                  "mapping": {\r\n                     "store": "no"\r\n                  }\r\n               }\r\n            }\r\n         ]\r\n      }\r\n   }\r\n}\'\r\n```\r\n\r\nNow get the mappings for the newly created type:\r\n```\r\ncurl -XGET "http://localhost:9200/test1/_mapping"\r\n```\r\n\r\nwhich gives (note that the t1 template is missing):\r\n````\r\n{\r\n   "test1": {\r\n      "type1": {\r\n         "dynamic_templates": [\r\n            {\r\n               "t2": {\r\n                  "mapping": {\r\n                     "store": "yes"\r\n                  },\r\n                  "match": "t*"\r\n               }\r\n            }\r\n         ],\r\n         "properties": {}\r\n      }\r\n   }\r\n}\r\n````\r\n\r\n'
3536,'brwe','multi term vectors similar to multi get\nCurrently, we can only retrieve [term vectors](https://github.com/elasticsearch/elasticsearch/issues/3114) for one document at a time. If term vectors for many documents are requested, a multi term vectors endpoint, similar to multi get, would be useful:\r\n\r\n    \r\n    ```\r\n    curl -XGET \'http://localhost:9200/index/type/_mtermvectors\' -d \'{\r\n        "fields": [\r\n            "field1",\r\n            "field2",\r\n            ...\r\n        ],\r\n        "ids": [\r\n            "docId1",\r\n            "docId2",\r\n            ...\r\n        ],\r\n        "offsets": false|true,\r\n        "payloads": false|true,\r\n        "positions": false|true,\r\n        "term_statistics": false|true,\r\n        "field_statistics": false|true\r\n    }\'\r\n    \r\n    ```\r\n    \r\n    The return format is an array, each entry of which conatins the term vector response for one document:\r\n    \r\n    ```\r\n    {\r\n       "docs": [\r\n          {\r\n             "_index": "index",\r\n             "_type": "type",\r\n             "_id": "docId1",\r\n             "_version": 1,\r\n             "exists": true,\r\n             "term_vectors": {\r\n             \t...\r\n             }\r\n          },\r\n          {\r\n             "_index": "index",\r\n             "_type": "type",\r\n             "_id": "docId2",\r\n             "_version": 1,\r\n             "exists": true,\r\n             "term_vectors": {\r\n             ...\r\n             }\r\n          }\r\n       ]\r\n    }\r\n    ```'
3534,'bleskes','Requesting the mapping of an index where no mappings are defined yet throws an Index Not found exception\nTo reproduce:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/index1"\r\n```\r\n\r\n```\r\ncurl -XGET "http://localhost:9200/index1/_mapping"\r\n```\r\n\r\nNow returns:\r\n```\r\n{\r\n   "error": "IndexMissingException[[index1] missing]",\r\n   "status": 404\r\n}\r\n```\r\n\r\nShould return:\r\n```\r\n{\r\n   "index1": {}\r\n}\r\n```\r\n\r\n\r\n'
3533,'brwe','function score usability\n```function_score``` ([Issue 3423 ](https://github.com/elasticsearch/elasticsearch/issues/3423)  and [Issue 3464](https://github.com/elasticsearch/elasticsearch/issues/3464) ) would be much easier to use, if the following issues were implemented:\r\n\r\n\r\n1. `reference` should also accept `{ lat: 11, lon: 12 }` and `[12,11]` for geopoints\r\n\r\n2. rename `scale_weight`, to `decay`?\r\n\r\n3. decay functions should support an `offset` value, eg anything within 10km is fine, then start decaying every 2km after that.\r\n\r\n4. rename the `score_mode` `total` to `sum`\r\n\r\n5. add `boost_mode`, to let the user define how the result of the score function is combined with the query score. allow options:\r\n\t* `multiply` (default)\r\n\t* `replace` \r\n\t* `sum`\r\n\t* `avg`\r\n\t* `min`\r\n\t* `max`\r\n\r\nmin/max because we may be combining this `function_score` query with another `function_score` query.\r\n\r\n6. make the default for `boost_mode` == `multiply`, even   with scripts, to make it consistent throughout.  This means, that `function_score` behaves different that `custom_score` before. To mimic the old functionality, change `boost_mode` to `replace`\r\n\r\n\r\n8. rename  `reference` to `origin`\r\n\r\n9. make nice java api for score function builders.\r\n'
3532,'dadoonet','old doku? \nhttp://www.elasticsearch.org/blog/the-river-searchable-couchdb/'
3529,'spinscale','#1745: Adds check to only write pidfile on succuess exec\nSee issue #1745.\r\n\r\n- Tom'
3527,'spinscale',"ZIP Version of 0.90.3 distribution not working on Mac OSX 10.8.3 (missing files?)\nHi,\r\n\r\nI downloaded this version and get the following error:\r\n````\r\nInitialization Failed ...\r\n- MissingResourceException[Can't find bundle for base name org.elasticsearch.common.joda.time.format.messages, locale en]\r\n````\r\n\r\nI then proceeded to download the tar.gz version and it worked perfectly!"
3526,'kimchy','Setting index/bulk thread pools with queue_size can cause replica shard failures\nWhen setting `queue_size` on the index/bulk thread pools, they can cause replica shard failures when a request ends up being rejected on the replica shard. We should not adhere to the `queue_size` limit when executing the operation on the replica (which is perfectly fine, since the primary shard will make sure to limit it).'
3521,'s1monw','odd scoring behaviour / inconsistent scoring\nunfortunately i wasn\'t able to reproduce this in the usual way, but here is the case:\r\n\r\nI have a query such as \r\n```\r\n{ "from": 0, "size": 10, "query": { "bool": {  "must": [ {"match_all":{}},{ "constant_score": { "filter": { "terms": { "id": [...] } } } } ] } }, "fields": "", "sort": [ { "_score": {} }, { "id": { "order": "desc" } } ] }\r\n```\r\nto the terms filter, i pass a list of ids(anywhere between 1 and 200k unique ids).\r\n\r\nwhen executing this query multiple times i get different results. so, investigating a little i traced it to the score not being constant sometimes(not what i expected at all).\r\n\r\ni ran the same query a few times with explain set to true and getting only the last document, and here is what i got:\r\n\r\n```\r\n_explanation: {\r\nvalue: 1.264911\r\ndescription: sum of:\r\ndetails: [\r\n{\r\nvalue: 0.94868326\r\ndescription: ConstantScore(*:*)^3.0, product of:\r\ndetails: [\r\n{\r\nvalue: 3\r\ndescription: boost\r\n}\r\n{\r\nvalue: 0.31622776\r\ndescription: queryNorm\r\n}\r\n]\r\n}\r\n{\r\nvalue: 0.31622776\r\ndescription: ConstantScore...\r\n```\r\n\r\nand then \r\n\r\n```\r\n_explanation: {\r\nvalue: 1.4142135\r\ndescription: sum of:\r\ndetails: [\r\n{\r\nvalue: 0.70710677\r\ndescription: ConstantScore(*:*), product of:\r\ndetails: [\r\n{\r\nvalue: 1\r\ndescription: boost\r\n}\r\n{\r\nvalue: 0.70710677\r\ndescription: queryNorm\r\n}\r\n]\r\n}\r\n{\r\nvalue: 0.70710677\r\ndescription: ConstantScore...\r\n```\r\n\r\nso, here i would expect this query to ALWAYS have the same score, and also, that every document scores exactly the same. \r\nit could even seem ok if the score wasnt constant across requests, but not really that documents score differently.\r\n\r\n\r\n* i do know i don\'t need the "match_all" query, but that\'s the way i managed to reproduce it on our cluster. without that, the score for the documents would always be 1 and i could not reproduce this behaviour.\r\n\r\n** hope thats clear enough... but let me know if you need more info, or even the complete output for the explain(its pretty big)'
3520,'dadoonet','Refactor the plugin manager to be more readable, focused\nRefactoring related to issue #3519\r\n\r\n'
3519,'dadoonet','Split download and extract in PluginManager\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/plugins/PluginManager.java#L91\r\n\r\nThis method tries to accomplish quite a lot; and if like us you are looking to install plugins from different protocols (sftp), it has some hard coded expectations around file protocols, locations and more.\r\n\r\nIt could benefit from:\r\n\r\n* Refactoring to split up \'fetch the file using some mechanism\' and "now that we have the .zip, install it"\r\n* Injecting some of the dependencies, like HTTPDownloader; so that other strategies can be implemented.'
3516,'martijnvg','id_cache memory footprint grows linearly with number of parent documents\nAs parent documents are indexed, the size of the parent/child `id_cache` grows linearly.  This applies even if the majority of parents do not have any associated children.'
3515,'s1monw','NullPointerException during discovery\nSome logs from during discovery of a brand new cluster today. The zen.multicast is set to disable and they are all given a list of the other nodes for unicast.\r\n\r\n[2013-08-15 02:41:17,744][INFO ][node                     ] [qphosphorus2] stopping ...\r\n[2013-08-15 02:41:18,237][INFO ][node                     ] [qphosphorus2] stopped\r\n[2013-08-15 02:41:18,237][INFO ][node                     ] [qphosphorus2] closing ...\r\n[2013-08-15 02:41:18,273][INFO ][node                     ] [qphosphorus2] closed\r\n[2013-08-15 02:57:51,994][INFO ][node                     ] [qphosphorus2] version[0.90.3], pid[72565], build[5c38d60/2013-08-06T13:18:31Z]\r\n[2013-08-15 02:57:51,995][INFO ][node                     ] [qphosphorus2] initializing ...\r\n[2013-08-15 02:57:52,004][INFO ][plugins                  ] [qphosphorus2] loaded [], sites [head]\r\n[2013-08-15 02:57:54,627][INFO ][node                     ] [qphosphorus2] initialized\r\n[2013-08-15 02:57:54,627][INFO ][node                     ] [qphosphorus2] starting ...\r\n[2013-08-15 02:57:54,856][INFO ][transport                ] [qphosphorus2] bound_address {inet[/192.168.72.120:9300]}, publish_address {inet[/192.168.72.120:9300]}\r\n[2013-08-15 02:58:24,865][WARN ][discovery                ] [qphosphorus2] waited for 30s and no initial state was set by the discovery\r\n[2013-08-15 02:58:24,866][INFO ][discovery                ] [qphosphorus2] QuizletProductionCluster/HstcP8sQRWGi6nwjZU5KHw\r\n[2013-08-15 02:58:24,949][INFO ][http                     ] [qphosphorus2] bound_address {inet[/192.168.72.120:9200]}, publish_address {inet[/192.168.72.120:9200]}\r\n[2013-08-15 02:58:24,950][INFO ][node                     ] [qphosphorus2] started\r\n[2013-08-15 02:58:54,909][INFO ][cluster.service          ] [qphosphorus2] new_master [qphosphorus2][HstcP8sQRWGi6nwjZU5KHw][inet[/192.168.72.120:9300]]{master=true}, reason: zen-disco-join (elected_as_master)\r\n[2013-08-15 02:59:30,213][INFO ][cluster.service          ] [qphosphorus2] added {[qaluminium2][YH9kVKH-Rgyc9tVcxQq_-g][inet[/192.168.72.115:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qaluminium2][YH9kVKH-Rgyc9tVcxQq_-g][inet[/192.168.72.115:9300]]{master=true}])\r\n[2013-08-15 03:00:35,465][INFO ][cluster.service          ] [qphosphorus2] added {[qargon2][Mif6T8WDT0Of0Td9vgrf_w][inet[/192.168.72.119:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qargon2][Mif6T8WDT0Of0Td9vgrf_w][inet[/192.168.72.119:9300]]{master=true}])\r\n[2013-08-15 03:00:35,533][DEBUG][action.admin.cluster.node.stats] [qphosphorus2] failed to execute on node [Mif6T8WDT0Of0Td9vgrf_w]\r\norg.elasticsearch.transport.RemoteTransportException: [qargon2][inet[/192.168.72.119:9300]][cluster/nodes/stats/n]\r\nCaused by: java.lang.NullPointerException\r\n        at org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:59)\r\n        at org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:215)\r\n        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:83)\r\n        at org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:62)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)\r\n        at org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:269)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\n[2013-08-15 03:00:37,546][INFO ][cluster.service          ] [qphosphorus2] added {[qchlorine2][PFq_0tNNSC6yvom-RNCzPw][inet[/192.168.72.114:9300]]{master=true},}, reason: zen-disco-receive(join from node[[qchlorine2][PFq_0tNNSC6yvom-RNCzPw][inet[/192.168.72.114:9300]]{master=true}])\r\n'
3514,'spinscale','0.90.2 RPM on Centos removes and recreates elasticsearch user without preserving the UID\nSteps:\r\n1. Install elasticsearch 0.19 on Centos 6.4 using the un-official rpm\r\n2. Do something which creates another uid.\r\n3. Now install 0.90.2 the same way.\r\n\r\nExpected results:\r\nThe elasticsearch remains with the same UID\r\n\r\nActual results:\r\nThe elasticsearch user gets removed and, when it is recreated, it not has a different uid - which means that it no longer owns its own files.'
3509,'spinscale','Error writing to elasticsearch\nWe are currently working with version 1.1.13  installed on ubuntu and elasticsearch version 0.90.2 of working on two-node cluster.\r\n\r\nWhen we write some traces with characters such as *é* or *í* we find the following error in log file logstash:\r\n\r\n>{: Message => "Error writing to elasticsearch",: response => # <FTW :: Response: FTW 0x6f229f11 @ headers = HTTP :: Headers :: <{"content-type" => "text / plain; charset = UTF-8 "," content-length "=>" 70 "}>, @ body = <FTW :: Connection (@ 4046) @ destinations = [" xxx.xxx.xxx.xxx: 9200 "] @ connected = true @ remote_address = "10.35.167.205" @ secure = false>, @ status = 400, @ logger = # <Cabin :: Channel: 0x22956f6b @ subscriber_lock = # <Mutex:0xc5eb8a>, @ metrics = # <Cabin :: Metrics: 0x41eab16b @ channel = # <Cabin::Channel:0x22956f6b ...>, @ metrics = {}, @ metrics_lock = # <Mutex:0x1726099c>>, @ data = {}, @ subscribers = {}, @ level =: info>, @ reason = "Bad Request", @ version = 1.1>,: response_body => "No handler found for uri [/ logstash-2013.08.15/test] and method [GET]",: level =>: error }\r\n\r\nThe elasticsearch log file shows the following error message:\r\n\r\n>[2013-08-14 13:53:59,652][DEBUG][action.index             ] [xxx.xxx.xxx.xxx] [logstash-2013.08.15][2], node[kAqv4xJHTB6uyWCmcWKyrw], [P], s[STARTED]: Failed to execute [index {[logstash-2013.08.15][test][_IpC9zxLTLiOvWFc4XZBow], source[{"@source":"file://control-node/tmp/test/test.log","@tags":["test"],"@fields":{"logLevel":["INFO"],"petitionID":["113002"],"userID":["Unknown"],"type":["Server"],"received":["2013-08-14T11:53:45.058Z"]},"@timestamp":"2013-08-15T06:27:58.615Z","@source_host":"control-node","@source_path":"/tmp/test/test.log","@message":"2013-08-15 08:27:58,615 [Server] INFO  - [113002] [Unknown] [Server] El usuario se ha registrado con éxito test@gmail.com nil null","@type":"test"]}]\r\norg.elasticsearch.index.mapper.MapperParsingException: Failed to parse\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:509)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:430)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.prepareCreate(InternalIndexShard.java:297)\r\n\tat org.elasticsearch.action.index.TransportIndexAction.shardOperationOnPrimary(TransportIndexAction.java:211)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:679)\r\nCaused by: org.elasticsearch.common.jackson.core.JsonParseException: Unexpected end-of-input: expected close marker for OBJECT (from [Source: [B@410f87d7; line: 1, column: 0])\r\n at [Source: [B@410f87d7; line: 1, column: 979]\r\n\tat org.elasticsearch.common.jackson.core.JsonParser._constructError(JsonParser.java:1378)\r\n\tat org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:599)\r\n\tat org.elasticsearch.common.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:532)\r\n\tat org.elasticsearch.common.jackson.core.base.ParserBase._handleEOF(ParserBase.java:479)\r\n\tat org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:2512)\r\n\tat org.elasticsearch.common.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:626)\r\n\tat org.elasticsearch.common.xcontent.json.JsonXContentParser.nextToken(JsonXContentParser.java:48)\r\n\tat org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:461)\r\n\tat org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:486)\r\n\t... 8 more\r\n\r\nlogstash.conf\r\n>input {\r\n      file {\r\n    \'debug\' => false\r\n    \'path\' => [\'/tmp/test/test.log\']\r\n    \'tags\' => \'test\'\r\n    \'type\' => \'test\'\r\n    \'discover_interval\' => 0\r\n    \'start_position\' => \'beginning\'\r\n  }\r\n}\r\nfilter {\r\n  multiline {\r\n    \'pattern\' => \'(^\\sat)|(^java)|(^com)|(^Cause)|(^\\s+.)|(^org)\'\r\n    \'tags\' => \'test\'\r\n    \'type\' => \'test\'\r\n    \'what\' => \'previous\'\r\n  }\r\n  grok {\r\n    \'add_field\' => [\'received\', \'%{@timestamp}\']\r\n    \'pattern\' => \'%{TIMESTAMP_ISO8601:date} \\[%{DATA}\\] %{USERNAME:logLevel}( )+- (\\[%{INT:petitionID}\\] ((\\[%{USERNAME:userID}\\] \\[%{USERNAME:type}\\] )|(\\[%{USERNAME:server}\\] )))?((?<iphone>%{USERNAME}\\.09)|%{GREEDYDATA})\'\r\n    \'tags\' => \'test\'\r\n    \'type\' => \'test\'\r\n  }\r\n  date {\r\n    \'match\' => [\'date\', \'yyyy-MM-dd HH:mm:ss,SSS\']\r\n    \'tags\' => \'test\'\r\n    \'type\' => \'test\'\r\n  }\r\n  mutate {\r\n    \'remove\' => [\'date\']\r\n    \'tags\' => \'test\'\r\n    \'type\' => \'test\'\r\n  }\r\n}\r\noutput {\r\n  elasticsearch_http { host => "x.x.x.x" flush_size => 1}\r\n}\r\n\r\nWe have verified that the data file was utf-8, and have tried to run the option logstash java *-Dfile.encoding = UTF8* with the same results.\r\n\r\nHow we could get to write the traces in elasticsearch. Any suggestions?\r\n'
3508,'bleskes','A previous dynamic change to mapping may cause a Put Mapping request to return prematurely \nIndex request which change the mapping of the index can cause a put mapping request which is executed quickly after it to return before all relevant nodes processed the change.\r\n'
3507,'bleskes','Concurrent Put Mapping API to multiple indices/types may return prematurely\nThe put mapping api waits for all the nodes that has the relevant indices to have processed the change. Concurrent put mapping requests to different indices/types may result in requests returning before all nodes processed the change.\r\n'
3506,'martijnvg','Add scoring support to percolate api \nAdding scoring support will allow the percolate matches to be sorted, or just assign a scores to percolate matches. Sorting by score can be very useful when millions of matches are being returned.\r\n\r\nThe scoring support hooks in into the percolate query option and adds two new boolean options:\r\n* `sort` - Whether to sort the matches based on the score. This will also include the score for each match. The `size` option is a required option when sorting percolate matches is enabled.\r\n* `score` - Whether to compute the score and include it with each match. This will not sort the matches.\r\n\r\nFor both new options the `query` option needs to be specified, which is used to produce the scores. The `query` option is normally used to control which percolate queries are evaluated. In order to give meaning to these score, the recently added `function_score` query in #3423 can be used to wrap these queries as is shown in the examples below.\r\n\r\n## Examples\r\n\r\n### Indexing dummy percolator queries:\r\n\r\nFirst query:\r\n```bash\r\ncurl -XPUT \'localhost:9200/my-index/_percolator/1\' -d \'{ \r\n    "query": { \r\n        "match_all" : {} \r\n    },\r\n    "priority" : 1,\r\n    "create_date" : "2010/01/01"\r\n}\'\r\n```\r\n\r\nSecond query:\r\n```bash\r\ncurl -XPUT \'localhost:9200/my-index/_percolator/2\' -d \'{ \r\n    "query": { \r\n        "match_all" : {} \r\n    },\r\n    "priority" : 2,\r\n    "create_date" : "2013/01/01"\r\n}\'\r\n```\r\n\r\nThese queries have two extra fields: \'priority\' and \'create_date\'. These fields can be used in the percolate api during sorting.\r\n\r\n### Script score example\r\nPercolate request using the `script_score` function:\r\n```json\r\n{ \r\n    "doc": { \r\n        "field" : "value" \r\n    },\r\n    "query" : {\r\n        "function_score" : {\r\n            "query" : { "match_all": {}},\r\n            "functions" : [\r\n                {\r\n                    "script_score" : {\r\n                        "script" : "doc[\'priority\'].value"\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    },\r\n    "sort" : true,\r\n    "size" : 10\r\n}\r\n```\r\n\r\n#### Response:\r\n```json\r\n{\r\n   "took": 118,\r\n   "_shards": {\r\n      "total": 5,\r\n      "successful": 5,\r\n      "failed": 0\r\n   },\r\n   "total": 2,\r\n   "matches": [\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "2",\r\n         "_score": 2\r\n      },\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "1",\r\n         "_score": 1\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\n\r\n### Decay function example\r\nPercolate request using the `exp` decay function:\r\n```json\r\n{ \r\n    "doc": { \r\n        "field" : "value" \r\n    },\r\n    "query" : {\r\n        "function_score" : {\r\n            "query" : { "match_all": {}},\r\n            "functions" : [\r\n                {\r\n                    "exp" : {\r\n                        "create_date" : {\r\n                            "reference" : "2013/08/14",\r\n                            "scale" : "1000d"\r\n                        }\r\n                    }\r\n                }\r\n            ]\r\n        }\r\n    },\r\n    "sort" : true,\r\n    "size" : 10\r\n}\r\n```\r\n#### Response:\r\n```json\r\n{\r\n   "took": 2,\r\n   "_shards": {\r\n      "total": 5,\r\n      "successful": 5,\r\n      "failed": 0\r\n   },\r\n   "total": 2,\r\n   "matches": [\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "2",\r\n         "_score": 0.85559505\r\n      },\r\n      {\r\n         "_index": "my-index",\r\n         "_id": "1",\r\n         "_score": 0.4002574\r\n      }\r\n   ]\r\n}\r\n```'
3504,'s1monw','Added support for random sort\n* Support seeds for consistent pagination. If no seed is provided, the current timestamp is used (at the "cost" of consistent pagination)\r\n* Note, just like normal search, the pagination will be consistent up to segment merges, for absolute consistency scroll should be used\r\n* order is supported for (reverse) consistent pagination\r\n* Enhanced the SortParser abstraction to enable sort parser with default configurations (enables the user to specify the sort as a simple string w\r\n\r\nCloses #1170'
3502,'s1monw','Add fuzzy feature to common terms query\nHi guys,\r\n\r\nI have a situation where i\'d like to remove common words based on a cutoff frequency and a low frequent term minimum_should_match.\r\n\r\nThe common terms query is just perfect for this job!\r\n\r\nBut i would also like to have a "fuzziness" on low frequent terms and only on low frequent terms. \r\n\r\nUsing the query string query don\'t give the expected results because words like "and" would match words like "brand", "band", ect... with a fuzziness of 0.6.\r\n\r\nI think adding a fuzzy features to common terms query would do the job.\r\n\r\nWhat do you think?\r\n\r\nThank you'
3501,'spinscale',"fixes #3500\nAdd **'/usr/lib/jvm/default-java'** to java_home candidates"
3500,'spinscale','Elasticsearch should include debian\'s standard java_home when looking for installed jdk \nI get that error when trying to install debian package\r\n```bash\r\nSetting up elasticsearch (0.90.2) ...\r\n * no JDK found - please set JAVA_HOME\r\ninvoke-rc.d: initscript elasticsearch, action "start" failed.\r\n```\r\nAfter looking in the source code I found out that the debian standard java_home **\'/usr/lib/jvm/default-java\'** isn\'t being looked up.'
3499,'dadoonet','Make RestSearchAction#parseSearchXXX(RestRequest) public\nWhen building a plugin with a new search endpoint, you need to parse the request as a searchRequest.\r\n\r\nMethods exist in RestSearchAction class but are private.\r\n\r\nWe will modify them to be public static. This applies to:\r\n\r\n* `RestSearchAction#parseSearchRequest(RestRequest)`\r\n* `RestSearchAction#parseSearchSource(RestRequest)`\r\n'
3498,'s1monw','Raise default DeleteIndex Timeout \nCurrently the timeout for an delete index operation is set to 10 seconds. Yet, if a full flush is running while we delete and index this can easily exceed 10 seconds. The timeout is not dramatic ie. the index will be deleted eventually but the client request is not acked which can cause confusion. We should raise it to prevent unnecessary confusion especially in client tests where this can happen if the machine is pretty busy.'
3496,'spinscale',"CentOS 6.3 x86_64 Unknown mlockall error 0\nInstalled with RPM from ES 0.90.3-1\r\n\r\nUpon startup logfile shows:\r\n     Unknown mlockall error 0\r\n\r\nIn order to stop it I've tried to - \r\n1.  enable common.jna: DEBUG and \r\n2.  'elasticsearch - memlimit unlimited' \r\n\r\nbut enabling either seems to cause retval for killproc in init script to FAIL."
3493,'spinscale',"'plugin' often returns 0 when error has occurred during install.\nI've written a Chef resource provider for [elasticsearch](https://github.com/SimpleFinance/chef-elasticsearch), and additionally the plugin management tool. Because I'm creating a resource that system(plugin)'s its very useful to check the exit status.\r\n\r\n```\r\nroot@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# ./plugin -i elasticsearch/elasticsearch-zookeeper/0.90.0\r\n-> Installing elasticsearch/elasticsearch-zookeeper/0.90.0...\r\nTrying http://download.elasticsearch.org/elasticsearch/elasticsearch-zookeeper/elasticsearch-zookeeper-0.90.0.zip...\r\nTrying http://search.maven.org/remotecontent?filepath=elasticsearch/elasticsearch-zookeeper/0.90.0/elasticsearch-zookeeper-0.90.0.zip...\r\nTrying https://oss.sonatype.org/service/local/repositories/releases/content/elasticsearch/elasticsearch-zookeeper/0.90.0/elasticsearch-zookeeper-0.90.0.zip...\r\nTrying https://github.com/elasticsearch/elasticsearch-zookeeper/zipball/v0.90.0... (assuming site plugin)\r\nFailed to install elasticsearch/elasticsearch-zookeeper/0.90.0, reason: failed to download out of all possible locations..., use -verbose to get detailed information\r\nroot@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# echo $?\r\n0\r\nroot@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin#\r\n```\r\n\r\n```\r\nroot@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# ./plugin -i sonian/elasticsearch-zookeeper\r\n-> Installing sonian/elasticsearch-zookeeper...\r\nTrying https://github.com/sonian/elasticsearch-zookeeper/zipball/master... (assuming site plugin)\r\nDownloading ..............DONE\r\nPlugin installation assumed to be site plugin, but contains source code, aborting installation...\r\nroot@config-ubuntu-1204-1144:/opt/es/es_test/0.90.2/bin# echo $?\r\n0\r\n```\r\n\r\nThanks!"
3491,'bleskes','Mapping: Allow to change _source exclude/include at runtime\nAllow to update the `_source` mapping exclude/include dynamically when we merge mappings.'
3490,'javanna','Different json response for arrays between get and search using fields\nI noticed a problem returning values for an array when specifying fields to return. I indexed a document containing an empty array and an array containing just one element. Getting the whole document works fine:\r\n\r\ncurl -XGET http://localhost:9300/picture/picture_object/16084571\r\n{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true, "_source" : {"URI":"16084571","CATEGORY":[],"MAINCOLORS":["#c0c0c0"]}}\r\n\r\nThis is the result as expected. Next thing I tried was reading just certain fields:\r\n\r\ncurl -XGET http://localhost:9300/picture/picture_object/16084571?fields=MAINCOLORS,CATEGORY\r\n{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true,"fields":{"MAINCOLORS":"#c0c0c0"}}\r\n\r\nCategory is not returned, I would have expected an empty array here. Maincolors became a single element, not an array anymore. Finally, I tried searching and returning fields:\r\n\r\ncurl -XGET http://localhost:9300/picture/picture_object/_search -d \'{"query":{"term":{"_id":16084571}},"fields":["CATEGORY","MAINCOLORS"]}\'\r\n{"took":2,"timed_out":false,"_shards":{"total":8,"successful":8,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"picture","_type":"picture_object","_id":"16084571","_score":1.0,"fields":{"MAINCOLORS":["#c0c0c0"],"CATEGORY":[]}}]}}\r\n\r\n\r\nThis test was done on elasticsearch 0.90.2 and 0.90.3. I compared it with 0.20.2, there was the result the expected one:\r\n\r\ncurl -XGET http://localhost:9300/picture/picture_object/16084571?fields=CATEGORY,PSCORE,MAINCOLORS\r\n{"_index":"picture","_type":"picture_object","_id":"16084571","_version":1,"exists":true,"fields":{"MAINCOLORS":["#c0c0c0"],"CATEGORY":[],"PSCORE":1515}}\r\n\r\nSo first there is a difference between search and get, second I would think that changing returned field from array to single element or not returning empty arrays is not right, the user probably had a reason for indexing an empty array or an array instead of just a single element.'
3489,'s1monw','More helpful error when specifying side=BACK on edge n-gram tokenizer/tokenfilter\nFrom what I can tell, as of Lucene 4.4/elasticsearch 0.90.2 the "side=BACK" variation of edgeNGram tokenizer/tokenfilter has not only been deprecated, but now throws an ElasticSearchIllegalArgumentException:\r\n\r\n>org.elasticsearch.ElasticSearchIllegalArgumentException: side=BACK is not supported anymore. Please fix your analysis chain or use an older compatibility version (<=4.2) but beware that it might cause highlighting bugs.\r\n\r\nThat\'s fine, but the exception message as well as the documentation page for edgeNGram on elsaticsearch.org don\'t provide any information on *how* you can/should fix your analysis chain.\r\n\r\nhttp://www.elasticsearch.org/guide/reference/index-modules/analysis/edgengram-tokenizer/\r\n\r\n>There used to be a side parameter up to 0.90.1 but it is now deprecated.\r\n\r\nThis change also wasn\'t mentioned/linked in any release notes (at least not that I could find).\r\n\r\nAfter some digging, I found this ticket in Lucene\'s Jira instance that explains a) what changed and b) how to work around this issue if you were using side=BACK previously:\r\n\r\nhttps://issues.apache.org/jira/browse/LUCENE-3907?focusedCommentId=13653011&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13653011\r\n\r\nThe elasticsearch docs and the exception message should provide some guidance on how to workaround this backwards-incompatible change.'
3488,'martijnvg','Add multi percolate api\nThe multi percolate allows the bundle multiple percolate requests into one request. This api works similar to the multi search api.\r\n\r\nThe request body format is line based. Each percolate request item takes two lines, the first line is the header and the second line is the body.\r\n\r\nThe header can contain any parameter that normally would be set via the request path or query string parameters. There are several percolate actions, because there are multiple types of percolate requests:\r\n* `percolate` - Action for defining a regular percolate request.\r\n* `count` - Action for defining a count percolate request.\r\n\r\nEach action has its own set of parameters that need to be specified in the percolate action.\r\nFormat:\r\n```\r\n{"[header_type]" : {[options...]}\r\n{[body]}\r\n```\r\n\r\nDepending on the percolate action different parameters can be specified. For example the percolate and percolate existing document actions support different parameters.\r\n\r\nThe following endpoints are supported:\r\n```\r\nPOST localhost:9200/[index]/[type]/_mpercolate\r\nPOST localhost:9200/[index]/_mpercolate\r\nPOST localhost:9200/_mpercolate\r\n```\r\n\r\nThe `index` and `type` defined in the url path are the default index and type.\r\n\r\n## Example\r\n\r\n#### Request:\r\n```bash\r\ncurl -XGET \'localhost:9200/twitter/tweet/_mpercolate\' --data-binary @requests.txt; echo\r\n```\r\nThe index twitter is the default index and the type tweet is the default type and will be used in the case a header doesn\'t specify an index or type.\r\n\r\n##### requests.txt:\r\n```\r\n{"percolate" : {"index" : twitter", "type" : "tweet"}}\r\n{"doc" : {"message" : "some text"}}\r\n{"percolate" : "index" : twitter", "type" : "tweet", "id" : "1"}\r\n{}\r\n{"percolate" : "index" : users", "type" : "user", "id" : "3", "percolate_index" : "users_2012" }\r\n{"size" : 10}\r\n{"count" : {"index" : twitter", "type" : "tweet"}}\r\n{"doc" : {"message" : "some other text"}}\r\n{"count" : "index" : twitter", "type" : "tweet", "id" : "1"}\r\n{}\r\n```\r\nFor a percolate existing document item (headers with the `id` field), the response can be an empty json object. All the required options are set in the header.\r\n\r\n#### Response:\r\n```json\r\n{\r\n    "items" : [\r\n        {\r\n            "took" : 24,\r\n            "_shards" : {\r\n                "total" : 5,\r\n                "successful" : 5,\r\n                "failed" : 0,\r\n            },\r\n            "total" : 3,\r\n            "matches" : ["1", "2", "3"]\r\n        },\r\n        {\r\n            "took" : 12,\r\n            "_shards" : {\r\n                "total" : 5,\r\n                "successful" : 5,\r\n                "failed" : 0,\r\n            },\r\n            "total" : 3,\r\n            "matches" : ["4", "5", "6"]\r\n        },\r\n        {\r\n            "error" : "[user][3]document missing"\r\n        },\r\n        {\r\n            "took" : 12,\r\n            "_shards" : {\r\n                "total" : 5,\r\n                "successful" : 5,\r\n                "failed" : 0,\r\n            },\r\n            "total" : 3\r\n        },\r\n        {\r\n            "took" : 14,\r\n            "_shards" : {\r\n                "total" : 5,\r\n                "successful" : 5,\r\n                "failed" : 0,\r\n            },\r\n            "total" : 3\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nEach item represents a percolate response, the order of the items maps to the order in where the percolate requests were specified. In case a percolate request failed, the item response is substituted with an error message. '
3487,'bleskes','PutMapping requests were prematurely acknowledged if other nodes were quicker than master\nIf other nodes completed process the new mapping before the master did, the put mapping request would acknowledge before master finished processing.\r\n\r\nThis can happen when running multiple nodes on one machine.\r\n\r\n'
3486,'s1monw','FastVectorHighlighter fails with StackOverflow on terms with large TermFrequency\nFVH deploys some recursive logic to extract terms from documents that need to highlighted. For documents that have terms with super large term frequency like a document that repeats a terms very very often this can produce some very large stacks when extracting the terms. Taken to an extreme this causes stack overflow errors when this grow beyond a term frequency >= 6000. \r\nThe ultimate solution is a iterative implementation of the extract logic but until then we should protect users from these massive term extractions which might be not very useful in the first place. \r\n\r\nI will attach a possible fix and a test case that reproduces the problem in a bit.'
3484,'brwe','Change return format of term vector request \nThe current output of the [term vector endpoint](https://github.com/elasticsearch/elasticsearch/issues/3114) is as follows (all options on):\r\n```\r\n{\r\n   "_index": "test",\r\n   "_type": "type1",\r\n   "_id": "1",\r\n   "_version": 1,\r\n   "exists": true,\r\n   "term_vectors": {\r\n      "field_with_positions_offsets": {\r\n         "field_statistics": {..},\r\n         "terms": {\r\n            "evil": {\r\n               "term_freq": 2,\r\n               "pos": [ 4 , 7 ],\r\n               "start": [ 17, 40 ],\r\n               "end": [ 21 , 44 ]\r\n            },\r\n            "orthodontist": {\r\n               "term_freq": 1,\r\n               "pos": [ 5 ]\r\n               ],\r\n               "start": [ 22 ],\r\n               "end": [ 34]\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nIf you look at the `evil` token, it has two occurrences which are currently represented by a two element array for all token properties (`pos`, `start` & `end`). This parallel array  approach makes sense from a java memory approach but for other languages (like Python, Ruby etc.) it\'s confusing and not really user friendly.  \r\n\r\nFurthermore, the term vector endpoint returns the nearly the same information as the [analyze endpoint](http://www.elasticsearch.org/guide/reference/api/admin-indices-analyze/) does and the response format of term vectors should be consistent with analyze. The response for the above example would the become:\r\n\r\n```\r\n{\r\n   "_index": "test",\r\n   "_type": "type1",\r\n   "_id": "1",\r\n   "_version": 1,\r\n   "exists": true,\r\n   "term_vectors": {\r\n      "field_with_positions_offsets": {\r\n         "field_statistics": {..},\r\n         "terms": {\r\n            "evil": {\r\n               "term_freq": 2,\r\n               "tokens": [\r\n                   { "position": 4, "start_offset": 17, "end_offset" : 21 },\r\n                   { "position": 7, "start_offset": 40, "end_offset" : 44 }\r\n               ]\r\n            },\r\n            "orthodontist": {\r\n               "term_freq": 1,\r\n               "tokens" : [ { "position": 5 , "start_offset" : 22, "end_offset" : 34 } ]\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```'
3483,'martijnvg','Change default operation_threading to thread_per_shard\n'
3482,'areek','Suggester: Phrase suggest option to limit suggestions to exising phrases\nWhen using phrase suggest API to provide "Did you mean ?" corrections it would be nice to include only suggestions that would return results.\r\n\r\nSo returned phrase must exist at least in one document in the index.'
3481,'spinscale','Java API BulkDelete returning isNotFound when run against index with multiple shards\nI\'ve reproduced this problem with both v 0.90.2 and 0.90.3.\r\n\r\nWhen I run some of my integration tests against an index that has multiple shards (running on multiple ES instances), some bulkDelete commands return isNotFound for documents that are in the index.  It works properly when I run against an index with a single shard.\r\n\r\nI\'ve not been able to reproduce this using the ElasticSearch REST API.  \r\n\r\nI\'ve attached images showing the BulkRequest in the debugger, the data in the index before the request runs, and how the shards are allocated.\r\n\r\nAll four DeleteResponses that come back have notFound set to true.  Notice that I\'m attempting to delete the documents from multiple indices and they only exist in one.  This doesn\'t cause a problem when I run with just one shard and even if I only try to delete it from the index that it actually resides in, I still get notFound.\r\n\r\nAlso note that the actual bulkRequest uses aliases to the actual indices.  Again, I\'ve tried it without aliases and I get the same behavior.\r\n\r\nIf I had to guess at what the problem is, I\'d say it\'s related to the fact that the documents I\'m attempting to delete are child documents of other documents in the index.  The parent of both these documents is the itemtype document with id "jmeter_000000_overwrite|@|file|@|id2|@|2"\r\n\r\n![bulkrequestbuilder](https://f.cloud.github.com/assets/1827700/943527/372d9d36-0227-11e3-8c65-9bbe7585d327.png)\r\n![eshead data](https://f.cloud.github.com/assets/1827700/943528/4668af84-0227-11e3-8796-67b705ef6917.png)\r\n![eshead indices](https://f.cloud.github.com/assets/1827700/943530/529e0e70-0227-11e3-8633-02fa6869c916.png)\r\n'
3480,'dakrone','Shard allocation to take into account free disk space\nSimon says:\r\n\r\n> I can imagine some sort of disk space allocation decider that can restrict a node from\r\n> allocating any further shards given the used / free disk space and / or move shards \r\n> away given a certain limit etc. We can also make allocation decision based on the size \r\n> of the shards or move shards around once they fill up and we see that certain \r\n> shards are much bigger than others\r\n\r\nMore at https://groups.google.com/forum/#!topic/elasticsearch/p-et4UxvcyU'
3479,'dadoonet','Null pointer exception for POST mode facets if facet_filter accepts no documents\nelasticsearch 0.90.3\r\norg.elasticsearch.search.facet.FacetExecutor$Post$Filtered, line 66\r\n```\r\nfor (int i = 0; i < docSets.size(); i++) {\r\n  ContextDocIdSet entry = docSets.get(i);\r\n  DocIdSet filteredSet = filter.getDocIdSet(entry.context, null);\r\n  filteredEntries.add(new ContextDocIdSet(\r\n    entry.context,\r\n    // TODO: can we be smart here, maybe AndDocIdSet is not always fastest?\r\n    new AndDocIdSet(new DocIdSet[]{entry.docSet, filteredSet})\r\n  ));\r\n}\r\n```\r\n\r\nFrom the JavaDoc for Filter:\r\n``NOTE: null can be returned if no documents are accepted by this Filter``\r\n\r\nSo, the filteredSet object can be null. It is then passed into the constructor of the AndDocIdSet, which can cause a null pointer exception during the execution of the POST facet.\r\n\r\nThe solution is that if filteredSet is null then the current ContextDocIdSet can be ignored.\r\n```\r\nif (filteredSet == null) continue;\r\n```\r\n\r\nI replicated this issue with the following query (where the filter facet does match a term in the index, but the facet filter does not):\r\n```\r\n{\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "size": 0,\r\n  "facets": {\r\n    "my_facet": {\r\n      "filter": {\r\n        "term": {\r\n          "text": "document"\r\n        }\r\n      },\r\n      "mode": "post",\r\n      "facet_filter": {\r\n        "term": {\r\n          "text": "DOES_NOT_MATCH_ANYTHING",\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```'
3478,'s1monw','Elasticsearch (0.90.2) fails in large core (Ex: ~48) machine \nelastic search creates number of threads based on the number of processors available. So when the elastic search  client library initialized,  it tries to create 350+ threads which is too much for the machine & results in OOM.\r\n\r\nApparently org.elasticsearch.threadpool.ThreadPool tries to assign threads based in cores available in the machines. \r\n\r\nExtrat from logs:\r\nthread_pool [index], type [fixed], size [48], \r\nqueue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [bulk], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [get], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [search], type [fixed], size [144], queue_size [1k], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [percolate], type [fixed], size [48], queue_size [null], reject_policy [abort], queue_type [linked] [org.elasticsearch.threadpool] : [xxxxxxxx] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m] \r\n\r\n& the stacktrace,\r\n\r\njava.lang.OutOfMemoryError: unable to create new native thread\r\n\tat java.lang.Thread.start0(Native Method)\r\n\tat java.lang.Thread.start(Thread.java:640)\r\n\tat java.util.concurrent.ThreadPoolExecutor.addThread(ThreadPoolExecutor.java:681)\r\n\tat java.util.concurrent.ThreadPoolExecutor.addIfUnderMaximumPoolSize(ThreadPoolExecutor.java:727)\r\n\tat java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:655)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:95)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:53)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:99)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:69)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)\r\n\tat org.elasticsearch.transport.netty.NettyTransport.doStart(NettyTransport.java:240)\r\n\tat org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)\r\n\tat org.elasticsearch.transport.TransportService.doStart(TransportService.java:90)\r\n\tat org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:85)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:179)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:119)\r\n\tat '
3477,'kimchy','Failure to execute search request with empty top level filter\nThe following causes failure to execute search request:\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/test/_search?pretty=true\' -d \'{ \r\n    "query" : {"query_string":{"query":"*","default_operator":"AND"}},\r\n    "filter":{} \r\n}\'\r\n```'
3476,'bleskes','_default_ mapping change is validated is if it was a normal type\nChanges to mappings of types in ES are subject to certain validation. For example, if a field was previously analyzed, it is impossible to set it to be not analyzed as this will create inconsistencies with existing data.\r\n\r\nThe `_default_` mapping are used for new type creation and therefore it shouldn\'t be problem to change any setting there.\r\n\r\nTo reproduced:\r\n```\r\ncurl -XPOST "http://localhost:9200/test" -d\'\r\n{\r\n   "mappings": {\r\n      "_default_": {\r\n         "properties": {\r\n            "f": {\r\n                "type": "string", \r\n                "index": "analyzed"\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\'\r\n```\r\nAnd then a change:\r\n\r\n```\r\ncurl -XPOST "http://localhost:9200/test/_default_/_mapping" -d\'\r\n{\r\n   "_default_": {\r\n        "properties": {\r\n            "f": {\r\n                "type": "string", \r\n                "index": "not_analyzed"\r\n            }\r\n        }\r\n   }\r\n}\'\r\n```\r\n\r\nWhich results in an `MergeMappingException`\r\n\r\n'
3474,'bleskes','When replacing an existing _default_ type, the old one get merged into the new\n.\r\n\r\nTo reproduce (contrived example):\r\n\r\n```\r\ncurl -XPOST "http://localhost:9200/test" -d\'\r\n{\r\n   "mappings": {\r\n      "_default_": {\r\n              "date_detection": false\r\n      }\r\n   }\r\n}\'\r\n```\r\n\r\nNow add an empty _default_ (to remove it): \r\n```\r\ncurl -XPUT "http://localhost:9200/test/_default_/_mapping" -d\'\r\n{\r\n   "_default_": {\r\n   }\r\n}\'\r\n```\r\n\r\nAnd get the mapping :\r\n```\r\ncurl -XGET "http://localhost:9200/test/_default_/_mapping"\r\n```\r\n\r\nwhich gives (note the `date_detection`) :\r\n```\r\n{\r\n   "test": {\r\n      "_default_": {\r\n         "date_detection": false,\r\n         "properties": {}\r\n      }\r\n   }\r\n}\r\n```\r\n\r\n\r\n'
3473,'dadoonet','Suggest should ignore empty shards\nFrom #3469.\r\nWhen running suggest on empty shards, it raises an error like:\r\n\r\n```\r\n"failures" : [ {\r\n      "status" : 400,\r\n      "reason" : "ElasticSearchIllegalArgumentException[generator field [title] doesn\'t exist]"\r\n    } ]\r\n```\r\n\r\nWe should ignore empty shards.'
3471,'brwe','term vector api crashes for documents that have no term vectors\nIf term vectors for a document are requested but the document does not contain any field with term vectors stored, this causes a null pointer exception.\r\nThe same happens if the document has the field, but the field only contains, for example, only the string "?". \r\n\r\nInstead of crashing, an empty response should be returned.'
3470,'s1monw','NPE in BytesRefOrdValComparator\nThis issue relates to issue #3189 and I\'m experiencing it with `v1.0.0.Beta`, but I guess that applies to `0.90` branch as well.\r\n\r\nWhile sorting on some trivial fields I get the following error message:\r\n\r\n```javascript\r\n{\r\n   "error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], all shards failed; shardFailures {[UIomKhmqSzumD0uzjmRbfA][brisa][0]: QueryPhaseExecutionException[[brisa][0]: query[filtered(ConstantScore(+NotFilter(cache(discarded:T))))->cache(_type:patients)],from[0],size[20],sort[<custom:\\"lastname\\": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@1d8f4c01>]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",\r\n   "status": 500\r\n}\r\n```\r\n\r\n`lastname` mapping is as follows:\r\n\r\n```javascript\r\n"lastname": {\r\n  "type": "string",\r\n  "index": "not_analyzed",\r\n  "omit_norms": true,\r\n  "index_options": "docs"\r\n}\r\n```\r\n\r\nFairly standard IMO.\r\n\r\nNow the funny thing is that I have another `name` mapping which is exactly the same and everything works just fine. From a data point of view, I cannot see a difference between one field and the other (upper case strings).\r\n\r\nThe NPE happens in `BytesRefOrdValComparator` at line 388 because `MultiDocs.ordinals()` returns `null`.\r\n\r\nNow, i don\'t know why `lastname` happens to use `MultiOrdinals` (or `MultiDocs` for that matter) to do sorting, but making `MultiDocs` return its `MultiOrdinals` instance seems to fix the issue, although I\'m not sure whether the null pointer was intentional and the problem is somewhere else.\r\n\r\nThoughts?\r\n\r\nThanks a lot in advance.\r\n'
3469,'s1monw','Multi-field and suggest api error\nThe following error occur while trying to do a simple phrase query on a multi-field using the suggest api:\r\n\r\n<pre>"failures" : [ {\r\n      "status" : 400,\r\n      "reason" : "ElasticSearchIllegalArgumentException[generator field [title] doesn\'t exist]"\r\n    }, {\r\n      "status" : 400,\r\n      "reason" : "RemoteTransportException[[Bella Donna][inet[/127.0.0.1:9301]][search/phase/query]]; nested: ElasticSearchIllegalArgumentException[generator field [title] doesn\'t exist]; "\r\n    } ]\r\n</pre>\r\n\r\nAnd here is the code:\r\n\r\n<pre>curl -XDELETE "http://localhost:9200/test?pretty"\r\ncurl -XPOST "http://localhost:9200/test?pretty" -d \'{\r\n  "settings": {\r\n    "index": {\r\n      "number_of_replicas": 0,\r\n      "analysis":{\r\n        "analyzer":{\r\n          "suggest":{\r\n            "type": "custom",\r\n            "tokenizer": "standard",\r\n            "filter": [ "standard", "lowercase", "suggest_shingle" ]\r\n          }\r\n        },\r\n        "filter":{\r\n          "suggest_shingle":{\r\n            "type": "shingle",\r\n            "min_shingle_size": 2,\r\n            "max_shingle_size": 5,\r\n            "output_unigrams": true\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\ncurl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d \'{\r\n  "test": {\r\n "properties" : {\r\n    "title": {\r\n      "path": "just_name",\r\n      "type": "multi_field",\r\n      "fields": \r\n        {\r\n          "title": {\r\n            "type": "string",\r\n            "index": "analyzed",\r\n            "similarity": "BM25",\r\n            "analyzer": "suggest"\r\n          }\r\n        }\r\n      }\r\n      \r\n    }\r\n  }\r\n}\'\r\n\r\n\r\ncurl -XPOST "http://localhost:9200/test/test?pretty" -d \'{\r\n  "title": "Just testing the suggestions api"\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/test/test?pretty" -d \'{\r\n  "title": "An other title"\r\n}\'\r\n\r\ncurl -XGET "http://localhost:9200/test/test/_search?pretty" -d \'{\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "suggest": {\r\n    "text": "tetsting sugestion",\r\n    "phrase":{\r\n      "phrase":{\r\n        "field": "title",\r\n        "max_errors": 5\r\n      }\r\n    }\r\n  }\r\n}\'</pre>'
3467,'s1monw','Make ScriptDocValues.Strings mockable in test code\n'
3465,'spinscale','Support FuzzySuggester for completion suggest\nRight now the completion suggester uses a custom version of the analyzing suggester. In order to improve completion queries we should support the fuzzy suggester as well.'
3464,'brwe',"Inconsistent usage of ScriptScoreFunction in FiltersFunctionScoreQuery\n\r\nScoring in `function_score` and `filters_function_score` queries is potentially inconsistent when using scripts.  \r\n\r\n## Brief overview of related classes\r\n`function_score` and `filters_function_score` allow a user to modify the score of a query (referred to as 'subQueryScore' from here on).  \r\n\r\nIn brief, there are two classes that compute scores based on a query and some function: `FunctionScoreQuery`, which only has one function and `FiltersFunctionScoreQuery` which combines the result of several functions. For both classes, the function can be a `ScriptScoreFunction`.\r\n\r\n\r\n\r\n`ScoreFunction`: Computes a score for a document. The two relevant methods are:\r\n\r\n-  `score(docId, subQueryScore)`: computes a new score taking into account the `subQueryScore` and some other properties of a documents.\r\n\r\n- `factor(docId)`: computes a score solely based on properties of the document.\r\n\t\r\n\r\n`FunctionScoreQuery.score()` computes:\r\n\r\nscore = `subQueryBoost` * `ScoreFunction.score(docId, subQueryScore)`\r\n\r\n\r\n`FiltersFunctionScoreQuery.CustomBoostFactorScorer.score()` computes: \r\n\r\nscore = `subQueryScore` * `subQueryBoost` * `combine(ScoreFunction1.factor(docId), ScoreFunction2.factor(docId),…)`\r\n\r\nwhere combine can mean add, multiply, lowest value, etc.\r\n\r\n## The problem\r\n\r\n`ScoreFunctions.factor(docId)` implies that the method computes a factor only and does not take into account the `subQueryScore`. This is the way the `ScoreFunctions` are used in `FiltersFunctionScoreQuery.CustomBoostFactorScorer`: The method `factor()` is called for each score function and the result is than later multiplied to the `subQueryScore`.\r\n\r\n\r\nHowever, the `ScriptScoreFunction` violates this principle: scripts can use the `_score` variable which should be initialized with the `subQueryScore` before the script is run, see `ScriptScoreFunction.score(..)`. If `ScriptScoreFunction.factor()` is called, then the behavior is undefined, since the `_score` variable is either wrong or maybe even not initialized.\r\n\r\n\r\nThis might cause unexpected behavior since this inconsistency is not transparent to the user.\r\n\r\n\r\n"
3463,'s1monw','plugin script does not exit with exit code != 0 on error\nSteps to reproduce:\r\n\r\n```\r\n$ sudo /usr/share/elasticsearch/bin/plugin -install foo\r\n-> Installing foo...\r\nFailed to install foo, reason: failed to download out of all possible locations..., use -verbose to get detailed information\r\n$ echo $?\r\n0\r\n```\r\n\r\nExpected result: The exit code should be != 0\r\n\r\nActual result: The exit code is 0'
3461,'s1monw','Expose IndexWriter#setUseCompundFile via Engine settings.\nLucene 4.4 shipped with a fundamental change in how the decision on when to write compound files is made. During segment flush the compound files are written by default which solely relies on a flag in the IndexWriterConfig. The merge policy has been factored out to only make decisions on merges and not on IW flushes. The default now is always writing CFS on flush to reduce resource usage like open files etc. if segments are flushed regularly. While providing a senseable default certain users / usecases might need to change this setting if re-packing flushed segments into CFS is not desired.'
3460,'spinscale',"Document that 50% of system memory is a good default for ES_HEAP_SIZE in production\nI'm new to ElasticSearch. I just installed ElasticSearch, inserted a bunch of documents, and tried to query it. Performance was horrible. I learned on the mailing list that a reasonable default for ES_HEAP_SIZE is 50% of system memory and that a much smaller value is used by default. It would be very nice if operational best practices were documented somewhere on the ElasticSearch site. It's hard to know as a beginner where to look to see what settings have defaults which are not suitable for production use. If I stick millions of records into MongoDB or MySQL with the default settings then they do not fall over even if they are not tuned exactly for my particular use case. It would be great for introducing people to ElasticSearch if it were as similarly easy to get started with."
3457,'javanna','Bad sorting by subfields\nI have a problem with sorting. My mapping looks like this:\r\n```\r\n"a" : {\r\n              "path" : "just_name",\r\n              "properties" : {\r\n                "content" : {\r\n                  "type" : "string",\r\n                  "analyzer" : "standard",\r\n                  "include_in_all" : false\r\n                },\r\n                "sortValue" : {\r\n                  "type" : "string",\r\n                  "index" : "not_analyzed",\r\n                  "omit_norms" : true,\r\n                  "index_options" : "docs",\r\n                  "include_in_all" : false\r\n                }\r\n              }\r\n            },\r\n"b" : {\r\n              "path" : "just_name",\r\n              "properties" : {\r\n                "content" : {\r\n                  "type" : "string",\r\n                  "analyzer" : "standard",\r\n                  "include_in_all" : false\r\n                },\r\n                "sortValue" : {\r\n                  "type" : "string",\r\n                  "index" : "not_analyzed",\r\n                  "omit_norms" : true,\r\n                  "index_options" : "docs",\r\n                  "include_in_all" : false\r\n                }\r\n              }\r\n            }\r\n```\r\n\r\nNow when I get documents sorted by "a.sortValue" I will get results that have values from b.sortValue in the SearchHit.getSortValues(). \r\nSearching by a.content and b.content works just fine and only matches the content in one field as expected. Also if I remove the "path" : "just_name", the sorting is done correctly. But once I set path to just_name, it seems that sorting only cares about the last part of the path (searchValue). \r\n\r\nThis is with elasticsearch version 0.90.2'
3456,'jpountz','Add support for left joining nested queries and filters\nContext\r\n----\r\n\r\nAssume you have a set of documents for which you have defined a [nested mapping][1]. Also assume that some documents have nested documents and some do not:\r\n\r\n**Document 1**\r\n```json\r\n{\r\n  "id":1,\r\n  "nested":[\r\n    {\r\n      "x":1\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**Document 2**\r\n```json\r\n{\r\n  "id":2,\r\n  "nested":[\r\n    {\r\n      "x":2\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n**Document 3**\r\n```json\r\n{\r\n  "id":3\r\n}\r\n```\r\n\r\nProblem\r\n----\r\n\r\nThere is currently no way to perform the equivalent of the following SQL query:\r\n```sql\r\nSELECT\r\n    r.id,\r\n    COUNT(*)\r\nFROM\r\n    root r\r\n    LEFT JOIN\r\n    nested n\r\n        ON parent(n) = r\r\nWHERE\r\n    n.x = 1 /* nested condition */\r\nGROUP BY\r\n    r.id\r\n```\r\n\r\nSomething that comes very close using ElasticSearch\'s query DSL is:\r\n```bash\r\ncurl -XGET http://localhost:9200/index/root/_search -d\'\r\n{\r\n   "fields":[ "id" ],\r\n   "query":{\r\n       "nested":{\r\n          "query":{\r\n              "constant_score":{\r\n                 "query":{\r\n                    "term":{"x":1}\r\n                 }\r\n              },\r\n              "boost":1.0\r\n           }\r\n        },\r\n        "path":"nested",\r\n        "score_mode":"total"\r\n      }\r\n   }\r\n}\'\r\n```\r\nHowever, since the semantics of [nested queries][2] and [nested filters][3] require a document to have [at least one nested document][4], **Document 3** would be filtered out here (i.e. "inner-join" semantics). \r\n\r\nApplication\r\n----\r\n\r\nTo note above is the condition on the nested documents. Although simple in this example, one can easily imagine situations where score based aggregations based on multiple dynamic conditions can not be precomputed in advance due to a combinatorial explosion.\r\n\r\nThe main value in the above query is that it includes results that have a score value of "0" and preserves a global document score sort and thus can not be computed using [facets][5] or the forthcoming [Aggregation Module][6].\r\n\r\nResources\r\n----\r\n\r\n- https://github.com/elasticsearch/elasticsearch/issues/3056\r\n- https://groups.google.com/forum/m/#!topic/elasticsearch/cZhIcZ7rxsY\r\n- https://groups.google.com/forum/m/#!topic/elasticsearch/Jlk7RgkhBG0\r\n- http://stackoverflow.com/questions/18199135/is-it-possible-to-left-join-nested-documents-in-elasticsearch-queries\r\n\r\n  [1]: http://www.elasticsearch.org/guide/reference/mapping/nested-type/\r\n  [2]: http://www.elasticsearch.org/guide/reference/query-dsl/nested-query/\r\n  [3]: http://www.elasticsearch.org/guide/reference/query-dsl/nested-filter/\r\n  [4]: https://groups.google.com/forum/m/#!topic/elasticsearch/Jlk7RgkhBG0\r\n  [5]: http://www.elasticsearch.org/guide/reference/java-api/facets/\r\n  [6]: https://github.com/elasticsearch/elasticsearch/issues/3300'
3455,'javanna','"term" vs "terms" inconsistency in term query/facet_filter ?\nv0.90.2\r\nWhen using the terms query API, one should use "terms" when searching for multiple terms.\r\n\r\nThis query works:\r\n```javascript\r\n"query": {\r\n  "terms": {\r\n    "user": [\r\n      "brianclozel",\r\n      "elasticsearch"\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nThis query throws an exception, because an array is given, but an "s" is missing in "term":\r\n```javascript\r\n"query": {\r\n  "term": {\r\n    "user": [\r\n      "brianclozel",\r\n      "elasticsearch"\r\n    ]\r\n  }\r\n}\r\n```\r\n\r\nWhen doing the same thing with a facet filter, we get a different behavior.\r\n\r\nThis query works as expected:\r\n```javascript\r\n"facets": {\r\n  "followers": {\r\n    "statistical": {\r\n      "field": "followers"\r\n    },\r\n    "facet_filter": {\r\n      "terms": {\r\n        "user": [\r\n          "elasticsearch",\r\n          "brianclozel"\r\n        ]\r\n      }\r\n    }\r\n  },\r\n}\r\n```\r\n\r\nWith an "s" missing in "term", this query does not raise an exception but takes only the first element in the array to apply the filter.\r\n```javascript\r\n"facets": {\r\n  "followers": {\r\n    "statistical": {\r\n      "field": "followers"\r\n    },\r\n    "facet_filter": {\r\n      "term": {\r\n        "user": [\r\n          "elasticsearch",\r\n          "brianclozel"\r\n        ]\r\n      }\r\n    }\r\n  },\r\n}\r\n```\r\n\r\nIs that behavior intended?'
3454,'martijnvg','Improve filtering by _parent field\nIn the _parent field the type and id of the parent are stored as type#id, because of this a term filter on the _parent field with the parent id is always resolved to a terms filter with a type / id combination for each type in the mapping.\r\n\r\nThis can be improved by automatically use the most optimized filter (either term or terms) based on the number of parent types in the mapping.\r\n\r\nAlso add support to use the parent type in the term filter for the _parent field. Like this:\r\n```json\r\n{\r\n   "term" : {\r\n        "_parent" : "parent_type#1"\r\n    }\r\n}\r\n```\r\nThis will then always automatically use the term filter.'
3453,'s1monw',"MLT returns all documents if non of the fields in the document are supported\nToday due the optimizations in the boolean query builder we adjust\r\na pure negative query with a 'match_all'. This is not the desired\r\nbehavior in the MLT API if all the fields in a document are unsupported ie. numeric fields etc. If that happens today we return all documents but the one MLT is \r\nexecuted on."
3452,'spinscale',"Older version of start-stop-daemon don't support --status\nOlder version of start-stop-daemon don't support --status so the new init.d script throws an error on Ubuntu 10.04\r\n\r\n```\r\nservice elasticsearch restart\r\n * Starting ElasticSearch Server                                                                                                                                                   start-stop-daemon: unrecognized option '--status'\r\nTry 'start-stop-daemon --help' for more information.\r\nstart-stop-daemon: unrecognized option '--status'\r\nTry 'start-stop-daemon --help' for more information.\r\n```\r\n\r\nOnce started running service elasticsearch stop doesn't stop the service it has to be killed manually. "
3449,'s1monw','Improved error message when the mapping document is malformed\n'
3448,'bleskes','Null pointer exceptions when bulk updates max out their retry on conflict\n\r\nReported on the mailing list:\r\n\r\n```\r\n[2013-08-04 21:48:54,972][WARN ][cluster.action.shard     ] [es003] sending failed shard for [samples][20], node[COpFaS8rRhSulgDPAq5xxg], [R], s[STARTED], reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException; nested: ResponseHandlerFailureTransportException; nested: NullPointerException; ]]\r\n[2013-08-04 21:48:55,028][WARN ][action.bulk              ] [es003] Failed to perform bulk/shard on replica [samples][82]\r\norg.elasticsearch.transport.RemoteTransportException\r\nCaused by: org.elasticsearch.transport.ResponseHandlerFailureTransportException\r\nCaused by: java.lang.NullPointerException\r\n  at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:247)\r\n  at org.elasticsearch.action.bulk.TransportBulkAction$2.onResponse(TransportBulkAction.java:242)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.finishIfPossible(TransportShardReplicationOperationAction.java:693)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.handleResponse(TransportShardReplicationOperationAction.java:679)\r\n\tat org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$4.handleResponse(TransportShardReplicationOperationAction.java:676)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\r\n\tat java.lang.Thread.run(Thread.java:662)\r\n[2013-08-04 21:48:55,029][WARN ][cluster.action.shard     ] [es003] sending failed shard for [samples][82], node[aUq8CNUeT_iEIkJ7rVw02w], [R], s[STARTED], reason [Failed to perform [bulk/shard] on replica, message [RemoteTransportException; nested: ResponseHandlerFailureTransportException; nested: NullPointerException; ]]\r\n\r\n```'
3447,'bleskes','Retry on conflict execution of updates *within* bulk request was off by one\nif the user set requestOnConflict to 3, we would only try again twice.\r\n\r\nNote: this does *not* hold for normal updates.'
3444,'javanna','Using parent property in update API with doc_as_upsert=true does not work\nIf I try to add a document using the bulk update API (doc_as_upsert=true) and set its parent property, I do not see the document as the child of the specified parent when I make a has_parent query. Can you please help see what could be wrong with the below. Thanks in advance.\r\n\r\nFollowing is the mapping:\r\n\r\nmappings: {\r\n    store: {\r\n        properties: {\r\n            name: {\r\n                type: string\r\n            }\r\n            owner: {\r\n                type: string\r\n            }\r\n        }\r\n    }\r\n    department: {\r\n        _routing: {\r\n            required: true\r\n        }\r\n        properties: {\r\n            name: {\r\n                type: string\r\n            }\r\n            numberOfProducts: {\r\n                type: long\r\n            }\r\n        }\r\n       _parent: {\r\n            type: store\r\n        }\r\n    }\r\n}\r\n\r\nFollowing is the bulk input file:\r\n\r\n{ "index" : { "_index" : "parent_child", "_type" : "store", "_id" : "store1" } }\r\n{ "name" : "auchan", "owner" : "chris" }\r\n{ "index" : { "_index" : "parent_child", "_type" : "department", "_id" : "department1", "parent" : "store1" } }\r\n{ "name" : "toys", "numberOfProducts" : 150  }\r\n{ "update" : { "_index" : "parent_child", "_type" : "department", "_id" : "department2", "parent" : "store1" } }\r\n{"doc" : { "name" : "dolls", "numberOfProducts" : 300  }, "doc_as_upsert" : "true"}\r\n\r\nQuery - \r\n\r\n{\r\n  "query": {\r\n    "has_parent": {\r\n      "type": "store",\r\n      "query": {\r\n        "match_all": {}\r\n      }\r\n    }\r\n  }\r\n}'
3442,'s1monw','Highlight suggestions\nI think it\'d be cool if the suggestion API returned the suggestions in a tokenized form so consumers could easily highlight the changed tokens.  This particular tokenization would have to ignore any shingle filters to in the analyzer because we\'d end up highlighting whole changed phrases and that really isn\'t how users think.\r\n\r\nI\'m thinking it should work like this:\r\n``` bash\r\ncurl -XDELETE "http://localhost:9200/test?pretty"\r\ncurl -XPOST "http://localhost:9200/test?pretty" -d \'{\r\n  "settings": {\r\n    "index": {\r\n      "number_of_replicas": 0\r\n      "analysis":{\r\n        "analyzer":{\r\n          "suggest":{\r\n            "type": "custom",\r\n            "tokenizer": "standard",\r\n            "filter": [ "standard", "lowercase", "suggest_shingle" ]\r\n          }\r\n        },\r\n        "filter":{\r\n          "suggest_shingle":{\r\n            "type": "shingle",\r\n            "min_shingle_size": 2,\r\n            "max_shingle_size": 5,\r\n            "output_unigrams": true\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\ncurl -XPOST "http://localhost:9200/test/test/_mapping?pretty" -d \'{\r\n  "test":{\r\n    "foo":{\r\n      "analyzer":"suggest"\r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XGET \'http://localhost:9200/_cluster/health?pretty=true&wait_for_status=yellow\'\r\n\r\ncurl -XPOST "http://localhost:9200/test/test?pretty" -d \'{\r\n  "foo": "I love shingles for suggestions"\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/test/test?pretty" -d \'{\r\n  "foo": "but not for finding differences"\r\n}\'\r\n\r\nsleep 1\r\n\r\ncurl -XGET "http://localhost:9200/test/test/_search?pretty" -d \'{\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "suggest": {\r\n    "text": "findning differrences",\r\n    "phrase":{\r\n      "phrase":{\r\n        "field": "foo",\r\n        "max_errors": 5\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\nThe final request would return something like this:\r\n``` json\r\n{\r\n  "took" : 39,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 2,\r\n    "failed" : 3,\r\n    "failures" : [ {\r\n      "status" : 400,\r\n      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn\'t exist]"\r\n    }, {\r\n      "status" : 400,\r\n      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn\'t exist]"\r\n    }, {\r\n      "status" : 400,\r\n      "reason" : "ElasticSearchIllegalArgumentException[generator field [foo] doesn\'t exist]"\r\n    } ]\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test",\r\n      "_type" : "test",\r\n      "_id" : "JBXVvQv6QUCHsUAc6XGvRg",\r\n      "_score" : 1.0, "_source" : {\r\n  "foo": "I love shingles for suggestions"\r\n}\r\n    }, {\r\n      "_index" : "test",\r\n      "_type" : "test",\r\n      "_id" : "VnE5ZKNNRp2V_mRKqASnYQ",\r\n      "_score" : 1.0, "_source" : {\r\n  "foo": "but not for finding differences"\r\n}\r\n    } ]\r\n  },\r\n  "suggest" : {\r\n    "phrase" : [ {\r\n      "text" : "differrences",\r\n      "offset" : 0,\r\n      "length" : 12,\r\n      "options" : [ {\r\n        "text" : "finding differences",\r\n        "score" : 0.49144784,\r\n        "changes" : [ {\r\n          "text" : "findning",\r\n          "is" : "finding",\r\n          "from" : 0,\r\n          "to" : 8\r\n        }, {\r\n          "text" : "differrences",\r\n          "is" : "differences",\r\n          "from" : 9,\r\n          "to" : 21\r\n        } ]\r\n      }, {\r\n        "text" : "findning differences",\r\n        "score" : 0.3803136,\r\n        "changes" : [ {\r\n          "text" : "differrences",\r\n          "is" : "differences",\r\n          "from" : 9,\r\n          "to" : 21\r\n        } ]\r\n      }, {\r\n        "text" : "finding differrences",\r\n        "score" : 0.37071815,\r\n        "changes" : [ {\r\n          "text" : "findning",\r\n          "is" : "finding",\r\n          "from" : 0,\r\n          "to" : 8\r\n        } ]\r\n      } ]\r\n    } ]\r\n  }\r\n}\r\n```'
3440,'martijnvg','Add size option to percolate api\nAdd a `size` option to the percolate api in order to limit the number of matches being returned:\r\n```bash\r\ncurl -XGET \'localhost:9200/my-index/my-type/_percolate\' -d \'{\r\n   "size" : 10,\r\n   "doc" : {...}\r\n}\'\r\n```\r\nIn the above request no more than 10 matches will be returned. The `count` field will still return the total number of matches the document matched with. \r\n\r\nThe `size` option is not applicable for the count percolate api. '
3439,'spinscale','Add client method to get a specific index template\nAs there is right now only `IndicesAdminClient.deleteTemplate()` and `IndicesAdminClient.createTemplate()`, it makes sense to add `IndicesAdminClient.getTemplates()`.\r\n\r\nThis also implies to create an API which does not need to use the cluster state.'
3435,'s1monw',"HighlightBuilder should be consistent with SearchContextHighlight.Field options\nToday the HighlighBuilder doesn't expose all the options that are possible via the REST  API. We need to expose the missing options via the Java API for consistency as well."
3434,'spinscale','Current Template API is not RESTful\nThe current template API does not behave RESTful\r\n\r\n* HEAD on a template does not work\r\n* GET on a non-existing template returns a 200 status message'
3432,'javanna','Add option to disable printing out readable size and time values\nWhen we generate responses, mainly in cluster monitoring apis, we currently print time and size values twice, in two formats: the raw number and the readable format. Although the readable version is easy to read for human beings, it generates a lot of garbage in memory, which is something that we want to disable if not needed.\r\n\r\nLet\'s then add support for a new "readable_format" parameter that allows to control whether we want to print out the readable version or not.\r\n\r\nThe plan is to keep it backward compatible on 0.90 (the default value will be true , meaning that readable values will be printed out by default). As of 1.0 we\'ll change the default to false, therefore in order to have readable values printed out you\'ll need to explicitly ask for them.'
3430,'martijnvg','Add count percolate api\nAdd a new percolate api that only returns the number of percolate queries that have matched with the document being percolated. The actual query ids are not included.\r\n\r\nThe percolate total count will be put in the `total` field and is the only result that will be returned from the dedicated count apis.\r\n\r\nThe `total` field will also be included in the already existing percolate and percolating existing document apis and are equal to the number of matches.\r\n\r\n## Count percolate api\r\nRequest:\r\n```bash\r\ncurl -XGET \'localhost:9200/my-index/my-type/_percolate/count\' -d \'{\r\n   "doc" : {\r\n       "message" : "some message"\r\n   }\r\n}\'\r\n```\r\nResponse:\r\n```json\r\n{\r\n   ... // header\r\n   "total" : 3\r\n}\r\n```\r\n\r\n## Percolate existing document count api\r\nRequest:\r\n```bash\r\ncurl -XGET \'localhost:9200/my-index/my-type/1/_percolate/count\'\r\n```\r\nResponse:\r\n```json\r\n{\r\n    ... // header\r\n   "total" : 5\r\n}\r\n```\r\n\r\n'
3428,'javanna','The ignore_indices=missing option should also work for indices queries and filters\nAt the moment, when you add the ignore_indices=missing option, it has no effect in indices that are missing within a indices query or filter.\r\n\r\nThe same ignore settings should be used there.'
3426,'dadoonet','Native custom score script: performance bottleneck in getting array fields\nFor my searches, I use a native script custom score query which processes array (multivalued) fields from the indexed documents.\r\n\r\nTo obtain them, it gets the\r\n- org.elasticsearch.index.fielddata.ScriptDocValues.Doubles\r\n- org.elasticsearch.index.fielddata.ScriptDocValues.Longs\r\nobjects from doc() --- which I understand to be the most efficient way of obtaining them (as they are cached in memory).\r\n\r\nWhen profiling, however, it appears that the script spends a very large amount of time in the Doubles.getValues() and Longs.getValues() methods. While the arrays themselves are not enormous (should only rarely exceed 200 entries), repeating the operation for all documents in the index proves to be the bottleneck in custom scoring.\r\n\r\nIs there a way of getting around this bottleneck? Does it matter that the getValues methods construct the output list by extending the underlying array (and copying the memory?) every time a new value is added?\r\n\r\n(Using 0.90.2)\r\n\r\n'
3425,'spinscale','pid file not properly overwritten\nI had an instance running on a server, and the PID file had a 5-digit process ID in it. I did a restart, and the new process ID was 4-digits, but the file sitll had the 5th digit from the prior process.\r\n\r\nWhen writing the pid file, it should truncate it first if it already exists.\r\n\r\nRunning elasticsearch 0.90.0 on FreeBSD.\r\n\r\nThanks!\r\n'
3423,'brwe','function score\n# Function score\r\n\r\n\r\n<code>function_score</code> allows to modify the score of documents that are retrieved by a query. This can be useful if, for example, a score function is computationally expensive and it is sufficient to compute the score on a filtered set of documents.\r\n\r\n<code>function_score</code> provides the same functionality that <code>custom_boost_factor</code>, <code>custom_score</code> and <code>custom_filters_score</code> provided but furthermore adds the option to score a document depending on the distance of a numeric field value from a user given reference (see description below).\r\n\r\n## Using function score\r\n\r\n<code>function_score</code> can be used with only one function like this:\r\n\r\n    "function_score": {\r\n        "(query|filter)": {},\r\n        "boost": "boost for the whole query",\r\n        "FUNCTION": {}\r\n    } \r\n    \r\n\r\nFurthermore, several functions can be combined. In this case one can optionally choose to apply the function only if a document matches a given filter:\r\n    \r\n    "function_score": {\r\n        "(query|filter)": {},\r\n        "boost": "boost for the whole query",\r\n        "functions": [\r\n            {\r\n                "filter": {},\r\n                "FUNCTION": {}\r\n            },\r\n            {\r\n                "FUNCTION": {}\r\n            }\r\n        ],\r\n        "score_mode": "(mult|max|...)"\r\n    }\r\n    \r\nIf no filter is given with a function this is equivalent to specifying <code>"match_all": {}</code>\r\n \r\n<code>score_mode</code> defines how functions are combined before multiplying to the score of the query:\r\n\r\n- "multiply":  all functions are multiplied \r\n-  "total":  functions are summed \r\n-  "avg":  average of functions is computed\r\n-  "first": the first function that has a matching filter with it is applied\r\n-  "max": the function yielding the maximum score is applied \r\n-   "min": the function yielding the minimum score is applied \r\n\r\nThe default is <code>"multiply"</code>.\r\n\r\n\r\n# Score functions\r\n<code>function_score</code> provides three types of score functions.\r\n\r\n## Script score\r\n\r\nThe <code>script_score</code> function allows to wrap another query and customize the scoring of it optionally with a computation derived from other field values in the doc (numeric ones) using script expression. Here is a simple sample:\r\n\r\n    "script_score" : {\r\n        "script" : "_score * doc[\'my_numeric_field\'].value"\r\n    }\r\n    \r\nOn top of the different scripting field values and expression, the <code>_score</code> script parameter can be used to retrieve the score based on the wrapped query.\r\n\r\n\r\n\r\nScripts are cached for faster execution. If the script has parameters that it needs to take into account, it is preferable to use the same script, and provide parameters to it:\r\n\r\n\r\n\r\n    "script_score": {\r\n    \t"lang": "lang",\r\n    \t"params": {\r\n        \t"param1": value1,\r\n        \t"param2": value2\r\n   \t\t },\r\n    \t"script": "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)"\r\n\t}\r\n\t\r\n## Boost factor\r\n\r\nThe <code>boost_factor</code> score allows to multiply the score by the provided <code>boost_factor</code>. This can sometimes be desired since boost value set on specific queries gets normalized, while for this score function it does not.\r\n\r\n\t"boost_factor" : number\r\n\r\n\r\n## Decay functions\r\n\r\n\r\nDecay functions score a document with a function that decays depending on the distance of a numeric field value of the document from a user given reference. This is similar to a range query, but with smooth edges instead of boxes.\r\n\r\n\r\nTo use distance scoring on a query that has numerical fields, the user has to define \r\n \r\n 1. a reference and\r\n 2. a scale\r\n \r\nfor each field. A reference is needed to define a distance for the document and a scale to define the rate of decay. The decay function is specified by\r\n\r\n    "DECAY_FUNCTION": {\r\n        "FIELD_NAME": {\r\n              "reference": "11, 12",\r\n              "scale": "2km"\r\n        }\r\n    }\r\n\r\nwhere <code>DECAY_FUNCTION</code> can be "linear", "exp" and "gauss".\r\n\r\n### Normal decay, keyword "gauss"\r\n\r\nThe score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\exp(-\\frac{(fieldvalue_{doc}-reference)^2}{2scale^2})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{(fieldvalue_{doc}-reference)^2}{2scale^2})" title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\n### Exponential decay, keyword "exp"\r\n\r\n\r\nThe score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\exp(-\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{|fieldvalue_{doc}-reference|}{scale})" title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\n###  \'Linear\' decay, keyword "linear"\r\n\r\n\r\nThe score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\max(\\frac{scale-|fieldvalue_{doc}-reference|}{scale},0)" title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\n\r\n\r\nIn contrast to the normal and exponential decay, this function actually sets the score to 0 if the field value exceeds the user given scale value.\r\n\r\n### Choosing an appropriate scale\r\n\r\nFor all three functions, it might not always be easy to define an appropriate scale. Rather than defining the scale parameter directly, one can optionally define a distance at which the function should compute a particular factor.\r\n\r\nFor example, your documents might represents hotels and contain a geo location field. You want to compute a decay function depending on how far the hotel is from a given location. You might not immediately see what scale to choose for the gauss function, but you can say something like: "At a distance of 2km from the desired location, the score should be reduced by half."\r\nYou can provide this parameter like this:\r\n\r\n      "DECAY_FUNCTION": {\r\n        "location": {\r\n              "reference": "11, 12",\r\n              "scale": "2km",\r\n              "scale_weight" : 0.5\r\n        }\r\n    }\r\n\r\nThe parameter "scale" will then be adjusted automatically to assure that the score function computes a score of 0.5 for hotels that are 2km away from the desired location.\r\n\r\n### Detailed example\r\n\r\nSuppose you are searching for a hotel in a certain town. Your budget is limited. Also, you would like the hotel to be close to the town center, so the farther the hotel is from the desired location the less likely you are to check in.\r\nYou would like the query results that match your criterion (for example, "hotel, Nancy, non-smoker") to be scored with respect to distance to the town center and also the price. \r\n\r\n\r\n\r\nIntuitively, you would like to define the town center as the origin and maybe you are willing to walk 2km to the town center from the hotel.\r\nIn this case your *reference* for the location field is the town center and the *scale* is ~2km.\r\n\r\n\r\nIf your budget is low, you would probably prefer something cheap above something expensive. \r\nFor the price field, the *reference* would be 0 Euros and the *scale* depends on how much you are willing to pay, for example 20 Euros. \r\n\r\nIn this example, the fields might be called "price" for the price of the hotel and "location" for the coordinates of this hotel. \r\n\r\nThe function for "price" in this case would be \r\n\r\n    "DECAY_FUNCTION": {\r\n        "price": {\r\n              "reference": "0",\r\n              "scale": "20"\r\n        }\r\n    }\r\n    \r\nand for "location"\r\n                    \r\n\t"DECAY_FUNCTION": {\r\n        "location": {\r\n              "reference": "11, 12",\r\n              "scale": "2km"\r\n        }\r\n    }\r\n\r\nwhere <code>DECAY_FUNCTION</code> can be "linear", "exp" and "gauss".\r\n\r\n\r\n\r\n\r\n\r\nSuppose you want to multiply these two functions on the original score, the request would look like this:\r\n\r\n\r\n\r\n    curl \'localhost:9200/hotels/_search/\' -d \'{\r\n    "query": {\r\n        "function_score": {\r\n            "functions": [\r\n                {\r\n                    "DECAY_FUNCTION": {\r\n                        "price": {\r\n                            "reference": "0",\r\n                            "scale": "20"\r\n                        }\r\n                    }\r\n                },\r\n                {\r\n                    "DECAY_FUNCTION": {\r\n                        "location": {\r\n                            "reference": "11, 12",\r\n                            "scale": "2km"\r\n                        }\r\n                    }\r\n                }\r\n            ],\r\n            "query": {\r\n                "match": {\r\n                    "properties": "balcony"\r\n                }\r\n            },\r\n            "score_mode": "multiply"\r\n        }\r\n    }\r\n    }\'\r\n\t\r\n\r\nNext, we show how the computed score looks like for each of the three possible decay functions.\r\n\r\n\r\n#### Normal decay, keyword "gauss"\r\n\r\nWhen choosing "gauss" as decay function in the above example, the multiplier to the original score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\mathcal{S}(doc)=\\exp(-\\frac{(location_{doc}-reference_{loc})^2}{2scale_{loc}})\\exp(-\\frac{(price_{doc}-reference_{price})^2}{2scale_{price}})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{(location_{doc}-reference_{loc})^2}{2scale_{loc}})\\exp(-\\frac{(price_{doc}-reference_{price})^2}{2scale_{price}})," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\nA contour and surface plot of the multiplier looks like this:\r\n\r\n\r\n![gausscontour](https://f.cloud.github.com/assets/4320215/768157/cd0e18a6-e898-11e2-9b3c-f0145078bd6f.png)\r\n![gausssurf](https://f.cloud.github.com/assets/4320215/768160/ec43c928-e898-11e2-8e0d-f3c4519dbd89.png)\r\n\r\n\r\nSuppose your original search results matches three hotels : "Backback Nap", "Drink n Drive" and "BnB Bellevue". \r\n"Drink n Drive" is pretty far from your defined location (nearly 2 km) and is not too cheap (about 13 Euros) so it gets a low factor a factor of 0.56. "BnB Bellevue" and "Backback Nap" are both pretty close to the defined location but "BnB Bellevue" is cheaper, so it gets a multiplier of 0.86 whereas "Backpack Nap" gets a value of 0.66."\r\n\r\n\r\n\r\n### Exponential decay, keyword "exp"\r\n\r\n\r\nWhen choosing "exp" as decay function in the above example, the multiplier to the original score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\exp(-\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{|location_{doc}-reference_{loc}|}{scale_{loc}})\\exp(-\\frac{|price_{doc}-reference_{price}|}{scale_{price}})," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\nA contour and surface plot of the multiplier looks like this:\r\n\r\n\r\n\r\n![expcontour](https://f.cloud.github.com/assets/4320215/768161/082975c0-e899-11e2-86f7-174c3a729d64.png)\r\n![expsurf](https://f.cloud.github.com/assets/4320215/768162/0b606884-e899-11e2-907b-aefc77eefef6.png)\r\n\r\n\r\n\r\n\r\n\r\n###  \'Linear\' decay, keyword "linear"\r\n\r\nWhen choosing "exp" as decay function in the above example, the multiplier to the original score is computed as\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\max(\\frac{scale_{loc}-|location_{doc}-reference_{loc}|}{scale_{loc}},0)\\max(\\frac{scale_{price}-|price_{doc}-reference_{price}|}{scale_{price}},0)," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\n\r\nA contour and surface plot of the multiplier looks like this:\r\n\r\n![lincontour](https://f.cloud.github.com/assets/4320215/768164/1775b0ca-e899-11e2-9f4a-776b406305c6.png)\r\n![linsurf](https://f.cloud.github.com/assets/4320215/768165/19d8b1aa-e899-11e2-91bc-6b0553e8d722.png)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n#### Supported fields for decay functions\r\nOnly single valued numeric fields, including time and geo locations, should be supported. \r\n\r\n#### What is a field is missing?\r\n\r\nIs the numeric field is missing in the document, the function will return 1.\r\n\r\n\r\n\r\n\r\n\t\r\n\r\n# Relation to custom_boost_factor, custom_score and custom_filters_score\r\n\r\n\r\n\r\n\r\nThe custom boost factor query \r\n\r\n\t"custom_boost_factor" : {\r\n    \t"query" : {\r\n        \t....\r\n    \t},\r\n    \t"boost_factor" : 5.2\r\n\t}\r\n\t\r\nbecomes\r\n\r\n\t"function_score" : {\r\n    \t"query" : {\r\n        \t....\r\n    \t},\r\n    \t"boost_factor" : 5.2\r\n\t}\r\n\t\r\nThe custom script score\r\n\r\n\t"custom_score" : {\r\n    \t"query" : {\r\n        \t....\r\n\t    },\r\n    \t"params" : {\r\n        \t"param1" : 2,\r\n \t       \t"param2" : 3.1\r\n    \t},\r\n\t    "script" : "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)"\r\n\t}\r\n\t\r\n becomes\r\n\r\n\t"function_score" : {\r\n    \t"query" : {\r\n        \t....\r\n\t    },\r\n\t    "script_score" : {\r\n\t    \r\n    \t\t"params" : {\r\n        \t\t"param1" : 2,\r\n \t       \t\t"param2" : 3.1\r\n    \t\t},\r\n\t    \t"script" : "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)"\r\n\t    }\r\n\t}\r\n\t\r\nand the custom filters score query \r\n\r\n\r\n\r\n\r\n    "custom_filters_score" : {\r\n        "query" : {\r\n            …\r\n       \t },\r\n        "filters" : [\r\n            {\r\n                "filter" : { …},\r\n                "boost" : "3"\r\n            },\r\n            {\r\n                "filter" : {…},\r\n                "script" : "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)"\r\n            }\r\n        ],\r\n        "params" : {\r\n        \t"param1" : 2,\r\n \t       \t"param2" : 3.1\r\n    \t}\r\n    \t"score_mode" : "first"\r\n    }  \t\t\r\n\r\n\t\r\nbecomes:\r\n\r\n\r\n\r\n\r\n    "function_score" : {\r\n        "query" : {\r\n            …\r\n       \t},\r\n        "functions" : [\r\n            {\r\n                "filter" : {…},\r\n                "boost" : "3"\r\n            },\r\n            {\r\n                "filter" : { … },\r\n                "script_score" : { \r\n                \t"script" : "_score * doc[\'my_numeric_field\'].value / pow(param1, param2)",\r\n                \t"params" : {\r\n        \t\t\t\t"param1" : 2,\r\n \t       \t\t\t\t"param2" : 3.1\r\n    \t\t\t\t}\r\n            \t\t\r\n            \t}\r\n            }\r\n        ],\r\n        "score_mode" : "first",  \t\r\n    }  \t\t\r\n\r\n\r\nThis issue replaces Issues #3307 and #3407 '
3421,'dadoonet','Plugin Manager should support -remove group/artifact/version naming\nWhen installing a plugin, we use:\r\n\r\n```sh\r\nbin/plugin --install groupid/artifactid/version\r\n```\r\n\r\nBut when removing the plugin, we only support:\r\n\r\n```sh\r\nbin/plugin --remove dirname\r\n```\r\n\r\nwhere `dirname` is the directory name of the plugin under `/plugins` dir.'
3420,'martijnvg','Improve alias support in the percolate api\nSupport for percolating via an alias is already implemented via #3173, if an alias is resolved to more than one index, then from the response it is impossible to know what id belongs to what concrete index. This can become even a bigger issue if percolate queries with the same id exist in different indices.\r\n\r\nFor this reason the response needs to be enhanced to include the concrete index per match:\r\n```json\r\n{\r\n  "matches" : [\r\n      {\r\n         "id" : "1",\r\n         "_index" : "my-index1"\r\n      },\r\n      ...\r\n  ]\r\n}\r\n```\r\n\r\nThe current format will still be supported by setting `percolate_format=ids` parameter in the query string. Useful in the case percolation is only executed on just one index.\r\n\r\nThis issue will also add support for specifying multiple indices in the percolate request:\r\n```bash\r\ncurl -XGET \'localhost:9200/my-index1,my-index2/my-type/_percolate\' -d \'...\'\r\n``` \r\n\r\n'
3414,'spinscale',"String values are stored in float field\n(Version 0.90.0) I have a float field defined in a mapping and I was able to put a string values into the float field as long as the string value can pass java.lang.Float.parserFloat(). Later sorting on this field will result in exception like this:\r\n\r\n```\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:573)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:484)\r\n        at org.elasticsearch.search.SearchService.createContext(SearchService.java:469)\r\n        at org.elasticsearch.search.SearchService.createAndPutContext(SearchService.java:462)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:234)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAc\r\ntion.java:141)\r\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstP\r\nhase(TransportSearchQueryThenFetchAction.java:80)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(Trans\r\nportSearchTypeAction.java:205)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(Trans\r\nportSearchTypeAction.java:192)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTy\r\npeAction.java:178)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:679)\r\nCaused by: org.elasticsearch.search.facet.FacetPhaseExecutionException: Facet [latitude]: value_field [loc_lat] isn't a number field, but a string\r\n        at org.elasticsearch.search.facet.termsstats.TermsStatsFacetParser.parse(TermsStatsFacetParser.java:127)\r\n        at org.elasticsearch.search.facet.FacetParseElement.parse(FacetParseElement.java:92)\r\n        at org.elasticsearch.search.SearchService.parseSource(SearchService.java:561)\r\n        ... 12 more\r\n```\r\n"
3410,'martijnvg','Improve how aliases are handled in the cluster state.\n'
3406,'imotov','Fix for #3399 - auto_expand_replicas causing very large amount of cluste...\n...r state changes when a node joins or leaves the cluster'
3404,'martijnvg','Add version support to get and mget apis\nAdd version support to mget and get apis, that only will perform the get operation if the version of the document to be fetched matches with the provided version.\r\n\r\nBoth get and mget apis will support the following parameters: `version` and `version_type`.'
3403,'martijnvg','Save percolation result to document itself\nReferencing:\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/issues/3173#issuecomment-21196963\r\nhttps://twitter.com/Argorak/statuses/357893281193017344\r\nhttps://twitter.com/jeroenrosenberg/status/361807942443479040\r\n\r\nI\'d like to propose an additional feature for the percolator: save the result in the document itself (or a child document, avoiding updating the document). This could be used e.g. for tagging of input data for later search by certain rules.\r\n\r\nI often use the percolator to categorize incoming data. This data is given by external services and is messy, though fixable by simple "search and clean". We use the percolator to register (sometimes user-created) queries that map those entries to our internal values.\r\n\r\nCurrently, this works like this:\r\n* Percolate the document\r\n* Write the result to the target field\r\n* Index the document\r\n\r\nAllowing this within the percolation step itself would vastly reduce our network overhead in this case and (if bulk percolation happens) also allow us to do bulk actions in one step.\r\n\r\nThe interface could be similar to certain queries (like the `terms` query):\r\n\r\n```{\r\n   ... percolation parameters\r\n   "save_to": { "path": "data.categories" }\r\n}```\r\n'
3402,'bleskes','MoreLikeThisFieldQueryBuilder defaulted failOnUnsupportedField inconsistently to the REST api\nbuilder defaulted to false while REST api defaults to true'
3401,'javanna','Added support for acknowledgement from other nodes in open/close index api\n'
3400,'javanna',"Open/Close indices to support acknowledgement from other nodes\nThe open/close index api currently returns back its response after the cluster state change has been digested by the master node, but the updated cluster state hasn't necessarily been published to all the nodes yet.\r\n\r\nAdd support for acknowledgement like we do with aliases. We wait for an ack from other nodes till the (configurable) timeout expires (default 10 seconds). The acknowledged flag in the response will reflect whether the cluster state change has been acknowledged by all the other nodes or not."
3396,'bleskes','dynamically updating index.gc_deletes not working\nIt seems that dynamically changing `index.gc_deletes` is broken. The corresponding field in code is `RobinEngine.gcDeletesInMillis` \r\n\r\n    this.gcDeletesInMillis = indexSettings.getAsTime(INDEX_GC_DELETES, TimeValue.timeValueSeconds(60)).millis();\r\n\r\nIt is only ever modified after a previous info log:\r\n\r\n    if (gcDeletesInMillis != RobinEngine.this.gcDeletesInMillis) {\r\n         logger.info("updating index.gc_deletes from [{}] to [{}]", TimeValue.timeValueMillis(RobinEngine.this.gcDeletesInMillis), TimeValue.timeValueMillis(gcDeletesInMillis));\r\n         RobinEngine.this.gcDeletesInMillis = gcDeletesInMillis;\r\n    }\r\n\r\nThis line never gets logged though. \r\n\r\nReopening the index works as expected.'
3390,'s1monw','Documentation Inconsistencies\nIt seems like a lot of recent settings/features are undocumented or not updated to reflect the new values in the elasticsearch.org documentation. It would be really great to see that become more consistent in general.\r\n\r\nSpecifically I noticed that on http://www.elasticsearch.org/guide/reference/modules/threadpool/ it says the default is unlimited however in 90.1 that changed to be 1000 by default.'
3389,'javanna',"FilterBuilder and QueryBuilder equals\nHey, I'm writing some middleware that parses some amount of input into query/filters using the builders in org.elasticsearch.index.query, and it seems that it uses default equals. This makes writing unit tests cumbersome afaik, is there any nice workaround for this?\r\n\r\nIf not, I don't mind submitting a pull request for this..."
3388,'dadoonet',"Routing with range values\nHi\r\n\r\nI want to index 30 days of data, in each day i will get GB's of data\r\nI Have 30 days of data, i have 10 shards, i want to route in such away , that first 3 days of data should go to first shard, next 3 days of data should go to next shard ..etc\r\n\r\nI am routing with date field, but i found data belongs to 1, 10,17 is going to 1 st 2, 11, .. going to next shard...etc . \r\n\r\nLooking for u r response"
3382,'spinscale','Expose recursion level for Hunspell token filter. Closes #3369\nAllows to override recursion level for the Hunspell token filter. By default the recursion level is set to 2 which is not suitable for certain languages/dictionaries. For example for `cs_CZ` (czech) language based on OpenOffice dictionaries it gives better results with recursion level 1 or even better 0.\r\n\r\nOne can override the `recursion_level` using the following configuration:\r\n\r\n```\r\n{\r\n    "filter" : {\r\n        "type" : "hunspell",\r\n        "locale" : "cs_CZ",\r\n        "recursion_level" : 0  // optional, defaults to 2\r\n    }\r\n}\r\n```\r\n\r\nNote: requires Lucene 4.4'
3381,'javanna','Updating mapping with `ignore_conflicts` parameter timeouts\nWhen running the following code against Elasticsearch built from 0.90 branch at b2d0802, the response never finishes:\r\n\r\n~~~bash\r\ncurl -X DELETE http://localhost:9200/mapped-index\r\n\r\ncurl -X POST http://localhost:9200/mapped-index -d \'{}\'\r\n\r\ncurl -X PUT "http://localhost:9200/mapped-index/article/_mapping" -d \'{"article":{"properties":{"body":{"type":"string"}}}}\'\r\n\r\ncurl -X PUT "http://localhost:9200/mapped-index/article/_mapping" -d \'{"article":{"properties":{"body":{"type":"integer"}}}}\'\r\n\r\n# => Conflict (OK)\r\n\r\ncurl -X PUT "http://localhost:9200/mapped-index/article/_mapping?ignore_conflicts=true" -d \'{"article":{"properties":{"body":{"type":"integer"}}}}\'\r\n\r\n# => Timeout (FAIL)\r\n~~~\r\n'
3380,'martijnvg',"Percolating existing document api\nA common use case is to percolate a document that has just been indexed. A big advantage would be to just specify the index, type and id of a document (which is returned with each index response) that has just been indexed, internally a get request can then fetch the content of the recently indexed document and then the percolate api will execute on that. \r\n\r\nPercolate an existing document with id 1 and type tweet from index twitter:\r\n```bash\r\ncurl -XGET 'localhost:9200/twitter/tweet/1/_percolate'\r\n```\r\n\r\nPercolate an existing document with id 2 and type tweet from index twitter-2013: (percolating 2013 tweet against 2012 queries)\r\n```bash\r\ncurl -XGET 'localhost:9200/twitter-2013/tweet/2/_percolate?percolate_index=twitter-2012'\r\n```\r\n\r\nAdditional options for percolating an existing document on top of existing percolator options:\r\n* `id` - The id of the document to retrieve the source for.\r\n* `percolate_index` - The index containing the percolate queries. Defaults to the `index` defined in the url.\r\n* `percolate_type` - The percolate type (used for parsing the document). Default to `type` defined in the url.\r\n* `routing` - The routing value to use when retrieving the document to percolate.\r\n* `preference` - Which shard to prefer when retrieving the existing document. \r\n* `percolate_routing` - The routing value to use when percolating the existing document.\r\n* `percolate_preference` - Which shard to prefer when executing the percolate request.\r\n* `version` - Enables a version check. If the fetched document's version isn't equal to the specified version then the request fails with a version conflict and the percolation request is aborted.\r\n* `version_type` - Whether internal or external versioning is used. Defaults to internal versioning.\r\n\r\nInternally the percolate api will issue a get request for fetching the`_source` of the document to percolate.\r\nFor this feature to work the `_source` for documents to be percolated need to be stored.\r\n"
3377,'spinscale','Analyze API ignores mapping when multi-field fully qualified path name is identical\nHere is a mapping setting up a multi-field.  We are going to try using the Analyze API on both "brand" and "brand.brand", which you would expect to give the same results.  Instead, it appears that the mapping is being ignored in the "brand.brand" case.\r\n\r\n```bash\r\ncurl -XPOST localhost:9200/test_multifield_same_name -d \'{\r\n    "mappings" : {\r\n        "type" : {\r\n            "properties" : {\r\n                "brand" : {\r\n                    "type" : "multi_field",\r\n                    "fields" : {\r\n                        "brand" : { "type" : "string", "analyzer" : "whitespace" },\r\n                        "untouched" : { "type" : "string", "index" : "not_analyzed" }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nThe field using just the simple path name, works fine (applies whitespace):\r\n```bash\r\ncurl -XPOST "localhost:9200/test_multifield_same_name/_analyze?pretty=true&field=brand" -d \'This Is Just A Test With Capitalized Words\'\r\n\r\n{\r\n  "tokens" : [ {\r\n    "token" : "This",\r\n    "start_offset" : 0,\r\n    "end_offset" : 4,\r\n    "type" : "word",\r\n    "position" : 1\r\n  }, {\r\n    "token" : "Is",\r\n    "start_offset" : 5,\r\n    "end_offset" : 7,\r\n    "type" : "word",\r\n    "position" : 2\r\n  }, {\r\n    "token" : "Just",\r\n    "start_offset" : 8,\r\n    "end_offset" : 12,\r\n    "type" : "word",\r\n    "position" : 3\r\n  }, {\r\n    "token" : "A",\r\n    "start_offset" : 13,\r\n    "end_offset" : 14,\r\n    "type" : "word",\r\n    "position" : 4\r\n  }, {\r\n    "token" : "Test",\r\n    "start_offset" : 15,\r\n    "end_offset" : 19,\r\n    "type" : "word",\r\n    "position" : 5\r\n  }, {\r\n    "token" : "With",\r\n    "start_offset" : 20,\r\n    "end_offset" : 24,\r\n    "type" : "word",\r\n    "position" : 6\r\n  }, {\r\n    "token" : "Capitalized",\r\n    "start_offset" : 25,\r\n    "end_offset" : 36,\r\n    "type" : "word",\r\n    "position" : 7\r\n  }, {\r\n    "token" : "Words",\r\n    "start_offset" : 37,\r\n    "end_offset" : 42,\r\n    "type" : "word",\r\n    "position" : 8\r\n  } ]\r\n}\r\n```\r\n\r\nBut if we refer to the field using the fully qualified path name, it looks like Elasticsearch applies the default (Standard) analyzer instead of whitespace:\r\n```bash\r\ncurl -XPOST "localhost:9200/test_multifield_same_name/_analyze?pretty=true&field=brand.brand" -d \'This Is Just A Test With Capitalized Words\'\r\n\r\n{\r\n  "tokens" : [ {\r\n    "token" : "just",\r\n    "start_offset" : 8,\r\n    "end_offset" : 12,\r\n    "type" : "<ALPHANUM>",\r\n    "position" : 3\r\n  }, {\r\n    "token" : "test",\r\n    "start_offset" : 15,\r\n    "end_offset" : 19,\r\n    "type" : "<ALPHANUM>",\r\n    "position" : 5\r\n  }, {\r\n    "token" : "capitalized",\r\n    "start_offset" : 25,\r\n    "end_offset" : 36,\r\n    "type" : "<ALPHANUM>",\r\n    "position" : 7\r\n  }, {\r\n    "token" : "words",\r\n    "start_offset" : 37,\r\n    "end_offset" : 42,\r\n    "type" : "<ALPHANUM>",\r\n    "position" : 8\r\n  } ]\r\n}\r\n```\r\n\r\n(Related to issue from https://github.com/polyfractal/elasticsearch-inquisitor/issues/21)'
3376,'spinscale','Completion prefix suggestion \n**Note**: This is an experimental feature!\r\n\r\nTraditionally FST suggesters needed to create an in-memory structure upfront, which needed to be in sync with the data inserted/deleted. This step to create a FST can be really expensive and long lasting on production systems.\r\n\r\nSo, why not trying to create an efficient FST alike structure on index time, load that quickly into memory and use this for suggestions?\r\n\r\nBefore deep diving into implementation details, let\'s start with a small sample\r\n\r\n# Sample\r\n\r\nCreate a simple mapping\r\n\r\n```\r\ncurl -X DELETE localhost:9200/music\r\ncurl -X PUT localhost:9200/music\r\n\r\ncurl -X PUT localhost:9200/music/song/_mapping -d \'{\r\n  "song" : {\r\n\t\t"properties" : {\r\n\t\t\t"name" : { "type" : "string" },\r\n            "suggest" : { "type" : "completion",\r\n                          "index_analyzer" : "stopword",\r\n                          "search_analyzer" : "simple",\r\n                          "payloads" : true\r\n            }\r\n\t\t}\r\n\t}\r\n}\'\r\n\r\ncurl -X PUT \'localhost:9200/music/song/1?refresh=true\' -d \'{\r\n\t"name" : "Nevermind",\r\n\t"suggest" : { \r\n\t\t"input": [ "Nevermind", "Nirvana" ],\r\n\t\t"output": "Nirvana - Nevermind",\r\n\t\t"payload" : { "artistId" : 2321 }\r\n\t}\r\n}\'\r\n```\r\n\r\nA request looks like this\r\n\r\n```\r\ncurl -X POST \'localhost:9200/music/_suggest\' -d \'{\r\n\t"song-suggest" : {\r\n\t\t"text" : "nev",\r\n\t\t"completion" : {\r\n\t\t\t"field" : "suggest"\r\n\t\t}\r\n\t}\r\n}\'\r\n```\r\n\r\nThis is the response\r\n\r\n```\r\n{\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "song-suggest" : [ {\r\n    "text" : "nev",\r\n    "offset" : 0,\r\n    "length" : 10,\r\n    "options" : [ {\r\n      "text" : "Nirvana - Nevermind",\r\n      "score" : 1.0, "payload" : {"artistId":2321}\r\n    } ]\r\n  } ]\r\n}\r\n```\r\n\r\nAs you can see, the text returned is the provided output during indexing. Also the payload is included, which might carry a reference ID to the artist and thus makes it easy to retrieve further information.\r\n\r\n# Mapping options\r\n\r\nIn order to support prefix suggestion the field has to be marked as type `completion`.\r\n\r\n```\r\n{\r\n    ...\r\n     "properties" : {\r\n        "suggestField" {\r\n            "type" : "completion"\r\n            "index_analyzer" : "stopword",\r\n            "search_analyzer" : "simple",\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nWhile the `type` field is mandatory, the `index_analyzer` and `search_analyzer` fields can be omitted. The `simple` analyzer is used by default.\r\n\r\n## Payloads\r\n\r\nIf you want to return payloads, you have to explicitely enable them by using `payloads: true` - payloads can contain arbitrary JSON, but must be a JSON object, with opening `{` and closing `}` - no pure strings or arrays allowed.\r\n\r\n## Preserve separators\r\n\r\nIn addition, you can set `preserve_separators: false` in case you in case you want to return "Foo Fighters" when searching for "foof" (using the correct analyzer of course).\r\n\r\n## Preserve position increments\r\n\r\nYou can set `preserve_position_increments: false` in order to not count increase position increments, which is needed if the first word is a stopword and you are using an analyzer to filter out stopwords. This would allow you to suggest for `b` and get back `The Beatles`\r\n\r\n# Indexing\r\n\r\n## Simple case\r\n\r\nThe most simple case to index is like this\r\n\r\n```\r\n"suggestField" : [ "The Prodigy Firestarter", "Firestarter"]\r\n```\r\n\r\nDepending on the analyzer used\r\n\r\n## Outputs\r\n\r\nDefining an output will always return the output for a found suggestion.\r\n\r\n```\r\n"suggestField" : { \r\n  "input" : [ "The Prodigy Firestarter", "Firestarter"],\r\n  "output" : "The Prodigy, Firestarter",\r\n}\r\n```\r\n\r\n## Weights\r\n\r\nYou should define custom weights instead of relying on the default one (see the drawbacks section). The weight must be an positive integer (**no float**) and defines the order of your suggestions.\r\n\r\n```\r\n"suggestField" : { \r\n  "input" : [ "The Prodigy Firestarter", "Firestarter"],\r\n  "output" : "The Prodigy, Firestarter",\r\n  "weight" : 42\r\n}\r\n```\r\n\r\nAlso custom weights can make your suggestions valuable. Using weights you could boost the most played song or the best rated hotel first in your suggestions.\r\n\r\n# Search\r\n\r\nSearches are working exactly like the phrase and term suggesters\r\n\r\n```\r\ncurl -X POST \'localhost:9200/music/_suggest\' -d \'{\r\n\t"song-suggest" : {\r\n\t\t"text" : "nev",\r\n\t\t"completion" : {\r\n\t\t\t"field" : "suggest"\r\n\t\t}\r\n\t}\r\n}\'\r\n```\r\n\r\n# Drawbacks\r\n\r\n## Using term frequency as default weight\r\n\r\nIf you do not specify a weight, the term frequency is used. This only makes sense if you optimize to a single segment or have large segments. If you do not, having custom weights might yield the results you are awaiting. So using term frequences as a weight indicator is not the best solution and you should set weight yourself.'
3373,'javanna','Validate query validates anything as true on empty cluster\n    # curl -X POST localhost:9200/_validate/query -d \'not a query\' \r\n    {"valid":true,"_shards":{"total":0,"successful":0,"failed":0}} '
3370,'kimchy',"Add Git build info when we build a distribution\nWould be nice to have git data (like commit hash, UTC time, ...) of a build of ES.\r\n\r\nInitial thoughts are we should have a `Build` class similar to `Version` class (we don't want to have this info as part of Version, since we serialize it on each request), that would read its info from a properties file that gets included in our jar file. We can use this plugin: http://mojo.codehaus.org/buildnumber-maven-plugin/ to do it potentially.\r\n\r\nWe should return the Build info on top of the Version info in the following cases:\r\n\r\n- when doing `GET /`\r\n- `bin/elasticsearch -v`\r\n- when the server starts up"
3369,'spinscale','Expose recursion level for Hunspell token filter (post Lucene 4.4 upgrade)\nAfter upgrade to Lucene 4.4 it would be very useful to expose recursion level in Hunspell as a configuration parameter in ES too. Here is related Lucene ticket:\r\nhttps://issues.apache.org/jira/browse/LUCENE-4542\r\n\r\nLet me know if you want patch for this, should be simple.'
3368,'martijnvg',"Add pending cluster tasks api\nAdd an api that shows how many cluster state related tasks (like create index, fail shard etc) are pending. \r\n\r\nUsually this will return an empty lists of tasks, because cluster state related tasks execute fast. This api is useful for diagnostic purposes. Rest api:\r\n\r\n```bash\r\ncurl -XGET 'localhost:9200/_cluster/pending_tasks'\r\n```"
3367,'martijnvg','Different numHits 0.90.1 vs 0.90.2 filter with lookup terms\nHi,\r\n\r\nWhen I run an identical query on 0.90.1 and 0.90.2 I get different results:\r\n\r\n```json\r\n{\r\n  "filter": {\r\n    "terms": {\r\n      "field": {\r\n        "id": "bar",\r\n        "type": "doctype",\r\n        "path": "lookupField"\r\n      }\r\n    }\r\n  },\r\n  "query": {\r\n    "match_all": {}\r\n  }\r\n}\r\n```\r\nIn 0.90.1 when filter returned no results the result of the query was also empty, i.e. 0 hits. \r\nIn 0.90.2 it returns everything. ~~The latter behavior is probably better and more expected~~, but I haven\'t seen it documented anywhere. Did I miss something?'
3364,'javanna',"Hang when modifying the cluster state and an uncaught exception is thrown\nAs mentioned in #3363 we don't decrement the CountDownLatch and never return a response if there's an uncaught exception in some of the TransportMasterNodeOperationActions that modify the cluster state. Some actions do catch Throwable and don't suffer from this issue, while some others don't.\r\n\r\nWe need to go over those and fix this issue so that we always decrement the CountDownLatch and give back the error that was thrown instead of hanging waiting for the thread to finish its execution."
3363,'javanna',"NullPointerException when trying to add single alias without index or alias\nMissing validation when trying to add a single alias and index or alias (or both) are missing. The following requests lead to a NullPointerException:\r\n\r\ncurl -XPUT localhost:9200/_alias\r\ncurl -XPUT localhost:9200/index1/_alias\r\ncurl -XPUT localhost:9200/_alias/alias1\r\n\r\nWe need to add proper validation for such cases. Also, in those cases the request never returns due to a missing CountDownLatch#countDown when there's an uncaught exception, which we are going to address on a separate issue.\r\n\r\n\r\n\r\n"
3359,'kimchy','[0.90.0, 0.90.2] Can\'t use empty replacement string in pattern_replace filter\nAfter upgrading from 0.20.0.rc1 to 0.90.2, this happens when we use blank replacement string in pattern_replace filter:\r\n\r\n```\r\n...\r\norg.elasticsearch.indices.IndexCreationException: [xxx] failed to create index\r\n\tat org.elasticsearch.indices.InternalIndicesService.createIndex(InternalIndicesService.java:382)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.applyNewIndices(IndicesClusterStateService.java:296)\r\n\tat org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:162)\r\n\tat org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:321)\r\n\tat org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:95)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:636)\r\nCaused by: org.elasticsearch.ElasticSearchIllegalArgumentException: replacement is missing for [whitespace_remove] token filter of type \'pattern_replace\'\r\n\tat org.elasticsearch.index.analysis.PatternReplaceTokenFilterFactory.<init>(PatternReplaceTokenFilterFactory.java:54)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:532)\r\n\tat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\r\n\tat org.elasticsearch.common.inject.InjectorImpl$4$1.call(InjectorImpl.java:763)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.InjectorImpl$4.get(InjectorImpl.java:759)\r\n\tat org.elasticsearch.common.inject.assistedinject.FactoryProvider2.invoke(FactoryProvider2.java:221)\r\n\tat $Proxy19.create(Unknown Source)\r\n\tat org.elasticsearch.index.analysis.AnalysisService.<init>(AnalysisService.java:152)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:532)\r\n\tat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat\r\n...\r\n```\r\n\r\nOur elasticsearch.json configuration looks like this (note `whitespace_remove` at the bottom, we need that to strip any whitespace in between characters):\r\n\r\n```json\r\n{\r\n  "cluster": {\r\n    "name": "1188_production_19_07"\r\n  },\r\n  "action": {\r\n    "auto_create_index": false\r\n  },\r\n  "indices": {\r\n    "memory": {\r\n      "index_buffer_size": "1024m"\r\n    }\r\n  },\r\n  "index": {\r\n    "number_of_replicas": 2,\r\n    "number_of_shards": 3,\r\n    "analysis": {\r\n      "analyzer": {\r\n        "autocomplete_exact_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "keyword",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "edge_ngram"\r\n          ]\r\n        },\r\n        "autocomplete_exact_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "keyword",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase"\r\n          ]\r\n        },\r\n        "autocomplete_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "edge_ngram"\r\n          ]\r\n        },\r\n        "autocomplete_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "keyword",\r\n          "filter": [\r\n            "whitespace_remove",\r\n            "lowercase"\r\n          ]\r\n        },\r\n        "ascii_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "czech_stem",\r\n            "asciifolding"\r\n          ]\r\n        },\r\n        "ascii_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "czech_stem",\r\n            "asciifolding"\r\n          ]\r\n        },\r\n        "untouched_ascii_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "asciifolding"\r\n          ]\r\n        },\r\n        "untouched_ascii_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "asciifolding"\r\n          ]\r\n        },\r\n        "czech_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "czech_stem"\r\n          ]\r\n        },\r\n        "czech_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase",\r\n            "czech_stem"\r\n          ]\r\n        },\r\n        "untouched_czech_search_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase"\r\n          ]\r\n        },\r\n        "untouched_czech_index_analyzer": {\r\n          "type": "custom",\r\n          "tokenizer": "standard",\r\n          "language": "Czech",\r\n          "filter": [\r\n            "standard",\r\n            "lowercase"\r\n          ]\r\n        }\r\n      },\r\n      "filter": {\r\n        "full_ngram": {\r\n          "type": "nGram",\r\n          "min_gram": "1",\r\n          "max_gram": "20"\r\n        },\r\n        "edge_ngram": {\r\n          "type": "edgeNGram",\r\n          "min_gram": 1,\r\n          "max_gram": 20,\r\n          "side": "front"\r\n        },\r\n        "whitespace_remove": {\r\n          "type": "pattern_replace",\r\n          "pattern": " ",\r\n          "replacement": ""\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI tried to search when this change occurred, so I tried it on 0.90.0, 0.90.1 and 0.90.2. They all produce the same result.\r\n\r\nI\'ve tried to look around and thought the JSON parser was the culprit, so I went digging and discovered, that JSON Loader returns null value for empty strings in JSONs, but YML loader does return empty strings for YMLs (which is by itself strange and should not happen imo :)) ) by adding relevant values to test-settings.json and test-settings.yml + their tests:\r\n\r\nhttps://gist.github.com/NoICE/6039088\r\n(note these added lines: https://gist.github.com/NoICE/6039088#file-jsonsettingsloadertests-java-L51-L53\r\nhttps://gist.github.com/NoICE/6039088#file-yamlsettingsloadertests-java-L51-L53)\r\n\r\nSo I rewrote our elasticsearch.json to elasticsearch.yml, but the error still remains.\r\n\r\nSo:\r\n\r\n- JSON parser returns null for "" values\r\n- YML parser does return "" for "" values\r\n- pattern replace does not allow empty string in either JSON or YML format (so we can rule out JSON parser is the culprit, maybe...)\r\n\r\nLet me know if I can provide some more tests or something...'
3357,'jpountz',"Highlighting doesn't work with term vectors enabled and some complex queries\nThis is related to https://issues.apache.org/jira/browse/LUCENE-4734. If you have term vectors enabled and try to highlight a proximity query or a phrase which has eg. 2 terms at the same position, no snippet will be returned."
3356,'bleskes','Missing filter works differently in top-level versus filtered query\nGiven the following script: https://gist.github.com/dakrone/6034875\r\n\r\nI was expecting the same output for both queries (no documents matched), but instead I get:\r\n\r\n```\r\n∴ ./missing-filter-bug.zsh\r\nRegular filter for null_value=true, existence=false\r\n{\r\n  "took" : 2,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "missing-test",\r\n      "_type" : "doc",\r\n      "_id" : "1",\r\n      "_score" : 1.0, "_source" : { "id": "1" }\r\n    }, {\r\n      "_index" : "missing-test",\r\n      "_type" : "doc",\r\n      "_id" : "2",\r\n      "_score" : 1.0, "_source" : { "id": "2", "myfield": "foo" }\r\n    } ]\r\n  }\r\n}\r\nFiltered query with filter for null_value=true, existence=false\r\n{\r\n  "took" : 1,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 0,\r\n    "max_score" : null,\r\n    "hits" : [ ]\r\n  }\r\n}\r\n```'
3355,'spinscale','Limits are not consumed using Systemd (ulimit -n / ulimit -l)\nSee http://www.freedesktop.org/software/systemd/man/systemd.exec.html\r\n* LimitNOFILE\r\n* LimitMEMLOCK'
3353,'imotov',"Don't allow unallocated indexes to be closed.\nRefuse to close indexes that have not had their primary shard shard allocated post api action because this leaves the index in an un-openable state.\r\n\r\nCloses #3313"
3351,'chilling','GeoPoint parsing\nThe `GeoPoint` class is used in several parsers. Each of these classes implements geopoint parsing on its own. To simplify the code and guarantee consistency these parsing methods must be refactored.'
3350,'s1monw',"Conflicting META-INF between elasticsearch & lucene-core/lucene-codecs for packaging into standalone binary\nWhen bundling lucene-core, lucene-codec, & elasticsearch into a single executable JAR, there are conflicting files in META-INF:\r\n\r\nelasticsearch:\r\n        0  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.Codec\r\n        0  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.DocValuesFormat\r\n      147  06-26-13 08:56   META-INF/services/org.apache.lucene.codecs.PostingsFormat\r\n\r\nlucene-core:\r\n      987  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.Codec\r\n      853  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.DocValuesFormat\r\n      909  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.PostingsFormat\r\n\r\nlucene-codecs:\r\n      897  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.Codec\r\n      909  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.DocValuesFormat\r\n     1079  06-09-13 12:01   META-INF/services/org.apache.lucene.codecs.PostingsFormat\r\n\r\nWhen packaging everything into a standalone binary, it becomes unclear which ones I need.  Using the ones from elasticsearch, I get:\r\nCaused by: java.lang.ExceptionInInitializerError\r\n\tat org.elasticsearch.index.codec.CodecModule.configure(CodecModule.java:118)\r\nCaused by: java.lang.IllegalArgumentException: A SPI class of type org.apache.lucene.codecs.PostingsFormat with name 'Direct' does not exist. You need to add the corresponding JAR file supporting this SPI to your classpath.The current classpath supports the following names: [XBloomFilter, es090]"
3346,'javanna','_id could become non-unique within a index when using _routing fields\n# Bug Description\r\nUsually, the _id field is known as global unique in a index, right?   \r\nBut I found it become non-unique when a doc\'s routing field is modified to another value and reindex to ES. Then, there will be two doc alive in diffrent shard but the same index.\r\n\r\nIt seems the delete operation is broadcasted to all shards, but the index operation not. \r\n\r\nSince it\'s hard to monitor if the routing filed is modified, the only thing I can do is do an delete operation before each index operation, I really don\'t like it >_<\r\n\r\n\r\n# how to reproduce the bug\r\n```Tested under both v0.90.0```\r\n## [1] Create A Index \r\n```shell\r\ncurl -XPUT \'http://localhost:9200/user\' -d \'\r\n{\r\n    "mappings": {\r\n        "User": {\r\n            "store": "no",\r\n            "_id": {\r\n                    "type": "string",\r\n                    "index": "not_analyzed",\r\n                    "store": "yes"\r\n            },\r\n            "_type": {\r\n                "enabled": true\r\n            },\r\n            "_routing": {\r\n                "path": "tag",\r\n                "required": true\r\n            },\r\n            "properties": {\r\n                "tag": {\r\n                    "type": "string",\r\n                    "index": "not_analyzed"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\'\r\n```\r\n\r\n## [2] Input Data \r\n```shell\r\ncurl -XPOST \'http://localhost:9200/user/User/123\' -d \'{"tag" : "good"}\'\r\n```\r\n{"ok":true,"_index":"user","_type":"User","_id":"123","_version":1}\r\n\r\n```shell\r\ncurl -XPOST \'http://localhost:9200/user/User/123\' -d \'{"tag" : "bad"}\'\r\n```\r\n{"ok":true,"_index":"user","_type":"User","_id":"123","_version":1}\r\n\r\n\r\n## [3] Search\r\n```shell\r\ncurl -XPOST \'http://localhost:9200/user/User/_search\' -d \'{\r\n  "query": {\r\n    "term": {\r\n      "_id": "123"\r\n    }\r\n  },\r\n  "facets": {\r\n    "tag": {\r\n      "terms": {\r\n        "field": "tag"\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nResult:\r\n\r\n```\r\n{\r\n    "took": 1,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 10,\r\n        "successful": 10,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 2,\r\n        "max_score": 0.30685282,\r\n        "hits": [\r\n            {\r\n                "_index": "user",\r\n                "_type": "User",\r\n                "_id": "123",\r\n                "_score": 0.30685282,\r\n                "_source": {\r\n                    "tag": "bad"\r\n                }\r\n            },\r\n            {\r\n                "_index": "user",\r\n                "_type": "User",\r\n                "_id": "123",\r\n                "_score": 0.30685282,\r\n                "_source": {\r\n                    "tag": "good"\r\n                }\r\n            }\r\n        ]\r\n    },\r\n    "facets": {\r\n        "tag": {\r\n            "_type": "terms",\r\n            "missing": 0,\r\n            "total": 2,\r\n            "other": 0,\r\n            "terms": [\r\n                {\r\n                    "term": "good",\r\n                    "count": 1\r\n                },\r\n                {\r\n                    "term": "bad",\r\n                    "count": 1\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n'
3345,'javanna',"Add local flag support to all the read operations that get executed on the master node\nAll the operations that involve the cluster state get executed on the master node by default, but it's usually possible to explicitly force their local execution through a local flag when it comes to reading from the cluster state. Some of those operations don't support the local flag (yet) though. We should add the local flag in the base class and support it by default so that all the classes that extend TransportMasterNodeOperationAction get it for free."
3344,'spinscale','IndexMissingException not thrown when doing a query on a non-existent index\nI\'m having a weird issue where the Java interface is not throwing an IndexMissingException in 0.90.2 like it used to in 0.20.4.\r\n\r\nWhen I use the REST interface, everything appears to be fine:\r\ncurl -XGET localhost:9200/twitter/_status\r\n{"error":"IndexMissingException[[twitter] missing]","status":404}\r\n\r\ncurl -XGET \'http://localhost:9200/twitter/tweet/_count?q=user:kimchy\'\r\n{"error":"IndexMissingException[[twitter] missing]","status":404}\r\n\r\nHowever, when I use the java interface, executing a CountRequestBuilder, I get the following exception:\r\n\r\norg.elasticsearch.transport.TransportSerializationException: Failed to deserialize exception response from stream\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.handlerResponseError(MessageChannelHandler.java:168)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:122)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\nCaused by: java.io.StreamCorruptedException: unexpected end of block data\r\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1370)\r\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1989)\r\n\tat java.io.ObjectInputStream.defaultReadObject(ObjectInputStream.java:499)'
3342,'bleskes','Thai language analyzer ignores stopwords configuration setting\n'
3336,'bleskes','memory leak while scrolling over index\n(I\'ve tried to report this via the google group. Either I\'m being moderated or google doesn\'t like me. sorry for posting it twice)\r\n\r\nI\'m trying to scroll over all documents of an ElasticSearch Index using\r\na match_all query. I\'ve set ES_HEAP_SIZE to 8G but I\'m not able to\r\ncomplete the operation, because elasticsearch runs out of memory (see\r\nlog at the end of this mail).\r\n\r\nThe head plugin tells me the index size is around 400GB with around 9.5M\r\ndocuments. I\'m using a single document type with the following mapping:\r\n\r\n<pre>\r\n{\r\n    "src_doc": {\r\n        "_all": {\r\n            "enabled": false\r\n        }, \r\n        "_source": {\r\n            "enabled": false\r\n        }, \r\n        "properties": {\r\n            "content": {\r\n                "type": "binary"\r\n            }, \r\n            "exception": {\r\n                "index": "no", \r\n                "store": true, \r\n                "type": "string"\r\n            }, \r\n            "last_update": {\r\n                "format": "YYYY-MM-dd", \r\n                "store": true, \r\n                "type": "date"\r\n            }, \r\n            "title": {\r\n                "index": "no", \r\n                "store": true, \r\n                "type": "string"\r\n            }, \r\n            "uid": {\r\n                "index": "no", \r\n                "type": "string"\r\n            }, \r\n            "url": {\r\n                "index": "no", \r\n                "store": true, \r\n                "type": "string"\r\n            }\r\n        }\r\n    }\r\n}\r\n</pre>\r\n\r\nI\'m using ElasticSearch 0.90.2, but the issue has already been in\r\n0.90.0. The Cluster is a single node, which is not being used otherwise.\r\n\r\nHere\'s the log:\r\n\r\n<pre>\r\n2013-07-12T13:59:39.40468 java.lang.OutOfMemoryError: Java heap space\r\n2013-07-12T13:59:39.43523 Dumping heap to java_pid8908.hprof ...\r\n2013-07-12T14:00:14.69555 Heap dump file created [8513498041 bytes in 35.278 secs]\r\n2013-07-12T14:00:14.73700 [2013-07-12 16:00:14,702][WARN ][http.netty               ] [graph.8908] Caught exception while handling client http traffic, closing connection [id: 0x8d823a3d, /127.0.0.1:52874 => /127.0.0.1:9250]\r\n2013-07-12T14:00:14.73702 java.lang.OutOfMemoryError: Java heap space\r\n2013-07-12T14:00:14.73702 \tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n2013-07-12T14:00:14.73702 \tat java.nio.ByteBuffer.allocate(ByteBuffer.java:331)\r\n2013-07-12T14:00:14.73702 \tat org.elasticsearch.common.netty.buffer.CompositeChannelBuffer.toByteBuffer(CompositeChannelBuffer.java:649)\r\n2013-07-12T14:00:14.73703 \tat org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.toByteBuffer(AbstractChannelBuffer.java:530)\r\n2013-07-12T14:00:14.73703 \tat org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:77)\r\n2013-07-12T14:00:14.73703 \tat org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:46)\r\n2013-07-12T14:00:14.73703 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:194)\r\n2013-07-12T14:00:14.73703 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:152)\r\n2013-07-12T14:00:14.73704 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)\r\n2013-07-12T14:00:14.73704 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\r\n2013-07-12T14:00:14.73705 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\r\n2013-07-12T14:00:14.73706 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)\r\n2013-07-12T14:00:14.73706 \tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n2013-07-12T14:00:14.73706 \tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n2013-07-12T14:00:14.73706 \tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n2013-07-12T14:00:14.73706 \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n2013-07-12T14:00:14.73707 \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n2013-07-12T14:00:14.73707 \tat java.lang.Thread.run(Thread.java:722)\r\n2013-07-12T14:00:18.29386 [2013-07-12 16:00:18,293][WARN ][http.netty               ] [graph.8908] Caught exception while handling client http traffic, closing connection [id: 0xd2b513bc, /127.0.0.1:52875 => /127.0.0.1:9250]\r\n2013-07-12T14:00:18.29388 java.lang.OutOfMemoryError: Java heap space\r\n2013-07-12T14:00:18.29388 \tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n2013-07-12T14:00:18.29388 \tat java.nio.ByteBuffer.allocate(ByteBuffer.java:331)\r\n2013-07-12T14:00:18.29388 \tat org.elasticsearch.common.netty.buffer.CompositeChannelBuffer.toByteBuffer(CompositeChannelBuffer.java:649)\r\n2013-07-12T14:00:18.29389 \tat org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.toByteBuffer(AbstractChannelBuffer.java:530)\r\n2013-07-12T14:00:18.29389 \tat org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:77)\r\n2013-07-12T14:00:18.29389 \tat org.elasticsearch.common.netty.channel.socket.nio.SocketSendBufferPool.acquire(SocketSendBufferPool.java:46)\r\n2013-07-12T14:00:18.29389 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.write0(AbstractNioWorker.java:194)\r\n2013-07-12T14:00:18.29389 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.writeFromTaskLoop(AbstractNioWorker.java:152)\r\n2013-07-12T14:00:18.29390 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioChannel$WriteTask.run(AbstractNioChannel.java:335)\r\n2013-07-12T14:00:18.29390 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)\r\n2013-07-12T14:00:18.29391 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)\r\n2013-07-12T14:00:18.29391 \tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)\r\n2013-07-12T14:00:18.29391 \tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n2013-07-12T14:00:18.29391 \tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n2013-07-12T14:00:18.29391 \tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n2013-07-12T14:00:18.29391 \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n2013-07-12T14:00:18.29392 \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n2013-07-12T14:00:18.29392 \tat java.lang.Thread.run(Thread.java:722)\r\n</pre>\r\n\r\n'
3330,'martijnvg','Rename existsAliases to aliasesExist\nRename IndicesAdminClient#existsAliases to IndicesAdminClient#aliasesExist. Also `IndicesExistsAliasesResponse` will be renamed to `AliasesExistResponse`.\r\n\r\nThis is a breaking change only for the Java api, and *not* for the rest api. '
3327,'martijnvg','Add found field for bulk deletes. Closes #3320\n'
3325,'bleskes','Mget: the fields parameter should accept "*" to retrieve all "store"d fields\nVery much like documented on http://www.elasticsearch.org/guide/reference/api/search/fields/ , it would be helpful if `_mget` would also support `"field": "*"` to retrieve all fields which have been marked as stored.'
3324,'spinscale','No logs\nI have 0.90.1 installed from the .deb, currently running with these options (ps output):\r\n\r\n108      19063  1.5  1.1 1630052 282388 ?      Sl   15:34   0:39 /usr/lib/jvm/java-6-openjdk/bin/java -Xms256m -Xmx1g -Xss256k -Djava.awt.headless=true -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -XX:+HeapDumpOnOutOfMemoryError -Delasticsearch -Des.pidfile=/var/run/elasticsearch.pid -Des.path.home=/usr/share/elasticsearch -cp :/usr/share/elasticsearch/lib/elasticsearch-0.90.1.jar:/usr/share/elasticsearch/lib/*:/usr/share/elasticsearch/lib/sigar/* -Des.default.config=/etc/elasticsearch/elasticsearch.yml -Des.default.path.home=/usr/share/elasticsearch -Des.default.path.logs=/var/log/elasticsearch -Des.default.path.data=/var/lib/elasticsearch -Des.default.path.work=/tmp/elasticsearch -Des.default.path.conf=/etc/elasticsearch org.elasticsearch.bootstrap.ElasticSearch\r\n\r\nHowever, in /var/log/elasticsearch there is only elasticsearch.log, which is empty.\r\n\r\nWhen I su to the elasticsearch user, I can append to elasticsearch.log and create a sibling file. I did lsof on 19063 and found no log file to be open. I was unable to lsof any child process (exit code 1).'
3322,'kimchy',"Search performance degrades after processing a large response\nIn our testing we've found that search performance can degrade quite a bit after processing a large search response.  We've tracked this down to the usage of TIntObjectHashMap in HandlesStreamInput.  We found that after processing a large search response, the capacity of these maps increased dramatically.  Even though clear() is called on the maps in the reset() methods, the capacities of the maps stay high and adversely impact the performance of subsequent clear() calls (since clear() must iterate over the entire capacity of each map).\r\n\r\nIn local testing we've found that this performance issue can be fixed by re-creating the hash maps in the reset() methods if the capacity exceeds a certain limit (say 10k).  We also believe that this could be fixed by calling compact() on the maps after clear()  (since that will also reduce their capacity and thus make operations fast again).\r\n\r\nPlease let me know if you'd like any additional information about this issue or the proposed fix."
3317,'jpountz',"Fix offsets handling of the n-gram and edge n-gram tokenizers and token filters\nLucene 4.4 will fix these tokenizers and token filters so that they don't break highlighting anymore. So it would be nice to import them into Elasticsearch. See https://issues.apache.org/jira/browse/LUCENE-3907"
3315,'javanna','Rename _boost "name" to "path"\nAll of the metadata fields (eg `_id`, `_routing` etc) allow you to configure the `path` to a document field, except the `_boost` field, which expects a `name` instead.\r\n\r\nPlease can we deprecate `name` and add support for `path` instead, to make all the settings consistent.'
3310,'chilling','geo_shape null geometry\nI have a geojson file generated using ogr2ogr from a shape file.Unfortunately some of the geometry objects are null but I still want to index the other meta data. Currently I get a parse error on these documents because the geo_shape parser is not handling json nulls. I think it would be nicer to not fail and simply not index the geo_shape field for those fields. Would it be possible to fix this?\r\n\r\nHere\'s a sample document. It fails on the geometry field (which is mapped to geo_shape). The error I get is: MapperParsingException[failed to parse [geometry]]; nested: ElasticSearchParseException[Shape must be an object consisting of type and coordinates];\r\n\r\n{ "type": "Feature", "properties": { "name": "吉井町宮田", "qs_id": 856730, "gn_id": null, "woe_id": 28484701, "gn_id_eh": null, "woe_id_eh": null, "gn_name": null, "gn_ascii": null, "gn_country": null, "gn_admin1": null, "gn_admin2": null, "gn_pop": null, "gn_fclass": null, "gn_fcode": null, "woe_name": "吉井町宮田", "woe_nameen": null, "placetype": "LocalAdmin", "iso": "JP", "language": "JPN", "parent_id": 28379393, "woe_local": 28379393, "woe_lau": 28484701, "woe_adm2": 0, "woe_adm1": 58646425, "woe_adm0": 23424856, "name_local": "うきは市", "name_lau": "吉井町宮田", "name_adm2": null, "name_adm1": "福岡県", "name_adm0": "日本", "gns_id": null, "accuracy": null, "matchtype": null, "geom_qual": null, "woe_funk": null, "photos": null, "photos_all": null, "woemembers": null, "photos_1k": null, "photos_9k": null, "photos_sr": 0, "photos_9r": 0, "pop_sr": 0 }, "geometry": null }'
3309,'s1monw','Script based sorting is applied only after pagination\nWhen a query requests both sorting and pagination, the expected behavior is that sorting is executed _before_ pagination. And indeed this is the actual behavior when the query requests sorting on a field level. However, when doing script based sorting, it seems that sorting is only executed _after_ pagination. This is behavior is unexpected and especially surprising because the same sort may yield different results depending on whether it is specified on a field level or via a script.\r\n\r\nExample\r\n\r\nSuppose we have 100 files indexed, named "file001", "file002", ..., "file100", with the file name being stored in a "filename" field. Then the following query\r\n\r\n```\r\n{\r\n  "from" : 0,\r\n  "size" : 10,\r\n  "query" : {\r\n    "match_all" : { }\r\n  },\r\n  "sort" : [ {\r\n    "filename" : {\r\n      "order" : "asc"\r\n    }\r\n  } ]\r\n}\r\n```\r\nyields the expected ordering "file001", "file002", ..., "file010". The seemingly equivalent query\r\n```\r\n{\r\n  "from" : 0,\r\n  "size" : 10,\r\n  "query" : {\r\n    "match_all" : { }\r\n  },\r\n  "sort" : [ {\r\n    "_script" : {\r\n      "script" : "doc[\'filename\'].value",\r\n      "type" : "string"\r\n    }\r\n  } ]\r\n}\r\n```\r\nmay return, for example, "file005", "file007", "file020", "file027", "file035", "file050", "file067", "file080", "file092", "file097". Which is pretty useless and certainly not what the client would expect.\r\n\r\nThis behavior was observed in ElasticSearch 0.90.2, but I didn\'t test previous versions.'
3307,'brwe','Distance Scoring\n# Distance scoring\r\n\r\nIt might sometimes be desirable to have a tool available that allows to multiply the original score for a document with a function that decays depending on the distance of a numeric field value of the document from a user given reference.\r\n\r\n\r\nIn the most simple case, for each field in each found document a decay function could be computed. The original score of the query could then be multiplied with the individual function values computed for the fields. Distance scoring in this case behaves like a range query with smoothed box edges.\r\n\r\nDistance scoring could be applied for an arbitrary number of numeric fields.\r\n\r\nTo use distance scoring on a query that has numerical fields, the user would have to define at least\r\n \r\n 1. a reference and\r\n 2. a scale\r\n \r\nfor each field. A reference is needed to define a distance for the document and a scale to define the rate of decay.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## Example\r\n\r\nSuppose you are searching for a hotel in a certain town. Your budget is limited. Also, you would like the hotel to be close to the town center, so the farther the hotel is from the desired location the less likely you are to check in.\r\nYou would like the query results that match your criterion (for example, "hotel, Nancy, non-smoker") to be scored with respect to distance to the town center and also the price. \r\n\r\n\r\n\r\nIntuitively, you would like to define the town center as the origin and maybe you are willing to walk 2km to the town center from the hotel.\r\nIn this case your *reference* for the location field is the town center and the *scale* is ~2km.\r\n\r\n\r\nIf your budget is low, you would probably prefer something cheap above something expensive. \r\nFor the price field, the *reference* would be 0 Euros and the *scale* depends on how much you are willing to pay, for example 20 Euros. \r\n\r\n## Usage\r\n\r\nIn the above example, the fields might be called "price" for the price of the hotel and "location" for the coordinates of this hotel. \r\nFor both fields, the user should be able to define a decay function and also how to combine the decay functions for different fields before the decay factor is multiplied to the score of the original query\r\nThe json request could look like this:\r\n\r\n\r\n\r\n    curl \'localhost:9200/hotels/_search/\' -d \'{\r\n    "query": {\r\n        "function_score": {\r\n            "functions": [\r\n                {\r\n                    "DECAY_FUNCTION": {\r\n                        "price": {\r\n                            "reference": "0",\r\n                            "scale": "20"\r\n                        }\r\n                    }\r\n                },\r\n                {\r\n                    "DECAY_FUNCTION": {\r\n                        "location": {\r\n                            "reference": "11, 12",\r\n                            "scale": "2km"\r\n                        }\r\n                    }\r\n                }\r\n            ],\r\n            "query": {\r\n                "match": {\r\n                    "properties": "balcony"\r\n                }\r\n            },\r\n            "score_mode": "multiply"\r\n        }\r\n    }\r\n    }\'\r\n\t\r\n\r\n\r\n## Decay Functions\r\nA huge variety of decay functions could be implemented. Here are three examples:\r\n\r\n### Normal decay \r\n\r\nA contour plot for the normal decay for two fields looks like this (if the decay functions are multiplied):\r\n\r\n\r\n![gausscontour](https://f.cloud.github.com/assets/4320215/768157/cd0e18a6-e898-11e2-9b3c-f0145078bd6f.png)\r\n![gausssurf](https://f.cloud.github.com/assets/4320215/768160/ec43c928-e898-11e2-8e0d-f3c4519dbd89.png)\r\n\r\n\r\nSuppose your original search results matches three hotels : "Backback Nap", "Drink n Drive" and "BnB Bellevue". \r\n"Drink n Drive" is pretty far from your defined location (nealy 2 km) and is not too cheap (about 13 Euros) so it gets a low factor a factor of 0.56. "BnB Bellevue" and "Backback Nap" are both pretty close to the defined location but "BnB Bellevue" is cheaper, so it gets a multiplier of 0.86 whereas "Backpack Nap" gets a value of 0.66."\r\n\r\n\r\nThe multiplier to the original score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\exp(-\\frac{(x-\\mu)^2}{\\sigma^2})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{(location_{doc}-\\mu_l)^2}{2\\sigma^2_l})\\exp(-\\frac{(price_{doc}-\\mu_p)^2}{2\\sigma^2_p})," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\nwhere <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_l"  /></a> is the town center, <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_l"  /></a> is the scale of the location (2km in this case), <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_p"  /></a> is the price reference (0 Euros since your budget is low) and <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_p"  /></a> is the scale of the price (20 Euros in this case).\r\n\r\n### Exponential decay \r\n\r\nA contour plot for the exponential decay for two fields looks like this:\r\n\r\n\r\n\r\n![expcontour](https://f.cloud.github.com/assets/4320215/768161/082975c0-e899-11e2-86f7-174c3a729d64.png)\r\n![expsurf](https://f.cloud.github.com/assets/4320215/768162/0b606884-e899-11e2-907b-aefc77eefef6.png)\r\n\r\n\r\n\r\n\r\nThe multiplier to the original score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\exp(-\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\exp(-\\frac{|location_{doc}-\\mu_l|}{\\sigma_l})\\exp(-\\frac{|price_{doc}-\\mu_p|}{\\sigma_p})," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\nwhere again <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_l"  /></a> is the town center, <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_l"  /></a> is the scale of the location (2km in this case), <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_p"  /></a> is the price reference (0 Euros since your budget is low) and <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_p"  /></a> is the scale of the price (20 Euros in this case).\r\n\r\n\r\n\r\n###  \'Linear\' decay\r\n\r\nA contour plot for the \'linear\' decay for two fields looks like this:\r\n\r\n![lincontour](https://f.cloud.github.com/assets/4320215/768164/1775b0ca-e899-11e2-9f4a-776b406305c6.png)\r\n![linsurf](https://f.cloud.github.com/assets/4320215/768165/19d8b1aa-e899-11e2-91bc-6b0553e8d722.png)\r\n\r\n\r\n\r\nThe multiplier to the original score is computed as\r\n\r\n<a href="http://www.codecogs.com/eqnedit.php?latex=\\frac{abs(x-\\mu)^2}{\\sigma})" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\\mathcal{S}(doc)=\\max(\\frac{\\sigma_l-|location_{doc}-\\mu_l|}{\\sigma_l},0)\\max(\\frac{\\sigma_p-|price_{doc}-\\mu_p|}{\\sigma_p},0)," title="\\exp(-\\frac{x-\\mu}{2\\sigma^2})," /></a>\r\n\r\nwhere again <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_l"  /></a> is the town center, <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_l"  /></a> is the scale of the location (2km in this case), <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\mu_p"  /></a> is the price reference (0 Euros since your budget is low) and <a href="http://www.codecogs.com/eqnedit.php" ><img src="http://latex.codecogs.com/gif.latex?\\sigma_p"  /></a> is the scale of the price (20 Euros in this case).\r\n\r\nIn contrast to the normal and exponential decay, this function actually sets the score to 0 if the field value exceeds the user gived scale value.\r\n\r\n\r\n\r\n\r\n\r\n## Supported fields\r\nOnly single valued numeric fields, including time and geo locations, should be supported. \r\n\r\n## What is a field is missing?\r\n\r\nIs the numeric field is missing in the document, that field should not be taken into account at all for this document.\r\n\r\n\r\n\r\n## Consolidate with custom_boost_factor, custom_score and custom_filters_score\r\n\r\nIt might make sense to consolidate this functionality with the custom_boost_factor, custom_score and custom_filters_score. \r\nSee https://github.com/elasticsearch/elasticsearch/issues/3407'
3305,'martijnvg','_explain endpoint does not work with has_child query\nSteps to reproduce:\r\n\r\nCreate a child type: \r\n\r\nPUT http://es:9200/hrtest/child/_mapping\r\n{\r\n    "child" : {\r\n        "_parent" : {\r\n            "type" : "parent"\r\n        }\r\n    }\r\n}\r\n\r\nIndex a parent:\r\nPOST http://es:9200/hrtest/parent/p1\r\n{\r\n\t"name" : "my parent" \r\n}\r\n\r\nIndex a child: \r\nPOST http://es:9200/hrtest/child/c1?parent=p1\r\n{\r\n\t"path" : "/top" \r\n}\r\n\r\nTry searching:\r\nPOST http://es:9200/hrtest/parent/_search\r\n{\r\n  "query": {\r\n    "has_child": {\r\n      "type": "child",\r\n      "query": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "term": {\r\n                "path": "/top"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nNow try _explain:\r\nPOST http://es:9200/hrtest/parent/p1/_explain\r\n{\r\n  "query": {\r\n    "has_child": {\r\n      "type": "child",\r\n      "query": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "term": {\r\n                "path": "/top"\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nYou get an error:\r\n{\r\n    "error": "ElasticSearchIllegalStateException[has_child filter hasn\'t executed properly]",\r\n    "status": 500\r\n}\r\n'
3304,'spinscale','bin/plugin in Debian package ignores path.plugins from /etc/elasticsearch/elasticsearch.yml\nbin/plugin script in Debian package does not pass -Des.default.config to ElasticSearch.\r\n\r\nAs a result, if path.plugins is changed in /etc/elasticsearch/elasticsearch.yml, bin/plugin will install plugins to the wrong path.\r\n'
3301,'bleskes','Add finer control over `_source` retrieval, in `get`, `mget`, `get_source`, `explain` & `search` API\nAt the moment all of the above API offer the `field` parameter to retrieve part of the stored documents. However, the `fields` option was built to expose Lucene\'s stored fields and thus has some limitations when use to extract data from `_source`. The most important one is potentially flatting the document structure. \r\n\r\nThis feature adds a new parameter that allows directly retrieving parts of the `_source`, without conforming to the store fields structure.\r\n\r\nTo maintain backward compatibility, you can still retrieve the `_source` by specifying `fields=["_source"]` but this special treatment will be removed in the future. \r\n\r\n\r\nGet API\r\n---\r\n\r\nThe Get api parameters are supplied via the query string. New `_source`,`_source_include` & `_source_exclude` parameters are added, according to the following:\r\n\r\n\r\n**A flag to control _source retrieval**\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/1?_source=false\'\r\n```\r\n\r\nor (default)\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/1?_source=true\'\r\n```\r\n\r\n\r\n**Only retrieve part of the source**\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/1?_source=title,author\'\r\n```\r\n\r\nor \r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/1?_source_include=title,content&_source_exclude=content.full_text\'\r\n```\r\n\r\nMulti Get API\r\n---\r\n\r\nThe Multi Get API allows you to control `_source` both on the query string (same syntax as the `get` API) or on a per document basis.\r\n\r\n\r\n**Query String defaults**\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/_mget?_source=false\' -d\'{\r\n  ids: [1, 2, 3]\r\n}\'\r\n```\r\n\r\nor\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/_mget?_source_include=title,content&_source_exclude=content.full_text\' -d\'{\r\n  ids: [1, 2, 3]\r\n}\'\r\n```\r\n\r\netc.\r\n\r\n**Per document settings**\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/_mget\' -d \'{\r\n\tdocs: [\r\n\t\t{ "_index": "test" , _type: "type1", "_id": "1", "_source": false },\r\n\t\t{ "_index": "test" , _type: "type1", "_id": "2", "_source": "title" },\r\n\t\t{ "_index": "test" , _type: "type1", "_id": "3", "_source": [ "title", "author" ] },\r\n\t\t{ "_index": "test" , _type: "type1", "_id": "4", \r\n\t\t  "_source": { "include": "content" , "exclude" : "content.full_text" }  \r\n\t\t},\r\n\t\t{ "_index": "test" , _type: "type1", "_id": "5", \r\n\t\t  "_source": { "include": [ "title", "content" ] , "exclude" : [ "content.full_text" ]}  \r\n\t\t}\r\n\t]\r\n}\'\r\n```\r\n\r\n\r\nGet_source API\r\n---\r\n\r\nThe `get/_source` API is an API that is already dedicated for `_source` retrieval. As such, it has a slightly different parameter naming:\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/index/type/1/_source?include=title,content&exclude=content.full_text\'\r\n```\r\n\r\nExplain API\r\n----\r\n\r\nThe `explain` API also offers the `fields` parameter. It is now extend with query string parameters, just like the `get` API:\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/index/type/1/_explain?_source=false\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } }\r\n}\'\r\n```\r\n\r\nor\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/index/type/1/_explain?_source=title,author\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } }\r\n}\'\r\n```\r\n\r\nand\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/index/type/1/_explain?_source_include=title,content&_source_exclude=content.full_text\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } }\r\n}\'\r\n```\r\n\r\nSearch API\r\n===\r\n\r\nThe `search` API was added an extra `_source` key in the body, with the same options as all the above:\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/_search\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } },\r\n\t"_source" : false\r\n}\'\r\n```\r\n\r\nand\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/_search\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } },\r\n\t"_source" : "title"\r\n}\'\r\n```\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/_search\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } },\r\n\t"_source" : [ "title" , "author" ]\r\n}\'\r\n```\r\n\r\n\r\n```\r\ncurl -XPOST \'http://localhost:9200/_search\' -d\'{\r\n\t"query" : { "term" : { "message" : "search" } },\r\n\t"_source": { "include": [ "title", "content" ] , "exclude" : [ "content.full_text" ]} \r\n}\'\r\n```\r\n\r\nAlso the `search` API supports accepting `_source` retrieval settings as query string parameters. The format is identical to the `get` API: `_source`, `_source_include` & `_source_exclude`. In the case where the parameters are supplied both in the request body and the query string, the query string parameter override the body.\r\n'
3300,'uboness','Aggregation Module - Phase 1 - Functional Design\n***NOTE: at this point we\'re focusing more on the functional design aspect rather than performance. Once we get this nailed down, we\'ll see how far we can push and optimize.***\r\n\r\n### Background\r\nThe new aggregations module is due to elasticsearch 1.0 release, and aims to serve as the next generation replacement for the functionality we currently refer to as "faceting". Facets, currently provide a great way to aggregate data within a document set context. This context is defined by the executed query in combination with the different levels of filters that are defined (filtered queries, top level filters, and facet level filters). Although powerful as is, the current facets implementation was not designed from ground up to support complex aggregations and thus limited. The main problem with the current implementation stem in the fact that they are hard coded to work on one level and that the different types of facets (which account for the different types of aggregations we support) cannot be mixed and matched dynamically at query time. It is not possible to compose facets out of other facet and the user is effectively bound to the top level aggregations that we defined and nothing more than that.\r\n\r\nThe goal with the new aggregations module is to break the barriers the current facet implementation put in place. The new name ("Aggregations") also indicate the intention here - a generic yet extremely powerful framework for defining aggregations - any type of aggregation. The idea here is to have each aggregation defined as a "standalone" aggregation that can perform its task within any context (as a top level aggregation or embedded within other aggregations that can potentially narrow its computation scope). We would like to take all the knowledge and experience we\'ve gained over the years working with facets and apply it when building the new framework.\r\n\r\nBefore we dive into the meaty part, it\'s important to set some key concepts and terminology first.\r\n\r\n### Key Concepts & Terminology\r\n\r\n- **Aggregation** - An aggregation is the result of an aggregation :). There are many types of aggregations, some look similar , others have their own unique structure (all depending on the nature of the aggregation). For example, a `terms` aggregation holds a list of objects (buckets), each holding information about a unique term. While an `avg` aggregation, just holds the avg number aggregated over all values of a specific field/s within a well defined set of documents.\r\n\r\n- **Aggregator** - An aggregator is the computation unit in elasticsearch which generates aggregations. It is effectively responsible for aggregating the data during query phase, and at the end of this phase, create the output aggregation. Each aggregation type has a dedicated aggregator which knows how to compute and generate it.\r\n\r\nThere are two types of aggregators/aggregations:\r\n\r\n- **Bucket** - A family of aggregators whos main responsibility is to define the current document set context and split it into buckets, where each bucket defines a well defined document set context. Typically, all aggregators of this type will also return the document count in each bucket. This aggregator is composable, meaning, one can define other aggregations under it. It will then perform these defined aggregations for each of the buckets it builds. It is therefore possible to create buckets within buckets within buckets... up to any level of hierarchy one desires. For example, one can define a filter bucket that holds all the "active" users (for example, if the documents represent website users/visitors), under which she\'ll define a range bucket that build 3 buckets to represent different user age groups, under each age group she\'ll define a terms bucket to narrow down the most common tags each age group is using on the website. As you can see, creating hierarchies of buckets can be extremely powerful can immensely help when sliding & dicing your data.\r\n\r\n- **Calc** - A family of aggregators whos sole responsibility is to perform computation and calculate numbers. It always operates in a well defined scope of a document set. This document set scope is either the top most level one - the scope defined by the search query, or otherwise defined by a higher level bucket aggregator (as discussed above). The Calc Aggregators typically work on field values, therefore utilizing the field data from which they extract these values. But one can utilise scripts to compute custom values which will be aggregated in different ways (depending on the specific calc aggregator that is used). If combining (mixing & matching) all different types of aggregators, while bucket aggregators can be placed anywhere in the aggregation definition "tree", calc aggregators are always "leaves" on the tree as (unlike bucket aggregators) they cannot contain other aggregators.\r\n\r\n\r\n\r\n#### Structuring Aggregations\r\n\r\nThe following snippet captures the basic structure of aggregations:\r\n\r\n```json\r\n"aggregations" : {\r\n\t"<aggregation_name>" : {\r\n\t\t"<aggregation_type>" : { \r\n\t\t\t<aggregation_body>\r\n\t\t},\r\n\t\t["aggregations" : { [<sub_aggregation>]* } ]\r\n\t}\r\n\t[,"<aggregation_name_2>" : { ... } ]*\r\n\t\r\n}\r\n```\r\nThe `aggregations` object (can also be `aggs` for short) in the json holds the aggregations you\'d like to be computed. Each aggregation is associated with a logical name that the user defines (e.g. if the aggregation computes the average price, then it\'ll make sense to call it `avg_price`). These logical names, also uniquely identify the aggregations you define (you\'ll use the same names/keys to identify the aggregations in the response). Each aggregation has a specific type (`<aggregation_type>` in the above snippet) and is typically the first key within the named aggregation body. Each type of aggregation define its own body, depending on the nature of the aggregation (eg. the `avg` aggregation will define the field on which the avg will be calculated). At the same level of the aggregation type definition, one can optionally define a set of additional aggregations, but this only makes sense if the aggregation you defined is a bucketing aggregation. In this scenario, the aggregation you define on the bucketing aggregation level will be computed for all the buckets built by the bucketing aggregation. For example, if the you define a set of aggregations under the `range` aggregation, these aggregations will be computed for each of the range buckets that are defined.\r\n\r\nIn this manner, you can mix & match bucketing and calculating aggregations any way you\'d like, create any set of complex hierarchies by embedding aggregations (of type bucket or calc) within other bucket aggregations. To better grasp how they can all work together, please refer to the examples section below.\r\n\r\n### Calc Aggregators\r\nIn this section will provide an overview of all calc aggregations available to date. \r\n\r\nAll the calc aggregators we have today belong to the same family which we like to call `stats`. All the aggregator in this family are based on values that can either come from the field data or from a script that the user defines. \r\n\r\nThese aggregators operate on the following context: { _D_, _FV_ } where _D_ is the the set of documents from which the field values are extracted, and _FV_ is the set of values that should be aggregated. The aggregations take all those field values and calculates statistical values (some only calculate on value - they\'re called `single value stats aggregators`, while others generate a set of values - these are called `multi-value stats aggregators`).\r\n\r\nHere are all currently available stats aggregators\r\n\r\n#### Avg\r\nSingle Value Aggregator - Will return the average over all field values in the aggregation context, or what ever values the script generates\r\n\r\n```json\r\n"aggs" : {\r\n\t"avg_price" : { "avg" : { "field" : "price" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"avg_price" : { "avg" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"avg_price" : { "avg" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n_NOTE: when `field` and `script` are both specified, the script will be called for every value of the field in the context, and within the script you can access this value using the reserved variable `_value`.\r\n\r\nOutput:\r\n\r\n```json\r\n"avg_price" : {\r\n\t"value" : 10\r\n}\r\n```\r\n\r\n\r\n#### Min\r\nSingle Value Aggregator - Will return the minimum value among all field values in the aggregation context, or what ever values the script generates\r\n\r\n```json\r\n"aggs" : {\r\n\t"min_price" : { "min" : { "field" : "price" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"min_price" : { "min" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"min_price" : { "min" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"min_price" : {\r\n\t"value" : 1\r\n}\r\n```\r\n\r\n\r\n#### Max\r\nSingle Value Aggregator - Will return the maximum value among all field values in the aggregation context, or what ever values the script generates\r\n\r\n```json\r\n"aggs" : {\r\n\t"max_price" : { "max" : { "field" : "price" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"max_price" : { "max" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"max_price" : { "max" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"max_price" : {\r\n\t"value" : 100\r\n}\r\n```\r\n\r\n#### Sum\r\nSingle Value Aggregator - Will return the sum of all field values in the aggregation context, or what ever values the script generates\r\n\r\n```json\r\n"aggs" : {\r\n\t"sum_price" : { "sum" : { "field" : "price" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"sum_price" : { "sum" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"sum_price" : { "sum" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"sum_price" : {\r\n\t"value" : 350\r\n}\r\n```\r\n\r\n#### Count\r\nSingle Value Aggregator - Will return the number of field values in the aggregation context, or what ever values the script generates\r\n\r\n```json\r\n"aggs" : {\r\n\t"prices_count" : { "count" : { "field" : "price" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"prices_count" : { "count" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"prices_count" : { "count" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"prices_count" : {\r\n\t"value" : 400\r\n}\r\n```\r\n\r\n#### Stats\r\nMulti Value Aggregator - Will return the following stats aggregated over the field values in the aggregation context, or what ever values the script generates:\r\n\r\n - avg\r\n - min\r\n - max\r\n - count\r\n - sum\r\n\r\n```json\r\n"aggs" : {\r\n\t"price_stats" : { "stats" : { "field" : "price" } }\r\n}\r\n```\r\n\r\n```json\r\n"aggs" : {\r\n\t"prices_stats" : { "stats" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n\r\n```json\r\n"aggs" : {\r\n\t"prices_stats" : { "stats" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"prices_stats" : {\r\n\t"min" : 1,\r\n\t"max" : 10,\r\n\t"avg" : 5.5,\r\n\t"sum" : 55,\r\n\t"count" : 10,\r\n}\r\n```\r\n\r\n#### Extended Stats\r\nMulti Value Aggregator - an extended version of the Stats aggregation above, where in addition to its aggregated statistics the following will also be aggregated:\r\n\r\n - sum_of_squares\r\n - variance\r\n - std_deviation\r\n\r\n```json\r\n"aggs" : {\r\n\t"price_stats" : { "extended_stats" : { "field" : "price" } }\r\n}\r\n```\r\n\r\n```json\r\n"aggs" : {\r\n\t"prices_stats" : { "extended_stats" : { "script" : "doc[\'price\']" } }\r\n}\r\n```\r\n```json\r\n"aggs" : {\r\n\t"prices_stats" : { "extended_stats" : { "field" : "price", "script" : "_value" } }\r\n}\r\n```\r\n\r\nOutput:\r\n\r\n```json\r\n"value_stats": {\r\n    "count": 10,\r\n    "min": 1.0,\r\n    "max": 10.0,\r\n    "avg": 5.5,\r\n    "sum": 55.0,\r\n    "sum_of_squares": 385.0,\r\n    "variance": 8.25,\r\n    "std_deviation": 2.8722813232690143\r\n}\r\n```\r\n\r\n### Bucket Aggregators\r\n\r\nBucket aggregators don\'t calculate values over fields like the `calc` aggregators do, but instead, they create buckets of documents. Each bucket defines a criteria (depends on the aggregation type) that determines whether or not a document in the current context "falls" in it. In other words, the buckets effectively define document sets (a.k.a docsets) on which the sub-aggregations are running on.\r\n\r\nThere a different bucket aggregators, each with a different "bucketing" strategy. Some define a single bucket, some define fixed number of multiple bucket, and others dynamically create the buckets while evaluating the docs.\r\n\r\nThe following describe the currently supported bucket aggregators.\r\n\r\n#### Global\r\nDefines a single bucket of all the documents within the search execution context. This context is defined by the indices and the document types you\'re searching on, but is **not** influenced by the search query itself.\r\n\r\n_Note, global aggregators can only be placed as top level aggregators (it makes no sense to embed a global aggregator within another bucket aggregator)_\r\n\r\n```json\r\n"aggs" : {\r\n    "global_stats" : {\r\n\t\t"global" : {}, // global has an empty body\r\n\t\t"aggs" : {\r\n\t\t\t"avg_price" : { "avg" : { "field" : "price" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nOutput\r\n\r\n```json\r\n"aggs" : {\r\n\t"global_stats" : {\r\n\t\t"doc_count" : 100,\r\n\t\t"avg_price" : { "value" : 56.3 }\r\n\t}\r\n}\r\n```\r\n\r\n\r\n#### Filter\r\nDefines a single bucket of all the documents in the current docset context which match a specified filter. Often this will be used to narrow down the current aggregation context to a specific set of documents.\r\n\r\n```json\r\n"aggs" : {\r\n    "active_items" : {\r\n\t\t"filter" : { "term" : { "active" : true } },\r\n\t\t"aggs" : {\r\n\t\t\t"avg_price" : { "avg" : { "field" : "price" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nOutput\r\n\r\n```json\r\n"aggs" : {\r\n\t"active_items" : {\r\n\t\t"doc_count" : 100,\r\n\t\t"avg_price" : { "value" : 56.3 }\r\n\t}\r\n}\r\n```\r\n\r\n#### Missing\r\nA field data based single bucket aggregator, that creates a bucket of all documents in the current docset context that are missing a field value. This aggregator will often be used in conjunction with other field data bucket aggregators (such as ranges) to return information for all the documents that could not be placed in any of the other buckets due to missing field data values. (The examples bellow show how well the range and the missing aggregators play together).\r\n\r\n```json\r\n"aggs" : {\r\n    "missing_price" : {\r\n\t\t"missing" : { "field" : "price" }\r\n\t}\r\n}\r\n```\r\nOutput\r\n\r\n```json\r\n"aggs" : {\r\n\t"missing_price" : {\r\n\t\t"doc_count" : 10\r\n\t}\r\n}\r\n```\r\n\r\n#### Terms\r\nA field data based multi-bucket aggregator where buckets are dynamically built - one per unique value (term) of a specific field. For each such bucket the document count will be aggregated (accounting for all the documents in the current docset context that have that term for the specified field). This aggregator is very similar to how the terms facet works except that it is an aggregator just like any other aggregator, meaning it can be embedded in other bucket aggregators and it can also hold any types of sub-aggregators itself.\r\n\r\n```json\r\n"aggs" : {\r\n    "genders" : {\r\n\t\t"terms" : { "field" : "gender" },\r\n\t\t"aggs" : {\r\n\t\t\t"avg_height" : { "avg" : { "field" : "height" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nOutput\r\n\r\n```json\r\n"aggs" : {\r\n\t"genders" : {\r\n\t    "terms" : [\r\n\t    \t{\r\n\t    \t\t"term" : "male",\r\n\t    \t\t"doc_count" : 10,\r\n\t    \t\t"avg_height" : 178.5\r\n\t    \t},\r\n\t    \t{\r\n\t    \t\t"term" : "female",\r\n\t    \t\t"doc_count" : 10,\r\n\t    \t\t"avg_height" : 165\r\n\t    \t},\r\n\t    ]\r\n\t}\r\n}\r\n```\r\n**TODO: do we want to get rid of the "terms" level in the response and directly put the terms array under the aggregation name? (we do that in range aggregation)**\r\n\r\n##### Options\r\n\r\nName | Default | Required | Description\r\n:------------- | :------------: | :------------: | :------------\r\nfield | - | yes/no | the name of the field from which the terms will be taken. It is required if there is no other field data based aggregator in the current aggregation context and the **script** option is also not set\r\nsize | 10 | no | Only the top _n_ terms will be returned, the size determines what this _n_ is\r\norder | count desc | no | the order in which the term bucket will be sorted, see bellow for possible values\r\nscript | - | no | one can choose to let a script generate the terms instead of extracting them verbatim from the field data. If the script is define along with the field, then this script will be executed for every term/value of the field data with a special variable **_value** which will provide access to that value from within the script (this is as opposed to specifying only the script, without the field, in which case the script will execute once per document in the aggregation context)\r\n\r\n##### About order\r\nOne can define the order in which the term buckets will be sorted and therefore return in the response. There are 4 fixed/pre-defined order types and one more dynamic:\r\n\r\nOrder by term (alphabetically) ascending/descending:\r\n\r\n```json\r\n"aggs" : {\r\n    "genders" : {\r\n\t\t"terms" : { "field" : "gender", "order": { "_term" : "desc" } }\r\n\t}\r\n}\r\n```\r\n\r\nOrder by count (alphabetically) ascending/descending:\r\n\r\n```json\r\n"aggs" : {\r\n    "genders" : {\r\n\t\t"terms" : { "field" : "gender", "order": { "_count" : "asc" } }\r\n\t}\r\n}\r\n```\r\n\r\nOrder by direct embedded calc aggregation, ascending/descending. For single value calc aggregation:\r\n\r\n```json\r\n"aggs" : {\r\n    "genders" : {\r\n\t\t"terms" : { "field" : "gender", "order": { "avg_price" : "asc" } },\r\n\t\t"aggs" : {\r\n\t\t\t"avg_price" : { "avg" : { "field" : "price" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\nOr, for multi-value calc aggregation:\r\n\r\n```json\r\n"aggs" : {\r\n    "genders" : {\r\n\t\t"terms" : { "field" : "gender", "order": { "price_stats.avg" : "desc" } },\r\n\t\t"aggs" : {\r\n\t\t\t"price_stats" : { "stats" : { "field" : "price" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n#### Range\r\nA field data bucket aggregation that enables the user to define a field on which the bucketing will work and a set of ranges. The aggregator will check each field data value in the current docset context against each bucket range and "bucket" the relevant document & values if they match. Note, that here, not only we\'re bucketing by document, we\'re also bucketing by value. For example, let\'s say we\'re bucketing on multi-value field, and document _D_ has values [1, 2, 3, 4, 5] for the field. In addition, there is a range bucket [ x < 4 ]. When evaluating document _D_, it seems to fall right in this range bucket, but it does so due to field values [1, 2, 3], not because values [4, 5]. Now… if this bucket will also have a sub-aggregators associated with it (say, sum aggregator), the system will make sure to only aggregate values [1, 2, 3] excluding [4, 5] (as 4 and 5 as values, don\'t really belong to this bucket). This is quite different than the other bucket aggregators we\'ve seen until now which mainly focused on whether the document falls in the bucket or not. Here we also keep track of the values belonging to each bucket.\r\n\r\n```json\r\n"aggs" : {\r\n    "age_groups" : {\r\n\t\t"range" : { \r\n\t\t\t"field" : "age",\r\n\t\t\t"ranges" : [\r\n\t\t\t\t{ "to" : 5 },\r\n\t\t\t\t{ "from" : 5, "to" : 10 },\r\n\t\t\t\t{ "from" : 10, "to" : 15 },\r\n\t\t\t\t{ "from" : 15}\r\n\t\t\t]\r\n\t\t},\r\n\t\t"aggs" : {\r\n\t\t\t"avg_height" : { "avg" : { "field" : "height" } }\r\n\t\t}\r\n\t}\r\n}\r\n\r\n```\r\nOutput\r\n\r\n```json\r\n"aggregations" : {\r\n\t"age_groups" : [\r\n\t    {\r\n\t    \t"to" : 5.0,\r\n\t    \t"doc_count" : 10,\r\n\t    \t"avg_height" : 95\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 5.0,\r\n\t    \t"to" : 10.0,\r\n\t    \t"doc_count" : 5,\r\n\t    \t"avg_height" : 130\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 10.0\r\n\t    \t"to" : 15.0,\r\n\t    \t"doc_count" : 4,\r\n\t    \t"avg_height" : 160\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 15.0,\r\n\t    \t"doc_count" : 10,\r\n\t    \t"avg_height" : 175.5\r\n\t    }\r\n\t]\r\n}\r\n```\r\nOf course, you normally don\'t want to store the **age** as a field, but store the birthdate instead. We can use scripts to generate the age:\r\n\r\n```json\r\n"aggs" : {\r\n    "age_groups" : {\r\n\t\t"range" : { \r\n\t\t\t"script" : "DateTime.now().year - doc[\'birthdate\'].date.year",\r\n\t\t\t"ranges" : [\r\n\t\t\t\t{ "to" : 5 },\r\n\t\t\t\t{ "from" : 5, "to" : 10 },\r\n\t\t\t\t{ "from" : 10, "to" : 15 },\r\n\t\t\t\t{ "from" : 15}\r\n\t\t\t]\r\n\t\t},\r\n\t\t"aggs" : {\r\n\t\t\t"avg_height" : { "avg" : { "field" : "height" } }\r\n\t\t}\r\n\t}\r\n}\r\n\r\n```\r\n\r\nAs with all other aggregations, leaving out the **field** from calc aggregator, will fall back on the field by which the range bucketing is done.\r\n\r\n```json\r\n"aggs" : {\r\n    "age_groups" : {\r\n\t\t"range" : { \r\n\t\t\t"field" : "age",\r\n\t\t\t"ranges" : [\r\n\t\t\t\t{ "to" : 5 },\r\n\t\t\t\t{ "from" : 5, "to" : 10 },\r\n\t\t\t\t{ "from" : 10, "to" : 15 },\r\n\t\t\t\t{ "from" : 15}\r\n\t\t\t]\r\n\t\t},\r\n\t\t"aggs" : {\r\n\t\t\t"min" : { "min" : { } },\r\n\t\t\t"max" : { "max" : { } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nOutput\r\n\r\n```json\r\n"aggregations" : {\r\n\t"age_groups" : [\r\n\t    {\r\n\t    \t"to" : 5.0,\r\n\t    \t"doc_count" : 10,\r\n\t    \t"min" : 4.0,\r\n\t    \t"max" : 5.0\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 5.0,\r\n\t    \t"to" : 10.0,\r\n\t    \t"doc_count" : 5,\r\n\t\t\t"min" : 5.0,\r\n\t\t\t"max" : 8.0\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 10.0\r\n\t    \t"to" : 15.0,\r\n\t    \t"doc_count" : 4,\r\n\t    \t"min" : 11.0,\r\n\t    \t"max" : 13.0\r\n\t    },\r\n\t    {\r\n\t    \t"from" : 15.0,\r\n\t    \t"doc_count" : 10,\r\n\t    \t"min" : 15.0,\r\n\t    \t"max" : 22.0\r\n\t    }\r\n\t]\r\n}\r\n```\r\nFurthermore, you can also define a value script which will serve as a transformation to the field data value:\r\n\r\n```json\r\n"aggs" : {\r\n    "age_groups" : {\r\n\t\t"range" : { \r\n\t\t\t"field" : "count",\r\n\t\t\t"script" : "_value - 3"\r\n\t\t\t"ranges" : [\r\n\t\t\t\t{ "to" : 6 },\r\n\t\t\t\t{ "from" : 6 }\r\n\t\t\t]\r\n\t\t},\r\n\t\t"aggs" : {\r\n\t\t\t"min" : { "min" : {} },\r\n\t\t\t"min_count" : { "min" : { "field" : "count" } }\r\n\t\t}\r\n\t}\r\n}\r\n```\r\nOutput\r\n```json\r\n"aggregations": {\r\n    "count_ranges": [\r\n      {\r\n        "to": 6.0,\r\n        "doc_count": 8,\r\n        "min": {\r\n          "value": -2.0\r\n        },\r\n        "min_count": {\r\n          "value": 1.0\r\n        }\r\n      },\r\n      {\r\n        "from": 6.0,\r\n        "doc_count": 2,\r\n        "min": {\r\n          "value": 6.0\r\n        },\r\n        "min_count": {\r\n          "value": 9.0\r\n        }\r\n      }\r\n    ]\r\n  }\r\n```\r\nNotice, the **min** aggregation above acts on the actual values that were used for the bucketing (after the transformation by the script), while the **min_count** aggregation act on the values of the count field that fall within their bucket.\r\n\r\n#### Date Range\r\nA range aggregation that is dedicated for date values. The main difference between this date range agg. to the normal range agg. is that the `from` and `to` values can be expressed in _Date Math_ expressions, and it is also possible to specify a date format by which the `from` and `to` json fields will be returned in the response:\r\n\r\n```json\r\n"aggs": {\r\n    "range": {\r\n        "date_range": {\r\n            "field": "date",\r\n            "format": "MM-yyy",\r\n            "ranges": [\r\n                {\r\n                    "to": "now-10M/M"\r\n                },\r\n                {\r\n                    "from": "now-10M/M"\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nIn the example above, we created two range buckets:\r\n - the first will bucket all documents dated prior to 10 months ago\r\n - the second will bucket all document dated since 10 months ago\r\n\r\n```json\r\n"aggregations": {\r\n    "range": [\r\n        {\r\n            "to": 1.3437792E+12,\r\n            "to_as_string": "08-2012",\r\n            "doc_count": 7\r\n        },\r\n        {\r\n            "from": 1.3437792E+12,\r\n            "from_as_string": "08-2012",\r\n            "doc_count": 2\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\n#### IP Range\r\nJust like the dedicated date range aggregation, there is also a dedicated range aggregation for IPv4 typed fields:\r\n\r\n```json\r\n"aggs" : {\r\n    "ip_ranges" : {\r\n        "ip_range" : {\r\n            "field" : "ip",\r\n            "ranges" : [\r\n                { "to" : "10.0.0.5" },\r\n                { "from" : "10.0.0.5" }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nOutput:\r\n```json\r\n"aggregations": {\r\n    "ip_ranges": [\r\n        {\r\n            "to": 167772165,\r\n            "to_as_string": "10.0.0.5",\r\n            "doc_count": 4\r\n        },\r\n        {\r\n            "from": 167772165,\r\n            "from_as_string": "10.0.0.5",\r\n            "doc_count": 6\r\n        }\r\n    ]\r\n}\r\n```\r\n\r\nIP ranges can also be defined as CIDR masks:\r\n\r\n```json\r\n"aggs" : {\r\n    "ip_ranges" : {\r\n        "ip_range" : {\r\n            "field" : "ip",\r\n            "ranges" : [\r\n                { "mask" : "10.0.0.0/25" },\r\n                { "mask" : "10.0.0.127/25" }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nOutput:\r\n```json\r\n"aggregations": {\r\n    "ip_ranges": [\r\n      {\r\n        "key": "10.0.0.0/25",\r\n        "from": 1.6777216E+8,\r\n        "from_as_string": "10.0.0.0",\r\n        "to": 167772287,\r\n        "to_as_string": "10.0.0.127",\r\n        "doc_count": 127\r\n      },\r\n      {\r\n        "key": "10.0.0.127/25",\r\n        "from": 1.6777216E+8,\r\n        "from_as_string": "10.0.0.0",\r\n        "to": 167772287,\r\n        "to_as_string": "10.0.0.127",\r\n        "doc_count": 127\r\n      }\r\n    ]\r\n}\r\n```\r\n\r\n#### Histogram\r\nAn aggregation that can be applied to numeric fields, and dynamically builds fixed size (a.k.a. interval) buckets over all the values of the document fields in the docset context. For example, if the documents have a field that holds a price (numeric), we can ask this aggregator to dynamically build buckets with interval 5 (in case of `price` it may represent $5). When the aggregation executes, the price field of every document within the aggregation context will be evaluated and will be **rounded** down to its closes bucket - for example, if the price is `32` and the bucket size is `5` then the rounding will yield `30` and thus the document will "fall" into the bucket the bucket that is associated withe the key `30`. To make this more formal, here is the rounding function that is used:\r\n\r\n`bucket_key = value - value % interval`\r\n\r\nA basic histogram aggergation on a single numeric field `value` (maybe be single or multi valued field)\r\n\r\n```json\r\n"aggs" : {\r\n\t"value_histo" : {\r\n        \t"histogram" : {\r\n                \t"field" : "value",\r\n                \t"interval" : 3\r\n        \t}\r\n\t}\r\n}\r\n```\r\n\r\nAn histogram aggregation on multiple fields\r\n\r\n```json\r\n"aggs" : {\r\n\t"value_histo" : {\r\n        \t"histogram" : {\r\n                \t"field" : [ "value", "values" ],\r\n                \t"interval" : 3\r\n        \t}\r\n\t}\r\n}\r\n```\r\n\r\nThe output of the histogram is an array of the buckets, where each bucket holds its key and the number of documents that fall in it. This array can be sorted based on different attributes in an ascending or descending order:\r\n\r\n - `_key` - The buckets will be sorted by their key\r\n - `_count` - The buckets will be sorted by the number of documents that fall in them\r\n - `aggName` - Bucket may hold other aggegations that will be applied to those documents that fall in them. It is possible to sort the buckets based on direct single-valued **calc** aggregations that they hold\r\n - `aggName` & `valueName` - It is also possible to sort buckets based on direct multi-valued **calc** aggregations that they hold\r\n\r\nSorting by bucket `key` descending\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "field" : "value",\r\n            "interval" : 3,\r\n            "order" : { "_key" : "desc" }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nSorting by document count ascending\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "field" : "value",\r\n            "interval" : 3,\r\n            "order" : { "_count" : "asc" }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAdding a sum aggregation (which is a single valued calc aggregation) to the buckets and sorting by it\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "field" : "value",\r\n            "interval" : 3,\r\n            "order" : { "value_sum" : "asc" }\r\n        },\r\n        "aggs" : {\r\n            "value_sum" : { "sum" : {} }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nAdding a stats aggregation (which is a multi-valued calc aggregation) to the buckets and sorting by the avg\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "field" : "value",\r\n            "interval" : 3,\r\n            "order" : { "value_stats.avg" : "desc" }\r\n        },\r\n        "aggs" : {\r\n            "value_stats" : { "stats" : {} }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nUsing value scripts to "preprocess" the values before the bucketing\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "field" : "value",\r\n            "script" : "_value * 4",\r\n            "interval" : 3,\r\n            "order" : { "sum" : "desc"}\r\n        },\r\n        "aggregations" : {\r\n            "sum" : { "sum" : {} }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIt\'s also possible to use document level scripts to compute the value by which the documents will be "bucketted"\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "histogram" : {\r\n            "script" : "doc[\'value\'].value + doc[\'value2\'].value",\r\n            "interval" : 3,\r\n            "order" : { "stats.sum" : "desc" }\r\n        },\r\n        "aggregations" : {\r\n            "stats" : { "stats" : {} }\r\n        }\r\n    }\r\n}\r\n```\r\nOutput:\r\n```json\r\n"aggregations": {\r\n  "histo": [\r\n    {\r\n      "key": 21,\r\n      "doc_count": 2,\r\n      "stats": {\r\n        "count": 2,\r\n        "min": 8.0,\r\n        "max": 9.0,\r\n        "avg": 8.5,\r\n        "sum": 17.0\r\n      }\r\n    },\r\n    {\r\n      "key": 15,\r\n      "doc_count": 2,\r\n      "stats": {\r\n        "count": 2,\r\n        "min": 5.0,\r\n        "max": 6.0,\r\n        "avg": 5.5,\r\n        "sum": 11.0\r\n      }\r\n    },\r\n    {\r\n      "key": 24,\r\n      "doc_count": 1,\r\n      "stats": {\r\n        "count": 1,\r\n        "min": 10.0,\r\n        "max": 10.0,\r\n        "avg": 10.0,\r\n        "sum": 10.0\r\n      }\r\n    },\r\n    {\r\n      "key": 18,\r\n      "doc_count": 1,\r\n      "stats": {\r\n        "count": 1,\r\n        "min": 7.0,\r\n        "max": 7.0,\r\n        "avg": 7.0,\r\n        "sum": 7.0\r\n      }\r\n    },\r\n    {\r\n      "key": 9,\r\n      "doc_count": 2,\r\n      "stats": {\r\n        "count": 2,\r\n        "min": 2.0,\r\n        "max": 3.0,\r\n        "avg": 2.5,\r\n        "sum": 5.0\r\n      }\r\n    },\r\n    {\r\n      "key": 12,\r\n      "doc_count": 1,\r\n      "stats": {\r\n        "count": 1,\r\n        "min": 4.0,\r\n        "max": 4.0,\r\n        "avg": 4.0,\r\n        "sum": 4.0\r\n      }\r\n    },\r\n    {\r\n      "key": 6,\r\n      "doc_count": 1,\r\n      "stats": {\r\n        "count": 1,\r\n        "min": 1.0,\r\n        "max": 1.0,\r\n        "avg": 1.0,\r\n        "sum": 1.0\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n#### Date Histogram\r\n\r\nDate histogram is a similar aggregation to the normal histogram (as described above) except that it can only work on date fields. Since dates are indexed internally as long values, it\'s possible to use the normal histogram on dates as well, but problem though stems in the fact that time based intervals are not fixed (think of leap years and on the number of days in a month). For this reason, we need a spcial support for time based data. From functionality perspective, this historam supports the same features as the normal histogram. The main difference though is that the interval can be specified by time expressions.\r\n\r\nBuilding a month length bucket intervals\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "date_histogram" : {\r\n            "field" : "date",\r\n            "interval" : "month"\r\n        }\r\n    }\r\n}\r\n```\r\nor based on 1.5 months\r\n\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "date_histogram" : {\r\n            "field" : "date",\r\n            "interval" : "1.5M"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nOther available expressions for interval: `year`, `quarter`, `week`, `day`, `hour`, `minute`, `second`\r\n\r\nSince internally, dates are represented as 64bit numbers, these numbers are returned as the bucket keys (each key representing a date). For this reason, it is also possible to define a date format, which will result in returning the dates as formatted strings next to the numeric key values:\r\n\r\n```json\r\n"aggs" : {\r\n    "histo" : {\r\n        "date_histogram" : {\r\n            "field" : "date",\r\n            "interval" : "1M",\r\n            "format" : "yyyy-MM-dd"\r\n        }\r\n    }\r\n}\r\n```\r\nOutput:\r\n```json\r\n"aggregations": {\r\n    "histo": [\r\n        {\r\n          "key_as_string": "2012-02-02",\r\n          "key": 1328140800000,\r\n          "doc_count": 1\r\n        },\r\n        {\r\n          "key_as_string": "2012-03-02",\r\n          "key": 1330646400000,\r\n          "doc_count": 2\r\n        },\r\n        ...\r\n    ]\r\n}\r\n```\r\nTimezones are also supported, enabling the user to define by which timezone they\'d like to bucket the documents (this support is very similar to the TZ support in the DateHistogram facet).\r\n\r\nSimilar to the current date histogram facet, pref_offset & post_offset will are also supported, for offsets applying pre rounding and post rounding. The values are time values with a possible `-` sign. For example, to offset a week rounding to start on Sunday instead of Monday, one can pass pre_offset of -1d to decrease a day before doing the week (monday based) rounding, and then have post_offset set to -1d to actually set the return value to be Sunday, and not Monday.\r\n\r\nLike with the normal histogram, both document level scripts and value scripts are supported. It is possilbe to control the order of the buckets that are returned. And of course, nest other aggregations within the buckets.\r\n\r\nBoth the normal `histogram` and the `date_histogram` now support computing/returning empty buckets. This can be controlled by setting the `compute_empty_buckets` parameter to `true` (defaults to `false`). \r\n\r\n\r\n#### Geo Distance\r\n\r\nAn aggregation that works on `geo_point` fields. Conceptually, it works very similar to range aggregation. The user can define a point of `origin` and a set of distance range buckets. The aggregation evaluate the distance of each document from the `origin` point and determine the bucket it belongs to based on the ranges (a document belongs to a bucket if the distance between the document and the `origin` falls within the distance range of the bucket).\r\n\r\n```json\r\n"aggs" : {\r\n    "rings" : {\r\n        "geo_distance" : {\r\n            "field" : "location",\r\n            "origin" : "52.3760, 4.894",\r\n            "ranges" : [\r\n                { "to" : 100 },\r\n                { "from" : 100, "to" : 300 },\r\n                { "from" : 300 }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nOutput\r\n```json\r\n"aggregations": {\r\n  "rings": [\r\n    {\r\n      "unit": "km",\r\n      "to": 100.0,\r\n      "doc_count": 3\r\n    },\r\n    {\r\n      "unit": "km",\r\n      "from": 100.0,\r\n      "to": 300.0,\r\n      "doc_count": 1\r\n    },\r\n    {\r\n      "unit": "km",\r\n      "from": 300.0,\r\n      "doc_count": 7\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\nThe specified `field` must be of type `geo_point` (which can only be set explicitly in the mappings). And it can also hold an array of `geo_point` fields, in which case all will be taken into account during aggregation. The `origin` point can accept all format `geo_point` supports:\r\n\r\n - Object format: `{ "lat" : 52.3760, "lon" : 4.894 }` - this is the safest format as it\'s the most explicit about the `lat` & `lon` values\r\n - String format: `"52.3760, 4.894"` - where the first number is the `lat` and the second is the `lon`\r\n - Array format: `[4.894, 52.3760]` - which is based on the GeoJson standard and where the first number is the `lon` and the second one is the `lat`\r\n\r\nBy default, the distance unit is `km` but it can also accept: `mi` (miles), `in` (inch), `yd` (yards), `m` (meters), `cm` (centimeters), `mm` (millimeters).\r\n\r\n```json\r\n"aggs" : {\r\n    "rings" : {\r\n        "geo_distance" : {\r\n            "field" : "location",\r\n            "origin" : "52.3760, 4.894",\r\n            "unit" : "mi",\r\n            "ranges" : [\r\n                { "to" : 100 },\r\n                { "from" : 100, "to" : 300 },\r\n                { "from" : 300 }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThere are two distance calculation modes: `arc` (the default) and `plane`. The `arc` calculation is the most accurate one but also the more expensive one in terms of performance. The `plane` is faster but less accurate. Consider using `plane` when your search context is narrow smaller areas (like cities or even countries). `plane` may return higher error mergins for searches across very large areans (e.g. cross atlantic search).\r\n\r\n```json\r\n"aggs" : {\r\n    "rings" : {\r\n        "geo_distance" : {\r\n            "field" : "location",\r\n            "origin" : "52.3760, 4.894",\r\n            "distance_type" : "plane",\r\n            "ranges" : [\r\n                { "to" : 100 },\r\n                { "from" : 100, "to" : 300 },\r\n                { "from" : 300 }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n#### Nested\r\nA special single bucket aggregation which enables aggregating nested documents:\r\n\r\nassuming the following mapping:\r\n\r\n```json\r\n"type" : {\r\n        "properties" : {\r\n            "nested" : { "type" : "nested" }\r\n        }\r\n    }\r\n}\r\n```\r\nHere\'s how a nested aggregation can be defined:\r\n\r\n```json\r\n"aggs" : {\r\n    "nested_value_stats" : {\r\n        "nested" : {\r\n            "path" : "nested"\r\n        },\r\n        "aggs" : {\r\n            "stats" : {\r\n                "stats" : { "field" : "nested.value" }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\nAs you can see above, the nested aggregation requires the path of the nested documents within the top level documents. Then one can define any type of aggregation over these nested documents.\r\n\r\nOutput:\r\n\r\n```json\r\n"aggregations": {\r\n    "employees_salaries": {\r\n        "doc_count": 25,\r\n        "stats": {\r\n            "count": 25,\r\n            "min": 1.0,\r\n            "max": 9.0,\r\n            "avg": 5.0,\r\n            "sum": 125.0\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n### Examples\r\n\r\n#### Filter + Range + Missing + Stats\r\nAnalyse the online product catalog web access logs. The following aggregation will only aggregate those logs from yesterday (the **filter** aggregation), providing information for different price ranges (the **range** aggregation), where per price range we\'ll return the price stats on that range and the total page views for those documents in the each range. We\'re also interested in finding all the bloopers - all those products that for some reason don\'t have prices associated with them and still they are exposed to the user and being accessed and viewed.\r\n\r\n```json\r\n"aggs" : {\r\n\t"yesterday" : {\r\n\t\t"filter" : { "range" : { "date" { "gt" : "now-1d/d", "lt" : "now/d" } } },\r\n\t\t"aggs" : {\r\n\t\t    "missing_price" : {\r\n\t\t    \t"missing" : { "field" : "price" },\r\n\t\t    \t"aggs" : {\r\n\t\t    \t\t"total_page_views" : { "sum" : { "field" : "page_views" } }\r\n\t\t    \t}\r\n\t\t    },\r\n\t\t\t"prices" : {\r\n\t\t\t\t"range" : {\r\n\t\t\t\t\t"field" : "price",\r\n\t\t\t\t\t"ranges" : [\r\n\t\t\t\t\t\t{ "to" : 100 },\r\n\t\t\t\t\t\t{ "from" : 100, "to" : 200 },\r\n\t\t\t\t\t\t{ "from" : 200, "to" 300 },\r\n\t\t\t\t\t\t{ "from" : 300 }\r\n\t\t\t\t\t]\r\n\t\t\t\t},\r\n\t\t\t\t"aggs" : {\r\n\t\t\t\t\t"price_stats" : { "stats" : {} },\r\n\t\t\t\t\t"total_page_views" : { "sum" : { "field" : "page_views" } }\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```\r\n\r\n#### Aggregating Hierarchical Data\r\nQuite often you\'d like to get aggregations on location in an hierarchical manner. For example, show all countries and how many documents fall within each country, and for each country show a breakdown by city. Here\'s a simple way to do it using hierarchical terms aggregations:\r\n\r\n```json\r\n"aggs" : {\r\n\t"country" : {\r\n\t\t"terms" : { "field" : "country" },\r\n\t\t"aggs" : {\r\n\t\t\t"city" : {\r\n\t\t\t\t"terms" : { "field" : "city" }\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}\r\n```'
3298,'martijnvg','Cannot create templates for _percolator indexes\nI posted an issue to the Google Group a few months ago:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/ZMG6wrN8ViM\r\n\r\nBasically, setting `index.mapper.dynamic` to false breaks the Percolators, because the types do not created automatically.\r\n\r\n@imotov suggested that a fix would be to create a template to override the value of `index.mapper.dynamic` back to true:\r\n\r\n> You can add a template for the _percolator index that would override the setting. All you need to do is to create file config/templates/percolator_template.json with the following content:\r\n\r\n```\r\n{\r\n    "percolator_template" : {\r\n        "template" : "_percolator",\r\n        "settings" : {\r\n            "index.mapper.dynamic": true\r\n        }\r\n}\r\n```\r\n\r\nHowever, it turns out that this file is not being read by Elastic Search. Trying to add the template manually results in the error "InvalidIndexTemplateException[index_template [percolator_test] invalid, cause [template must not start with \'_\']]".\r\n\r\nIt looks like this is [returned by the MetaDataCreateIndexService](https://github.com/elasticsearch/elasticsearch/blob/0.90/src/main/java/org/elasticsearch/cluster/metadata/MetaDataIndexTemplateService.java#L160). Is this correct? Is there a valid reason why templates cannot be created that refer to the percolator index? We\'ve tried to \'hack around\' the validation by using a regular expression but that didn\'t seem to work.\r\n\r\nIt\'s worth noting that https://github.com/elasticsearch/elasticsearch/issues/788 would also fix this problem for us, and in a nicer way.\r\n\r\nIt would be good to get some feedback on this as it means we have to manually intervene every time a new index is created that needs percolators.\r\n\r\n(Additionally, it\'s not clear from the documentation that the templates are only read when the index is originally created. If I get a chance I will put in a pull request against the docs.)'
3296,'spinscale',"Set max open files for systemd startup\n...elasticsearch\r\n\r\nNote that EnfironmentFile isn't loaded in time for:\r\n\r\nLimitNOFILE=$MAX_OPEN_FILES\r\n\r\nto work. Without this elasticsearch has the system default limit (likely\r\n1-4K)."
3295,'spinscale','Could not create TransportClient object\nHi Guys\r\n\r\nHere\'s java code:\r\n\r\n        final Settings settings = ImmutableSettings.settingsBuilder()\r\n                .put("cluster.name", "elasticsearch")\r\n                .put("client.transport.sniff", false)\r\n                .put("client", true)\r\n                .put("data", true)\r\n                .build();\r\n\r\n        final TransportClient client = new TransportClient(settings);\r\nabove line throws following exception:\r\n\r\nException in thread "main" java.lang.NoSuchMethodError: org.apache.lucene.util.UnicodeUtil.UTF16toUTF8(Ljava/lang/CharSequence;IILorg/apache/lucene/util/BytesRef;)V\r\n\tat org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1502)\r\n\tat org.elasticsearch.common.Strings.toUTF8Bytes(Strings.java:1498)\r\n\tat org.elasticsearch.search.facet.filter.InternalFilterFacet.<clinit>(InternalFilterFacet.java:40)\r\n\tat org.elasticsearch.search.facet.TransportFacetModule.configure(TransportFacetModule.java:40)\r\n\tat org.elasticsearch.common.inject.AbstractModule.configure(AbstractModule.java:60)\r\n\tat org.elasticsearch.common.inject.spi.Elements$RecordingBinder.install(Elements.java:201)\r\n\tat org.elasticsearch.common.inject.spi.Elements.getElements(Elements.java:82)\r\n\tat org.elasticsearch.common.inject.InjectorShell$Builder.build(InjectorShell.java:130)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:99)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)\r\n\tat org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:177)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:119)\r\n\tat com.intentline.elasticsearch.ElasticSearchHelper.initDashboardIndexClient(ElasticSearchHelper.java:37)\r\n\r\nFYI, I\'m using lucene-core-4.3.1.jar and elasticsearch-0.90.2.jar'
3290,'martijnvg','The top_children, has_child and has_parent query can cause error when cached.\nIn the case that the `top_children`, `has_child` or `has_parent` queries are cached via the `fquery` filter then an class cast exception error occurs. '
3288,'bleskes','Partial fields filtering may return false matches and doesn\'t allow selecting complete objects\nTo reproduce:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/test/type1/1" -d\'\r\n{\r\n   "field": "value",\r\n   "array" : [ 1 , 2],\r\n   "obj" : {\r\n       "field": "value"\r\n   }\r\n}\'\r\n```\r\n\r\nThen search:\r\n\r\n```\r\ncurl -XPOST "http://localhost:9200/test/_search" -d\'\r\n{\r\n  "partial_fields": {\r\n     "obj_selection": {\r\n        "include": [ "obj" ]\r\n     },\r\n     "match_problem": {\r\n        "include": [ "field_which_doesnt_exist" ]\r\n     }\r\n  }\r\n}\'\r\n```\r\n\r\nResults in:\r\n\r\n```\r\n{\r\n  ...\r\n   "hits": {\r\n      "total": 1,\r\n      "max_score": 1,\r\n      "hits": [\r\n         {\r\n            ...\r\n            "fields": {\r\n               "obj_selection": {}, // not selected.\r\n               "match_problem": {  // should be empty\r\n                  "field": "value"\r\n               }\r\n            }\r\n         }\r\n      ]\r\n   }\r\n}\r\n```'
3287,'markharwood','Number of results from fuzzy_like_this_field influence "good" results being returned\nHi,\r\n\r\nSorry for the lousy title, but let me explain the problem I\'m having.  I\'m currently working on a project, and a part of this includes a search engine for addresses, which I have in elasticsearch.  What I\'m trying to do is use fuzzy_like_this_field queries when a new character is entered in my search bar to generate autocomplete results and try to "guess" which of the (~1 million) addresses the user is typing.\r\n\r\nMy issue is that I currently have a size limit on my query, as returning all of the results was both unnecessary and expensive, time-wise.  My issue, is that I often am not getting the "correct" result unless I return 1000 or more results from the query.  For example, if I enter "100 broad" in trying to search for "100 broadway" and I only return 200 results (about the max that I can do without it taking too long), 100 broadway is nowhere to be found, even though all of the returned results have a higher levenshtein distance than the result that I want.  I get "100 broadway" as the first result if I return 2000 results from my query, but it takes too long.  I can\'t even filter the results that got returned to bring the correct one to the top, because it\'s not being returned.  \r\n\r\nShouldn\'t putting a size limit of N on the query return the best N results, not a seemingly random subset of them?\r\n\r\n\r\nSorry if this is poorly worded or too vague.'
3284,'spinscale','Debian package dependencies can result in java uninstallation\nThe discussion happened at https://github.com/elasticsearch/elasticsearch/pull/3105\r\n\r\nThe Java dependency was changed from "Depends" to "Suggest" - which sounded good, but could result in uninstallation of java, in case no other package depended on java.\r\n\r\nMaybe the \'Enhances\' flag can help here, need to investigate, see https://github.com/elasticsearch/elasticsearch/pull/3105#issuecomment-20370219\r\n\r\n'
3283,'bleskes','Incorrect response on concurrent indexing of the same document.\nWhen doing concurrent re-indexing of an existing document with the version parameter, sometimes the response has status 200 and is neither a success response (with "ok": true), nor a conflict.\r\n\r\nThe following are the relevant requests/responses when keeping a counter in a document and running 10 parallel processes to increment it:\r\n\r\n    curl -XPUT \'http://localhost:9200/test_storage_index/storage/key?version=22158&pretty=true\' -d \'{"value": "113"}\'\r\n    Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22158,"exists":true, "_source" : {"value": "112"}}\r\n    ...\r\n    curl -XPUT \'http://localhost:9200/test_storage_index/storage/key?version=22367&pretty=true\' -d \'{"value": "322"}\'\r\n    Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22367,"exists":true, "_source" : {"value": "321"}}\r\n    ...\r\n    curl -XPUT \'http://localhost:9200/test_storage_index/storage/key?version=22500&pretty=true\' -d \'{"value": "455"}\'\r\n    Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22501,"exists":true, "_source" : {"value": "455"}}\r\n    ...\r\n    curl -XPUT \'http://localhost:9200/test_storage_index/storage/key?version=22771&pretty=true\' -d \'{"value": "726"}\'\r\n    Got 200: {"_index":"test_storage_index","_type":"storage","_id":"key","_version":22771,"exists":true, "_source" : {"value": "725"}}\r\n\r\nFrom the final value of the counter, it seems that in these cases sometimes the indexing operation goes through, sometimes it doesn\'t.\r\nThis happens on ES version 0.90.2.\r\n'
3276,'javanna','Parent is ignored in exists request\nThe `parent` param is ignored in `HEAD` requests on `/index/type/id`:\r\n\r\n    curl -XPUT \'localhost:9200/test_1?pretty=1\'  -d \'\r\n    {\r\n       "mappings" : {\r\n          "test" : {\r\n             "_parent" : {\r\n                "type" : "foo"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'localhost:9200/_cluster/health?wait_for_status=yellow&pretty=1\'\r\n\r\n    curl -XPUT \'localhost:9200/test_1/test/1?parent=5&pretty=1\'  -d \'\r\n    {\r\n       "foo" : "bar"\r\n    }\r\n    \'\r\n\r\n    curl -XHEAD \'localhost:9200/test_1/test/1?parent=5&pretty=1\'\r\n\r\n    # [Mon Jul  1 14:50:27 2013] ERROR: Not Found (404)\r\n    #\r\n'
3275,'s1monw','Update HighlightBuilder.Field API, it should allow for the same API as S...\n...earchConstextHighlight.Field. In other words, what is possible to setup using DSL in highlighting at the field level is also possible via the Java API.\r\nBTW we would like to use this :-)'
3274,'spinscale','Mget: no support for "parent"\nThe `parent` param is not supported by `mget` either at the top-level or at the per-document level  (while `routing` is):\r\n\r\n    curl -XPUT \'localhost:9200/test_1?pretty=1\'  -d \'\r\n    {\r\n       "mappings" : {\r\n          "test" : {\r\n             "_parent" : {\r\n                "type" : "foo"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'localhost:9200/_cluster/health?wait_for_status=yellow&pretty=1\'\r\n\r\n    curl -XPUT \'localhost:9200/test_1/test/1?parent=4&pretty=1\'  -d \'\r\n    {\r\n       "foo" : "bar"\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'localhost:9200/test_1/test/_mget?pretty=1\'  -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "parent" : "4",\r\n             "_id" : "1"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "docs" : [\r\n    #       {\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : false\r\n    #       }\r\n    #    ]\r\n    # }\r\n\r\n\r\n    curl -XGET \'localhost:9200/test_1/test/_mget?parent=4&pretty=1\'  -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "_id" : "1"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "docs" : [\r\n    #       {\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : false\r\n    #       }\r\n    #    ]\r\n    # }\r\n\r\n'
3273,'spinscale','Parsing fields in 0.90.2 is changed.\nSomething have changed in the way search requests are parsed. Haven\'t had time to look into the code for it. Can do that later tonight if no one is jumps on this.\r\nThis query works as expected\r\n```json\r\n{    \r\n    "fields": [\r\n        "Title"\r\n    ],\r\n    "query": {\r\n        "query_string": {\r\n            "query": "Banana"\r\n        },\r\n     "language":"en"\r\n    }\r\n}\r\n```\r\nThis query ignores the field param and returns `_source`\r\n```json\r\n{   \r\n    "query": {\r\n        "query_string": {\r\n            "query": "Banana"\r\n        },\r\n     "language":"en"\r\n    }, \r\n    "fields": [\r\n        "Title"\r\n    ],\r\n}\r\n```\r\n\r\nShould be the same query. Have previously been the same query. '
3271,'spinscale','Supports mget fields parameter given as string.\nCloses #3270'
3270,'spinscale','Mget: the fields parameter should accept a string\nThe fields parameter in the `mget` API only accepts an array of field names - it should accept a single string, like it does for the `search` API:\r\n\r\n    curl -XPUT \'localhost:9200/test_1/test/1?pretty=1\'  -d \'\r\n    {\r\n       "foo" : "bar"\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'localhost:9200/_cluster/health?wait_for_status=yellow&pretty=1\'\r\n\r\n    curl -XGET \'localhost:9200/test_1/test/_mget?pretty=1\'  -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "fields" : "foo",\r\n             "_id" : "1"\r\n          },\r\n          {\r\n             "fields" : [\r\n                "foo"\r\n             ],\r\n             "_id" : "1"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "docs" : [\r\n    #       {\r\n    #          "_source" : {\r\n    #             "foo" : "bar"\r\n    #          },\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : true,\r\n    #          "_version" : 1\r\n    #       },\r\n    #       {\r\n    #          "fields" : {\r\n    #             "foo" : "bar"\r\n    #          },\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : true,\r\n    #          "_version" : 1\r\n    #       }\r\n    #    ]\r\n    # }\r\n'
3268,'javanna','Fix search shards count method when targeting concrete and aliased indices\nThis is related to the same example as issue #2682.\r\n\r\nThis bug can lead to a forced query_and_fetch search type which might not be the expected behavior.\r\n\r\nI propose to fix it this way because it is a quick 2 lines patch. But maybe we should refactor a bit and add a shared method for searchShards and searchShardsCount to avoid divergent behavior in the future.\r\n\r\nReading the code it seems like searchShardsCount is mainly implemented to test if we are hitting 1 shard or more than 1 shards so maybe it could be renamed and optimized for that specific use case.\r\n\r\nFeel free to fix the problem otherwise or to tell me what do you prefer and I will be happy to change this pull request.'
3267,'spinscale','Mget aborting request if index missing\n`mget` returns an error for each doc if the type or id is not found, but throws a top-level error if the index is not found.  This seems inconsistent:\r\n\r\n    curl -XPUT \'localhost:9200/test_1/test/1?pretty=1\'  -d \'\r\n    {\r\n       "foo" : "bar"\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'localhost:9200/_mget?pretty=1\'  -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "2",\r\n             "_type" : "test"\r\n          },\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "1",\r\n             "_type" : "none"\r\n          },\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "1",\r\n             "_type" : "test"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "docs" : [\r\n    #       {\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "2",\r\n    #          "_type" : "test",\r\n    #          "exists" : false\r\n    #       },\r\n    #       {\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "none",\r\n    #          "exists" : false\r\n    #       },\r\n    #       {\r\n    #          "_source" : {\r\n    #             "foo" : "bar"\r\n    #          },\r\n    #          "_index" : "test_1",\r\n    #          "_id" : "1",\r\n    #          "_type" : "test",\r\n    #          "exists" : true,\r\n    #          "_version" : 1\r\n    #       }\r\n    #    ]\r\n    # }\r\n\r\n    curl -XGET \'localhost:9200/_mget?pretty=1\'  -d \'\r\n    {\r\n       "docs" : [\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "2",\r\n             "_type" : "test"\r\n          },\r\n          {\r\n             "_index" : "test_2",\r\n             "_id" : "1",\r\n             "_type" : "test"\r\n          },\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "1",\r\n             "_type" : "none"\r\n          },\r\n          {\r\n             "_index" : "test_1",\r\n             "_id" : "1",\r\n             "_type" : "test"\r\n          }\r\n       ]\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "status" : 404,\r\n    #    "error" : "IndexMissingException[[test_2] missing]"\r\n    # }\r\n\r\n\r\n'
3266,'martijnvg',"Don't ignore doc_as_upsert value and simplify. Closes #3265\nThis simplifies the code and should close #3265"
3265,'martijnvg','The behavior of doc_as_upsert is incorrect\nHere is the gist recreation\r\n\r\n* First some cleaning\r\n```bash\r\n$ curl -XDELETE \'localhost:9200/index1\'\r\n```\r\n\r\n* This insert the document where it shouldn\'t\r\n```bash\r\n$ curl -XPOST \'localhost:9200/index1/test1/1/_update\' -d \'\r\n{\r\n  doc : { field : "value" },\r\n  doc_as_upsert : false \r\n}\'\r\n```\r\n\r\n* Changing the order change the behavior of the param\r\n```bash\r\n$ curl -XPOST \'localhost:9200/index1/test1/2/_update\' -d \'\r\n{\r\n  doc_as_upsert : false,\r\n  doc : { field : "value" }\r\n}\'\r\n```'
3264,'spinscale',"custom query parser registration\n\r\nHi,\r\n\r\n\r\nIndicesQueriesModule's addQuery and addFilter method's signatures seems weird..\r\nI can not register my custom parser's class..\r\n\r\nThey should be changed to  Class<? extends QueryParser> and Class<? extends FilterParser> respectively..\r\n\r\n    public synchronized IndicesQueriesModule addQuery(Class<QueryParser> queryParser) {\r\n        queryParsersClasses.add(queryParser);\r\n        return this;\r\n    }\r\n\r\n  \r\n    public synchronized IndicesQueriesModule addFilter(Class<FilterParser> filterParser) {\r\n        filterParsersClasses.add(filterParser);\r\n        return this;\r\n    }\r\n\r\nthanks."
3262,'drewr',"unicast.hosts has to seed enough nodes to satisfy minimum_master_nodes\nIf you have `minimum_master_nodes` set and you don't supply enough nodes in `d.z.p.unicast.hosts`, a node can fail to join the cluster. Described at http://thread.gmane.org/gmane.comp.search.elasticsearch.user/740.\r\n"
3260,'spinscale',"suggest api options enhancement\nHi,\r\n\r\nis it possible to extend suggest API's Option class so that custom suggesters return arbitrary values rather than just text and score?\r\n\r\nFor example I want to create a custom suggester that returns:\r\n- suggested text\r\n- highligted version of the suggested text\r\n- A few concrete products (promotions) for each suggested text.\r\n\r\n\r\nMy purposed solution:\r\n- Define an interface (StreamableToXContent) that extends both Streamable and ToXContent\r\n- Modify Option.class as fallows:\r\n                \r\n\tprivate Text text;\r\n\tprivate Text label; //this is presented to the user\r\n\tprivate float score;\r\n\tprivate StreamableToXContent data; //this is for arbitrary data\r\n\r\nany thoughts?\r\n\r\nregards..\r\n "
3258,'s1monw','Add a score_mode to the rescore\nI started working on adding a score_mode parameter to the rescore query, something like :\r\n\r\n```json\r\n{\r\n    "rescore" : {\r\n      "window_size" : 50,\r\n      "query" : {\r\n         "rescore_query" : {\r\n            "match" : {\r\n               "field1" : {\r\n                  "query" : "the quick brown",\r\n                  "type" : "phrase",\r\n                  "slop" : 2\r\n               }\r\n            }\r\n         },\r\n         "query_weight" : 0.7,\r\n         "rescore_query_weight" : 1.2,\r\n         "score_mode" : "multiply"\r\n      }\r\n   }\r\n}\r\n```\r\n\r\nDefault is "total", possible values are : avg, max, min, total and multiply.\r\n\r\nMy use case is for the "multiply", because I want to do something like:\r\n\r\n```json\r\n{\r\n   "query" : {\r\n      // lot of complex queries\r\n      // default scoring\r\n   },\r\n   "rescore" : {\r\n      "window_size" : 500,\r\n      "query" : {\r\n         "rescore_query" : {\r\n            "custom_score" : {\r\n              "query" : { "match_all" : { } },\r\n              "script" : "complex_scoring",\r\n              "lang" : "native"\r\n            }\r\n          },\r\n         "query_weight" : 1.0,\r\n         "rescore_query_weight" : 1.2\r\n      }\r\n   }\r\n}\r\n```\r\nSo rather than having to duplicate my complex query in the rescore query, I just multiply the first score with the second one.\r\n\r\nWhat do you think? I will post my code.'
3257,'martijnvg','The parent option is ignored in delete requests\nThe `parent` option is ignored in the delete api (rest only) and for delete actions in the bulk api.\r\nThis bug occurs in the case that the `_parent` field enabled in a mapping, and only the `parent` option is used. This results in a situation that documents are deleted even if the specified parent value is incorrect. \r\n\r\nIn the case that routing is required and no routing is specified for a delete request, then the delete is executed on all shards. The also applies when the `_parent` field is configured on a mapping for a type the delete is executed. The parent id also acts as a routing value.\r\n\r\nCurrent work around is to not use the `parent` option and just use the `routing` option, which has the same effect.\r\n\r\nTest case:\r\n```bash\r\ncurl -XDELETE \'localhost:9200/test%2A?pretty=1\'\r\n \r\ncurl -XPUT \'localhost:9200/test_1?pretty=1\'  -d \'\r\n{\r\n   "mappings" : {\r\n      "test" : {\r\n         "_parent" : {\r\n            "type" : "foo"\r\n         }\r\n      }\r\n   }\r\n}\r\n\'\r\n \r\ncurl -XGET \'localhost:9200/_cluster/health?wait_for_status=yellow&pretty=1\'\r\n \r\ncurl -XPUT \'localhost:9200/test_1/test/1?parent=3&pretty=1\'  -d \'\r\n{\r\n   "foo" : "bar"\r\n}\r\n\'\r\n \r\ncurl -XDELETE \'localhost:9200/test_1/test/1?parent=2&pretty=1\'\r\n \r\n# {\r\n#    "ok" : true,\r\n#    "_index" : "test_1",\r\n#    "_id" : "1",\r\n#    "_type" : "test",\r\n#    "found" : true,\r\n#    "_version" : 2\r\n# }\r\n \r\ncurl -XDELETE \'localhost:9200/test_1/test/1?parent=3&pretty=1\'\r\n \r\n# {\r\n#    "ok" : true,\r\n#    "_index" : "test_1",\r\n#    "_id" : "1",\r\n#    "_type" : "test",\r\n#    "found" : false,\r\n#    "_version" : 0\r\n# }\r\n```'
3253,'dadoonet','NPE in PluginManager when asking for list on non existing dir\nAsking for list of installed plugins with no existing plugin dir:\r\n\r\n```sh\r\n$ bin/plugin --list\r\n```\r\n\r\nIt causes a NPE in PluginManager.'
3252,'jpountz','Error on MoreLikeThis API with Non Stored Numeric Fields\nAccording to the documentation:\r\n\r\nNote: In order to use the mlt feature a mlt_field needs to be either be stored, store term_vector or source needs to be enabled.\r\n\r\n\r\nBut,running this:\r\n```\r\ncurl -XPOST http://localhost:9200/foo\r\ncurl -XPUT http://localhost:9200/foo/bar/_mapping -d \'{ "bar": { "dynamic": "strict", "properties": { "id": { "type": "integer", "index": "not_analyzed" }, "content": { "type": "string", "analyzer": "standard" }}}}\'\r\n\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{"id":1, "content":"foo bar foo2 bar2 foo3 bar3"}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/2 -d \'{"id":2, "content":"foo3 bar3 foo4 bar4"}\'\r\n\r\n\r\ncurl -XGET \'http://localhost:9200/foo/bar/1/_mlt?mlt_fields=content&min_term_freq=1&min_doc_freq=1\'\r\ncurl -XGET \'http://localhost:9200/foo/bar/1/_mlt?min_term_freq=1&min_doc_freq=1\'\r\n```\r\n\r\nfails(second query) with:\r\n{"error":"MapperParsingException[failed to parse [id]]; nested: ElasticSearchIllegalStateException[Field should have either a string, numeric or binary value]; ","status":400}\r\n\r\nThis is basically because here(for example):\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/mapper/core/IntegerFieldMapper.java#L356-L360\r\n\r\nThe numeric value is not actually used unless the field is stored.\r\n\r\nThen here:\r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/action/mlt/TransportMoreLikeThisAction.java#L293-L303\r\n\r\nif you can\'t read it, it will just thrown an exception.\r\n'
3247,'spinscale','Invalid "ElasticSearchIllegalArgumentException[The required text option is missing]" Error\nIf I precisely follow the docs & send the following, I get a successful request (200).\r\n\r\n    curl -XPOST \'localhost:9200/_search\' -d \'{\r\n      "query": {\r\n        "query_string": {\r\n            "query": "hello"\r\n        }\r\n      },\r\n      "suggest": {\r\n        "suggest": {\r\n          "text": "*:*",\r\n          "term": {\r\n            "field": "_all"\r\n          }\r\n        }\r\n      }\r\n    }\'\r\n\r\nHowever, if I just change the ordering of the ``text`` & ``term`` keys, I get the "Required text option is missing error" (& a resulting HTTP 500), even though it is present in the JSON. AFAICT, the ordering within the JSON object shouldn\'t matter.\r\n\r\n    curl -XPOST \'localhost:9200/_search\' -d \'{\r\n      "query": {\r\n        "query_string": {\r\n            "query": "hello"\r\n        }\r\n      },\r\n      "suggest": {\r\n        "suggest": {\r\n          "term": {\r\n            "field": "_all"\r\n          },\r\n          "text": "*:*"\r\n        }\r\n      }\r\n    }\''
3246,'spinscale','Index Warmer Setting is not dynamic anymore on 0.90.1\nAccording to the documentation; Index warmup can be disabled by setting index.warmer.enabled to false. It is supported as a realtime setting using update settings API.\r\n\r\n\r\n\r\nWhen I try to set it back to true;\r\n```\r\n{"index.warmer.enabled":"true"}\r\n```\r\nI get to following exception;\r\n```\r\n{"error":"RemoteTransportException[[xxx][inet[/xx.x.xx.xx:9300]][indices/settings/update]]; nested: ElasticSearchIllegalArgumentException[Can\'t update non dynamic settings[[index.warmer.enabled]] for open indices[[public_20120701]]]; ","status":400}\r\n```\r\n\r\n'
3245,'dadoonet','PluginManager fails with unknown command when passing url or verbose parameters\nTo reproduce try:\r\n> bin/plugin -v\r\n\r\n[...]\r\n\r\nMessage:\r\n   Command [-v] unknown.\r\n\r\nThe problem is that in the main function the commands are cycled twice; the first time to check for url and verbose, the second time to check for the others. If the second time a command that is not install, remove, list or help is found, the application exits (but url and verbose were never filtered out from the list of commands).'
3242,'chilling','Geoshape filter can\'t handle multiple shapes\nThe `geo_shape` filter seems to be unable to handle multiple `geo_shape` fields in a single document if this document is used as indexed filter.\r\n\r\nAssume a mapping with multiple `geo_shape` fields:\r\n```\r\n{\r\n    "type1" : {\r\n        "properties" : {\r\n            "location1" : {\r\n                 "type" : "geo_shape"\r\n            },\r\n            "location2" : {\r\n                "type" : "geo_shape"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nand a document\r\n\r\n```\r\n{\r\n    "location1" : {\r\n        "type":"polygon",\r\n        "coordinates":[[[-10,-10],[10,-10],[10,10],[-10,10],[-10,-10]]]\r\n    },\r\n    "location2" : {\r\n        "type":"polygon",\r\n        "coordinates":[[[-20,-20],[20,-20],[20,20],[-20,20],[-20,-20]]]\r\n    }\r\n}\r\n```\r\n\r\nIf a `geo_shape` filter is applied to the `location2` field\r\n\r\n```\r\n{\r\n    "geo_shape": {\r\n        "location2": {\r\n            "indexed_shape": { \r\n                "id": "1",\r\n                "type": "type1",\r\n                "index": "test",\r\n                "shape_field_name": "location2"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nparsing fails with\r\n\r\n```\r\nElasticSearchIllegalStateException[Shape with name [1] found but missing location2 field];\r\n```'
3239,'dadoonet','plugin version in admin api / feature request\nwhen running _nodes?plugin=true it would be great to be able to get the version of the installed plugins'
3237,'jpountz',"IndexUpgraderMergePolicy doesn't assign a field number to _version correctly\nMartijn found a bug in IndexUpgraderMergePolicy which attributes fieldInfos.size() as a field number although this field number might already be taken in the current segment."
3231,'spinscale','Add Arabic/PersianNormalizationFilters from Lucene\nTracking pull request https://github.com/elasticsearch/elasticsearch/pull/3227 with this bug report (for changelog).\r\n\r\nCommits\r\nMaster: https://github.com/elasticsearch/elasticsearch/commit/c561b1bbcfd8ae016921a0277a40d1aad41b47c5\r\n0.90: https://github.com/elasticsearch/elasticsearch/commit/3aaf31159bba25bb4d7eab02a17409145a2d1115'
3228,'spinscale','Reproducible UnavailableShardsException on type-creation via POST\nI\'m trying to put together some tests on a module I\'ve been writing with ElasticSearch and as a result I\'ve been trying to create and delete types and indices in reliable reproducible ways. I\'ve run into an error though that seems to make it very difficult to truly decouple index settings and type mappings. Process for reproducing bug is as follows:\r\n\r\nStartup ES server\r\nPUT request to http://localhost:9200/test-index w/ data: \r\n{\r\n\t"settings": {\r\n\t\t"number_of_shards": 10,\r\n\t\t"number_of_replicas": 5\r\n\t}\r\n}\r\nresponse:\r\n{\r\n   "ok": true,\r\n   "acknowledged": true\r\n}\r\nPOST request to http://localhost:9200/test-index/test-type w/ data:\r\n`{\r\n\t"mappings": {\r\n\t\t"properties": {\r\n\t\t\t"test-string": {\r\n\t\t\t\t"type": "string",\r\n\t\t\t\t"store": "yes",\r\n\t\t\t\t"index": "analyzed",\r\n\t\t\t\t"term_vector": "with_positions_offsets"\r\n\t\t\t},\r\n\r\n\t\t\t"test-float": {\r\n\t\t\t\t"type": "float",\r\n\t\t\t\t"store": "yes"\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n}`\r\n\r\nResponse:\r\n{\r\n   "error": "UnavailableShardsException[[test-index][3] [6] shardIt, [1] active : Timeout waiting for [1m], request: index {[test-index][test-type][S_RZkZekR2iiFBnhAhVSpQ], source[{\\n    \\"mappings\\": {\\n\\t\\t\\"properties\\": {\\n\\t\\t\\t\\"test-string\\": {\\n\\t\\t\\t\\t\\"type\\": \\"string\\",\\n\\t\\t\\t\\t\\"store\\": \\"yes\\",\\n\\t\\t\\t\\t\\"index\\": \\"analyzed\\",\\n\\t\\t\\t\\t\\"term_vector\\": \\"with_positions_offsets\\"\\n\\t\\t\\t},\\n\\n\\t\\t\\t\\"test-float\\": {\\n\\t\\t\\t\\t\\"type\\": \\"float\\",\\n\\t\\t\\t\\t\\"store\\": \\"yes\\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}]}]",\r\n   "status": 503\r\n}\r\n\r\nI know I could simple hard-code my mapping and settings into the same file with type names and PUT that to the index, but I feel like that makes for a significantly less clean implementation. Do I have to close the index down, then POST, then reopen it? Or is this a bug with a known workaround? Sorry if I\'m doing something silly here, but I\'m fairly new to ElasticSearch, though I\'ve really been loving it.\r\n\r\nUpdate: Tried on a closed index and got a ClusterBlockException, which is understandable.\r\n\r\nUpdate: This seems to be the result of the settings used in the index. Is there some reason for that? Is 10 shards with 5 replicas far larger than I think it is?'
3220,'jpountz',"Merge integer field data implementations together\nElasticsearch has 4 similar field data implementations for its integer types: byte, short, int and long. These implementations could be merged together and even be made a little more memory-efficient by using Lucene's PackedInts API."
3217,'javanna','Close/Open Index API to support multiple indices and wildcard on index names\nClose and Open index APIs should follow other indices APIs where they should support wildcard notation and multiple indices.'
3211,'jpountz','ElasticSearch 0.90 fails when "highlight" contains a field of type "long"\nIf the "highlight" section of a searh query contains a field of type "long", an error occurs. That didn\'t happen with the old version.\r\nThis is a sample script I used to test this behaviour:\r\n\r\n``` bash\r\ncurl -s -o /dev/null -X DELETE "http://${hostname}:9200/test?pretty"\r\n\r\ncurl -s -o /dev/null -X PUT "http://${hostname}:9200/test/?pretty"  -d \'\r\n{\r\n   "mappings" : {\r\n      "test1" : {\r\n        "properties" : {\r\n            "text" : {\r\n                "store": "yes",\r\n                "type": "string"\r\n            }\r\n         }\r\n      },\r\n      "test2" : {\r\n        "properties" : {\r\n            "text" : {\r\n                "store": "yes",\r\n                "type": "string"\r\n            },\r\n            "number" : {\r\n                "store": "yes",\r\n                "type": "long"\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\'\r\n\r\ncurl -s -o /dev/null -X POST "http://${hostname}:9200/test/test1?pretty"  -d \'\r\n{\r\n   "text" : "test one"\r\n}\r\n\'\r\n\r\ncurl -s -o /dev/null -X POST "http://${hostname}:9200/test/test2?pretty"  -d \'\r\n{\r\n   "text" : "test two",\r\n   "number" : 100\r\n}\r\n\'\r\n\r\ncurl -s -o /dev/null -X POST "http://${hostname}:9200/test1/_refresh"\r\ncurl -s -o /dev/null -X POST "http://${hostname}:9200/test2/_refresh"\r\n\r\nsleep 3\r\n\r\ncurl -s -X GET "http://${hostname}:9200/test/_search?pretty" -d \'\r\n{\r\n  "query": {\r\n        "prefix": {\r\n          "text": "test"\r\n        }\r\n  },\r\n  "highlight": {\r\n    "number_of_fragments": 0,\r\n    "fields": {\r\n        "text": {},\r\n        "number": {}\r\n    }\r\n  }\r\n\r\n}\'\r\n```\r\n\r\n\r\nIf you run it against an elasticsearch 0.20.6 server, it returns the two hits, correctly highlighted.\r\nHowever, if you run it against an elasticsearch 0.90 server, this happens:\r\n\r\n``` bash\r\n{\r\n  "took" : 6,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 4,\r\n    "failed" : 1,\r\n    "failures" : [ {\r\n      "index" : "test",\r\n      "shard" : 0,\r\n      "status" : 500,\r\n      "reason" : "FetchPhaseExecutionException[[test][0]: query[text:test*],from[0],size[10]: Fetch Failed [Failed to highlight field [number]]]; nested: StringIndexOutOfBoundsException[String index out of range: -1]; "\r\n    } ]\r\n  },\r\n  "hits" : {\r\n    "total" : 2,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test",\r\n      "_type" : "test1",\r\n      "_id" : "y51XzuuaQ6uYnigzhq5EtA",\r\n      "_score" : 1.0, "_source" :\r\n{\r\n   "text" : "test one"\r\n}\r\n,\r\n      "highlight" : {\r\n        "text" : [ "<em>test</em> one" ]\r\n      }\r\n    } ]\r\n  }\r\n}\r\n```\r\n\r\n'
3203,'martijnvg','min_score does not work with HasChild queries (0.90.1)\nhow to replicate:\r\n\r\nCreate index test\r\ncurl -XPOST ‘http://localhost:9200/test’\r\n\r\nParent docs\r\ncurl -XPOST \'http://localhost:9200/test/testP/1\' -d \'{"yes" : "1"}\'\r\ncurl -XPOST \'http://localhost:9200/test/testP/2\' -d \'{"yes" : "2"}\'\r\n\r\nCreate child mapping\r\ncurl -XPUT \'http://localhost:9200/test/testC/_mapping\' -d \'{"testC" : {"type" : "object", "_parent" : {"type" : "testP"}}}\'\r\n\r\nChild docs\r\ncurl -XPOST \'http://localhost:9200/test/testC?parent=1\' -d \'{"yes" : "2"}\'\r\ncurl -XPOST \'http://localhost:9200/test/testC?parent=1\' -d \'{"yes" : "2"}\'\r\ncurl -XPOST \'http://localhost:9200/test/testC?parent=1\' -d \'{"yes" : "2"}\'\r\ncurl -XPOST \'http://localhost:9200/test/testC?parent=2\' -d \'{"yes" : "2"}\'\r\n\r\nQuery returns empty:\r\n{\r\n  "min_score": 1,\r\n  "query": {\r\n    "has_child": {\r\n      "query": {\r\n        "constant_score": {\r\n          "filter": {\r\n            "match_all": {}\r\n          }\r\n        }\r\n      },\r\n      "child_type": "testC",\r\n      "score_type": "sum"\r\n    }\r\n  }\r\n}\r\n\r\n'
3202,'jpountz',"Add support for Lucene Common Grams token filter\nIt's like the Shingle tokenfilter but only for common words."
3200,'jpountz','Elasticsearch Highlighting problem (0.90.1)\nI am working on  upgradion of elasticsearch from 0.20.2 to 0.90.1 and come across the following issue.\r\nElasticsearch Highlighting was working fine(getting results as we expected) in 0.20.2, but the same doesn’t work in ES 0.90.1 "type": "pattern",\r\n\r\nSteps to reproduce the issue\r\n\r\nEnvironment:\r\nJDK 1.7,Windows 7, elasticsearch 0.90.1, used elasticsearch head plugin to create/query documents\r\n\r\nStep 1:-\r\nDefined mappings and settings for index(test_hightlight)/type(hightlight).\r\n\r\nhttp://localhost:9200/test_hightlight   [POST]\r\n\r\n{\r\n  "settings": {\r\n    "index": {\r\n      "number_of_shards": 6,\r\n      "number_of_replicas": 2,\r\n      "analysis": {\r\n        "analyzer": {\r\n          "CommaAnalyzer": {\r\n            "type": "pattern",\r\n            "flags": "DOTALL",\r\n            "lowercase": "true",\r\n            "pattern": "\\\\,",\r\n            "stopwords": "_none_"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "mappings": {\r\n    "hightlight": {\r\n      "properties": {\r\n        "documentName": {\r\n          "analyzer": "CommaAnalyzer",\r\n          "type": "string"\r\n        },\r\n        "description": {\r\n          "analyzer": "CommaAnalyzer",\r\n          "type": "string"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nStep2: \r\nIndexed following documents to newly created Index.\r\n\r\n\thttp://localhost:9200/test_hightlight/hightlight/1001  [POST]\r\n\t{\r\n\t\t"documentName":"business Contract JSON business vendor and rep credentialing program ensures that Kutti Kumar and reps you are doing business with meet your requirements and are sound business partners With the business Small Business Package you can be where the buyers business are Not only do you get Business access to more than Business 1800 business hospitals you can BUSINESS promote your business in the only credentialed supplier sourcing tool used by Business healthcare organizations across business the country",\r\n\t\t"description":"Manage Kutti Kumar access and influence permissions Monitor Kutti Kumar sanction and financial details"\r\n\r\n\t}\r\n\r\n\thttp://localhost:9200/test_hightlight/hightlight/1002  [POST]\r\n\t{\r\n\t\t"documentName":"business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for you Kutti Kumar",\r\n\t\t"description":"notifications and management enable you to have better insight to your business Kutti Kumar"\r\n\r\n\t}\r\n\r\n\thttp://localhost:9200/test_hightlight/hightlight/1003  [POST]\r\n\t{\r\n\t  "documentName": "Kutti Kumar business JSON Communicating and managing those standards across all of your vendors can be a coordination nightmare Let business manage it for Kutti Kumar",\r\n\t  "description": "Kutti Kumar notifications and management enable Kutti Kumar to have better insight to your business"\r\n\t}\r\nStep3:\r\nExecuted the following query \r\n\t\r\nURL:http://localhost:9200/test_hightlight/\t\r\nQuery:\r\n\r\n{\r\n  "timeout": 60000,\r\n  "query": {\r\n    "bool": {\r\n      "must": {\r\n        "query_string": {\r\n          "query": "business",\r\n          "default_operator": "and"\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "explain": false,\r\n  "highlight": {\r\n    "pre_tags": [\r\n      "<em>"\r\n    ],\r\n    "post_tags": [\r\n      "</em>"\r\n    ],\r\n    "fields": {\r\n      "documentName": {\r\n        "fragment_size": 20,\r\n        "number_of_fragments": 5,\r\n        "fragment_offset": 0\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nGot the following error\r\n\t\r\n{\r\n\r\n    took: 20\r\n    timed_out: false\r\n    _shards: {\r\n        total: 6\r\n        successful: 3\r\n        failed: 3\r\n        failures: [\r\n            {\r\n                index: test_hightlight\r\n                shard: 3\r\n                status: 500\r\n                reason: FetchPhaseExecutionException[[test_hightlight][3]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; \r\n            }\r\n            {\r\n                index: test_hightlight\r\n                shard: 4\r\n                status: 500\r\n                reason: FetchPhaseExecutionException[[test_hightlight][4]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; \r\n            }\r\n            {\r\n                index: test_hightlight\r\n                shard: 5\r\n                status: 500\r\n                reason: FetchPhaseExecutionException[[test_hightlight][5]: query[_all:business],from[0],size[10]: Fetch Failed [Failed to highlight field [documentName]]]; nested: IOException[Stream closed]; \r\n            }\r\n        ]\r\n    }\r\n    hits: {\r\n        total: 3\r\n        max_score: 0.12557761\r\n        hits: [ ]\r\n    }\r\n\r\n}\r\n\r\nNote: The same query is working as we expected with Elasticsearch 0.20.2\r\n'
3196,'s1monw','IndexOutOfBoundsException while call suggest api\njava.lang.IndexOutOfBoundsException: Index: 2, Size: 2\r\n        at java.util.ArrayList.rangeCheck(ArrayList.java:571)\r\n        at java.util.ArrayList.get(ArrayList.java:349)\r\n        at org.elasticsearch.search.suggest.Suggest$Suggestion.reduce(Suggest.java:251)\r\n        at org.elasticsearch.search.suggest.Suggest.reduce(Suggest.java:179)\r\n        at org.elasticsearch.action.suggest.TransportSuggestAction.newResponse(TransportSuggestAction.java:145)\r\n        at org.elasticsearch.action.suggest.TransportSuggestAction.newResponse(TransportSuggestAction.java:60)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.finishHim(TransportBroadcastOperationAction.java:369)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.onOperation(TransportBroadcastOperationAction.java:306)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:265)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.performOperation(TransportBroadcastOperationAction.java:242)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$1.run(TransportBroadcastOperationAction.java:218)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n'
3195,'martijnvg','Add `doc_as_upsert` option to update api\nIssue for PR #3153.'
3193,'jpountz','encount a java.lang.ClassCastException: org.elasticsearch.index.fielddata.plain.PagedBytesIndexFieldData cannot be cast to org.elasticsearch.index.fielddata.IndexNumericFieldData\nHi ther,  I post this problem first at google group to found reason of problem.\r\n\r\nand now,I think I know where the problem is.\r\n\r\nThis happes, when I delete all data and changed mappings .\r\n\r\ninsert data is right.but when search again. ES throws cast exception. \r\n\r\neg. my time filed is string first time. and delete all data and change time to date.\r\n\r\nafter that search again will cause exception.\r\n\r\nso restart server will solve problem. this is a small bug.u can ignore:).\r\n'
3190,'martijnvg','optimize has_child query when matching parent count is low\nCurrently the has_child query loops over every single document of the parent type looking for the parents matched in child query.  In situations where the child query only matches few parents, this loop is expensive.  \r\n\r\nI feel this loop can be eliminated or short-circuited early since we already know the matching parent ids (the keys of the uidToScore map).\r\n\r\nI am currently testing a few approaches and will submit a PR when ready.\r\n\r\n/cc @martijnvg @s1monw '
3189,'jpountz','Field data should support more than 2B ordinals per segment\nField data currently uses integers to store ordinals although a Lucene index can have more than 2B unique values per index. We should use longs instead in the APIs and fix implementations to actually support more than 2B unique values.'
3188,'s1monw','Add a minimum_should_match parameter when Common query has only high frequent terms\nWhen a Common query has only high frequent terms, they are combined in a boolean MUST query.\r\n\r\nI\'d like to add the possibility to specify a minimum_should_match for this specific case. Because when I use a very low cutoff_frequency, the boolean fallback query is too restrictive in this case.\r\n\r\nIt requires to override the buildQuery method in the Lucene CommonTermsQuery class. I called the parameter "high_freq_only_minimum_should_match", but that might be a bit far-fetched... what do you think?'
3186,'jpountz',"Compress PagedBytesAtomicFieldData's termOrdToBytesOffset\nMonotonicAppendingLongBuffer is an in-memory append-only data-structure which compresses efficiently monotonically increasing sequence of longs. We would save memory by using it in PagedBytesAtomicFieldData to store the term ordinal -> offset mapping."
3185,'martijnvg','Pack the ordinals in field data for single valued fields\nIn field data ordinals for singe valued fields are always represented as plain integer arrays. Using packed integers to store those ordinals will reduce the memory usage.'
3183,'spinscale','Too many open files issue even after increasing limit to 65K on Ubuntu\nStill getting a stack trace when I try to copy an index with just 143 entries over to another index.  ulimit -a shows 65K set for the max open files.d\r\n\r\nCaused by: java.io.FileNotFoundException: /usr/local/share/elasticsearch/data/elasticsearch/nodes/0/indices/contacts-new/\r\n1/index/_5.fdx (Too many open files)\r\n        at java.io.RandomAccessFile.open(Native Method)\r\n        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)\r\n        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput$Descriptor.<init>(SimpleFSDirectory.java:71)\r\n        at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.<init>(SimpleFSDirectory.java:98)\r\n        at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.<init>(NIOFSDirectory.java:92)\r\n        at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:79)\r\n        at org.elasticsearch.index.store.Store$StoreDirectory.openInput(Store.java:537)\r\n        at org.apache.lucene.index.FieldsReader.<init>(FieldsReader.java:133)\r\n        at org.apache.lucene.index.SegmentCoreReaders.openDocStores(SegmentCoreReaders.java:234)\r\n        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:118)\r\n        at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:696)\r\n        at org.apache.lucene.index.IndexWriter$ReaderPool.getReadOnlyClone(IndexWriter.java:654)\r\n        at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:142)\r\n        at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:36)\r\n        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:451)\r\n        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:399)\r\n        at org.apache.lucene.index.IndexReader.open(IndexReader.java:296)\r\n        at org.apache.lucene.search.SearcherManager.<init>(SearcherManager.java:82)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.buildSearchManager(RobinEngine.java:1428)\r\n        at org.elasticsearch.index.engine.robin.RobinEngine.start(RobinEngine.java:271)\r\n        ... 6 more\r\n[2013-06-14 04:45:28,671][WARN ][cluster.action.shard     ] [Emplate] sending failed shard for [contacts-new][1], node[gbLiKgx3T0qcVQpmsmj3gQ], [P], s[INITIALIZING], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[contacts-new][1] failed recovery]; nested: EngineCreationFailureException[[contacts-new][1] failed to open reader on writer]; nested: FileNotFoundException[/usr/local/share/elasticsearch/data/elasticsearch/nodes/0/indices/contacts-new/1/index/_5.fdx (Too many open files)]; ]]'
3181,'bleskes','Pull request #3002 creates issue when getting array fields from _source\nPull request #3002 creates issue when getting array fields from _source.\r\n\r\nIf a field in _source is an array with just one element, the field gets extracted as the element\'s type and not an array with the element inside.\r\n\r\nSo, instead of getting this:\r\n\r\n```javascript\r\n{\r\n    "_index": "someindex",\r\n    "_type": "sometype",\r\n    "_id": "someid",\r\n    "_version": 2,\r\n    "exists": true,\r\n    "fields": {\r\n        "tags": [\r\n            {\r\n                "id": "someid",\r\n                "value": "somevalue"\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nI get this:\r\n```javascript\r\n{\r\n    "_index": "someindex",\r\n    "_type": "sometype",\r\n    "_id": "someid",\r\n    "_version": 2,\r\n    "exists": true,\r\n    "fields": {\r\n        "tags": {\r\n            "id": "someid",\r\n            "value": "somevalue"\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nThis causes, for example, de-serialization exceptions when trying to convert the JSON "fields" object into a strongly typed class.\r\n\r\nDo let me know if I you need any further information or assistance.'
3177,'s1monw','NPE in query execution of boolean filter in 0.90.1\nAttempting to upgrade to 0.90.1 (from 0.90.0) and seeing a new NPE for certain queries (specific generated boolean query with `not` section).\r\n\r\nDetails below -- happy to provide more information to help track down but haven\'t had any luck so far reducing this to simpler case.\r\n\r\nThe error is:\r\n```\r\n[2013-06-13 11:04:12,411][DEBUG][action.search.type       ] [Mop Man] [product-development-1][0], node[2njk8xyQQUSqmjVuG51JiA], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@4e299813]\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [product-development-1][0]: query[filtered(ConstantScore(+QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(properties.name.id:Product Category) +cache(properties.value.id:13351)))->cache(_type:__properties))) QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(categories.name.id:Product Category) +cache(categories.value.id:13351)))->cache(_type:__categories))) +NotFilter(QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(properties.name.id:ProductName) +cache(properties.value.id:drive converter)))->cache(_type:__properties))) QueryWrapperFilter(ToParentBlockJoinQuery (filtered(ConstantScore(+cache(categories.name.id:ProductName) +cache(categories.value.id:drive converter)))->cache(_type:__categories))))))->cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@953b6c6c)],from[0],size[10]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:138)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:206)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:193)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:179)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\r\n\tat java.lang.Thread.run(Thread.java:680)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.lucene.docset.NotDocIdSet$IteratorBasedIterator.cost(NotDocIdSet.java:168)\r\n\tat org.elasticsearch.common.lucene.docset.AndDocIdSet$IteratorBasedIterator.<init>(AndDocIdSet.java:140)\r\n\tat org.elasticsearch.common.lucene.docset.AndDocIdSet.iterator(AndDocIdSet.java:82)\r\n\tat org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:135)\r\n\tat org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:538)\r\n\tat org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:133)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:609)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:161)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:482)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:438)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:134)\r\n\t... 9 more\r\n```\r\n\r\nQuery is:\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "and": [\r\n          {\r\n            "or": [\r\n              {\r\n                "nested": {\r\n                  "path": "properties",\r\n                  "query": {\r\n                    "filtered": {\r\n                      "filter": {\r\n                        "and": [\r\n                          {\r\n                            "term": {\r\n                              "properties.name.id": "Product Category"\r\n                            }\r\n                          },\r\n                          {\r\n                            "term": {\r\n                              "properties.value.id": "13351"\r\n                            }\r\n                          }\r\n                        ]\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              },\r\n              {\r\n                "nested": {\r\n                  "path": "categories",\r\n                  "query": {\r\n                    "filtered": {\r\n                      "filter": {\r\n                        "and": [\r\n                          {\r\n                            "term": {\r\n                              "categories.name.id": "Product Category"\r\n                            }\r\n                          },\r\n                          {\r\n                            "term": {\r\n                              "categories.value.id": "13351"\r\n                            }\r\n                          }\r\n                        ]\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          {\r\n            "not": {\r\n              "or": [\r\n                {\r\n                  "nested": {\r\n                    "path": "properties",\r\n                    "query": {\r\n                      "filtered": {\r\n                        "filter": {\r\n                          "and": [\r\n                            {\r\n                              "term": {\r\n                                "properties.name.id": "ProductName"\r\n                              }\r\n                            },\r\n                            {\r\n                              "term": {\r\n                                "properties.value.id": "drive converter"\r\n                              }\r\n                            }\r\n                          ]\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                },\r\n                {\r\n                  "nested": {\r\n                    "path": "categories",\r\n                    "query": {\r\n                      "filtered": {\r\n                        "filter": {\r\n                          "and": [\r\n                            {\r\n                              "term": {\r\n                                "categories.name.id": "ProductName"\r\n                              }\r\n                            },\r\n                            {\r\n                              "term": {\r\n                                "categories.value.id": "drive converter"\r\n                              }\r\n                            }\r\n                          ]\r\n                        }\r\n                      }\r\n                    }\r\n                  }\r\n                }\r\n              ]\r\n            }\r\n          }\r\n        ]\r\n      }\r\n    }\r\n  },\r\n  "fields": [\r\n    "id"\r\n  ]\r\n}\r\n```'
3173,'martijnvg','Distributed percolator engine\n## Background\r\nRedesigning the percolate engine is targeted for version 1.0. The main reason why the rewrite is necessary is that the current perculate engine doesn\'t scale. The idea is that perculating a document should be executed in the same manner as a distributed search request. \r\n\r\nIn the current approach queries are stored in a single primary shard index, that is auto replicated to each data node. This allows the percolation to happen locally. In the case that large amount of queries are index into this `_percolator` index, percolating document just start to take to long. Also all queries are loaded into memory (Map: query uid -> Lucene Query), so in this case heap space issues can occur. On top of this with the current api the query always need to get index into the `_percolator` index and the type is the name of the index the query is percolated for. So scaling out the percolator feature is needed for sharing the percolator execution and memory load.\r\n\r\nBecause of the fact that percolation will be a distributed request, the perculate option in the index api is scheduled to be removed. The main reason behind this is that we can\'t block and wait in the index api for a distributed percolate request to complete. The perculate request may take longer to complete then the actual index request (we currently perculate during replication) and thus slowing down the actual index request.\r\n\r\nTo substitute the percolate while indexing option, one just needs to run percolate api directly after the index api returned. The percolate api will remain to be a realtime api.\r\n\r\n## Implementation plan\r\n\r\nThe percolator index type approach stores the percolate queries in a special `_percolator` type with its own mapping in the same index where the actual data is or in a different index (dedicated percolation index, which might require different sharding behavior compared to the index that holds actual data and being search on). This approach also allows percolator to scale beyond the single shard exection we have today, meaning we both partition the percolated queries, and distribute the percolate execution.\r\n\r\nStore a query in the twitter index:\r\n```\r\ncurl -XPUT \'localhost:9200/twitter/_percolator/p_es\' -d \'{\r\n\t"query" : {\r\n\t\t"match" : {\r\n\t\t\t"message" : "elasticsearch"\r\n\t\t}\r\n\t}\r\n}\'\r\n```  \r\n\r\nPercolating a document uses the same rest end point:\r\n```\r\ncurl -XGET \'localhost:9200/twitter/tweet/_percolate\' -d \'{\r\n    "doc" : {\r\n        "message" : "Bonsai tree in elasticsearch office"\r\n    }\r\n}\'\r\n```\r\n\r\nThe response initially doesn\'t change. The rest endpoint will also support a routing query string parameter, to allow documents to only be percolated on queries in specific shards. \r\n\r\nDuring regular searches, we will automatically filter out documents with the `_percolator` type (only if it exists, so its only added as an overhead if explicitly used). We won\'t filter `_percolator` type if explciitly specified in the search request since users might still want to search and get back the percolated queries.\r\n\r\n## Backwards compatibility\r\n\r\nThe plan is not to keep backwards compatibility with the current percolate implementation. Percolate queries indexed via the old infrastructure will need to be migrated into the new planned infrastructure. The \'old\' `_percolate` index won\'t be removed, so the queries can easily be copied to the new infrastructure by using a scan search request.\r\n\r\n## Post redesign\r\n\r\nAfter the redesign has been implemented adding more features to the percolator is next. One of them is to highlight what parts of the query matched with the document. \r\n\r\nThe idea is have different response modes. For example:\r\n* `count` - A count of how many queries matched with the document.\r\n* `compact` - Returns a list of query ids that have matched with the document. (just like we do today)\r\n* `verbose` - Returns a body per matched query. This body can for example hold a query highlight in the future.\r\n \r\nHere are a few thoughts on post features for percolator:\r\n* Support additional operations such as highlighting\r\n* Allow to do bulk percolation. Two options, simple bulk and use the MemoryIndex to do it one by one, or somehow bulk index the docs into an in memory index (MemoryIndex does\'t support more than one index, possible RAM based dir?), and then execute the queries against it. Bulk percolation will still need to be distributed and broken down into shard level bulks.\r\n* Support percolating an existing document, by specifying an index, type and id and optionally an version instead of an actual document.\r\n'
3172,'martijnvg','Make get mapping response consistent\nThe get mapping api response wraps the mappings in a type object and the type objects in an index object. If the mapping for only one index and type is requested the index object is omitted, for the sake of brevity, but this can also be confusing for application parsing it. The top level index object should always be included'
3171,'martijnvg',"Improve get mapping and warmers api\nImprove the way the get mapping and the get warmer api get their data from the master's copy of the cluster state.\r\n\r\nRelates to #3100 "
3168,'bleskes',"MVEL infinite loop in its error handling causing cluster to degrade\nRecently I've seen requests trigger a failure, where MVEL gets into an infinite loop trying to report the exception.\r\n\r\nThis failure causes service to degrade across the entire cluster, causing timeouts on requests handled by all nodes. These may be just for indexes with a shard on the affected machine, but it feels bigger than that.\r\n\r\nShared some logs and a repro with @kimchy."
3167,'martijnvg',"custom_score could support a filter directly\ncustom_score only supports a query, but lot's of scripts probably don't use the `_score` variable anyway. So it makes sense to use a filter (no scoring overhead and better caching).\r\n\r\nIt would be nice if you could use a `filter` directly instead of wrapping it in a `filtered` query (see an example in #3165), it looks better and could be a little bit more optimized (maybe)."
3166,'s1monw','Merge Settings are misleading\ntoday we have a settings `index.compound_format` that if set to `true` tells lucene to use a compound index format. Yet, this might be confusing to many folks since what lucene does is it only uses CFS if the segment that is written is < 10% of the rest of the index. This is an impl detail of lucene and ES should manage user expectation and use CFS all the time if set to true.'
3164,'s1monw','Phrase suggest run 100% CPU when giving long sentences\nHi,\r\n\r\nI run into an issue with the Suggester, using "phrase". Here is a gist, but I have to warn you: your CPU will melt if you do not stop ES :fire:!\r\n\r\nhttps://gist.github.com/damienalexandre/5763504\r\n\r\nYou can test with:\r\n\r\n```json\r\n{"query": {\r\n    "match" : {\r\n      "_all": {\r\n        "query": "queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen queen "\r\n      }\r\n    }\r\n   },\r\n  "suggest": {\r\n    "text": "queen",\r\n    "my-suggest": {\r\n      "phrase": {\r\n        "field": "name",\r\n        "analyzer" : "francais"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nAnd that will works. So the issue is only when the `text` is long. \r\n\r\nTo prevent serious issues in production, I have to implement a truncate on the client side, avoiding long user contributed sentences.\r\n\r\nI do not know if this is an ES bug or a bad usage from me - using a suggester on a phrase is not clever BUT as this is a public search engine, I can\'t expect user to use simple search terms.\r\n\r\nES 0.90.1 on Ubuntu 12.04 (but same issue on FreeBSD).'
3157,'spinscale','_ttl listed in index template is ignored\n0.90.1\r\n\r\nhttps://gist.github.com/unthingable/5708136 — the mapping and the template that produced it. The mapping has no _ttl.'
3154,'bleskes','Add a created flag to IndexResponse & UpdateResponse\nThe new flag indicates whether the operation added the document to the index (true) or replaced an existing one (false).'
3153,'martijnvg',"Upsert shouldn't require an extra document\nWhen posting a document for Update should be possible to tell the command to do an upsert of the same document. Something like upsert_doc: true. Reduces the amount of data sent over the wire significantly. This scenario happens a lot when having several data stores for the same document in the index. "
3148,'spinscale','match_phrase_prefix missing results in some circumstances\nI am seeing this issue in 0.90.1 and 0.20.5 (I have not tested on any release in between or before 0.20.5).\r\n\r\nFirst, I create an index called `test`.\r\n\r\nThen I insert 2 documents:\r\n\r\n```json\r\nPOST http://localhost:9200/test/people/1\r\n\r\n{\r\n    "user": "Anna Citizen",\r\n    "post_date": "2009-11-15T14:12:12",\r\n    "message": "ES is great!"\r\n}\r\n```\r\n\r\n```json\r\nPOST http://localhost:9200/test/people/2\r\n\r\n{\r\n    "user": "Jerry Smith",\r\n    "post_date": "2009-11-15T14:12:12",\r\n    "message": "Testing ES!"\r\n}\r\n```\r\n\r\nThen I run a simple `match_phrase_prefix` query:\r\n```json\r\nPOST http://localhost:9200/test/_search/\r\n\r\n{\r\n   "query":{\r\n      "match":{\r\n         "people.user":{\r\n            "type":"phrase_prefix",\r\n            "query":"j"\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\nThe correctly finds the `Jerry Smith` document:\r\n```json\r\n{\r\n   "took":2,\r\n   "timed_out":false,\r\n   "_shards":{\r\n      "total":5,\r\n      "successful":5,\r\n      "failed":0\r\n   },\r\n   "hits":{\r\n      "total":1,\r\n      "max_score":0.19178301,\r\n      "hits":[\r\n         {\r\n            "_index":"test",\r\n            "_type":"people",\r\n            "_id":"2",\r\n            "_score":0.19178301,\r\n            "_source":{\r\n               "user":"Jerry Smith",\r\n               "post_date":"2009-11-15T14:12:12",\r\n               "message":"ES is great!"\r\n            }\r\n         }\r\n      ]\r\n   }\r\n}\r\n```\r\n\r\nHowever, if I search for `a`, I get nothing:\r\n```json\r\nPOST http://localhost:9200/test/_search/\r\n\r\n{\r\n   "query":{\r\n      "match":{\r\n         "people.user":{\r\n            "type":"phrase_prefix",\r\n            "query":"a"\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n```json\r\n{\r\n   "took":1,\r\n   "timed_out":false,\r\n   "_shards":{\r\n      "total":5,\r\n      "successful":5,\r\n      "failed":0\r\n   },\r\n   "hits":{\r\n      "total":0,\r\n      "max_score":null,\r\n      "hits":[\r\n\r\n      ]\r\n   }\r\n}\r\n```\r\n\r\nThe only way to get the `Anna Citizen` document is to search for `ann`:\r\n```json\r\nPOST http://localhost:9200/test/_search/\r\n\r\n{\r\n   "query":{\r\n      "match":{\r\n         "people.user":{\r\n            "type":"phrase_prefix",\r\n            "query":"ann"\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n```json\r\n{\r\n   "took":1,\r\n   "timed_out":false,\r\n   "_shards":{\r\n      "total":5,\r\n      "successful":5,\r\n      "failed":0\r\n   },\r\n   "hits":{\r\n      "total":1,\r\n      "max_score":0.19178301,\r\n      "hits":[\r\n         {\r\n            "_index":"test",\r\n            "_type":"people",\r\n            "_id":"1",\r\n            "_score":0.19178301,\r\n            "_source":{\r\n               "user":"Anna Citizen",\r\n               "post_date":"2009-11-15T14:12:12",\r\n               "message":"trying out Elastic Search"\r\n            }\r\n         }\r\n      ]\r\n   }\r\n}\r\n```\r\n\r\nWhy is this happening? Could this possibly be a bug?'
3144,'martijnvg',"has_child & has_parent queries don't take deletes into account\nIf score mode: `max`, `avg` or `sum` is specified for `has_child` and if score mode `score` is specified for `has_parent` query then deletes aren't taken into account.\r\n\r\nThis affects all version from 0.90.0.Beta1"
3143,'javanna','MultiGetRequestBuilder toString implementation\nI reported this in the Google group about a month ago.  Unlike other builders, MultiGetRequestBuilder does not log the JSON used to query the server when the toString method is called.\r\n\r\nDavid Pilato responded:\r\n\r\n>I think we just need to add:\r\n>\r\n    @Override\r\n    public String toString() {\r\n        return internalBuilder().toString();\r\n    }\r\n\r\n>In MultiGetRequestBuilder and MultiSearchRequestBuilder as well.'
3142,'s1monw',"FVH can result in massive CPU & RAM usage if MultPhraseQuery is large\nin the case of a MultiPhraseQuery (multiple terms on the same position) the highlighter can result in a very big cpu and memory cosumption. We should cut off if there are too many terms and just highlight term by term instead which might result in slightly different highlights but doesn't potentially kill a node."
3141,'kimchy','missing/exists filters should also work for objects\nThe missing filter is not working, when the field expected to be missing is an object. Consider this sample\r\n\r\n```sh\r\ncurl -X DELETE http://localhost:9200/test\r\n\r\ncurl -XPUT \'http://localhost:9200/test/test/3?refresh=true\' -d \'{\r\n    "foo" : {\r\n        "bar" : "baz"\r\n    }\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/test/_search?pretty" -d \'{\r\n    "filter" : {\r\n        "missing" : {\r\n            "field" : "foo"\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nThis could be implemented by checking the mapping for the type to be queried and create a bool filter full of missing filters, when the mapping shows an object type.\r\n\r\nThis feature also supports wildcards notation, for example `obj1.obj2.field_pre*`.'
3140,'s1monw',"FVH produces StringArrayIndexOutOfBounds if stored field is used\nWe still use Lucene's SimpleFragmentsBuilder which doesn't try to correct the offsets is a broken analysis chain is used like the old NGramTokenFilter. SimpleFragmentsBuilder also needs to call into FragmentBuilderHelper.fixWeightedFragInfo() to detect broken chains and do best effort to resort the tokens."
3136,'javanna','Highlight is not working on _source\nI am trying to perform a query on all fields.\r\nI would like to know which fields contain the matched string.\r\n\r\nI am using the default mapping for ES. I am passing a json object when putting the documents into ES.\r\n\r\nI am then searching on all fields. It works, but I cannot manage to highlight the matched string and/or the field containing the matched string.\r\n\r\n'
3134,'jpountz','Static analysis\nFixes some problems discovered by findbugs, metrics projects.'
3132,'javanna','API privison to delete index level settings\nUse case - Routing settings are made to not use a machine , but later we need to use that machine. \r\n\r\nWe should be able to delete settings related to routing.'
3130,'bleskes','Elasticsearch not responding. Possible GC issue.\nI am trying to upgrade elasticsearch from 0.19.12 to 0.90.x. After upgrading elasticsearch on the workers of my testing environment I found out that elasticsearch server started to become unresponsive on every distinct worker. The workers are regular PCs with one elasticsearch instance with its own cluster name. They are not related at all.\r\n\r\nI managed to figure out that each of the elasticsearch instances were run out of heap memory.\r\n\r\nI wrote a script that does the same thing that the tests of our application perform to the elasticsearch server(creating the proper indexes before the start of the suite, performing some queries and truncating the contents of each index after each run).\r\n\r\nI compared the heap memory usage between the 0.19.12 and 0.90.1 versions with this script. I found out that in the case of the version 0.19.12, the garbage collector was applied and the heap memory was freed, and the server continued to run without any problem.\r\n\r\nBut in the case of 0.90.1, although the garbage collector was applied, the memory was not freed.\r\n\r\nI attach the screenshots of bigdesk for both versions and the script that I used for conducting this comparison.\r\n\r\n    #!/bin/bash\r\n    \r\n    while true; do\r\n    \r\n      echo "Deleting all indexes"\r\n      curl -XDELETE \'http://localhost:9200/_all/?pretty\' > /dev/null  2>&1\r\n    \r\n      echo "Creating index"\r\n      curl -XPUT \'http://localhost:9200/test/?pretty\' -d \'{\r\n       "index" : true,\r\n       "settings": {\r\n         "number_of_shards" : 1\r\n       },\r\n       "_default_": {\r\n         "include_in_all" : true\r\n       },\r\n       "mappings" : {\r\n         "blog_post" : {\r\n           "type" : "object",\r\n           "_all" : {"enabled" : true },\r\n           "_source": {"enabled" : true, "compress" : true}\r\n         }\r\n       }\r\n      }\' > /dev/null  2>&1\r\n    \r\n      for i in {1..10}\r\n      do\r\n        echo "Truncating $i times"\r\n        curl -XDELETE \'http://localhost:9200/test/blog_post/_query?pretty\' -d \'{\r\n          "match_all" : {}\r\n        }\' > /dev/null  2>&1\r\n    \r\n        echo "Refreshing $i times"\r\n        curl -XPOST \'http://localhost:9200/test/_refresh?pretty\' > /dev/null  2>&1\r\n      done\r\n    \r\n    done\r\n\r\nElasticsearch 0.19.12:\r\n![heap-memory-0-19-12](https://f.cloud.github.com/assets/827828/605783/f4e58206-cd1d-11e2-8b58-962d5b6790c8.png)\r\n\r\nElasticsearch 0.90.1:\r\n![heap-memory-0-90-1](https://f.cloud.github.com/assets/827828/605784/faecf986-cd1d-11e2-9cb8-0a030bdbea39.png)\r\n\r\nI have mentioned this issue at the google group of elasticsearch: [Elasticsearch becomes unresponsive after updating to 0.90.x](https://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/yqhCO-NNOXs)\r\n\r\n    \r\n'
3128,'s1monw','Unusual CPU/Memory Usage while Highlighting\nSo, trying to run this on ES 0.90 results in an OOM...\r\nIts quite an unusual query I agree, but still the behavior seems wrong. \r\n\r\n```\r\ncurl -XPOST http://localhost:9200/test_hl -d \'{ "index": { "number_of_shards": "1", "number_of_replicas": "0", "analysis": { "filter": { "wordDelimiter": { "type": "word_delimiter", "split_on_numerics": "false", "generate_word_parts": "true", "generate_number_parts": "true", "catenate_words": "true", "catenate_numbers": "true", "catenate_all": "false" } }, "analyzer": { "custom_analyzer": { "tokenizer": "whitespace", "filter": [ "lowercase", "wordDelimiter" ] } } } } }\'\r\n\r\ncurl -XPUT http://localhost:9200/test_hl/profile/_mapping -d \'{ "profile": { "dynamic": "strict", "properties": { "id": { "type": "integer", "index": "not_analyzed", "store": "yes" }, "content": { "type": "string", "index_analyzer": "custom_analyzer", "search_analyzer": "custom_analyzer", "store": "yes", "term_vector": "with_positions_offsets" } } } }\'\r\n\r\ncurl -XPUT http://localhost:9200/test_hl/profile/2 -d \'{"content": "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature", "id": 2}\'\r\n\r\ncurl -XGET http://localhost:9200/test_hl/profile/_search -d \'{ "from": 0, "size": 10, "query": { "match": { "content": { "query": "Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature Test: http://www.facebook.com http://elasticsearch.org http://xing.com http://cnn.com http://quora.com http://twitter.com this is a test for highlighting feature", "type": "phrase" } } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }\'\r\n```'
3127,'spinscale','Suggest Doesn\'t work with latest .deb package (0.91)\nI installed elastic search on a vm (Ubuntu 12.04) (from scratch) using the .deb package\r\n\r\nI followed the instructions here:\r\n\r\nhttp://elasticsearchserverbook.com/elasticsearch-0-90-using-suggester/\r\n\r\nI am getting no options back for suggestions.\r\n\r\n  "suggest" : {\r\n    "check1" : [ {\r\n      "text" : "crume",\r\n      "offset" : 0,\r\n      "length" : 5,\r\n      "options" : [ ]\r\n    } ]\r\n  }\r\n\r\nI should be getting:\r\n\r\n"suggest" : {\r\n  "check1" : [ {\r\n    "text" : "crume",\r\n    "offset" : 0,\r\n    "length" : 5,\r\n    "options" : [ {\r\n      "text" : "crime",\r\n      "score" : 0.8,\r\n      "freq" : 1 \r\n     } ] \r\n   } ] \r\n }\r\n\r\nThere is a general paucity of info about this feature. I tried this as a last measure (i.e install from scratch with online examples) when I couldn\'t get suggest to work on a current install.\r\n\r\nAnyone have any idea why it might be behaving like this?\r\n'
3124,'spinscale','Change Version methods to be more readable\nCurrent behaviour\r\n\r\n```\r\nVersion.V_0_90_1.before(Version.V_0_90_0) == true\r\n```\r\n\r\nThis is confusing and more readable the other way around.\r\n\r\n**Note**: This is a breaking change! Test with care\r\n\r\nAlso all official plugins need to checked, in order to make sure, we use it right apart from elasticsearch core.'
3123,'spinscale',"rpm upgrade to 0.90.1 overwrote /etc/sysconfig/elasticsearch\nrpm -Uvh https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-0.90.1.noarch.rpm from 0.90.0 overwrites /etc/sysconfig/elasticsearch without warning. It doesn't overwrite /etc/elasticsearch/elasticsearch.yml however."
3121,'martijnvg','Percolator requests return inconsistent/empty results\nWe\'re experience an issue where calls to _percolate return inconsistent results based on which node we hit. Unfortunately I don\'t have a fully reproducible case; this seems to have happened in two cases, (1) a node fails and leaves/re-enters the cluster (2) a percolator shard goes in to a \'recovering\' state.\r\n\r\nHere\'s an example of what we see:\r\n\r\n    ~$ curl -XGET \'node1.endpoint:9200/abc/def/_percolate\' -d \'{"doc":{"foo":"bar"}}\'\r\n    {"ok":true,"matches":["some-result"]}\r\n    ~$ curl -XGET \'node2.endpoint:9200/abc/def/_percolate\' -d \'{"doc":{"foo":"bar"}}\'\r\n    {"ok":true,"matches":[]}\r\n\r\nFurthermore, we\'ll see for the "docs" entry in the below calls:\r\n\r\n    ~$ curl -XGET endpoint.node1:9200/_percolator/_status\r\n    "docs" : { "num_docs" : 8, "max_doc" : 8, "deleted_docs" : 0 }\r\n    ~$ curl -XGET endpoint.node2:9200/_percolator/_status\r\n    "docs" : { "num_docs" : 8, "max_doc" : 11, "deleted_docs" : 3 }\r\n\r\nSome notes on the occurrence: \r\n\r\n- We\'re running 5 nodes, and we will see "matches": [] across some number of them (2 during the most recent event).\r\n- The results returned from each node are consistent, meaning that we will receive empty matches repeatedly from the same bad node.\r\n- Flushing or refreshing do not resolve this.\r\n- The appropriate documents exist on disk in the machines (maybe expected):\r\n  `/location/elasticsearch/nodes/0/indices/_percolator/0/index/_0.fdt`\r\n  contained the same document on both machines.\r\n\r\nResolution:\r\nThe only way we can resolve this at the moment is by deleting the _percolator index and re-submitting entries to it.\r\n\r\nThis is running on Ubuntu with java version 1.6.0_27. We have 5 nodes, 3 indices. ES: 0.20.4 with s3 as the gateway.type.'
3116,'dadoonet','Add more information in PluginManager\nNew option -list display list of existing plugins\r\nCatch ArraysOutOfBoundException when no arg given to install or remove option\r\nAdd description on plugin name structure:\r\n- elasticsearch/plugin/version for official elasticsearch plugins (download from download.elasticsearch.org)\r\n- groupId/artifactId/version   for community plugins (download from maven central or oss sonatype)\r\n- username/repository          for site plugins (download from github master)\r\nPR for #3112.'
3114,'brwe','return term vectors and some statistics for a document\nThis feature seems to be useful as can be seen by typing "term vectors elasticsearch" in google. \r\n\r\nHere is how it should work: \r\n\r\n\r\nReturns information and statistics on terms in the fields of a particular document as stored in the index.\r\n\r\n        curl -XGET \'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true\'\r\n\r\nTree types of values can be requested: term information, term statistics and field statistics.\r\nBy default, all term information and field statistics are returned for all fields but no term statistics.\r\n\r\nOptionally, you can specify the fields for which the information is retrieved either with a parameter in the url\r\n\r\n\tcurl -XGET \'http://localhost:9200/twitter/tweet/1/_termvector?fields=text,...\'\r\n\r\nor adding by adding the requested fields in the request body (see example below).\r\n\r\n## YOU MUST ENABLE TERM VECTOR STORING FOR USING THE API\r\n\r\n\r\nSee [mapping](http://www.elasticsearch.org/guide/reference/mapping/core-types/) doc and the example below on how to do that.\r\nTerm information\r\n-------------------------\r\n\r\n- term frequency in the field (always returned)\r\n- term positions ("positions" : true)\r\n- start and end offsets ("offsets" : true)\r\n- term payloads ("payloads" : true), as base64 encoded bytes\r\n\r\nIf the requested information wasn\'t stored in the index, it will be omitted without further warning.\r\nSee [mapping](http://www.elasticsearch.org/guide/reference/mapping/core-types/) on how to configure your index to store term vectors.\r\n\r\nTerm statistics\r\n-------------------------\r\n\r\nSetting "term_statistics" to "true" (default is "false") will return\r\n\r\n- total term frequency (how often a term occurs in all documents)\r\n- document frequency (the number of documents containing the current term)\r\n\r\nBy default these values are not returned since term statistics can have a serious performance impact.\r\n\r\nField statistics\r\n-------------------------\r\n\r\nSetting "field_statistics" to "false" (default is "true") will omit\r\n\r\n- document count (how many documents contain this field)\r\n- sum of document frequencies (the sum of document frequencies for all terms in this field)\r\n- sum of total term frequencies (the sum of total term frequencies of each term in this field)\r\n\r\nBehavior\r\n-------------------------\r\n\r\nThe term and field statistics are not accurate. Deleted documents are not taken into account. The information is only retrieved for the shard the requested document resides in. The term and field statistics are therefore only useful as relative measures whereas the absolute numbers have no meaning in this context.\r\n\r\nExample\r\n-------------------------\r\n\r\nFirst, we create an index that stores term vectors, payloads etc. :\r\n```\r\n    curl -s -XPUT \'http://localhost:9200/twitter/\' -d \'{\r\n        "mappings": {\r\n            "tweet": {\r\n                "properties": {\r\n                    "text": {\r\n                                "type": "string",\r\n                                "term_vector": "with_positions_offsets_payloads",\r\n                                "store" : "yes",\r\n                                "index_analyzer" : "fulltext_analyzer"\r\n                         },\r\n                     "fullname": {\r\n                                "type": "string",\r\n                                "term_vector": "with_positions_offsets_payloads",\r\n                                "index_analyzer" : "fulltext_analyzer"\r\n                         }\r\n                 }\r\n            }\r\n        },\r\n        "settings" : {\r\n            "index" : {\r\n                "number_of_shards" : 1,\r\n                "number_of_replicas" : 0\r\n            },\r\n            "analysis": {\r\n                    "analyzer": {\r\n                        "fulltext_analyzer": {\r\n                            "type": "custom",\r\n                            "tokenizer": "whitespace",\r\n                            "filter": [\r\n                                "lowercase",\r\n                                "type_as_payload"\r\n                            ]\r\n                        }\r\n                    }\r\n            }\r\n         }\r\n    }\'\r\n```\r\nSecond, we add some documents:\r\n```\r\n    curl -XPUT \'http://localhost:9200/twitter/tweet/1?pretty=true\' -d \'{\r\n      "fullname" : "John Doe",\r\n      "text" : "twitter test test test "\r\n\r\n    }\'\r\n\r\n    curl -XPUT \'http://localhost:9200/twitter/tweet/2?pretty=true\' -d \'{\r\n      "fullname" : "Jane Doe",\r\n      "text" : "Another twitter test ..."\r\n\r\n    }\'\r\n```\r\nThe following request returns all information and statistics for field "text" in document "1" (John Doe):\r\n```\r\n     curl -XGET \'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true\' -d \'{\r\n                    "fields" : ["text"],\r\n                    "offsets" : true,\r\n                    "payloads" : true,\r\n                    "positions" : true,\r\n                    "term_statistics" : true,\r\n                    "field_statistics" : true\r\n            }\'\r\n```\r\nEquivalently, all parameters can be passed as URI parameters:\r\n     curl -GET \'http://localhost:9200/twitter/tweet/1/_termvector?pretty=true&fields=text&offsets=true&payloads=true&positions=true&term_statistics=true&field_statistics=true\'\r\n\r\nResponse:\r\n```\r\n  {\r\n    "_index" : "twitter",\r\n    "_type" : "tweet",\r\n    "_id" : "1",\r\n    "_version" : 1,\r\n    "exists" : true,\r\n    "term_vectors" : {\r\n      "text" : {\r\n        "field_statistics" : {\r\n          "sum_doc_freq" : 6,\r\n          "doc_count" : 2,\r\n          "sum_ttf" : 8\r\n        },\r\n        "terms" : {\r\n          "test" : {\r\n            "doc_freq" : 2,\r\n            "ttf" : 4,\r\n            "term_freq" : 3,\r\n            "pos" : [ 1, 2, 3 ],\r\n            "start" : [ 8, 13, 18 ],\r\n            "end" : [ 12, 17, 22 ],\r\n            "payload" : [ "d29yZA==", "d29yZA==", "d29yZA==" ]\r\n          },\r\n          "twitter" : {\r\n            "doc_freq" : 2,\r\n            "ttf" : 2,\r\n            "term_freq" : 1,\r\n            "pos" : [ 0 ],\r\n            "start" : [ 0 ],\r\n            "end" : [ 7 ],\r\n            "payload" : [ "d29yZA==" ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\n\r\nThis is similar to Issue #2691'
3113,'s1monw',"Don't fail highlighting if the field to highlight is missing.\nPlainHighlighter fails at highlighting with a hard exception in case the field\r\nto highlight is missing. This patch fixes this issue by\r\n - making FieldsVisitor.fields() return an empty list instead of null when no\r\n   stored field was found,\r\n - replacing the fields to highlight with an empty list in case they are absent.\r\n\r\nCloses: #3109"
3112,'dadoonet','Update plugin manager\nI searched using Google and found nothing.\r\n\r\nI tried this and got nothing:\r\n```\r\n$ ./plugin -help\r\n```\r\nI tried this and got nothing:\r\n```\r\n$ ./plugin -nonsense\r\n```\r\n\r\nI tried this and got incompete information:\r\n```\r\n$ ./plugin\r\nUsage:\r\n    -url     [plugin location]   : Set exact URL to download the plugin from\r\n    -install [plugin name]       : Downloads and installs listed plugins\r\n    -remove  [plugin name]       : Removes listed plugins\r\n    -verbose                     : Prints verbose messages\r\n```\r\n\r\nI tried this and got nothing:\r\n```\r\n$ ./plugin -verbose\r\n```\r\n\r\nI tried this and got nothing:\r\n```\r\n$ ./plugin -verbose -nonsense\r\n```\r\n\r\nJust to make my own sense of frustrated indignation complete, I tried this:\r\n```\r\n$ ./plugin -this "is bullshit"\r\n$ echo $?\r\n```\r\n\r\nThe result was *0*.\r\n\r\nAnd the icing on the cake:\r\n```\r\n$ ./plugin -install\r\nException in thread "main" java.lang.ArrayIndexOutOfBoundsException: 1\r\n        at org.elasticsearch.plugins.PluginManager.main(PluginManager.java:321)\r\n```\r\n\r\nI see lots of examples around the web where *plugin name* is something like **mobz/elasticsearch-head** or **elasticsearch/mapper-attachments/1.6.0** which clearly has some semantically important syntax embedded in it.  Nowhere is it documented what the components of this syntax are or what valid values are.'
3111,'bleskes','Update api doesn\'t support versioning\nTo reproduce\r\n\r\n```sh\r\ncurl -XPOST http://localhost:9200/test/test/1 -d\'{\r\n    "field": "value1"\r\n}\'\r\n```\r\n\r\nThe internal version is now 1.\r\n\r\nNow index again:\r\n\r\n```sh\r\ncurl -XPOST http://localhost:9200/test/test/1 -d\'{\r\n    "field": "value2"\r\n}\'\r\n```\r\n\r\nInternal version is now 2. \r\n\r\nTry to update using version=1 (should fail)\r\n\r\n```sh\r\ncurl -XPOST "http://localhost:9200/test/test/1/_update?version=1" -d\'{\r\n    "doc": { "field": "value3" }\r\n}\'\r\n```\r\n\r\nWhich doesn\'t fail with a version conflict but returns:\r\n```json\r\n{"ok":true,"_index":"test","_type":"test","_id":"1","_version":3,"_previous_version":2}\r\n```\r\n\r\nPS. The java api\'s UpdateRequestBuilder doesn\'t have a setVersion method\r\n'
3109,'jpountz','Highlighter exception (0.90.0)\nWhen applying highlight on fields that are declared as stored at index mapping but do not actually exists the following exception occurs (in version 0.90.x)\r\n```javascript\r\n{\r\n   "took":6,\r\n   "timed_out":false,\r\n   "_shards":{\r\n      "total":3,\r\n      "successful":2,\r\n      "failed":1,\r\n      "failures":[\r\n         {\r\n            "index":"myindex",\r\n            "shard":1,\r\n            "status":500,\r\n            "reason":"FetchPhaseExecutionException[[myindex][1]: query[filtered(name:p*)->cache(_type:myindextype)],from[0],size[10]: Fetch Failed [Failed to highlight field [surname]]]; nested: NullPointerException; "\r\n         }\r\n      ]\r\n   },\r\n   "hits":{\r\n      "total":1,\r\n      "max_score":1.0,\r\n      "hits":[\r\n\r\n      ]\r\n   }\r\n}.\r\n```\r\nYou can reproduce the error by doing the following on an empty cluster\r\n\r\n```bash\r\n # create index \r\n\r\ncurl -XPUT \'http://192.168.56.150:9200/myindex/\' -d \'{\r\n    "settings" : {\r\n        "number_of_shards" : 3,\r\n        "number_of_replicas" : 0\r\n    }\r\n}\'\r\n\r\n# create the mapping\r\n\r\ncurl -XPUT http://192.168.56.150:9200/myindex/myindextype/_mapping -d \'\r\n\t{ "myindextype" : {"properties" : { "name":{"type":"string", "store":"yes", "analyzer":"simple"}, "surname":{"type":"string", "store":"yes", "analyzer":"simple"} } } }\r\n\'\r\n\r\n# add a record \r\ncurl -XPUT http://192.168.56.150:9200/myindex/myindextype/1 -d \'{ "name":"panagiotis" }\'\r\n\r\n# query with highlighting\r\n\r\ncurl -XGET \'http://192.168.56.150:9200/myindex/myindextype/_search\' -d \'\r\n{\r\n  "query": {\r\n    "query_string": {\r\n      "query": "name:p*"\r\n    }\r\n  },\r\n  "highlight": {\r\n    "order": "score",\r\n    "fields": {\r\n      "name": {},\r\n      "surname": {}\r\n    }\r\n  }\r\n}\r\n\'\r\n```\r\nI think that ES should not throw an exception on that case (as did in 0.20.X and earlier versions) due to the schema-free philosophy that is built on. \r\n\r\nThe problematic code is **HighlightPhase.java:191** and one easy patch is to to replace that line with the following lines\r\n\r\n```java\r\nif (fieldVisitor.fields() == null)\r\n\ttextsToHighlight = new ArrayList<Object>();\r\nelse\r\n\ttextsToHighlight = fieldVisitor.fields().get(mapper.names().indexName());\r\n```\r\nI think that the same logic where in previous versions.\r\n\r\nThank you,\r\nAlex\r\n'
3105,'spinscale','Changed Java dependency from Depends to Suggest for Debian. \nSince people are also using the Oracle JAVA distribution and not the OpenJDK.\r\n\r\nNow the installation will at least continue, and it will not give dependency errors when installing other packages after the installation of ElasticSearch.'
3103,'jpountz',"Store _version as a numeric doc values field\n_version is today stored as a payload alongside the postings of the _uid field.\r\n\r\nStoring it as a numeric doc values field would help save space, since _uid could be indexed with IndexOptions.DOCS_ONLY and numeric doc values formats can be very compact since they don't need to be byte-aligned. For example, all doc values formats in Lucene but SimpleText store numbers into blocks of packed integers."
3100,'martijnvg',"Aliases: Add indices aliases exists api\nAdd indices aliases exists api that allows to check to existence of an index alias. This api redirects to the master to check for the existence of one or multiple index aliases.\r\n\r\nPossible options:\r\n* `index` - The index name to check index aliases for. Partially names are supported via wildcards, also multiple index names can be specified separated with a comma. Also the alias name for an index can be used.\r\n* `alias` - The name of alias to check the existence for. Like the index option, this option supports wildcards and the option the specify multiple alias names separated by a comma. This is a required option.\r\n* `ignore_indices` - What to do is an specified index name doesn't exist. If set to `missing` then those indices are ignored.\r\n\r\nThe rest head endpoint is: `/{index}/_alias/{alias}`\r\n\r\nExamples:\r\nCheck existence for any aliases with the name 2013 in any index:\r\n```\r\ncurl -XHEAD 'localhost:9200/_alias/2013\r\n```\r\nCheck existence for any aliases that start with 2013_01 in any index\r\n```\r\ncurl -XHEAD 'localhost:9200/_alias/2013_01*\r\n```\r\nCheck existence for any aliases in the users index.\r\n```\r\ncurl -XHEAD 'localhost:9200/users/_alias/*\r\n```"
3097,'javanna',"No matched_filters being returned when using named filters\nSee the request and response here:\r\n\r\nhttps://gist.github.com/Mpdreamz/5656950\r\n\r\nI'm not seeing a matched_filters property on any of the hits, am I doing something wrong here?"
3094,'s1monw','IllegalAccessError when using an MVEL script filter in the query\nHi,\r\n\r\nWe have a 2-node ES cluster running 0.90.0 release in our QA environment.  When we started using a script filter in our query, we got the following stack trace after a few queries to ES.  After looking into the issue a bit, the issue seemed to be caused by the use of MVEL in compiled or accelerated mode.  Further research seems to indicate that disabling JIT for MVEL worked around the problem (via -Dmvel2.disable.jit=true).\r\n\r\nDisabling MVEL JIT would also mean a performance concern for us.  What\'s root cause of this issue?  How should we address it?\r\n\r\nOur QA environment.\r\n\r\n<b>OS</b>\r\nUbuntu 12.10 (GNU/Linux 3.5.0-17-generic x86_64)\r\n\r\n<b>Java version, vendor</b>\r\njava version "1.7.0_21"\r\nOpenJDK Runtime Environment (IcedTea 2.3.9) (7u21-2.3.9-0ubuntu0.12.10.1)\r\nOpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)\r\n\r\n<b>Script</b>\r\n<pre>\r\nif(doc[\'targeting.keys\'] == null || doc[\'targeting.keys\'].values.length == 0)\r\n      return true;\r\nforeach (kv : doc[\'targeting.keys\'].values) {\r\n       var found = 0;\r\n       foreach (qkey : qkeys) {\r\n            if (kv == qkey) {\r\n                 found = 1;\r\n            }\r\n       }\r\n       if(found == 0){\r\n            return false;\r\n       }\r\n }\r\nreturn true;\r\n</pre>\r\n\r\n<b>Stack Trace</b>\r\n\r\n<pre>\r\n[2013-05-23 21:04:55,650][DEBUG][action.search.type       ] [Omen] [indexedbuyitem][0], node[-3mMO7JjTKKhIk92AJt_xQ], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1d5e469d]\r\n\r\njava.lang.IllegalAccessError: org/elasticsearch/index/fielddata/ScriptDocValues$Strings$1\r\n        at ASMAccessorImpl_1471145441369339522430.getValue(Unknown Source)\r\n        at org.elasticsearch.common.mvel2.optimizers.dynamic.DynamicGetAccessor.getValue(DynamicGetAccessor.java:73)\r\n        at org.elasticsearch.common.mvel2.ast.ASTNode.getReducedValueAccelerated(ASTNode.java:108)\r\n        at org.elasticsearch.common.mvel2.ast.BinaryOperation.getReducedValueAccelerated(BinaryOperation.java:108)\r\n        at org.elasticsearch.common.mvel2.ast.Or.getReducedValueAccelerated(Or.java:34)\r\n        at org.elasticsearch.common.mvel2.ast.Or.getReducedValueAccelerated(Or.java:34)\r\n        at org.elasticsearch.common.mvel2.MVELRuntime.execute(MVELRuntime.java:85)\r\n        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getDirectValue(CompiledExpression.java:123)\r\n        at org.elasticsearch.common.mvel2.compiler.CompiledExpression.getValue(CompiledExpression.java:119)\r\n        at org.elasticsearch.script.mvel.MvelScriptEngineService$MvelSearchScript.run(MvelScriptEngineService.java:192)\r\n        at org.elasticsearch.index.query.ScriptFilterParser$ScriptFilter$ScriptDocSet.matchDoc(ScriptFilterParser.java:184)\r\n        at org.elasticsearch.common.lucene.docset.MatchDocIdSet.get(MatchDocIdSet.java:67)\r\n        at org.apache.lucene.search.FilteredDocIdSet$1.get(FilteredDocIdSet.java:65)\r\n        at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndBits.get(AndDocIdSet.java:106)\r\n        at org.elasticsearch.common.lucene.docset.AndDocIdSet$AndBits.get(AndDocIdSet.java:106)\r\n        at org.elasticsearch.common.lucene.docset.BitsDocIdSetIterator$FilteredIterator.match(BitsDocIdSetIterator.java:59)\r\n        at org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:60)\r\n        at org.apache.lucene.search.FilteredDocIdSetIterator.nextDoc(FilteredDocIdSetIterator.java:59)\r\n        at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.nextDoc(ConstantScoreQuery.java:185)\r\n        at org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorScorer.nextDoc(FiltersFunctionScoreQuery.java:283)\r\n        at org.apache.lucene.search.Scorer.score(Scorer.java:63)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:605)\r\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:156)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)\r\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)\r\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:127)\r\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:239)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)\r\n        at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:281)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onFailure(TransportSearchTypeAction.java:213)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$2.handleException(SearchServiceTransportAction.j\r\n</pre>'
3091,'kimchy',"Dates passed to the script terms facet are now in the default time zone\nThis may be on purpose, but it's a breaking change from 0.20.6, where dates were always UTC. See discussion at  https://groups.google.com/d/topic/elasticsearch/a_Jj7d4Uk6c/discussion"
3089,'spinscale','Make suggest API implementations pluggable\nIn order to create a foundation for more possible suggesters, we need to open up the suggest API and support the possibility of registering suggester types (similar to registering facets for example).'
3088,'s1monw','"no index mapper found for field" bulk indexing in 0.90\nToday I\'ve been getting horrible performance on 0.90 server I upgraded last week. Running on Ubuntu, Ruby 1.9.3, Rails 3.2.3, with the Stretcher gem.\r\n\r\nThe issue is in bulk indexing Twitter data, with JSON very similar to the response shown at: https://dev.twitter.com/docs/api/1.1/get/statuses/user_timeline, except the user node of each item is removed and is instead a parent document (although I don\'t think that is the issue).\r\n\r\nMy mappings are intentionally kept simple, using dynamic mappings to add fields. Everything has been working fine to this point. Now I\'m getting the following error, bringing the server to its knees.\r\n\r\nThe error is:\r\n[2013-05-23 22:13:01,792][WARN ][index.merge.scheduler    ] [Rocket Racer] [twitter_history_76816072][1] failed to merge\r\norg.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]\r\n        at org.elasticsearch.index.codec.PerFieldMappingPostingFormatCodec.getPostingsFormatForField(PerFieldMappingPostingFormatCodec.java:52)\r\n        at org.apache.lucene.codecs.lucene42.Lucene42Codec$1.getPostingsFormatForField(Lucene42Codec.java:59)\r\n        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.addField(PerFieldPostingsFormat.java:102)\r\n        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:71)\r\n        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)\r\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116)\r\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3693)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3296)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:401)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:478)\r\n[2013-05-23 22:13:01,939][WARN ][index.engine.robin       ] [Rocket Racer] [twitter_history_76816072][1] failed engine\r\norg.apache.lucene.index.MergePolicy$MergeException: org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]\r\n        at org.elasticsearch.index.merge.scheduler.ConcurrentMergeSchedulerProvider$CustomConcurrentMergeScheduler.handleMergeException(ConcurrentMergeSchedulerProvider.java:100)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:514)\r\nCaused by: org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]\r\n        at org.elasticsearch.index.codec.PerFieldMappingPostingFormatCodec.getPostingsFormatForField(PerFieldMappingPostingFormatCodec.java:52)\r\n        at org.apache.lucene.codecs.lucene42.Lucene42Codec$1.getPostingsFormatForField(Lucene42Codec.java:59)\r\n        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsWriter.addField(PerFieldPostingsFormat.java:102)\r\n        at org.apache.lucene.codecs.FieldsConsumer.merge(FieldsConsumer.java:71)\r\n        at org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:383)\r\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:116)\r\n        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3693)\r\n        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3296)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:401)\r\n        at org.apache.lucene.index.TrackingConcurrentMergeScheduler.doMerge(TrackingConcurrentMergeScheduler.java:91)\r\n        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:478)\r\n[2013-05-23 22:13:04,187][WARN ][cluster.action.shard     ] [Rocket Racer] sending failed shard for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[INITIALIZING], reason [engine failure, message [MergeException[org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]]; nested: ElasticSearchIllegalStateException[no index mapper found for field: [entities.description.urls.display_url]]; ]]\r\n[2013-05-23 22:13:04,234][WARN ][cluster.action.shard     ] [Rocket Racer] received shard failed for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[INITIALIZING], reason [engine failure, message [MergeException[org.elasticsearch.ElasticSearchIllegalStateException: no index mapper found for field: [entities.description.urls.display_url]]; nested: ElasticSearchIllegalStateException[no index mapper found for field: [entities.description.urls.display_url]]; ]]\r\n[2013-05-23 22:13:07,433][WARN ][indices.cluster          ] [Rocket Racer] [twitter_history_76816072][1] master [[Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]]] marked shard as started, but shard have not been created, mark shard as failed\r\n[2013-05-23 22:13:07,446][WARN ][cluster.action.shard     ] [Rocket Racer] sending failed shard for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[STARTED], reason [master [Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]] marked shard as started, but shard have not been created, mark shard as failed]\r\n[2013-05-23 22:13:07,446][WARN ][cluster.action.shard     ] [Rocket Racer] received shard failed for [twitter_history_76816072][1], node[_1cPivHASUOzVcgQ2fK0_A], [P], s[STARTED], reason [master [Rocket Racer][_1cPivHASUOzVcgQ2fK0_A][inet[/10.178.13.201:9300]] marked shard as started, but shard have not been created, mark shard as failed]\r\n\r\nThe best guess I have is that some random data is breaking the creation of new mappings. The same is happening on the same index in both production and test environments. Other indexes with the same initial mappings but different Twitter data content are not exhibiting the same error. \r\n\r\nI\'m trying to capture the data, although there is a mass of it. Therefore before I go too far, has anybody got any clue how I can start to identify the source of the problem?\r\n\r\n'
3084,'s1monw',"Index endpoint doesn't return a 201 Created status code when using an external version type\n"
3082,'dadoonet','Unicast Cluster on Azure Virtual Machines not working (0.90)\nI am trying to build a unicast cluster on Azure Virtual machines and it does not seem to work. I don\'t see any errors. I have enabled debug mode and here are the logs.\r\nAlso, both the machines can talk to each other. I looked at the source and the hosts array (using initial hosts [],) during initialization seems to be empty even though I am specifying 2 hosts in it.\r\n\r\nNode 1: Config\r\n-----------------\r\n\r\n```yml\r\n#################################### Node #####################################\r\n\r\n# Node names are generated dynamically on startup, so you\'re relieved\r\n# from configuring them manually. You can tie this node to a specific name:\r\n#\r\n node.name: "xES1"\r\n\r\n################################## Discovery ##################################\r\n\r\n# Discovery infrastructure ensures nodes can be found within a cluster\r\n# and master node is elected. Multicast discovery is the default.\r\n\r\n# Set to ensure a node sees N other master eligible nodes to be considered\r\n# operational within the cluster. Set this option to a higher value (2-4)\r\n# for large clusters (>3 nodes):\r\n#\r\n# discovery.zen.minimum_master_nodes: 1\r\n\r\n# Set the time to wait for ping responses from other nodes when discovering.\r\n# Set this option to a higher value on a slow or congested network\r\n# to minimize discovery failures:\r\n#\r\n discovery.zen.ping.timeout: 10s\r\n\r\n# See <http://elasticsearch.org/guide/reference/modules/discovery/zen.html>\r\n# for more information.\r\n\r\n# Unicast discovery allows to explicitly control which nodes will be used\r\n# to discover the cluster. It can be used when multicast is not present,\r\n# or to restrict the cluster communication-wise.\r\n#\r\n# 1. Disable multicast discovery (enabled by default):\r\n#\r\n discovery.zen.ping.multicast.enabled: false\r\n#\r\n# 2. Configure an initial list of master nodes in the cluster\r\n#    to perform discovery when new nodes (master or data) are started:\r\n#discovery.zen.ping.unicast.hosts: \r\ndiscovery.zen.ping.unicast.hosts:["elasticsearch3","rnynjpxyhcfhxdm"]\r\n#discovery.zen.ping.unicast.hosts:["10.78.76.39:9300","10.78.26.64:9300"]\r\n```\r\n\r\n\r\nNode 1 logs:\r\n-----------------\r\n\r\n```\r\n[2013-05-23 19:35:12,433][INFO ][node                     ] [xES1] {0.90.0}[3136]: initializing ...\r\n[2013-05-23 19:35:12,435][DEBUG][node                     ] [xES1] using home [C:\\ddapplications\\elasticsearch-0.90.0], config [C:\\ddapplications\\elasticsearch-0.90.0\\config], data [[C:\\ddapplications\\elasticsearch-0.90.0\\data]], logs [C:\\ddapplications\\elasticsearch-0.90.0\\logs], work [C:\\ddapplications\\elasticsearch-0.90.0\\work], plugins [C:\\ddapplications\\elasticsearch-0.90.0\\plugins]\r\n[2013-05-23 19:35:12,454][INFO ][plugins                  ] [xES1] loaded [], sites [head]\r\n[2013-05-23 19:35:12,524][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder\r\n[2013-05-23 19:35:12,567][DEBUG][env                      ] [xES1] using node location [[C:\\ddapplications\\elasticsearch-0.90.0\\data\\elasticsearch\\nodes\\0]], local_node_id [0]\r\n[2013-05-23 19:35:15,035][DEBUG][threadpool               ] [xES1] creating thread_pool [generic], type [cached], keep_alive [30s]\r\n[2013-05-23 19:35:15,057][DEBUG][threadpool               ] [xES1] creating thread_pool [index], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:35:15,059][DEBUG][threadpool               ] [xES1] creating thread_pool [bulk], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:35:15,060][DEBUG][threadpool               ] [xES1] creating thread_pool [get], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:35:15,065][DEBUG][threadpool               ] [xES1] creating thread_pool [search], type [fixed], size [2], queue_size [1k], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:35:15,066][DEBUG][threadpool               ] [xES1] creating thread_pool [percolate], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:35:15,067][DEBUG][threadpool               ] [xES1] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]\r\n[2013-05-23 19:35:15,069][DEBUG][threadpool               ] [xES1] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:35:15,070][DEBUG][threadpool               ] [xES1] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:35:15,071][DEBUG][threadpool               ] [xES1] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:35:15,072][DEBUG][threadpool               ] [xES1] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:35:15,073][DEBUG][threadpool               ] [xES1] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:35:15,147][DEBUG][transport.netty          ] [xES1] using worker_count[2], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb->512kb]\r\n[2013-05-23 19:35:15,166][DEBUG][discovery.zen.ping.unicast] [xES1] using initial hosts [], with concurrent_connects [10]\r\n[2013-05-23 19:35:15,169][DEBUG][discovery.zen            ] [xES1] using ping.timeout [10s], master_election.filter_client [true], master_election.filter_data [false]\r\n[2013-05-23 19:35:15,171][DEBUG][discovery.zen.elect      ] [xES1] using minimum_master_nodes [-1]\r\n[2013-05-23 19:35:15,174][DEBUG][discovery.zen.fd         ] [xES1] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n[2013-05-23 19:35:15,185][DEBUG][discovery.zen.fd         ] [xES1] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n[2013-05-23 19:35:15,264][DEBUG][monitor.jvm              ] [xES1] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name=\'default\', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name=\'ParNew\', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name=\'ConcurrentMarkSweep\', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]\r\n[2013-05-23 19:35:15,782][DEBUG][monitor.os               ] [xES1] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@4ec48e7] with refresh_interval [1s]\r\n[2013-05-23 19:35:15,805][DEBUG][monitor.process          ] [xES1] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@26d6221b] with refresh_interval [1s]\r\n[2013-05-23 19:35:15,825][DEBUG][monitor.jvm              ] [xES1] Using refresh_interval [1s]\r\n[2013-05-23 19:35:15,827][DEBUG][monitor.network          ] [xES1] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@2d923a8f] with refresh_interval [5s]\r\n[2013-05-23 19:35:16,019][DEBUG][monitor.network          ] [xES1] net_info\r\nhost [rnynjpxyhcfhxdm]\r\nlo\tdisplay_name [Software Loopback Interface 1]\r\n\t\taddress [/127.0.0.1] [/0:0:0:0:0:0:0:1] \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [true] up [true] virtual [false]\r\nnet0\tdisplay_name [WAN Miniport (L2TP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet1\tdisplay_name [WAN Miniport (SSTP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet2\tdisplay_name [WAN Miniport (IKEv2)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet3\tdisplay_name [WAN Miniport (PPTP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nppp0\tdisplay_name [WAN Miniport (PPPOE)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth0\tdisplay_name [WAN Miniport (IP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth1\tdisplay_name [WAN Miniport (IPv6)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth2\tdisplay_name [WAN Miniport (Network Monitor)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth3\tdisplay_name [Microsoft Kernel Debug Network Adapter]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nppp1\tdisplay_name [RAS Async Adapter]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth4\tdisplay_name [Microsoft Hyper-V Network Adapter]\r\n\t\taddress [/10.78.76.39] [/fe80:0:0:0:4ccd:6139:c38f:e027%12] \r\n\t\tmtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]\r\neth5\tdisplay_name [WAN Miniport (IP)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth6\tdisplay_name [WAN Miniport (IPv6)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth7\tdisplay_name [WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth8\tdisplay_name [Microsoft Hyper-V Network Adapter-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth9\tdisplay_name [Microsoft Hyper-V Network Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth10\tdisplay_name [Microsoft Hyper-V Network Adapter-WFP Native MAC Layer LightWeight Filter-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet4\tdisplay_name [Microsoft ISATAP Adapter]\r\n\t\taddress [/fe80:0:0:0:0:5efe:a4e:4c27%19] \r\n\t\tmtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]\r\n\r\n[2013-05-23 19:35:16,105][DEBUG][monitor.fs               ] [xES1] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@e1b054c] with refresh_interval [1s]\r\n[2013-05-23 19:35:16,703][DEBUG][indices.store            ] [xES1] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]\r\n[2013-05-23 19:35:16,718][DEBUG][cache.memory             ] [xES1] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]\r\n[2013-05-23 19:35:16,744][DEBUG][script                   ] [xES1] using script cache with max_size [500], expire [null]\r\n[2013-05-23 19:35:16,836][DEBUG][cluster.routing.allocation.decider] [xES1] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\r\n[2013-05-23 19:35:16,840][DEBUG][cluster.routing.allocation.decider] [xES1] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\r\n[2013-05-23 19:35:16,841][DEBUG][cluster.routing.allocation.decider] [xES1] using [cluster_concurrent_rebalance] with [2]\r\n[2013-05-23 19:35:16,846][DEBUG][gateway.local            ] [xES1] using initial_shards [quorum], list_timeout [30s]\r\n[2013-05-23 19:35:17,100][DEBUG][indices.recovery         ] [xES1] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]\r\n[2013-05-23 19:35:17,254][DEBUG][http.netty               ] [xES1] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb->512kb]\r\n[2013-05-23 19:35:17,263][DEBUG][indices.memory           ] [xES1] using index_buffer_size [101.5mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]\r\n[2013-05-23 19:35:17,279][DEBUG][indices.cache.filter     ] [xES1] using [node] weighted filter cache with size [20%], actual_size [203.1mb], expire [null], clean_interval [1m]\r\n[2013-05-23 19:35:17,282][DEBUG][indices.fielddata.cache  ] [xES1] using size [-1] [-1b], expire [null]\r\n[2013-05-23 19:35:17,297][DEBUG][gateway.local.state.meta ] [xES1] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]\r\n[2013-05-23 19:35:17,358][DEBUG][gateway.local.state.meta ] [xES1] took 60ms to load state\r\n[2013-05-23 19:35:17,359][DEBUG][gateway.local.state.shards] [xES1] took 0s to load started shards state\r\n[2013-05-23 19:35:17,367][DEBUG][bulk.udp                 ] [xES1] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]\r\n[2013-05-23 19:35:17,370][INFO ][node                     ] [xES1] {0.90.0}[3136]: initialized\r\n[2013-05-23 19:35:17,371][INFO ][node                     ] [xES1] {0.90.0}[3136]: starting ...\r\n[2013-05-23 19:35:17,484][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500\r\n[2013-05-23 19:35:17,485][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false\r\n[2013-05-23 19:35:17,588][DEBUG][transport.netty          ] [xES1] Bound to address [/0:0:0:0:0:0:0:0:9300]\r\n[2013-05-23 19:35:17,659][INFO ][transport                ] [xES1] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.78.76.39:9300]}\r\n[2013-05-23 19:35:27,756][DEBUG][discovery.zen            ] [xES1] filtered ping responses: (filter_client[true], filter_data[false]) {none}\r\n[2013-05-23 19:35:27,767][DEBUG][cluster.service          ] [xES1] processing [zen-disco-join (elected_as_master)]: execute\r\n[2013-05-23 19:35:27,769][DEBUG][cluster.service          ] [xES1] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]\r\n[2013-05-23 19:35:27,772][INFO ][cluster.service          ] [xES1] new_master [xES1][QVFMXbh8Tt6A25sY8eZauA][inet[/10.78.76.39:9300]], reason: zen-disco-join (elected_as_master)\r\n[2013-05-23 19:35:27,845][DEBUG][transport.netty          ] [xES1] connected to node [[xES1][QVFMXbh8Tt6A25sY8eZauA][inet[/10.78.76.39:9300]]]\r\n[2013-05-23 19:35:27,851][DEBUG][cluster.service          ] [xES1] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state\r\n[2013-05-23 19:35:27,852][INFO ][discovery                ] [xES1] elasticsearch/QVFMXbh8Tt6A25sY8eZauA\r\n[2013-05-23 19:35:27,875][DEBUG][cluster.service          ] [xES1] processing [local-gateway-elected-state]: execute\r\n[2013-05-23 19:35:27,897][DEBUG][cluster.service          ] [xES1] cluster state updated, version [2], source [local-gateway-elected-state]\r\n[2013-05-23 19:35:27,982][INFO ][gateway                  ] [xES1] recovered [0] indices into cluster_state\r\n[2013-05-23 19:35:27,984][DEBUG][cluster.service          ] [xES1] processing [local-gateway-elected-state]: done applying updated cluster_state\r\n[2013-05-23 19:35:27,985][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: execute\r\n[2013-05-23 19:35:27,985][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-05-23 19:35:27,987][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: execute\r\n[2013-05-23 19:35:27,988][DEBUG][river.cluster            ] [xES1] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-05-23 19:35:28,059][INFO ][http                     ] [xES1] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.78.76.39:9200]}\r\n[2013-05-23 19:35:28,061][INFO ][node                     ] [xES1] {0.90.0}[3136]: started\r\n[2013-05-23 19:35:37,849][DEBUG][cluster.service          ] [xES1] processing [routing-table-updater]: execute\r\n[2013-05-23 19:35:37,851][DEBUG][cluster.service          ] [xES1] processing [routing-table-updater]: no change in cluster_state\r\n[2013-05-23 19:40:51,172][TRACE][action.admin.cluster.health] [xES1] Calculating health based on state version [2]\r\n```\r\n\r\n\r\nNode2 Config\r\n-----------------\r\n\r\n```yml\r\n#################################### Node #####################################\r\n\r\n# Node names are generated dynamically on startup, so you\'re relieved\r\n# from configuring them manually. You can tie this node to a specific name:\r\n#\r\n node.name: "xES2"\r\n\r\n################################## Discovery ##################################\r\n\r\n# Discovery infrastructure ensures nodes can be found within a cluster\r\n# and master node is elected. Multicast discovery is the default.\r\n\r\n# Set to ensure a node sees N other master eligible nodes to be considered\r\n# operational within the cluster. Set this option to a higher value (2-4)\r\n# for large clusters (>3 nodes):\r\n#\r\n# discovery.zen.minimum_master_nodes: 1\r\n\r\n# Set the time to wait for ping responses from other nodes when discovering.\r\n# Set this option to a higher value on a slow or congested network\r\n# to minimize discovery failures:\r\n#\r\n discovery.zen.ping.timeout: 10s\r\n\r\n# See <http://elasticsearch.org/guide/reference/modules/discovery/zen.html>\r\n# for more information.\r\n\r\n# Unicast discovery allows to explicitly control which nodes will be used\r\n# to discover the cluster. It can be used when multicast is not present,\r\n# or to restrict the cluster communication-wise.\r\n#\r\n# 1. Disable multicast discovery (enabled by default):\r\n#\r\n discovery.zen.ping.multicast.enabled: false\r\n#\r\n# 2. Configure an initial list of master nodes in the cluster\r\n#    to perform discovery when new nodes (master or data) are started:\r\n#\r\n# discovery.zen.ping.unicast.hosts: ["host1", "host2:port", "host3[portX-portY]"]\r\ndiscovery.zen.ping.unicast.hosts:["rnynjpxyhcfhxdm","elasticsearch3"]\r\n```\r\n\r\nNode 2 Logs\r\n-----------------\r\n\r\n```\r\n====================================================================\r\n[2013-05-23 19:58:28,732][INFO ][node                     ] [xES2] {0.90.0}[3604]: initializing ...\r\n[2013-05-23 19:58:28,734][DEBUG][node                     ] [xES2] using home [C:\\ddapplications\\elasticsearch-0.90.0], config [C:\\ddapplications\\elasticsearch-0.90.0\\config], data [[C:\\ddapplications\\elasticsearch-0.90.0\\data]], logs [C:\\ddapplications\\elasticsearch-0.90.0\\logs], work [C:\\ddapplications\\elasticsearch-0.90.0\\work], plugins [C:\\ddapplications\\elasticsearch-0.90.0\\plugins]\r\n[2013-05-23 19:58:28,751][INFO ][plugins                  ] [xES2] loaded [], sites [head]\r\n[2013-05-23 19:58:28,821][DEBUG][common.compress.lzf      ] using [UnsafeChunkDecoder] decoder\r\n[2013-05-23 19:58:28,863][DEBUG][env                      ] [xES2] using node location [[C:\\ddapplications\\elasticsearch-0.90.0\\data\\elasticsearch\\nodes\\0]], local_node_id [0]\r\n[2013-05-23 19:58:31,712][DEBUG][threadpool               ] [xES2] creating thread_pool [generic], type [cached], keep_alive [30s]\r\n[2013-05-23 19:58:31,742][DEBUG][threadpool               ] [xES2] creating thread_pool [index], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:58:31,743][DEBUG][threadpool               ] [xES2] creating thread_pool [bulk], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:58:31,744][DEBUG][threadpool               ] [xES2] creating thread_pool [get], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:58:31,749][DEBUG][threadpool               ] [xES2] creating thread_pool [search], type [fixed], size [2], queue_size [1k], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:58:31,750][DEBUG][threadpool               ] [xES2] creating thread_pool [percolate], type [fixed], size [1], queue_size [null], reject_policy [abort], queue_type [linked]\r\n[2013-05-23 19:58:31,751][DEBUG][threadpool               ] [xES2] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]\r\n[2013-05-23 19:58:31,753][DEBUG][threadpool               ] [xES2] creating thread_pool [flush], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:58:31,754][DEBUG][threadpool               ] [xES2] creating thread_pool [merge], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:58:31,755][DEBUG][threadpool               ] [xES2] creating thread_pool [refresh], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:58:31,756][DEBUG][threadpool               ] [xES2] creating thread_pool [warmer], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:58:31,757][DEBUG][threadpool               ] [xES2] creating thread_pool [snapshot], type [scaling], min [1], size [1], keep_alive [5m]\r\n[2013-05-23 19:58:31,827][DEBUG][transport.netty          ] [xES2] using worker_count[2], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1], receive_predictor[512kb->512kb]\r\n[2013-05-23 19:58:31,852][DEBUG][discovery.zen.ping.unicast] [xES2] using initial hosts [], with concurrent_connects [10]\r\n[2013-05-23 19:58:31,855][DEBUG][discovery.zen            ] [xES2] using ping.timeout [10s], master_election.filter_client [true], master_election.filter_data [false]\r\n[2013-05-23 19:58:31,857][DEBUG][discovery.zen.elect      ] [xES2] using minimum_master_nodes [-1]\r\n[2013-05-23 19:58:31,860][DEBUG][discovery.zen.fd         ] [xES2] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n[2013-05-23 19:58:31,871][DEBUG][discovery.zen.fd         ] [xES2] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n[2013-05-23 19:58:31,962][DEBUG][monitor.jvm              ] [xES2] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name=\'default\', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name=\'ParNew\', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name=\'ConcurrentMarkSweep\', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]\r\n[2013-05-23 19:58:32,483][DEBUG][monitor.os               ] [xES2] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@4ec48e7] with refresh_interval [1s]\r\n[2013-05-23 19:58:32,495][DEBUG][monitor.process          ] [xES2] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@26d6221b] with refresh_interval [1s]\r\n[2013-05-23 19:58:32,515][DEBUG][monitor.jvm              ] [xES2] Using refresh_interval [1s]\r\n[2013-05-23 19:58:32,526][DEBUG][monitor.network          ] [xES2] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@2d923a8f] with refresh_interval [5s]\r\n[2013-05-23 19:58:32,711][DEBUG][monitor.network          ] [xES2] net_info\r\nhost [elasticsearch3]\r\nlo\tdisplay_name [Software Loopback Interface 1]\r\n\t\taddress [/127.0.0.1] [/0:0:0:0:0:0:0:1] \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [true] up [true] virtual [false]\r\nnet0\tdisplay_name [WAN Miniport (L2TP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet1\tdisplay_name [WAN Miniport (SSTP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet2\tdisplay_name [WAN Miniport (IKEv2)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet3\tdisplay_name [WAN Miniport (PPTP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nppp0\tdisplay_name [WAN Miniport (PPPOE)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth0\tdisplay_name [WAN Miniport (IP)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth1\tdisplay_name [WAN Miniport (IPv6)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth2\tdisplay_name [WAN Miniport (Network Monitor)]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth3\tdisplay_name [Microsoft Kernel Debug Network Adapter]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nppp1\tdisplay_name [RAS Async Adapter]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth4\tdisplay_name [Microsoft Hyper-V Network Adapter]\r\n\t\taddress [/10.78.26.64] [/fe80:0:0:0:e024:1b29:9f61:eda%12] \r\n\t\tmtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]\r\neth5\tdisplay_name [WAN Miniport (IP)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth6\tdisplay_name [WAN Miniport (IPv6)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth7\tdisplay_name [WAN Miniport (Network Monitor)-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth8\tdisplay_name [Microsoft Hyper-V Network Adapter-QoS Packet Scheduler-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth9\tdisplay_name [Microsoft Hyper-V Network Adapter-WFP 802.3 MAC Layer LightWeight Filter-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\neth10\tdisplay_name [Microsoft Hyper-V Network Adapter-WFP Native MAC Layer LightWeight Filter-0000]\r\n\t\taddress \r\n\t\tmtu [-1] multicast [true] ptp [false] loopback [false] up [false] virtual [false]\r\nnet4\tdisplay_name [Microsoft ISATAP Adapter]\r\n\t\taddress [/fe80:0:0:0:0:5efe:a4e:1a40%19] \r\n\t\tmtu [1280] multicast [false] ptp [true] loopback [false] up [false] virtual [false]\r\n\r\n[2013-05-23 19:58:32,786][DEBUG][monitor.fs               ] [xES2] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@e1b054c] with refresh_interval [1s]\r\n[2013-05-23 19:58:33,377][DEBUG][indices.store            ] [xES2] using indices.store.throttle.type [none], with index.store.throttle.max_bytes_per_sec [0b]\r\n[2013-05-23 19:58:33,391][DEBUG][cache.memory             ] [xES2] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]\r\n[2013-05-23 19:58:33,418][DEBUG][script                   ] [xES2] using script cache with max_size [500], expire [null]\r\n[2013-05-23 19:58:33,515][DEBUG][cluster.routing.allocation.decider] [xES2] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [4]\r\n[2013-05-23 19:58:33,517][DEBUG][cluster.routing.allocation.decider] [xES2] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\r\n[2013-05-23 19:58:33,518][DEBUG][cluster.routing.allocation.decider] [xES2] using [cluster_concurrent_rebalance] with [2]\r\n[2013-05-23 19:58:33,523][DEBUG][gateway.local            ] [xES2] using initial_shards [quorum], list_timeout [30s]\r\n[2013-05-23 19:58:33,775][DEBUG][indices.recovery         ] [xES2] using max_size_per_sec[0b], concurrent_streams [3], file_chunk_size [512kb], translog_size [512kb], translog_ops [1000], and compress [true]\r\n[2013-05-23 19:58:33,931][DEBUG][http.netty               ] [xES2] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb], receive_predictor[512kb->512kb]\r\n[2013-05-23 19:58:33,941][DEBUG][indices.memory           ] [xES2] using index_buffer_size [101.5mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]\r\n[2013-05-23 19:58:33,954][DEBUG][indices.cache.filter     ] [xES2] using [node] weighted filter cache with size [20%], actual_size [203.1mb], expire [null], clean_interval [1m]\r\n[2013-05-23 19:58:33,957][DEBUG][indices.fielddata.cache  ] [xES2] using size [-1] [-1b], expire [null]\r\n[2013-05-23 19:58:33,973][DEBUG][gateway.local.state.meta ] [xES2] using gateway.local.auto_import_dangled [YES], with gateway.local.dangling_timeout [2h]\r\n[2013-05-23 19:58:34,033][DEBUG][gateway.local.state.meta ] [xES2] took 59ms to load state\r\n[2013-05-23 19:58:34,034][DEBUG][gateway.local.state.shards] [xES2] took 0s to load started shards state\r\n[2013-05-23 19:58:34,049][DEBUG][bulk.udp                 ] [xES2] using enabled [false], host [null], port [9700-9800], bulk_actions [1000], bulk_size [5mb], flush_interval [5s], concurrent_requests [4]\r\n[2013-05-23 19:58:34,051][INFO ][node                     ] [xES2] {0.90.0}[3604]: initialized\r\n[2013-05-23 19:58:34,052][INFO ][node                     ] [xES2] {0.90.0}[3604]: starting ...\r\n[2013-05-23 19:58:34,259][DEBUG][netty.channel.socket.nio.SelectorUtil] Using select timeout of 500\r\n[2013-05-23 19:58:34,260][DEBUG][netty.channel.socket.nio.SelectorUtil] Epoll-bug workaround enabled = false\r\n[2013-05-23 19:58:34,263][DEBUG][transport.netty          ] [xES2] Bound to address [/0:0:0:0:0:0:0:0:9300]\r\n[2013-05-23 19:58:34,340][INFO ][transport                ] [xES2] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.78.26.64:9300]}\r\n[2013-05-23 19:58:44,430][DEBUG][discovery.zen            ] [xES2] filtered ping responses: (filter_client[true], filter_data[false]) {none}\r\n[2013-05-23 19:58:44,439][DEBUG][cluster.service          ] [xES2] processing [zen-disco-join (elected_as_master)]: execute\r\n[2013-05-23 19:58:44,440][DEBUG][cluster.service          ] [xES2] cluster state updated, version [1], source [zen-disco-join (elected_as_master)]\r\n[2013-05-23 19:58:44,442][INFO ][cluster.service          ] [xES2] new_master [xES2][Nss1G-bwT0aU3UYeF-A0Lg][inet[/10.78.26.64:9300]], reason: zen-disco-join (elected_as_master)\r\n[2013-05-23 19:58:44,592][DEBUG][transport.netty          ] [xES2] connected to node [[xES2][Nss1G-bwT0aU3UYeF-A0Lg][inet[/10.78.26.64:9300]]]\r\n[2013-05-23 19:58:44,599][DEBUG][cluster.service          ] [xES2] processing [zen-disco-join (elected_as_master)]: done applying updated cluster_state\r\n[2013-05-23 19:58:44,600][INFO ][discovery                ] [xES2] elasticsearch/Nss1G-bwT0aU3UYeF-A0Lg\r\n[2013-05-23 19:58:44,629][DEBUG][cluster.service          ] [xES2] processing [local-gateway-elected-state]: execute\r\n[2013-05-23 19:58:44,640][DEBUG][cluster.service          ] [xES2] cluster state updated, version [2], source [local-gateway-elected-state]\r\n[2013-05-23 19:58:44,722][INFO ][gateway                  ] [xES2] recovered [0] indices into cluster_state\r\n[2013-05-23 19:58:44,723][DEBUG][cluster.service          ] [xES2] processing [local-gateway-elected-state]: done applying updated cluster_state\r\n[2013-05-23 19:58:44,727][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: execute\r\n[2013-05-23 19:58:44,728][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-05-23 19:58:44,729][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: execute\r\n[2013-05-23 19:58:44,730][DEBUG][river.cluster            ] [xES2] processing [reroute_rivers_node_changed]: no change in cluster_state\r\n[2013-05-23 19:58:44,796][INFO ][http                     ] [xES2] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.78.26.64:9200]}\r\n[2013-05-23 19:58:44,797][INFO ][node                     ] [xES2] {0.90.0}[3604]: started\r\n```\r\n'
3081,'s1monw','Limited facet search OutOfMemoryError[Java heap space]; ( core dump link attached )\nLoaded 300mil data points to do some statistical analysis.  ES java process is only using 1.05 GB on a 12GB server with 6 GB free memory.\r\nThe search is available below.  Other searches work fine even after the error.\r\n\r\nCore dump available here\r\nhttps://ten10.box.com/s/hynkizn4qtfiyryh434a\r\n\r\n\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [dfs], total failure; shardFailures {[QwoFm6gqSi6TEFjefRkKaw][touches][6]: SearchParseException[[touches][6]: query[ConstantScore(*:*)],from[0],size[100]: Parse Failure [Failed to parse source [{ \r\n"from":0,\r\n"size": 100,\r\n   "query" : {\r\n        "match_all" : {  }\r\n    },\r\n    "facets" : {\r\n        "sp_user_distinct" : {\r\n            "terms" : {\r\n                "field" : "sp_user",\r\n                "all_terms" : true,\r\n\t\t\t\t\t\t\t\t"size": 100\r\n            }\r\n        }\r\n    }\r\n\r\n}]]]; nested: ElasticSearchException[java.lang.OutOfMemoryError: Java heap space]; nested: ExecutionError[java.lang.OutOfMemoryError: Java heap space]; nested: OutOfMemoryError[Java heap space];'
3080,'spinscale','template index name prefix bug\nAs posted in this thread:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/rlONWRp_iXU\r\n\r\nThe following template:\r\n\r\n```\r\n{\r\n  "template": "logs*",\r\n  "settings": {\r\n    "query.default_field": "@message",\r\n    "action.auto_create_index": "+logs*",\r\n    "number_of_shards": 1,\r\n    "number_of_replicas": 1,\r\n    "routing.allocation.total_shards_per_node": 1,\r\n    "auto_expand_replicas": false,\r\n    "index": {\r\n      "analysis": {\r\n        "analyzer": {\r\n          "my_log_analyser": {\r\n            "tokenizer": "whitespace",\r\n            "filter": [\r\n              "standard",\r\n              "lowercase"\r\n            ],\r\n            "type": "custom"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "mappings": {\r\n    "_default_": {\r\n      "_all": {\r\n        "enabled": false\r\n      },\r\n      "_source": {\r\n        "compress": false\r\n      },\r\n      "dynamic_templates": [\r\n        {\r\n          "fields_template": {\r\n            "mapping": {\r\n              "type": "string",\r\n              "index": "not_analyzed"\r\n            },\r\n            "path_match": "@fields.*"\r\n          }\r\n        }\r\n      ],\r\n      "properties": {\r\n        "@time": {\r\n          "type": "date",\r\n          "index": "not_analyzed"\r\n        },\r\n        "@message": {\r\n          "type": "string",\r\n          "index": "analyzed",\r\n          "analyzer": "my_log_analyser"\r\n        },\r\n        "@fields": {\r\n          "type": "object",\r\n          "dynamic": true,\r\n          "path": "full"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWill cause queries issued with lowercase words to produce no hits whatsoever.\r\nIt does seem that \r\n- logs*\r\n- logs-*\r\nis maybe a reserved word and hence the lowercase filter is not applied for some reason.\r\n\r\nChanging the prefix name to ANY other value will produce the desired results.\r\n'
3078,'martijnvg','String sorting incorrect after reindex\nAfter reindexing a doc, it is not being returned in the correct sort order (when sorting on a string field)\r\n\r\nFirst, index docs 1..100 with a string field `user`:\r\n\r\n    curl -XPOST \'http://127.0.0.1:9200/test/test/_bulk?pretty=1\'  -d \'\r\n    {"index" : {"_id" : "1"}}\r\n    {"user" : "1"}\r\n    {"index" : {"_id" : "2"}}\r\n    {"user" : "2"}\r\n    {"index" : {"_id" : "3"}}\r\n    {"user" : "3"}\r\n    {"index" : {"_id" : "4"}}\r\n    {"user" : "4"}\r\n    {"index" : {"_id" : "5"}}\r\n    {"user" : "5"}\r\n    {"index" : {"_id" : "6"}}\r\n    {"user" : "6"}\r\n    {"index" : {"_id" : "7"}}\r\n    {"user" : "7"}\r\n    {"index" : {"_id" : "8"}}\r\n    {"user" : "8"}\r\n    {"index" : {"_id" : "9"}}\r\n    {"user" : "9"}\r\n    {"index" : {"_id" : "10"}}\r\n    {"user" : "10"}\r\n    {"index" : {"_id" : "11"}}\r\n    {"user" : "11"}\r\n    {"index" : {"_id" : "12"}}\r\n    {"user" : "12"}\r\n    {"index" : {"_id" : "13"}}\r\n    {"user" : "13"}\r\n    {"index" : {"_id" : "14"}}\r\n    {"user" : "14"}\r\n    {"index" : {"_id" : "15"}}\r\n    {"user" : "15"}\r\n    {"index" : {"_id" : "16"}}\r\n    {"user" : "16"}\r\n    {"index" : {"_id" : "17"}}\r\n    {"user" : "17"}\r\n    {"index" : {"_id" : "18"}}\r\n    {"user" : "18"}\r\n    {"index" : {"_id" : "19"}}\r\n    {"user" : "19"}\r\n    {"index" : {"_id" : "20"}}\r\n    {"user" : "20"}\r\n    {"index" : {"_id" : "21"}}\r\n    {"user" : "21"}\r\n    {"index" : {"_id" : "22"}}\r\n    {"user" : "22"}\r\n    {"index" : {"_id" : "23"}}\r\n    {"user" : "23"}\r\n    {"index" : {"_id" : "24"}}\r\n    {"user" : "24"}\r\n    {"index" : {"_id" : "25"}}\r\n    {"user" : "25"}\r\n    {"index" : {"_id" : "26"}}\r\n    {"user" : "26"}\r\n    {"index" : {"_id" : "27"}}\r\n    {"user" : "27"}\r\n    {"index" : {"_id" : "28"}}\r\n    {"user" : "28"}\r\n    {"index" : {"_id" : "29"}}\r\n    {"user" : "29"}\r\n    {"index" : {"_id" : "30"}}\r\n    {"user" : "30"}\r\n    {"index" : {"_id" : "31"}}\r\n    {"user" : "31"}\r\n    {"index" : {"_id" : "32"}}\r\n    {"user" : "32"}\r\n    {"index" : {"_id" : "33"}}\r\n    {"user" : "33"}\r\n    {"index" : {"_id" : "34"}}\r\n    {"user" : "34"}\r\n    {"index" : {"_id" : "35"}}\r\n    {"user" : "35"}\r\n    {"index" : {"_id" : "36"}}\r\n    {"user" : "36"}\r\n    {"index" : {"_id" : "37"}}\r\n    {"user" : "37"}\r\n    {"index" : {"_id" : "38"}}\r\n    {"user" : "38"}\r\n    {"index" : {"_id" : "39"}}\r\n    {"user" : "39"}\r\n    {"index" : {"_id" : "40"}}\r\n    {"user" : "40"}\r\n    {"index" : {"_id" : "41"}}\r\n    {"user" : "41"}\r\n    {"index" : {"_id" : "42"}}\r\n    {"user" : "42"}\r\n    {"index" : {"_id" : "43"}}\r\n    {"user" : "43"}\r\n    {"index" : {"_id" : "44"}}\r\n    {"user" : "44"}\r\n    {"index" : {"_id" : "45"}}\r\n    {"user" : "45"}\r\n    {"index" : {"_id" : "46"}}\r\n    {"user" : "46"}\r\n    {"index" : {"_id" : "47"}}\r\n    {"user" : "47"}\r\n    {"index" : {"_id" : "48"}}\r\n    {"user" : "48"}\r\n    {"index" : {"_id" : "49"}}\r\n    {"user" : "49"}\r\n    {"index" : {"_id" : "50"}}\r\n    {"user" : "50"}\r\n    {"index" : {"_id" : "51"}}\r\n    {"user" : "51"}\r\n    {"index" : {"_id" : "52"}}\r\n    {"user" : "52"}\r\n    {"index" : {"_id" : "53"}}\r\n    {"user" : "53"}\r\n    {"index" : {"_id" : "54"}}\r\n    {"user" : "54"}\r\n    {"index" : {"_id" : "55"}}\r\n    {"user" : "55"}\r\n    {"index" : {"_id" : "56"}}\r\n    {"user" : "56"}\r\n    {"index" : {"_id" : "57"}}\r\n    {"user" : "57"}\r\n    {"index" : {"_id" : "58"}}\r\n    {"user" : "58"}\r\n    {"index" : {"_id" : "59"}}\r\n    {"user" : "59"}\r\n    {"index" : {"_id" : "60"}}\r\n    {"user" : "60"}\r\n    {"index" : {"_id" : "61"}}\r\n    {"user" : "61"}\r\n    {"index" : {"_id" : "62"}}\r\n    {"user" : "62"}\r\n    {"index" : {"_id" : "63"}}\r\n    {"user" : "63"}\r\n    {"index" : {"_id" : "64"}}\r\n    {"user" : "64"}\r\n    {"index" : {"_id" : "65"}}\r\n    {"user" : "65"}\r\n    {"index" : {"_id" : "66"}}\r\n    {"user" : "66"}\r\n    {"index" : {"_id" : "67"}}\r\n    {"user" : "67"}\r\n    {"index" : {"_id" : "68"}}\r\n    {"user" : "68"}\r\n    {"index" : {"_id" : "69"}}\r\n    {"user" : "69"}\r\n    {"index" : {"_id" : "70"}}\r\n    {"user" : "70"}\r\n    {"index" : {"_id" : "71"}}\r\n    {"user" : "71"}\r\n    {"index" : {"_id" : "72"}}\r\n    {"user" : "72"}\r\n    {"index" : {"_id" : "73"}}\r\n    {"user" : "73"}\r\n    {"index" : {"_id" : "74"}}\r\n    {"user" : "74"}\r\n    {"index" : {"_id" : "75"}}\r\n    {"user" : "75"}\r\n    {"index" : {"_id" : "76"}}\r\n    {"user" : "76"}\r\n    {"index" : {"_id" : "77"}}\r\n    {"user" : "77"}\r\n    {"index" : {"_id" : "78"}}\r\n    {"user" : "78"}\r\n    {"index" : {"_id" : "79"}}\r\n    {"user" : "79"}\r\n    {"index" : {"_id" : "80"}}\r\n    {"user" : "80"}\r\n    {"index" : {"_id" : "81"}}\r\n    {"user" : "81"}\r\n    {"index" : {"_id" : "82"}}\r\n    {"user" : "82"}\r\n    {"index" : {"_id" : "83"}}\r\n    {"user" : "83"}\r\n    {"index" : {"_id" : "84"}}\r\n    {"user" : "84"}\r\n    {"index" : {"_id" : "85"}}\r\n    {"user" : "85"}\r\n    {"index" : {"_id" : "86"}}\r\n    {"user" : "86"}\r\n    {"index" : {"_id" : "87"}}\r\n    {"user" : "87"}\r\n    {"index" : {"_id" : "88"}}\r\n    {"user" : "88"}\r\n    {"index" : {"_id" : "89"}}\r\n    {"user" : "89"}\r\n    {"index" : {"_id" : "90"}}\r\n    {"user" : "90"}\r\n    {"index" : {"_id" : "91"}}\r\n    {"user" : "91"}\r\n    {"index" : {"_id" : "92"}}\r\n    {"user" : "92"}\r\n    {"index" : {"_id" : "93"}}\r\n    {"user" : "93"}\r\n    {"index" : {"_id" : "94"}}\r\n    {"user" : "94"}\r\n    {"index" : {"_id" : "95"}}\r\n    {"user" : "95"}\r\n    {"index" : {"_id" : "96"}}\r\n    {"user" : "96"}\r\n    {"index" : {"_id" : "97"}}\r\n    {"user" : "97"}\r\n    {"index" : {"_id" : "98"}}\r\n    {"user" : "98"}\r\n    {"index" : {"_id" : "99"}}\r\n    {"user" : "99"}\r\n    {"index" : {"_id" : "100"}}\r\n    {"user" : "100"}\r\n    \'\r\n\r\nSearch, sorting on `user`:\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/_search?pretty=1\'  -d \'\r\n    {\r\n       "sort" : {\r\n          "user" : "asc"\r\n       },\r\n       "fields" : [],\r\n       "size" : 10\r\n    }\r\n    \'\r\n\r\nResults show that `user:1` is in first position:\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "sort" : [\r\n    #                "1"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "1",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "10"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "10",\r\n    #             "_type" : "test"\r\n    #          },\r\n    # ....\r\n\r\nNow reindex the first doc, with the same values:\r\n \r\n    curl -XPUT \'http://127.0.0.1:9200/test/test/1?pretty=1\'  -d \'\r\n    {\r\n       "user" : "1"\r\n    }\r\n    \'\r\n\r\nAnd search again:\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/_search?pretty=1\'  -d \'\r\n    {\r\n       "sort" : {\r\n          "user" : "asc"\r\n       },\r\n       "fields" : [],\r\n       "size" : 10\r\n    }\r\n    \'\r\n\r\nDoc with `user:1` no longer appears in the correct position, in fact it doesn\'t appear anywhere in the first 10 results:\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "sort" : [\r\n    #                "10"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "10",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "100"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "100",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "11"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "11",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "12"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "12",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "13"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "13",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "14"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "14",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "15"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "15",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "16"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "16",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "17"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "17",\r\n    #             "_type" : "test"\r\n    #          },\r\n    #          {\r\n    #             "sort" : [\r\n    #                "18"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "18",\r\n    #             "_type" : "test"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : null,\r\n    #       "total" : 100\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 3\r\n    # }\r\n\r\n\r\nHowever, if you return all 100 docs, then it appears in the first position again (correctly):\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/_search?pretty=1\'  -d \'\r\n    {\r\n       "sort" : {\r\n          "user" : "asc"\r\n       },\r\n       "fields" : [],\r\n       "size" : 100\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "sort" : [\r\n    #                "1"\r\n    #             ],\r\n    #             "_score" : null,\r\n    #             "_index" : "test",\r\n    #             "_id" : "1",\r\n    #             "_type" : "test"\r\n    #          },\r\n\r\nwhich leads me to think that it is the shard level sorting which is incorrect.'
3077,'martijnvg',"Aliases: Add delete index alias api for deleting a single alias.\nAdd delete index alias api for deleting a single alias.\r\n\r\nOptions:\r\n* `index` - The index the alias is in, the needs to be deleted. This is a required option.\r\n* `alias` - The name of the alias to delete. This is a required option.\r\n\r\nThe rest endpoint is: /{index}/_alias/{alias}\r\n\r\nExample:\r\ncurl -XDELETE 'localhost:9200/users/_alias/user_12'"
3076,'martijnvg','Aliases: Add endpoint to add one specific index alias\nAdd put index alias api for adding a single index alias.\r\n\r\nOptions:\r\n* `index` - The index to alias refers to. This is a required option.\r\n* `alias` - The name of the alias. This is a required option.\r\n* `routing` - An optional routing that can be associated with an alias.\r\n* `filter` - An optional filter that can be associated with an alias. \r\n\r\nThe rest endpoint is: `/{index}/_alias/{alias}`\r\n\r\nExamples:\r\nAdding time based alias:\r\n```\r\ncurl -XPUT \'localhost:9200/logs_201305/_alias/2013\'\r\n```\r\n\r\nAdding user alias:\r\n```\r\ncurl -XPUT \'localhost:9200/users/_alias/user_12\' -d \'{\r\n\t"routing" : "12",\r\n\t"filter" : {\r\n\t\t"term" : {\r\n\t\t\t"user_id" : 12\r\n\t\t}\r\n\t}\t\r\n}\'\r\n```'
3075,'martijnvg','Aliases: Add get index alias api that allows get specific aliases\nAdd get index alias api that allows to filter by alias name and index name. This api redirects to the master and fetches the requested index aliases, if available. This api only serialises the found index aliases.\r\n\r\nPossible options:\r\n* `index` - The index name to get aliases for. Partially names are supported via wildcards, also multiple index names can be specified separated with a comma. Also the alias name for an index can be used.\r\n* `alias` - The name of alias to return in the response. Like the index option, this option supports wildcards and the option the specify multiple alias names separated by a comma. This is a required option.\r\n* `ignore_indices` - What to do is an specified index name doesn\'t exist. If set to `missing` then those indices are ignored.\r\n\r\nThe rest endpoint is: `/{index}/_alias/{alias}`\r\n\r\nExamples:\r\nAll aliases for the index users:\r\n```\r\ncurl -XGET \'localhost:9200/users/_alias/*\r\n {\r\n  "users" : {\r\n    "aliases" : {\r\n      "user_13" : {\r\n        "filter" : {\r\n          "term" : {\r\n            "user_id" : 13\r\n          }\r\n        },\r\n        "index_routing" : "13",\r\n        "search_routing" : "13"\r\n      },\r\n      "user_14" : {\r\n        "filter" : {\r\n          "term" : {\r\n            "user_id" : 14\r\n          }\r\n        },\r\n        "index_routing" : "14",\r\n        "search_routing" : "14"\r\n      },\r\n      "user_12" : {\r\n        "filter" : {\r\n          "term" : {\r\n            "user_id" : 12\r\n          }\r\n        },\r\n        "index_routing" : "12",\r\n        "search_routing" : "12"\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nAll aliases with the name 2013 in any index:\r\n```\r\ncurl -XGET \'localhost:9200/_alias/2013\r\n{\r\n  "logs_201304" : {\r\n    "aliases" : {\r\n      "2013" : { }\r\n    }\r\n  },\r\n  "logs_201305" : {\r\n    "aliases" : {\r\n      "2013" : { }\r\n    }\r\n  }\r\n}\r\n```\r\nAll aliases that start with 2013_01 in any index\r\n```\r\ncurl -XGET \'localhost:9200/_alias/2013_01*\r\n{\r\n  "logs_20130101" : {\r\n    "aliases" : {\r\n      "2013_01" : { }\r\n    }\r\n  }\r\n}\r\n```'
3073,'chilling','Regresion: geo distance filter - filters out proper geohashes\nMapping of searched field:\r\n```\r\nlocation: {\r\n   type: geo_point\r\n}\r\n```\r\nI tried also with:\r\n```\r\nlocation: {\r\n   lat_lon: true,\r\n   type: geo_point,\r\n   geohash: true,\r\n   geohash_precision: 24\r\n}\r\n````\r\n\r\nQuery:\r\n```\r\n{\r\n  "from": 0,\r\n  "fields": [\r\n    "_id",\r\n    "_parent",\r\n    "_routing",\r\n    "_source"\r\n  ],\r\n  "filter": {\r\n        "geo_distance": {\r\n          "distance": "50mi",\r\n          "optimize_bbox": "memory",\r\n          "location": {\r\n            "lat": 40.720611,\r\n            "lon": -73.998776\r\n          }\r\n        }\r\n  },\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "size": 20\r\n}\r\n```\r\n\r\nData:\r\n```\r\n{\r\n "location": "dr5rshgwz81eqnfrrrhz"\r\n}\r\n```\r\n\r\n\r\nIt works for 0.20.2 and don\'t work for current release 0.90.0\r\n\r\nEDIT: Sorry I post it to fast, data attached :)'
3071,'martijnvg','percolate requires action.auto_create_index and index.mapper.dynamic\nWhen disabling automatic creation of indexes/mappings (action.auto_create_index:false and index.mapper.dynamic:false in elasticsearch.yml), percolation fails.\r\n\r\nPercolation is a feature provided by elasticsearch, and as such, should not depend on the fact it\'s implemented with an index.\r\n\r\nTo reproduce, just run a server with this configuration, and execute the examples in the percolate documentation (guide/reference/api/percolate/). When trying to register a query in the percolator, error is: \r\n```{"error":"IndexMissingException[[_percolator] missing]","status":404}```\r\nOk, there is a workaround: set action.auto_create_index to +_* or +_percolator. Then,\r\n```{"error":"TypeMissingException[[_percolator] type[test] missing: trying to auto create mapping, but dynamic mapping is disabled]","status":404}```\r\n\r\nIs there any way to use percolation while preventing indexing from creating mappings on the fly?'
3066,'bleskes',"Report previous version in IndexResponse when external version type used\nWhen indexing, you get back in the response the version number that was assigned to the document you put. This makes sense, and quite useful in many scenarios - for example in ours, where we check to see if it == 1 and if it does we know its a new entry and can do perform some extra actions.\r\n\r\nWhen specifying a version yourself by using the external version type, you already know what version to expect in the response if the indexing operation succeeds. Hence, the version returned is quite meaningless. In this scenario, I would really like to get the PREVIOUS version. For us this would mean knowing whether this is a new entry or not.\r\n\r\nI'll be happy to provide a pull request with this change if this helps :)"
3065,'s1monw','QueryStringQuery overwrites parsed boost value\nSet the query boost of a parsed query string query to the product of\r\nthe parsed query boost and the boost value specified in the "boost"\r\nquery string parameter.\r\n\r\nfixes #3024'
3064,'s1monw','Regex on terms facet doesn\'t work\nI have objects with region_ids with mapping:\r\n```        \r\n"region_ids" : {\r\n          "type" : "integer"\r\n        },\r\n```\r\nFor example:\r\n```\r\n{\r\n\'_id\': 52,\r\n\'region_ids\': [3, 4, 12, 13, 14, 18, 30, 31, 30067]\r\n}\r\n```\r\n\r\nFacet Query:\r\n```\r\n{\'facets\': {\'regions\': {\'terms\': {\'field\': \'region_ids\',\r\n    \'regex\': \'62191|75|30\',\r\n    \'regex_flags\': \'DOTALL\',\r\n    \'size\': 10}}},\r\n \'fields\': [\'_id\', \'_parent\', \'_routing\', \'_source\'],\r\n \'from\': 0,\r\n \'query\': {\'match_all\': {}},\r\n \'size\': 10}\r\n```\r\n\r\nAnd it returns facet terms not filtered by regex:\r\n```\r\n{\'regions\': {\'_type\': \'terms\',\r\n  \'missing\': 5,\r\n  \'other\': 570,\r\n  \'terms\': [{\'count\': 52, \'term\': 30067},\r\n   {\'count\': 48, \'term\': 4},\r\n   {\'count\': 44, \'term\': 31},\r\n   {\'count\': 43, \'term\': 29902},\r\n   {\'count\': 43, \'term\': 75},\r\n   {\'count\': 40, \'term\': 30},\r\n   {\'count\': 32, \'term\': 13},\r\n   {\'count\': 23, \'term\': 12},\r\n   {\'count\': 23, \'term\': 3},\r\n   {\'count\': 19, \'term\': 29398}],\r\n  \'total\': 937}}\r\n```\r\n\r\nIt works with script and exlude, but not with regex, so I assume it\'s a bug.\r\nI try regexs:\r\n```\r\n62191|75|30\r\n^(62191|75|30)$\r\n75\r\n[75]*\r\n```\r\nIt just doesn\'t work.\r\n\r\nES newest build 0.90'
3063,'kimchy','Query DSL: External terms doesn\'t work with _id field\n    curl -XPUT http://localhost:9200/index1/t1/123 -d \'{ "name": "123" }\'\r\n    curl -XPUT http://localhost:9200/index1/t1/456 -d \'{ "name": "456" }\'\r\n    curl -XPUT http://localhost:9200/index1/t2/1 -d \'{ "ids": ["123", "456"] }\'\r\n\r\nQuery with external terms returns no results:\r\n\r\n    curl http://localhost:9200/index1/t1/_search?pretty -d \'{ "query": { "filtered": { "filter": { "terms": { "_id": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }\'\r\n\r\nQuery with listed terms works:\r\n\r\n    curl http://localhost:9200/index1/t1/_search ?pretty -d \'{ "query": { "filtered": { "filter": { "terms": { "_id": ["123", "456"] } } } } }\'\r\n\r\nExternal terms on `name` field works:\r\n\r\n    curl http://localhost:9200/index1/t1/_search?pretty -d \'{ "query": { "filtered": { "filter": { "terms": { "name": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }\'\r\n\r\n\r\nSide issue: unmapped field throws NPE:\r\n\r\n    curl http://localhost:9200/index1/t1/_search?pretty -d \'{ "query": { "filtered": { "filter": { "terms": { "XXX": { "index": "index1", "type": "t2", "id": "1", "path": "ids" } } } } } }\'\r\n\r\n'
3061,'s1monw',"MultiGetRequest does not expose its list of items\nThe class MultiGetRequest does not provide a way to access the list of get request. However, it's sometimes useful to look at the individual item before executing the request. Note that the class MultiSearchRequest provides a method called request() to expose the list of individual request.\r\n"
3060,'spinscale',"Wrong lucene-core version being pulled in by maven.\nUsing ES 0.90.0 Java API, Maven pulls in lucene-core 3.6 instead of 4.2.1\r\nHere's the snipplet from the output of mvn dependency:tree for a project using ES API 0.90.0\r\n\r\n```\r\n 789 [INFO] +- org.elasticsearch:elasticsearch:jar:0.90.0:compile\r\n 790 [INFO] |  +- org.apache.lucene:lucene-core:jar:3.6.1:compile (version managed from 4.2.1)\r\n 791 [INFO] |  +- org.apache.lucene:lucene-analyzers-common:jar:4.2.1:compile\r\n 792 [INFO] |  +- org.apache.lucene:lucene-codecs:jar:4.2.1:compile\r\n 793 [INFO] |  +- org.apache.lucene:lucene-queries:jar:4.2.1:compile\r\n 794 [INFO] |  +- org.apache.lucene:lucene-memory:jar:4.2.1:compile\r\n 795 [INFO] |  +- org.apache.lucene:lucene-highlighter:jar:4.2.1:compile\r\n 796 [INFO] |  +- org.apache.lucene:lucene-queryparser:jar:4.2.1:compile\r\n 797 [INFO] |  |  \\- org.apache.lucene:lucene-sandbox:jar:4.2.1:compile\r\n 798 [INFO] |  +- org.apache.lucene:lucene-suggest:jar:4.2.1:compile\r\n 799 [INFO] |  +- org.apache.lucene:lucene-join:jar:4.2.1:compile\r\n 800 [INFO] |  |  \\- org.apache.lucene:lucene-grouping:jar:4.2.1:compile\r\n 801 [INFO] |  \\- org.apache.lucene:lucene-spatial:jar:4.2.1:compile\r\n```\r\n\r\nlucene-core should be 4.2.1 , but 3.6.1 is being pulled in, which results in  error when loading org.elasticsearch.Version class."
3059,'s1monw','Integrate forbiddenAPIs checks into ElasticSearch\nhttps://code.google.com/p/forbidden-apis/ integrates with maven and checks Java byte code against a list of "forbidden" API signatures. It fails the builds if deprecated JDK methods are used or if Streams are created with default charsets, if default locales or timezones are used or if encodings are missing on String formatting etc. \r\n\r\nThis helps to prevent issues that occur on the users systems if locals are different or charsets are oddly set.'
3055,'spinscale','ClasscastException in GetResponse for binary data\nSomeone on the mailinglist mentioned a seemingly random occuring issue when executing a GetRequest...\r\n\r\nI managed to track down the issue, but the exception still occurs randomly (my assumption is, that it depends whether the data needs to be fetched remotely).\r\n\r\nThis is the test case\r\n\r\n```java\r\npublic class SearchClassCastTest extends AbstractNodesTests {\r\n\r\n    private Client client;\r\n\r\n    @BeforeClass\r\n    public void createNodes() throws Exception {\r\n        startNode("node1");\r\n        startNode("node2");\r\n        client = node("node1").client();\r\n    }\r\n\r\n    @AfterClass\r\n    public void closeNodes() {\r\n        client.close();\r\n        closeAllNodes();\r\n    }\r\n\r\n    @Test\r\n    public void flakyAssertion() throws Exception {\r\n        try {\r\n            client.admin().indices().prepareDelete("test").execute().actionGet();\r\n        } catch (IndexMissingException e) {}\r\n        client.admin().indices().prepareCreate("test").execute().actionGet();\r\n\r\n        String mapping = jsonBuilder().startObject()\r\n                .startObject("test")\r\n                    .startObject("properties")\r\n                        .startObject("data")\r\n                            .field("type","binary")\r\n                        .endObject()\r\n                    .endObject()\r\n                .endObject()\r\n                .string();\r\n        client.admin().indices().preparePutMapping("test").setType("test").setSource(mapping).execute().actionGet();\r\n\r\n        client.prepareIndex("test", "test", "0").setSource(jsonBuilder().startObject()\r\n                .field("data", "dm9vZG9v")\r\n            .endObject())\r\n            .setRefresh(true)\r\n            .execute().actionGet();\r\n\r\n        GetResponse g = client.prepareGet("test", "test", "0").setFields("data").execute().actionGet();\r\n        assertThat(g.isExists(), is(true));\r\n\r\n        Object data= g.getField("data").getValue();\r\n        // FLAKY ASSERTION HERE\r\n        assertThat(data, instanceOf(BytesArray.class));\r\n    }\r\n\r\n\r\n```\r\n\r\nThe assertion error is this\r\n\r\n```\r\njava.lang.AssertionError: \r\nExpected: an instance of org.elasticsearch.common.bytes.BytesArray\r\n     but: <org.elasticsearch.common.bytes.ChannelBufferBytesReference@cfac04ae> is a org.elasticsearch.common.bytes.ChannelBufferBytesReference\r\n\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n\tat org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:8)\r\n\tat org.elasticsearch.test.integration.search.simple.SearchClassCastTest.flakyAssertion(SearchClassCastTest.java:87)\r\n```\r\n\r\nMy assumption is, that we just forgot to call `ChannelBufferBytesReference.toBytesArray()` somewhere when getting the data.\r\n\r\nAlso, the test fails never when ES_LOCAL_MODE is set. And you need more than one node.\r\n\r\n'
3051,'s1monw','ArrayIndexOutOfBoundsException in org.elasticsearch.index.fielddata.ScriptDocValues.Strings\ngetValues() causes an ArrayIndexOutOfBoundsException in case the list is larger than 10 elements because the SlicedObjectList.grow() method is never called. ES-version 0.90.0'
3049,'spinscale','Changed daemon command to use $ES_USER\nI just installed `elasticsearch-0.90.0-1.noarch.rpm` and have not been able to start/stop elasticsearch using `service elasticsearch start`.\r\n\r\nAt line 67 where it tries to exec the `daemon` command, it is using a variable `$USER`, which is not set anywhere in the script, which causes that command to fail.\r\n\r\nThis should be changed to `$ES_USER`.'
3047,'s1monw','Date parsing is locale dependant with no way to configure\nSome date formats are locale dependent, e.g. the one given in the enron emails (http://www.cs.cmu.edu/~enron/). The format is:\r\n\r\n"E, d MMM yyyy HH:mm:ss Z"\r\ne.g.\r\n"Wed, 06 Dec 2000 02:55:00 -0800"\r\n\r\nE is locale dependent, in the German locale, it is "Mi" (afaik, its certainly not "Wed"). Parsing this date fails with `de_DE.UTF-8` and succeeds with `en_US.UTF-8` as environment setting. Sadly, this is not settable for date parsing, e.g.:\r\n\r\n```\r\n"type" : "date",\r\n"format" : "E, d MMM yyyy HH:mm:ss Z",\r\n"locale" : "US"\r\n```\r\n(where locale is a java.util.Locale)\r\n\r\nThis means that elasticsearch can only ever parse dates in one format per cluster. (or worse, depending on the locale of the node).'
3044,'martijnvg','Sorting: Also support nested sorting for sorting by script and geo distance sorting\n'
3039,'martijnvg',"Mlt api doesn't serialize routing\nIn some cases the mlt api redirects the request to a different node and because of the fact that the routing option isn't serialized during the redirecting it can end up going to the a node that doesn't hold the right shard (based on routing option). This results in a document missing error."
3037,'s1monw','Phrase suggest direct generator possibly not obeying min_word_len 0.90 \nI ran into an issue where the phrase suggester does not seem to be generating terms for words of length less than the default of four even with the min_word_len set to 0,1,2, or 3. When I run a term suggest, the term comes back as expected.\r\n\r\nHere is a gist reproducing the issue:\r\nhttps://gist.github.com/jtreher/5577747'
3035,'s1monw','Config: Use Recovery Throttling by default in 0.90.1\nalong the same lines as #3033 recovery throttling is very frequently used in production for cluster stability. we should use it by default as well to prevent usually unexpected impact on search performance due to recovery operations.'
3033,'s1monw',"Config: Use Merge Throttling by default in 0.90.1\nMerge Throttling is one of the most recommended settings and crucial in the RealTime indexing case. We should set the default to a  reasonable setting that allows folks to index in a production index and don't see large merge peaks by default. Yet, the default here is hard to calculate and solely relies on experience from production environments. The `right` settings depends on the actual hardware used and can't be easily predicted.\r\n\r\nif somebody has experience with a good setting, sharing HW setup and throttle setting would be much appreciated."
3032,'s1monw','Config: Raise Search ThreadPool Size to 3x availableProcessors\n2x available processors seems kind of lowish and in practice 3x seems to be a sweetspot. We should raise the limit to 3x as a default value'
3028,'martijnvg','Parent-Child: Improve memory usage id cache\nThe id cache loads the ids of all documents. If we only load the parent ids, then we can reduce the memory usage of the parent/child support.\r\n\r\nUpdate: The amount of memory that will be reduced when upgrading to version `0.90.1` depends on the amount of child documents in an index. For example if you have one child doc for every parent doc, then it the memory usage of the id cache should be reduced by around half. The more child docs per parent doc the larger the difference. '
3024,'s1monw','MatchQueryParser doesn\'t allow field boosting on query when included in a _GET request\nIn the following query, we try to boost the FirstLast field by 7 and it fails:\r\n\r\n/_search?q=(FirstLast%3A"johnsmith")^7&explain=true\r\n...\r\n_explanation": {\r\n\r\n    "value": 10,\r\n    "description": "weight(FirstLast:johnsmith in 0) [PerFieldSimilarity], result of:"\r\n\r\nThis behavior works correctly in the MultiMatchQueryParser:\r\n_search?q=(FirstLast%3A"johnsmith")^7+(State%3A"wa")&explain=true\r\n...\r\n"_explanation": {\r\n\r\n    "value": 80,\r\n    "description": "sum of:",\r\n    "details": [\r\n        {\r\n            "value": 70,\r\n            "description": "weight(FirstLast:johnsmith^7.0 in 0) [PerFieldSimilarity], result of:",\r\n\r\n\r\nNote that the boost does work when the request is a _POST as so:\r\n_search" -d \'{"query":{"term":{"State":{"value":"wa","boost":7.0}}}}\'\r\n{"took":2,"timed_out":false,"_shards":{"total":1,"successful":1,"failed":0},"hits":{"total":1,"max_score":70.0,"hits":[{"_index":"3","_type":"people","_id":"XXXXXXXXXX","_score":70.0, "_source" : {"FirstLast":["johnsmith"],"State":["wa"]}}]}}\r\n\r\nI\'m just getting used to the code and have no debugger setup yet so forgive me if I\'m mistaken or incorrectly using this.'
3017,'imotov',"Invalid internal transport message format between 0.20.6 master and client caused re-election\nA client connecting to a master resulted in \r\n\r\n ```\r\njava.io.StreamCorruptedException: invalid internal transport message format\r\n``` \r\n\r\nBoth client and server are running 0.20.6 (stack trace from the master below). This seems to be a truncated message (client died or network problem), but it's difficult to say. It looks to have badly hung the master, which became unresponsive on the 9300 port but was honouring requests on 9200, claiming the cluster was green.  In the meantime other nodes in the cluster were failing to get responses from it. The two other master eligible nodes in the cluster excised it and performed a re-election a few minutes afterwards. The original master was unable to join the cluster until it was restarted - up to that point it was reporting the cluster as green. \r\n\r\nWhat happened after the exception is a concern as the cluster seems to have cascaded into a bad state -\r\n\r\n - The rest of cluster went yellow as a mass re-allocation of replicas occurred.\r\n\r\n - This is an example of what showed up in the logs, which stopped after about 1 minute after the election - \r\n\r\n```\r\n[2013-05-09 11:34:52,232][WARN ][indices.cluster          ] [ip-10-239-70-202] [profiles_0001][26] master [[ip-10-34-144-149][IsP0kjtRS6KJ-9R3hZehwQ][inet[/10.34.144.149:9300]]{data=false, master=true, zone=eu-west-1c}] marked shard as started, but shard have not been created, mark shard as failed\r\n[2013-05-09 11:34:52,232][WARN ][cluster.action.shard     ] [ip-10-239-70-202] sending failed shard for [profiles_0001][26], node[nUOPQBwwTdihgBPosOdbxA], [P], s[STARTED], reason [master [ip-10-34-144-149][IsP0kjtRS6KJ-9R3hZehwQ][inet[/10.34.144.149:9300]]{data=false, master=true, zone=eu-west-1c} marked shard as started, but shard have not been created, mark shard as failed]\r\n```\r\n\r\n - After about 7hours the 3 replicas in progress remain unassigned. We concluded the allocation process had failed at this point. \r\n\r\nSome details -\r\n\r\n - Cluster is running in AWS with ec2 discovery\r\n - 6 data nodes, 3 master nodes \r\n - The masters are dedicated (`node.master: true`; `node.data: false`)\r\n - The data nodes are dedicated (`node.master: false`; `node.data: true`)\r\n - `discovery.zen.minimum_master_nodes: 2`\r\n\r\n\r\n\r\n```\r\n2013-05-09 10:32:42,015][WARN ][discovery.ec2            ] [ip-10-208-11-218] received a join request for an existing node [[ip-10-36-129-154][jaqf1e_AS0aE_6CXupb2IQ][inet[/10.36.129.154:9300]]{client=true, data=false}]\r\n[2013-05-09 11:32:51,646][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xbab451d1, /10.36.129.154:37310 => /10.208.11.218:9300]], closing connection\r\njava.io.StreamCorruptedException: invalid internal transport message format\r\n\tat org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-05-09 11:32:51,648][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xbab451d1, /10.36.129.154:37310 :> /10.208.11.218:9300]], closing connection\r\njava.io.StreamCorruptedException: invalid internal transport message format\r\n\tat org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:570)\r\n\tat org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)\r\n\tat org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)\r\n\tat org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:505)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:227)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)\r\n\tat org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:654)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:562)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-05-09 11:32:54,717][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xd9e316de, /10.36.129.154:37311 => /10.208.11.218:9300]], closing connection\r\njava.io.StreamCorruptedException: invalid internal transport message format\r\n\tat org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-05-09 11:32:54,720][WARN ][transport.netty          ] [ip-10-208-11-218] exception caught on transport layer [[id: 0xd9e316de, /10.36.129.154:37311 :> /10.208.11.218:9300]], closing connection\r\njava.io.StreamCorruptedException: invalid internal transport message format\r\n\tat org.elasticsearch.transport.netty.SizeHeaderFrameDecoder.decode(SizeHeaderFrameDecoder.java:27)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:425)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.cleanup(FrameDecoder.java:482)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.channelDisconnected(FrameDecoder.java:365)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:102)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireChannelDisconnected(Channels.java:396)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.close(AbstractNioWorker.java:336)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:81)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:36)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendDownstream(DefaultChannelPipeline.java:570)\r\n\tat org.elasticsearch.common.netty.channel.Channels.close(Channels.java:812)\r\n\tat org.elasticsearch.common.netty.channel.AbstractChannel.close(AbstractChannel.java:197)\r\n\tat org.elasticsearch.transport.netty.NettyTransport.exceptionCaught(NettyTransport.java:505)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler.exceptionCaught(MessageChannelHandler.java:227)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.exceptionCaught(FrameDecoder.java:377)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:112)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireExceptionCaught(Channels.java:525)\r\n\tat org.elasticsearch.common.netty.channel.AbstractChannelSink.exceptionCaught(AbstractChannelSink.java:48)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.notifyHandlerException(DefaultChannelPipeline.java:654)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:562)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:74)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n````"
3013,'s1monw',"Analysis: Expose LimitTokenCountFilter in ElasticSearch\nExpose Lucene's LimitTokenCountFilter to ElasticSeach so we can pick the number of tokens to index on multi-valued data (array fields)."
3012,'s1monw',"DFS modes can cause undefined behaviour in 0.90 \nCurrently some of the transport protocols are broken if Term/FieldStatistics don't provide all optional information. For instance we use `totalTermFrequency` in the TermStats but if you omit the term frequency the `totalTermFrequency` is a negative 1 (`-1`). This also happens if an index gets upgrade to 0.90 from a 0.20 version that runs lucene 3.6 and doesn't have these stats so they are -1 by default.  This seems to be the cause for #3008 as well as #2932 which eventually fail with NPE since the reading behavior of vlongs is undefined."
3008,'s1monw','NPE when using java client with DFS_QUERY_THEN_FETCH\nhere\'s the request.toString() content, this request works fine with REST API, but not work with java client.\r\n{\r\n  "from" : 0,\r\n  "size" : 5000,\r\n  "query" : {\r\n    "bool" : {\r\n      "must" : [ {\r\n        "match" : {\r\n          "tag" : {\r\n            "query" : "Custom",\r\n            "type" : "phrase"\r\n          }\r\n        }\r\n      }, {\r\n        "range" : {\r\n          "timestamp" : {\r\n            "from" : 1367251200000,\r\n            "to" : 1367942400000,\r\n            "include_lower" : true,\r\n            "include_upper" : true\r\n          }\r\n        }\r\n      } ]\r\n    }\r\n  },\r\n  "explain" : false,\r\n  "fields" : [ "tag", "buildId", "timestamp", "day", "count" ],\r\n  "sort" : [ {\r\n    "timestamp" : {\r\n      "order" : "desc"\r\n    }\r\n  } ]\r\n}\r\nbelow is the response:\r\n{\r\n  "took" : 5,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 0,\r\n    "failed" : 5,\r\n    "failures" : [ {\r\n      "status" : 500,\r\n      "reason" : "SendRequestTransportException[[es-node-6][inet[/10.217.41.206:9300]][search/phase/query/id]]; nested: NullPointerException; "\r\n    }, {\r\n      "status" : 500,\r\n      "reason" : "SendRequestTransportException[[es-node-5][inet[/10.8.199.3:9300]][search/phase/query/id]]; nested: NullPointerException; "\r\n    }, {\r\n      "status" : 500,\r\n      "reason" : "SendRequestTransportException[[es-node-5][inet[/10.8.199.3:9300]][search/phase/query/id]]; nested: NullPointerException; "\r\n    }, {\r\n      "status" : 500,\r\n      "reason" : "SendRequestTransportException[[es-node-7][inet[/10.143.135.88:9300]][search/phase/query/id]]; nested: NullPointerException; "\r\n    }, {\r\n      "status" : 500,\r\n      "reason" : "SendRequestTransportException[[es-node-7][inet[/10.143.135.88:9300]][search/phase/query/id]]; nested: NullPointerException; "\r\n    } ]\r\n  },\r\n  "hits" : {\r\n    "total" : 0,\r\n    "max_score" : 0.0,\r\n    "hits" : [ ]\r\n  }\r\n}\r\n'
3007,'s1monw','Query DSL: field_masking_span query parser not registered\nfield_masking_span  has been introduced via (now closed) issue #471.\r\n\r\nThe FieldMaskingSpanQueryParser is not registred in org/elasticsearch/indices/query/IndicesQueriesModule.java (version 0.90.0). I guess that adding\r\n```java\r\nqpBinders.addBinding().to(FieldMaskingSpanQueryParser.class).asEagerSingleton();\r\n```\r\nfixes the issue.'
3006,'s1monw','Highlighting: Highlighter still fails if broken analysis chains are  used with fast vector highlighter\nTokenFilters like `word_delimiter_filter` might produce broken term_vectors and mess up highlighting. We still fail with `StringIndexOutOfBoundsException` which is no good. Even if those filters are broken we should try at least best effort to not fail with a SIOOBException'
3004,'bleskes','key_script for term stats facets\nI cannot find a script for key fields in term stats. That can also be very helpful.\r\n\r\nThanks'
3003,'s1monw','empty sort parameter slows down query time\nHi there,\r\n\r\nI\'m using elasticsearch 0.20.4.\r\n\r\nWhen applying an empty sort, query time slows down:\r\n\r\n```\r\n# [Mon May  6 14:47:39 2013] Protocol: http, Server: 157.193.59.144:9200\r\ncurl -XGET \'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&pretty=1&search_type=query_then_fetch\'  -d \'\r\n{\r\n   "sort" : [],\r\n   "query" : {\r\n      "has_child" : {\r\n         "query" : {\r\n            "query_string" : {\r\n               "fields" : [\r\n                  "_all"\r\n               ],\r\n               "query" : "ugent",\r\n               "default_operator" : "AND",\r\n               "use_dis_max" : "true"\r\n            }\r\n         },\r\n         "type" : "child"\r\n      }\r\n   },\r\n   "from" : 0\r\n}\r\n\'\r\n\r\n# [Mon May  6 14:48:03 2013] Response:\r\n# {\r\n#    "hits" : {\r\n#       "hits" : [\r\n#         ..          \r\n#       ],\r\n#       "max_score" : 1,\r\n#       "total" : 1402728\r\n#    },\r\n#    "timed_out" : false,\r\n#    "_shards" : {\r\n#       "failed" : 0,\r\n#       "successful" : 5,\r\n#       "total" : 5\r\n#    },\r\n#    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMjE4Ok5KLXZkQkZkUnp1Wl\r\n# >    dObDJLQ2VNcFE7MTIyMDpOSi12ZEJGZFJ6dVpXTmwyS0NlTXBROzEyMTY6Tk\r\n# >    otdmRCRmRSenVaV05sMktDZU1wUTsxMjE3Ok5KLXZkQkZkUnp1WldObDJLQ2\r\n# >    VNcFE7MTIxOTpOSi12ZEJGZFJ6dVpXTmwyS0NlTXBROzA7",\r\n#    "took" : 24675\r\n# }\r\n```\r\n\r\nWhen the parameter is not set it runs a LOT faster:\r\n\r\n```\r\n# [Tue May  7 08:43:30 2013] Protocol: http, Server: 157.193.59.144:9200\r\ncurl -XGET \'http://127.0.0.1:9200/meercat/parent/_search?scroll=1m&pretty=1&search_type=scan\'  -d \'\r\n{\r\n   "query" : {\r\n      "has_child" : {        \r\n         "query" : {\r\n            "query_string" : {\r\n               "fields" : [\r\n                  "_all"\r\n               ],\r\n               "query" : "ugent",\r\n               "default_operator" : "AND",\r\n               "use_dis_max" : "true"\r\n            }\r\n         },        \r\n         "type" : "child"\r\n      }\r\n   },\r\n   "from" : 0\r\n}\r\n# [Tue May  7 08:43:31 2013] Response:\r\n# {\r\n#    "hits" : {\r\n#       "hits" : [],\r\n#       "max_score" : 0,\r\n#       "total" : 1405509\r\n#    },\r\n#    "timed_out" : false,\r\n#    "_shards" : {\r\n#       "failed" : 0,\r\n#       "successful" : 5,\r\n#       "total" : 5\r\n#    },\r\n#    "_scroll_id" : "c2Nhbjs1Ozg6OGdNdGZwNzFTck9sSm1OSEZQY2dlZzs2Oj\r\n# >    hnTXRmcDcxU3JPbEptTkhGUGNnZWc7MTA6OGdNdGZwNzFTck9sSm1OSEZQY2\r\n# >    dlZzs5OjhnTXRmcDcxU3JPbEptTkhGUGNnZWc7Nzo4Z010ZnA3MVNyT2xKbU\r\n# >    5IRlBjZ2VnOzE7dG90YWxfaGl0czoxNDA1NTA5Ow==",\r\n#    "took" : 357\r\n# }\r\n\r\n```\r\n\r\nSo that is 24675 for a empty sort, while it takes 357 milliseconds for not supplying the sort parameter at all.\r\n\r\nAny idea what causes this behaviour?\r\n\r\nThanks in advance\r\n\r\n'
3000,'imotov','Get doc fails for some array fields\nSee the following two scripts to reproduce:\r\nhttps://gist.github.com/dakrone/5528301\r\nhttps://gist.github.com/dakrone/5528298\r\n\r\nWhen run:\r\n\r\n```\r\n∴ ./broken-get.zsh \r\n{"ok":true,"acknowledged":true}{"ok":true,"acknowledged":true}\r\n{"ok":true,"_index":"get-test","_type":"doc","_id":"1","_version":1}\r\n{"ok":true,"_index":"get-test","_type":"doc","_id":"2","_version":1}\r\n{\r\n  "_index" : "get-test",\r\n  "_type" : "doc",\r\n  "_id" : "1",\r\n  "_version" : 1,\r\n  "exists" : true, "_source" : {"date":"2010-01-01"}\r\n}\r\n{\r\n  "_index" : "get-test",\r\n  "_type" : "doc",\r\n  "_id" : "2",\r\n  "_version" : 1,\r\n  "exists" : true, "_source" : {"date":["2010-01-01","2011-01-01"]}\r\n}\r\n{\r\n  "error" : "MapperParsingException[failed to parse date field [[2010-01-01, 2011-01-01]], tried both date format [yyyy-MM-dd], and timestamp number]; nested: IllegalArgumentException[Invalid format: \\"[2010-01-01, 2011-01-01]\\"]; ",\r\n  "status" : 400\r\n}\r\n```\r\n\r\nand:\r\n\r\n```\r\n∴ ./broken-get2.zsh \r\n{"ok":true,"acknowledged":true}{"ok":true,"acknowledged":true}\r\n{"ok":true,"_index":"get-test","_type":"doc","_id":"1","_version":1}\r\n{"ok":true,"_index":"get-test","_type":"doc","_id":"2","_version":1}\r\n{\r\n  "_index" : "get-test",\r\n  "_type" : "doc",\r\n  "_id" : "1",\r\n  "_version" : 1,\r\n  "exists" : true, "_source" : {"num":2}\r\n}\r\n{\r\n  "_index" : "get-test",\r\n  "_type" : "doc",\r\n  "_id" : "2",\r\n  "_version" : 1,\r\n  "exists" : true, "_source" : {"num":[2,1]}\r\n}\r\n{\r\n  "error" : "NumberFormatException[For input string: \\"[2, 1]\\"]",\r\n  "status" : 500\r\n}\r\n```'
2994,'s1monw','Query DSL: span_near query not working\nHi,\r\n\r\nMaking a simple span_near query just fails. Running\r\n  query: span_near : { clauses : [ { span_term : { index.field : value } } ] }\r\nI get\r\n  QueryParsingException[[index] spanNear [clauses] must be of type span query]\r\nI don\'t get any error if I put an unknown field.\r\n\r\nNB I\'m using version 0.90.0.\r\n\r\nBy looking at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/query/SpanTermQueryParser.java\r\n```java\r\n      33. import static org.elasticsearch.index.query.support.QueryParsers.wrapSmartNameQuery;\r\n     106. SpanTermQuery query = new SpanTermQuery(new Term(fieldName, valueBytes));\r\n     108. return wrapSmartNameQuery(query, smartNameFieldMappers, parseContext);\r\n```\r\nI supposed that QueryParsers#wrapSmartNameQuery() does not return a span query (but a XFilteredQuery).\r\n\r\n\r\nTo reproduce (bash):\r\n\r\n```bash\r\nHOSTPORT="127.0.0.1:9200"\r\n\r\nfunction create_schema () {\r\n    local SCHEMA=\'\r\n{\r\n    "mappings" : {\r\n        "bugnearspanindex" : {\r\n            "properties" : {\r\n                "myField" : { "type" : "string" }\r\n            }\r\n        }\r\n    }\r\n}\r\n\'\r\n    curl --noproxy \'*\' -XPUT "http://${HOSTPORT}/bugnearspanindex/" -d "${SCHEMA}"\r\n}\r\n\r\n\r\nfunction make_and_run_query () {\r\n    local FIELD=$1\r\n    local QUERY=\'\r\n{\r\n    "query" : {\r\n        "span_near" : {\r\n            "clauses" : [\r\n                { "span_term" : { "bugnearspanindex.\'${FIELD}\'" : "foobar" } }\r\n            ],\r\n            "slop" : 0,\r\n            "in_order" : false,\r\n            "collect_payloads" : false\r\n        }\r\n    }\r\n}\r\n\'\r\n    curl --noproxy \'*\' -XGET "http://${HOSTPORT}/bugnearspanindex/_search?pretty=true&format=yaml" -d "${QUERY}"\r\n}\r\n\r\n# create the schema\r\ncreate_schema\r\n\r\n# run the request on a non existing field\r\nmake_and_run_query nonExistingField\r\n  # -> hits: { total: 0 }\r\n\r\n# run the request on an existing field\r\nmake_and_run_query myField\r\n  # -> error: QueryParsingException[[bugnearspanindex] spanNear [clauses] must be of type span query]\r\n```'
2991,'s1monw','BytesRefOrdValComparator ignores highest value in a segment during binarySearch\nThe BytesRefOrdValComparator uses `Ordinals.Docs.getNumOrdinals() -1` as the upperbound for the binarysearch. The `-1` causes that we ignore the last value in the segment. \r\n\r\nThis is kind of a very tricky bug since it only happens if we need to binary-search to align ords and the bottom of the sort queue is greater than the largest value in the segment but less than the second largest. This was causing an issue reported by a user on the mailing list: https://groups.google.com/d/msg/elasticsearch/W5s1KypYcYw/L1UgixO_gQ4J'
2990,'s1monw','Upgrade to Lucene 4.3.0\nLucene 4.3.0 release vote has passed. Yet once the release is officially announce we can upgrade master and 0.90'
2989,'spinscale','Unable to run elasticsearch - Exception in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version\nI got an error using elasticsearch 0.90.0\r\n\r\nI am unable to run it using elasticsearch sh script.\r\n\r\nException in thread "main" java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.Version\r\n\tat org.elasticsearch.bootstrap.Bootstrap.buildErrorMessage(Bootstrap.java:251)\r\n\tat org.elasticsearch.bootstrap.Bootstrap.main(Bootstrap.java:235)\r\n\tat org.elasticsearch.bootstrap.ElasticSearch.main(ElasticSearch.java:32)'
2988,'s1monw','Expose underlying Lucene version in API\nSometimes it would be nice to be able to query the underlying Lucene version, fex. because the necessary query expression escaping rules depend on it.'
2987,'spinscale','MoreLikeThis request + CustomNumericField + store=no\nHello,\r\n\r\nI get a ElasticSearchIllegalStateException("Field should have either a string, numeric or binary value") throwed from TransportMoreLikeThisAction.convertField(Field field).\r\n\r\nActually, i have a index field which is a CustomIntegerNumericField for which field.numericValue() returns null when the "store" option is not set to "yes".\r\n\r\nField.numericValue() is using the "fieldsData" which is obviously null when not storing.\r\nWhen debugging, i can see the "fieldsData" is indeed null, but the "number" variable is not.\r\n\r\nShouldn\'t the method numericValue() be overriden in CustomIntegerNumericField to use the "number" variable instead ? Also in other classes extending CustomNumericField.\r\n\r\n(I\'m using version 0.90.0)\r\n\r\nThank you already,\r\n\r\nOlivier'
2984,'s1monw','PrimaryBalance in BalancedShardsAllocator can trigger unneeded relocation\ntoday if two nodes have very similar weights but only differ in the number of primaries a relocation can happen due to tie-breaking on the primaries per node. This might happen only if lots of relocations have happened before but still can trigger a unnecessary relocation.'
2979,'martijnvg','Query DSL: Wrong result on bool filter with \'must\' and \'should\' clauses\nES version is 0.90.0 (from deb package). Count of products in test - 667 (333 in stock and 334 out of stock). 50 products was added less than a month ago (37 in stock and 13 out of stock). One product (in stock) have a "Hot Offer" label.\r\n\r\nSo, my first query (search_type = count):\r\n```JSON\r\n{\r\n    "query": {\r\n        "match_all": {}\r\n    },\r\n    "filter": {\r\n        "bool": {\r\n            "should": [\r\n                {"term": {"hotOffer": true}},\r\n                {"range": {"created": {"from": "2013-04-02"}}}\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nThe result is 51 hits. It\'s correct result.\r\nNow add a <code>\'must\'</code> clause to <code>\'bool\'</code> filter:\r\n```JSON\r\n{\r\n    "query": {\r\n        "match_all": {}\r\n    },\r\n    "filter": {\r\n        "bool": {\r\n            "must": {\r\n                "term": {"inStock": true}\r\n            },\r\n            "should": [\r\n                {"term": {"hotOffer": true}},\r\n                {"range": {"created": {"from": "2013-04-02"}}}\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nThe result is 332 hits. It\'s wrong result (should be 38).\r\nRemove <code>\'range\'</code> filter from <code>\'should\'</code> clause:\r\n```JSON\r\n{\r\n    "query": {\r\n        "match_all": {}\r\n    },\r\n    "filter": {\r\n        "bool": {\r\n            "must": {\r\n                "term": {"inStock": true}\r\n            },\r\n            "should": [\r\n                {"term": {"hotOffer": true}}\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\nThe result is 319 hits. It\'s wrong result, again (should be 1).\r\nIf a <code>\'bool\'</code> filter replace by a combination of <code>\'and\'</code> and <code>\'or\'</code> filters then all is OK.\r\n```JSON\r\n{\r\n    "query": {\r\n        "match_all": {}\r\n    },\r\n    "filter": {\r\n        "and": [\r\n            {"term": {"inStock": true}},\r\n            {\r\n                "or": [\r\n                    {"term": {"hotOffer": true}},\r\n                    {"range": {"created": {"from": "2013-04-02"}}}\r\n                ]\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\nThe result is 38 hits.\r\n\r\nIt seems that if both clause (<code>must</code> and <code>should</code>) are in <code>bool</code> filter then results are incorrect. In 0.20.6 all works fine. '
2978,'s1monw','Add support for Collections to TermsQuery/InQuery \nCurrently passing in a collection such as List<Integer> to search a field in will not work unless you first convert it to an array. '
2974,'jpountz','JSON parsing tolerance level\nThis is mostly a possible enhancement.\r\n\r\nI have spent a significant amount of time searching for a problem that ended up being trivial. I build the query from PHP and use json_encode to serialize to JSON before sending it to ElasticSearch.\r\n\r\nI could not get highlighting to work. It turns out the built query was:\r\n\r\n    {"query":{"match":{"contents":{"query":"description","boost":1}}},"highlight":{"fields":{"contents":[]}}}\r\n\r\nThe problem is that the serialization used []\xa0instead of {}.\r\n\r\nTechnically there is no problem with ElasticSearch, but it would have saved a tremendous amount of time if it had simply rejected the query, or accepted the empty array.\r\n\r\nWould it be possible to have some sort of strict mode enabled to reject keys containing invalid values or some other mechanisms to help identify this type of issue?\r\n\r\nSeems like I was not alone on the IRC channel that encountered this sort of problem.'
2973,'s1monw','BalancedShardAllocator looses custom settings if un-related settings changed\nThe `BalancedShardAllocator` uses the default settings as the settings that are used if the relevant setting is not present in the settings object. Yet, this can cause lost settings in the allocator if the custom settings are not always passed in. '
2969,'uboness','Analysis: Have the hunspell filters do dedup by default\nFor better out-of-the-box experience, the hunspell token filters should be set with ```"dedup" : true``` by default (it\'s always possible to set it to ```false``` in the filter configuration)'
2967,'martijnvg','Update API doesn\'t support both script and doc\nIn my application I want to update a document by incrementing a counter using `script` and by passing a partial document to be merged in using `doc`. However, it appears that only one of these is applied:\r\n\r\n```\r\ncurl -XPOST http://localhost:9200/foo/bar/aoeu -d \'{"counter": 0, "foo": "bar"}\'\r\ncurl -XPOST http://localhost:9200/foo/bar/aoeu/_update -d \'{"script": "ctx._source.counter += 1", "doc": {"foo": "barbaz"}}\'\r\ncurl  http://localhost:9200/foo/bar/aoeu\r\n# Returns: {"_index":"foo","_type":"bar","_id":"aoeu","_version":2,"exists":true, "_source" : {"counter":1,"foo":"bar"}}\r\n```\r\n\r\nThe counter is incremented but the foo key\'s value isn\'t updated.\r\n\r\nI expected that my document would be merged and that my script would be executed. But it seems that if `doc` is passed, then `script` (as well as `upsert`) is disregarded. The documentation doesn\'t mention this and the server doesn\'t provide a warning (as far as I can tell). \r\n\r\nThis should either be described in the documentation or combinations of doc, script, and upsert should be accepted and applied. '
2966,'martijnvg','Add index based nested sort mode\nAdd index based nested `sort_mode`, that allows the sort by fields from specified nested inner objects identified by an index. This can be a useful `sort_mode` if the order of the nested inner objects has a meaning.\r\n\r\nThe index based `sort_mode` should look like this:\r\n* `index_1` - Instructs the nested sorting to only look at the second element.\r\n* `index_9` - Instructs the nested sorting to only look at the tenth element.\r\n* `index_first` - Instructs the nested sorting to only use the first nested inner object. \r\n* `index_last` - Instructs the nested sorting to only use the last nested inner object. \r\n\r\nRelates to #2662 '
2959,'s1monw','Alternate model for field AND logic within MultiMatch query\nI wrote a patch to MultiMatch query that provides more natural and processing when considering multiple fields.\r\n\r\nConsider document with fields:\r\ntitle:  Something\r\ndescription: featured on their 1969 album Abbey Road\r\nauthor: Beatles\r\nNow if I take user\'s input and run a query to match my documents, it would be natural to consider ether the dreaded _all field or a multi_match query like:\r\nmulti_match:{"query":"Something Beatles", "fields":["title", "description", "author"], "operator":"and"}\r\n\r\nWhich would get transformed into a boolean query such as:\r\n(+title:something +title:beatles) (+description:something +description:beatles) (+author:something +author: beatles)\r\n\r\nThere is no match for our document! From human input perspective often the most natural way to AND multi-field search is to ensure each term is matched somewhere across all fields such as:\r\n+(title:something description:something author:something) +(title:beatles description:beatles author:beatles)\r\n\r\nMy patch does exactly that and it also accounts for use of multiple analyzers which may remove tokens from some fields (ex: The Beatles). If a token is skipped by an analyzer it will be turned into a should requirement on remaining fields instead of a must.\r\n\r\nI am using facilities of match query for minimum should match as well as fuzzy processing so a new match type felt natural. \r\nmulti_match:{"query":"Something Beatles", "fields":["title", "description", "author"], "type":"across"}'
2956,'spinscale','Timing problem with aliases and index status\nHi everyone,\r\n\r\nI tested elastic search 0.90 today and some of my test failed. I\'ve isolated the minimum reproductible test case: https://gist.github.com/Lothiraldan/5488931.\r\n\r\nI create an index without specific settings named test.1.1 I set two aliases test.read and test.write to test.1.1.\r\n\r\nThen I get the index status of each alias (via localhost:9200/test.read/status) and extract the real index name behind alias (it\'s exactly how the python driver pyes does the job).\r\n\r\nThen I create a second one named test.1.2 I move the two aliases (test.read and test.write) to the freshly created index and try to do the same requests (get index status and extract real index name).\r\n\r\nOn elastic search 0.20.6, no problem. On elastic search 0.90, when I do the request, I get this result:\r\n\r\n{\r\n    "_shards": {\r\n        "failed": 0, \r\n        "successful": 0, \r\n        "total": 10\r\n    }, \r\n    "indices": {}, \r\n    "ok": true\r\n}\r\n\r\nIt looks like a timing issue, I tried to add a sleep in my python code but I must add a ~300ms minimum else my test will fail.\r\n\r\nI don\'t know if it\'s a bug or a desired behaviour, if someone can confirm the difference of output between the two version.\r\n\r\nThanks'
2953,'s1monw',"TransportAnalyzeAction causes StringIndexOutOfBoundsException on first attempt to analyze a numeric field\nI'm seeing the following in Elastic 0.90.0 during the first attempt attempt to initiate an AnalyzeRequest against a numeric field:\r\n\r\n```\r\nCaused by: java.util.concurrent.ExecutionException: java.lang.StringIndexOutOfBoundsException: String index out of range: -1\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:272)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\r\n        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\r\n        ... 38 more\r\nCaused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1\r\n        at java.lang.String.<init>(String.java:207)\r\n        at org.elasticsearch.index.analysis.NumericTokenizer.reset(NumericTokenizer.java:59)\r\n        at org.elasticsearch.index.analysis.NumericTokenizer.reset(NumericTokenizer.java:54)\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:202)\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)\r\n        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        ... 1 more\r\n```\r\n\r\nThe cause of the problem appears to be that the `reset()` method of the `NumericTokenizer` is called twice the first time `analyzer.tokenStream()` is called for a the field in a thread:\r\n\r\n* The first call happens as a result of Lucene calling `createComponents()` on the `NumericAnalyzer`.  This eventually results in the construction of a `NumericTokenizer` with a `char[]` buffer, which calls `reset()` during construction.  This first `reset()` call leaves the `FastStringReader` associated with the Tokenizer with a `next` value that is equal to the  length of that buffer because it reads all the chars out of the reader.\r\n* The second call happens as a result of the explicit call to `reset()` in `TransportAnalyzeAction` immediately after the `TokenStream` has been retrieved from the analyzer.  Unfortunately calling the method a second time triggers the `if (next >= length)` check in the `read()` method of the associated `FastStringReader` to return -1.  NumericTokenizer then tries to use -1 as the number of chars to use when constructing a `String`, which throws the `StringIndexOutOfBoundsException` above.\r\n"
2952,'s1monw',"TransportAnalyzeAction causes IllegalArgumentException: NumericTokenStream does not support CharTermAttribute\nI'm getting the following Exception in Elastic 0.90.0 when I attempt to initiate an AnalyzeRequest against a numeric field:\r\n\r\n```\r\nCaused by: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: NumericTokenStream does not support CharTermAttribute.\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:285)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:272)\r\n        at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:113)\r\n        at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:45)\r\n        ... 38 more\r\nCaused by: java.lang.IllegalArgumentException: NumericTokenStream does not support CharTermAttribute.\r\n        at org.apache.lucene.analysis.NumericTokenStream$NumericAttributeFactory.createAttributeInstance(NumericTokenStream.java:136)\r\n        at org.apache.lucene.util.AttributeSource.addAttribute(AttributeSource.java:271)\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:203)\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)\r\n        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$2.run(TransportSingleCustomOperationAction.java:175)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        ... 1 more\r\n```\r\n\r\nI believe that this is probably caused by the following change introduced in Lucene 4:\r\n\r\n_NumericTokenStream now works directly on byte[] terms. If you plug a TokenFilter on top of this stream, you will likely get an IllegalArgumentException, because the NTS does not support TermAttribute/CharTermAttribute_\r\n\r\n(From http://lucene.apache.org/core/4_2_1/changes/Changes.html#4.0.0-alpha.changes_in_backwards_compatibility_policy)\r\n\r\nLine 203 of TransportAnalyzeAction is attempting to add a CharTermAttribute to a TokenStream instance.\r\n\r\n"
2946,'bleskes','Current external versioning semantics not suitable for all use cases\nAs discussed in issue #2938 and pull request #2939, the versioning semantics are not suiteable for all cases. For instance, deleting an externally versioned document with its current version fails ("version conflict, current [4], provided [4]]") which is allowed in internal versioning. For instance, when mirroring changes to a database, using the last version of a just deleted object should work as expected.'
2944,'spinscale','source exclusion mapping prevents geo shape coordinates to be returned in query result source field\nES Version: 0.90.0.RC2\r\n\r\nSteps to reproduce:\r\n\r\nok case, without source exclusion:\r\n\r\n```bash\r\ncurl -XDELETE \'http://localhost:9200/geo_test\'\r\ncurl -XPUT \'http://localhost:9200/geo_test\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/_mapping\' -d \'{\r\n    "item" : {\r\n        "properties" : {\r\n            "location" : {\r\n                "type" : "object",\r\n                "properties": {\r\n                     "point": {"type": "geo_point"},\r\n                     "area": {"type": "geo_shape"}\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/1\' -d \'{\r\n    "location": {"point": [45.0, 45.0]}\r\n}\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/2\' -d \'{\r\n    "location": {\r\n        "area": {\r\n            "type" : "envelope",\r\n            "coordinates" : [[44.0, 46.0], [45.0, 45.0]]\r\n        }\r\n    }\r\n}\'\r\ncurl -XPOST \'http://localhost:9200/geo_test/item/_search?pretty\' -d \'{\r\n    "query": {"match_all": {}}\r\n}\'\r\n```\r\n\r\nreturns the coordinates for geo_point and geo_shape items:\r\n```JSON \r\n{\r\n\t"took": 1,\r\n\t"timed_out": false,\r\n\t"_shards": {\r\n\t\t"total": 1,\r\n\t\t"successful": 1,\r\n\t\t"failed": 0\r\n\t},\r\n\t"hits": {\r\n\t\t"total": 2,\r\n\t\t"max_score": 1.0,\r\n\t\t"hits": [{\r\n\t\t\t"_index": "geo_test",\r\n\t\t\t"_type": "item",\r\n\t\t\t"_id": "1",\r\n\t\t\t"_score": 1.0,\r\n\t\t\t"_source": {\r\n\t\t\t\t"location": {\r\n\t\t\t\t\t"point": [45.0, 45.0]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}, {\r\n\t\t\t"_index": "geo_test",\r\n\t\t\t"_type": "item",\r\n\t\t\t"_id": "2",\r\n\t\t\t"_score": 1.0,\r\n\t\t\t"_source": {\r\n\t\t\t\t"location": {\r\n\t\t\t\t\t"area": {\r\n\t\t\t\t\t\t"type": "envelope",\r\n\t\t\t\t\t\t"coordinates": [\r\n\t\t\t\t\t\t\t[45.0, 45.0],\r\n\t\t\t\t\t\t\t[44.0, 46.0]\r\n\t\t\t\t\t\t]\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}]\r\n\t}\r\n}\r\n```\r\n\r\nhowever the same scenario but with a source exclusion mapping of an arbitrary field does not return the geo_shape coordinates any longer:\r\n\r\n```bash\r\ncurl -XDELETE \'http://localhost:9200/geo_test\'\r\ncurl -XPUT \'http://localhost:9200/geo_test\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/_mapping\' -d \'{\r\n    "item": {\r\n        "_source": {\r\n            "excludes": ["body"]\r\n        },\r\n        "properties" : {\r\n            "location" : {\r\n                "type" : "object",\r\n                "properties": {\r\n                     "point": {"type": "geo_point"},\r\n                     "area": {"type": "geo_shape"}\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/1\' -d \'{\r\n    "location": {"point": [45.0, 45.0]}\r\n}\'\r\ncurl -XPUT \'http://localhost:9200/geo_test/item/2\' -d \'{\r\n    "location": {\r\n        "area": {\r\n            "type" : "envelope",\r\n            "coordinates" : [[44.0, 46.0], [45.0, 45.0]]\r\n        }\r\n    }\r\n}\'\r\ncurl -XPOST \'http://localhost:9200/geo_test/item/_search?pretty\' -d \'{\r\n    "query": {"match_all": {}}\r\n}\'\r\n```\r\n\r\nreturns only the geo_point coordinates but not the geo_shape coordinates:\r\n\r\n```JSON\r\n{\r\n\t"took": 1,\r\n\t"timed_out": false,\r\n\t"_shards": {\r\n\t\t"total": 1,\r\n\t\t"successful": 1,\r\n\t\t"failed": 0\r\n\t},\r\n\t"hits": {\r\n\t\t"total": 2,\r\n\t\t"max_score": 1.0,\r\n\t\t"hits": [{\r\n\t\t\t"_index": "geo_test",\r\n\t\t\t"_type": "item",\r\n\t\t\t"_id": "1",\r\n\t\t\t"_score": 1.0,\r\n\t\t\t"_source": {\r\n\t\t\t\t"location": {\r\n\t\t\t\t\t"point": [45.0, 45.0]\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}, {\r\n\t\t\t"_index": "geo_test",\r\n\t\t\t"_type": "item",\r\n\t\t\t"_id": "2",\r\n\t\t\t"_score": 1.0,\r\n\t\t\t"_source": {\r\n\t\t\t\t"location": {\r\n\t\t\t\t\t"area": {\r\n\t\t\t\t\t\t"type": "envelope",\r\n\t\t\t\t\t\t"coordinates": []\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}]\r\n\t}\r\n}\r\n```'
2938,'bleskes','Bulk delete not working with versioning\nPerforming delete operations via bulk API does not work as it is causing wrong VersionConflictEngineExceptions:\r\n\r\n    VersionConflictEngineException: [default][2] [tweet][1]: version conflict, current [4], provided [4]\r\n\r\nthe error is in this code snippet of [RobinEngine](https://github.com/elasticsearch/elasticsearch/blob/v0.20.6/src/main/java/org/elasticsearch/index/engine/robin/RobinEngine.java#L652):\r\n\r\n    } else if (currentVersion >= delete.version()) {\r\n      throw new VersionConflictEngineException(shardId, delete.type(), delete.id(), currentVersion, delete.version());\r\n    }\r\n\r\n`currentVersion` is the currently stored version while `delete.version()` is the version given in the delete request. Obviously equal versions must not cause an exception. Therefore the condition must be changed to `currentVersion > delete.version()`'
2935,'s1monw','Fixing possible NoClassDefFoundError bubbling from SettingsBuilder\nIn order to handle exceptions correctly, when classes are not found, one\r\nneeds to handle ClassNotFoundException as well as NoClassDefFoundError\r\nin order to be sure to have caught every possible case. We did not cater\r\nfor the latter in ImmutableSettings yet.\r\n\r\nThis fix is just executing the same logic for both exceptions instead of\r\nsimply bubbling up NoClassDefFoundError.\r\n\r\nI still do not like the notion of loading classes via the SettingsBuilder, but that is a another issue :-)'
2932,'s1monw','NullPointerException when using DFS_QUERY_AND/THEN_FETCH using 0.90RC2\nThe following Exception is logged:\r\n\r\n    org.elasticsearch.transport.SendRequestTransportException: [es7][inet[/10.66.110.57:9300]][search/phase/query/id]\r\n        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:199)\r\n        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:171)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:181)\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeQuery(TransportSearchDfsQueryThenFetchAction.java:148)\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchDfsQueryThenFetchAction.java:107)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:229)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:208)\r\n        at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:205)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:122)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$1.handleResponse(SearchServiceTransportAction.java:113)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:156)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:127)\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:787)\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296)\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462)\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443)\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:560)\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:555)\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268)\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)\r\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:679)\r\n    Caused by: java.lang.NullPointerException\r\n\r\n\r\nWhen the following query is executed:\r\n\r\n    SearchResponse response = client.prepareSearch("heartbeat")\r\n        .setSearchType(SearchType.DFS_QUERY_THEN_FETCH)\r\n        .setQuery(\r\n            QueryBuilders.boolQuery()\r\n                .must(QueryBuilders.termQuery("online", true))\r\n                .must(QueryBuilders.boolQuery()\r\n                    .should(QueryBuilders.boolQuery()\r\n                        .must(QueryBuilders.rangeQuery("ts").lt(now - (MAX_AGE_BS * 1000)))\r\n                        .must(QueryBuilders.termQuery("_type", "bs"))\r\n                        )\r\n                    .should(QueryBuilders.boolQuery()\r\n                        .must(QueryBuilders.rangeQuery("ts").lt(now - (MAX_AGE_SENSOR * 1000)))\r\n                        .must(QueryBuilders.termQuery("_type", "s"))\r\n                    )\r\n                )\r\n        )\r\n        .setVersion(true)\r\n        .setFrom(0).setSize(100).setExplain(true)\r\n        .execute()\r\n        .actionGet();\r\n\r\nThe query does return the expected results. So other then the log entry there seems to be nothing wrong with it.\r\nChanging the SearchType fixes the issue.\r\n\r\nThe fields have the following properties:\r\n\r\n    "online": {\r\n        "type": "boolean",\r\n    },\r\n    "ts": {\r\n        "type": "date",\r\n        "ignore_malformed": False,\r\n        "format": "dateOptionalTime"\r\n    },\r\n\r\nAnd the "s" type has a routing defined:\r\n\r\n\r\n    "_routing": {\r\n        "required": True,\r\n        "path": "bs"\r\n    },\r\n\r\n\r\nthe "bs" field is only in the "s" type defined:\r\n\r\n    "bs": {\r\n        "type": "string",\r\n        "index": "not_analyzed"\r\n    },\r\n'
2931,'s1monw','StringIndexOutOfBoundsException[String index out of range: -8] while Highlighting\nThis issue happens on 0.20.4 and 0.90RC2 and probably every other version(?) since it I guess its related to:\r\n\r\nhttps://issues.apache.org/jira/browse/LUCENE-4899\r\n\r\nHere is a test that manages to reproduce the error. First 2 queries should execute ok, but third should fail.\r\n\r\n```\r\ncurl -XPOST \'http://127.0.0.1:9200/test?\' -d \'{ "mappings" : { "test" : { "properties" : { "name" : { "type": "string", "index_analyzer": "name_index_analyzer", "search_analyzer": "name_search_analyzer", "term_vector" : "with_positions_offsets" } } } }, "settings" : { "analysis" : { "filter" : { "my_ngram" : { "max_gram" : 20, "min_gram" : 1, "type" : "ngram" } }, "analyzer" : { "name_index_analyzer": { "tokenizer": "whitespace", "filter": [ "my_ngram" ] }, "name_search_analyzer": { "tokenizer": "whitespace" } } } }}\'\r\n\r\ncurl -XPUT \'http://localhost:9200/test/test/1\' -d \'{"name": "logicacmg ehemals avinci - the know how company"}\'\r\n\r\ncurl -XGET \'http://localhost:9200/test/test/_search\' -d \'{ "query": { "match": { "name": { "query": "logica" } } }, "highlight": { "fields": { "name": {} } }}\'\r\n\r\ncurl -XGET \'http://localhost:9200/test/test/_search\' -d \'{ "query": { "match": { "name": { "query": "logica ma" } } }, "highlight": { "fields": { "name": {} } }}\'\r\n\r\ncurl -XGET \'http://localhost:9200/test/test/_search\' -d \'{ "query": { "match": { "name": { "query": "logica m" } } }, "highlight": { "fields": { "name": {} } }}\'\r\n```\r\n\r\nMaybe its possible a work around, or a Lucene upgrade to 4.3(since it seems to be fixed there)?'
2929,'s1monw','Create a circuit breaker to prevent searches from bringing down a node\nOne of the fears that I have when using ElasticSearch is that expensive queries can bring down nodes in my cluster. \r\n\r\nIt would be really nice if ElasticSearch could detect this type of node-killing event by adding logic that would trigger a circuit breaker and kill the offending query, leaving my node intact.  For example, if a search takes X% of the heap, the query would be killed by ElasticSearch.  It would be useful to expose the X% of heap_size as a configurable value since the level of concurrency of the system would vary by ES installation.\r\n\r\nAnother feature that would be helpful is when the circuit breaker is tripped, a response is generated from ElasticSearch saying that the query died from using excess memory.'
2928,'s1monw','Documentation: specify how to remove a setting\nLooking at:\r\n\r\nhttp://www.elasticsearch.org/guide/reference/api/admin-indices-update-settings/\r\n\r\nIt is not clear to me how to set total_shards_per_node back to it\'s default once it has been modified. The ML provided the (as yet untested) answer of 0, but the documentation should probably specify the default value instead of "Unbounded".\r\n\r\nAlso, I looked for a way to remove settings altogether (undefine them) and found no way to do this. It would be nice if there was a documented way to REMOVE an arbitrary setting (or any object key for that matter) via the REST API.'
2927,'spinscale','Log conflicting options / directives\nI ran into an issue today with ElasticSearch. The problem was my own bone-headedness. However, it took me longer to realize this than it should have.\r\n\r\nMy problem in a nutshell was that while troubleshooting some time ago, I had set total_shards_per_node to 4. For me, with 3 nodes and 12 shards (primaries) this number was too low. I had (incorrectly) assumed that shards referred to just primaries. I was recently trying to figure out where my replicas had all gone, and went to the mailing list for help.\r\n\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/vCZN2LR7K0U\r\n\r\nI have seen other systems (like Heartbeat) log a message when it has received conflicting instructions. On the one hand, I asked ES to make a replica for each shard. But on the other hand, I did not allocate enough shards per node to accommodate the replicas. The ES log file was completely silent, so I had nowhere except the ML to look for more information. I propose that in this situation, ES logs something like:\r\n\r\n"WARNING: I need to store 24 shards, but total_shards_per_node * node_count == 12."\r\n\r\nP.S. I think that the terminology is a bit vague in regards to shards / replicas. Replicas are shards in some sense, but not in others. I would suggest separating the two, or introducing a term that applies to shards+replicas. For example, shard refers to primaries, replica refers to a copy of a shard, and __insert term here__ refers to both (shards+replicas).'
2926,'s1monw','Error executing must_not queries on sharded index\nRunning this in 0.90.0 RC2(also latest build) fails with:\r\n\r\nQuery Failed [Failed to execute main query]]; nested: ElasticSearchIllegalArgumentException[Not distributed collection statistics for field: description]; "}]}\r\n\r\nWorks fine in 0.20.4. \r\n\r\nAlso works if using only one shard, or if changing the must_not query for a must query. \r\n\r\n```\r\ncurl -XPOST http://localhost:9200/test -d \'{"settings": { "index.number_of_replicas": 0, "index.number_of_shards": 2}}\'\r\n\r\ncurl -XPOST http://localhost:9200/test/test/1 -d \'{"id":1,"description":"foo other anything bar"}\'\r\ncurl -XPOST http://localhost:9200/test/test/2 -d \'{"id":2,"description":"foo other anything"}\'\r\ncurl -XPOST http://localhost:9200/test/test/3 -d \'{"id":3,"description":"foo other"}\'\r\n\r\ncurl -XPOST http://localhost:9200/test/test/_search?search_type=dfs_query_then_fetch -d \'{ "query": { "bool": { "must_not": [ { "match": { "description": { "query": "anything", "type": "boolean", "operator": "AND" } } } ] } }}\'\r\n```'
2925,'imotov','network.host (elasticsearch.yml): _eth1_ appears to bind to IPV6 only\nIn order to get IPV4 access, I have to explicitly declare _eth1:ipv4_ - is that intended?\r\n\r\nOS: CentOS release 6.3 (Final)'
2924,'imotov',"Config: elasticsearch.yml doesn't accept _lo_ or _lo0_ for network.host\nAn explicit 127.0.0.1 works for me, while lo/lo0 don't.\r\n\r\nRan into this because I'm deploying ES using configuration management (Puppet), and would find it cleaner to go with all-names as opposed to a hybrid of numbers for localhost and names for non-localhost."
2921,'spinscale','It may lost exception  in response message  when submitting  StateUpdateTask in InternalClusterService\nIf throw a exception when submitting StateUpdateTask in InternalClusterService,\r\n\r\nSystem will log the exception ,\r\nthen return the mesage (acknowledged=false),\r\nbut i want to know the detailed information of exception from response message.\r\n\r\nMaybe it is necessary to return exception information to the client.\r\n\r\nthe following is my idea.\r\n\r\n1   add a method in ProcessedClusterStateUpdateTask.\r\n     void failed(Throwable t);\r\n\r\n2  implement this method in sub class\r\n    public void failed(Throwable t) {\r\n          listener.onFailure(t); \r\n    }\r\n3  call this method in InternalClusterService\r\n    ...\r\n    try {\r\n         ...\r\n    } catch (Exception e) {\r\n         updateTask.failed(e);\r\n         ...\r\n    }\r\n\r\n\r\n\r\n '
2920,'s1monw','Error on Script Based Sorting\nWhen used script based sorting on an index with mora than 1 shard, I get:\r\n\r\n{"error":"ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: ClassCastException[java.lang.String cannot be cast to org.apache.lucene.util.BytesRef]; ","status":500}\r\n\r\n\r\nEg:\r\n```\r\ncurl -XPOST http://localhost:9200/script -d \'{"settings": { "index.number_of_replicas": 0, "index.number_of_shards": 2}}\'\r\n\r\ncurl -XPUT http://localhost:9200/script/test/_mapping -d \'{"profile":{"dynamic":"strict","properties":{"id":{"type":"integer","index":"not_analyzed","store":"yes"},"groups_code":{"properties":{"id":{"type":"integer","index":"not_analyzed"},"date":{"type":"date","index":"not_analyzed","format":"date_time_no_millis"}}}}}}\'\r\n\r\ncurl -XPUT http://localhost:9200/script/test/1 -d \'{"groups_code":[{"id":47642,"date":"2010-08-12T07:54:55Z"}]}\'\r\n\r\ncurl -XPUT http://localhost:9200/script/test/2 -d \'{"groups_code":[{"id":47642,"date":"2010-05-04T12:10:54Z"}]}\'\r\n\r\n\r\ncurl -XGET http://localhost:9200/script/test/_search -d \'{"query":{"match_all":{}},"fields":"","sort":[{"_script":{"script":"if ( ! _source.groups_code.empty ) { result = ($.date in _source.groups_code if $.id == id); if ( ! result.empty ) { result[0] } else { \'\' } }","type":"string","reverse":true,"params":{"id":47642}}}]}\'\r\n```'
2919,'s1monw','Apply minimum_should_match to inner clauses of multi_match query\nWhen specifying minimum_should_match in a multi_match query it was being applied\r\nto the outer bool query instead of to each of the inner field-specific bool queries.\r\n\r\nCloses #2918'
2918,'s1monw','minimum_should_match applied to wrong query in multi_match\nWhen using `minimum_should_match` with a `multi_match` query, it is being applied to the ``bool` query which wraps the per-field queries. It should be applied to each per-field query instead:\r\n\r\n\r\n    curl -XPOST \'http://127.0.0.1:9200/test/test?pretty=1\'  -d \'\r\n    {\r\n       "foo" : "one two three"\r\n    }\r\n    \'\r\n\r\nWith a `match` query, the minimum of 70% doesn\'t find any results (correctly):\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/test/_search?pretty=1\'  -d \'\r\n    {\r\n       "query" : {\r\n          "match" : {\r\n             "foo" : {\r\n                "minimum_should_match" : "70%",\r\n                "query" : "three four five"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [],\r\n    #       "max_score" : null,\r\n    #       "total" : 0\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 10\r\n    # }\r\n\r\nWith `multi_match`, it finds results (incorrectly):\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/test/_search?pretty=1\'  -d \'\r\n    {\r\n       "query" : {\r\n          "multi_match" : {\r\n             "minimum_should_match" : "70%",\r\n             "fields" : [\r\n                "foo",\r\n                "bar"\r\n             ],\r\n             "query" : "three four five",\r\n             "use_dis_max": true\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "foo" : "one two three"\r\n    #             },\r\n    #             "_score" : 0.009060421,\r\n    #             "_index" : "test",\r\n    #             "_id" : "sa8shEUoR5SRtME0EA4Gyw",\r\n    #             "_type" : "test"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 0.009060421,\r\n    #       "total" : 1\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 5\r\n    # } \r\n'
2913,'martijnvg','Allow boost fields to be indexed and stored\nCurrently there is no way to change the default behavior of boost fields being not indexed and not stored. If a boosted field can be also indexed, then it can be sorted/faceted on if there is a use case that requires it. This change will make boost fields behave like the timestamp field, which can be indexed and stored.\r\n\r\nI know that Shay is not a fan of document-level boosts and in fact, Lucene 4 has gotten rid of them. But the boost field feature is there, so it would be great to use it to its full potential.'
2912,'s1monw','If a value/field is a Calendar, it will be converted to a Date using getTime()\nCloses #2911'
2911,'lucaslward','XContentBuilder doesn\'t handle Java Calendar\n```Java\r\nCalendar calendar = new GregorianCalendar();\r\nString expectedCalendar = XContentBuilder.defaultDatePrinter.print(calendar.getTimeInMillis());\r\nXContentBuilder builder = XContentFactory.contentBuilder(XContentType.JSON);\r\nbuilder.startObject().field("calendar", calendar).endObject();\r\nassertThat(builder.string(), equalTo("{\\"calendar\\":\\"" + expectedCalendar + "\\"}"));\r\n```\r\n\r\nThe above will fail because the builder would have just called toString() on Calendar, even though it\'s trivial to say calendar.getTime() (which returns a normal Date)'
2907,'s1monw','Empty response when updating cluster settings\nThis happens with at least 0.20.4 and 0.90.0.RC2, so I guess it happens on every version.\r\nIs that the desired behaviour? It\'s just a bit confusing not getting any response and not being sure if the new value was really applied or not(or having to do another request and check if it was applied)\r\n\r\nEg:\r\ncurl -XPUT http://localhost:9200/_cluster/settings -d \'{"transient":{"cluster.routing.allocation.disable_allocation":"true"}}\'\r\n\r\n'
2901,'javanna','_msearch use source parameter if no body is given\nPermit `_msearch` to take the body content from the REST parameter `source` if no request body is given, just like `_search` does.'
2899,'s1monw','PolygonBuilder does not support holes\nThe `PolygonBuilder` does not support holes within a polygon. Since holes are supported by the GeoJson this function should be implemented in `PolygonBuilders` as well.'
2896,'s1monw','NullPointerException in count and search with preference set to _primary\nversion 0.90 RC2\r\n\r\nsteps to reproduce:\r\n\r\nstart 2 nodes\r\ncreate an index with shards on both nodes\r\nstop 1 node, so that the index state is red\r\n\r\nissue a _count or a _search request like this:\r\n```\r\ncurl -XGET $idx_url\'/_count?pretty=1&preference=_primary\'\r\n\r\ncurl -XGET $idx_url\'/_search?pretty=1&preference=_primary\' -d \'{\r\n"query":{\r\n"match_all":{}\r\n }\r\n }\r\n\'\r\n```\r\n\r\nthe result for both is \r\n```\r\n{\r\n  "error" : "NullPointerException[null]",\r\n  "status" : 500\r\n}\r\n```\r\n\r\n'
2895,'s1monw','Add UNICODE_CHARACTER_CLASS flag to Regex flag parsing\nHi,\r\n\r\nJava\'s Regex have issue with character classes when it comes to unicode content ( \r\nhttp://stackoverflow.com/questions/4304928/unicode-equivalents-for-w-and-b-in-java-regular-expressions\r\n)\r\n\r\nApparently they solved things in Java7 by adding a special flag: UNICODE_CHARACTER_CLASS ( http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#UNICODE_CHARACTER_CLASS )\r\n\r\nSadly org.elasticsearch.common.regex doesn\'t "understand" it. As Java7 is the recommended runtime for ES I think it\'s OK to add it even if not supported by earlier JDK.\r\n\r\nCheers,\r\nBoaz'
2893,'martijnvg','Better error message when using bloom codec\nIf the `postings_format` is specified as just `bloom`, then it throws a class-not-found exception. Instead, would be better to have a message like "The bloom codec cannot be used by itself. Use bloom_default or bloom_pulsing instead"'
2892,'uboness',"Inconsistent locations returned by _source field, doc['_source'] script field and doc['location'].lats and doc['location'].lons script fields\nHere's a gist that demonstrates the issue (tested in v0.19.10 and v0.20.6):\r\n\r\nhttps://gist.github.com/richardpoole/5379348\r\n\r\nIt's almost like scripted fields are coming from different documents. Changing the size parameter of the search request seems to have an effect (e.g. problem disappears with size=1)."
2887,'dadoonet','Missing FilterBuilder.toString()\nhttp://www.elasticsearch.org/guide/reference/java-api/query-dsl-filters/ states\r\n\r\n    Note that you can easily print (aka debug) JSON generated queries\r\n    using toString() method on FilterBuilder object.\r\n\r\nThis is !true! Tried this for debugging and got Object.toString() output.\r\nQueryBuilder.toString() instead works as advertised so the toString() from BaseQueryBuilder.java might just be pasted into BaseFilterBuilder.java.'
2883,'s1monw','Rename Suggester prefix_length to prefix_len for consistency with other params\nwe already have `min_word_len` and other params like `min_doc_freq` that use short forms. We should be consistent here and use `prefix_len` rather than `prefix_length`'
2882,'martijnvg','Fielddata stats incorrect for multi-value fields\nI index 100,000 docs with two numeric fields, one value in each field:\r\n\r\n* `unique` contains vals 1 to 100,000\r\n* `repeated` contains vals 1 to 30\r\n\r\nWhen sorting on `unique`, the fielddata size is reported as 800,020 bytes. When sorting on `repeated`, the fielddata size is reported as 100,020 bytes.\r\n\r\nThen I index another document with `unique` set to the array [1..100] and `repeated` set to [1..30], and optimize down to one segment.\r\n\r\nWhen sorting on `unique`, the fielddata size is reported as 400,032 bytes. When sorting on `repeated`, the fielddata size is reported as 59 bytes!\r\n\r\nSee this gist for a recreation: https://gist.github.com/clintongormley/5353777\r\n\r\n'
2878,'imotov','Terms facets may return negative "other" count for script field\nRepro: https://gist.github.com/imotov/983383fd17b215c44c71'
2876,'s1monw','Configure fielddata using a hash, not a string\nWith the changes in #2874 we are forced to specify a lot of config options in a long unreadable string.  The fielddata param should use a JSON object like all the other config in the mapping.\r\n\r\neg instead of: \r\n\r\n    "fielddata": "format=paged_bytes;filter.frequency.min=0.001;filter.frequency.max=0.1;filter.frequency.min_segment_size=1000;filter.regex=^en_"\r\n\r\nit should be:\r\n\r\n    "fielddata": {\r\n        "format": "paged_bytes",\r\n        "filter": {\r\n            "min_freq": 0.001,\r\n            "max_freq": 0.1,\r\n            "min_segment_size":  1000,\r\n            "regex": "^en_"\r\n        }\r\n    }'
2874,'s1monw','Allow FieldData loading to be filtered\n# FieldData Filter\r\n\r\nFieldData is an in-memory representation of the term dictionary in an uninverted form. Under certain circumstances this FieldData representation can grow very large on high-cardinality fields like tokenized full-text. Depending on the use-case filtering the terms that are hold in the FieldData representation can heavily improve execution performance and application stability.\r\n\r\nFieldData Filters can be applied on a per-segment basis. During FieldData loading the terms enumeration is passed through a filter predicate that  either accepts or rejects a term.\r\n\r\n__Note: this feature is only supported on string fields__\r\n\r\n## Frequency Filter\r\n\r\nThe Frequency Filter acts as a high / low pass filter based on the document frequencies of a certain term within the segment that is loaded into field data. It allows to reject terms that are very high or low frequent based on absolute frequencies or percentages relative to the number of documents in the segment or more precise the number of document that have at least one value in the field that is loaded in the current segment.\r\n\r\nHere is an example mapping:\r\n\r\n```json\r\n{\r\n    "tweet" : {\r\n        "properties" : {\r\n            "locale" : {\r\n                "type" : "string",\r\n                "fielddata" : "format=paged_bytes;filter.frequency.min=0.001;filter.frequency.max=0.1",\r\n                "index" : "analyzed",\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n### Paramters\r\n\r\n * `filter.frequency.min` - the minimum document frequency (inclusive) in order to be loaded in to memory. Either a percentage if < `1.0` or an absolute value. `0` if omitted.\r\n * `filter.frequency.max` - the maximum document frequency (inclusive) in order to be loaded in to memory. Either a percentage if < `1.0` or an absolute value. `0` if omitted. \r\n * `filter.frequency.min_segment_size` - the minimum number of documents in a segment in order for the filter to be applied. Small segments might be omitted with this setting.\r\n\r\n## Regular Expression Filter\r\n\r\nThe regular expression filter applies a regular expression to each term  during loading and only loads terms into memory that match the given regular expression. \r\n\r\nHere is an example mapping:\r\n\r\n```json\r\n{\r\n    "tweet" : {\r\n        "properties" : {\r\n            "locale" : {\r\n                "type" : "string",\r\n                "fielddata" : "format=paged_bytes;filter.regex=^en_.*",\r\n                "index" : "analyzed",\r\n            }\r\n        }\r\n    }\r\n}\r\n```'
2873,'imotov','0.90 RC2 can no longer fetch template\n```\r\ncurl -XPUT http://localhost:9200/_template/test -d \'{template: "session*", settings: { "foo.bar": 1 } }\'\r\n\r\ncurl \'http://localhost:9200/_template/test?pretty\'\r\n```\r\n\r\nreturns\r\n\r\n```\r\n{\r\n  "error" : "IndexMissingException[[_na] missing]",\r\n  "status" : 404\r\n}%\r\n```'
2868,'imotov',"Get template does not seem to return warmers\nUnless of course I am doing something wrong, it seems that registering a warmer using PUT template seems to work but if i then GET the template it does not return the warmers.\r\n\r\nSee: https://gist.github.com/Mpdreamz/5330654\r\n\r\nFor the raw HTTP calls.\r\n\r\nI'm using elasticsearch 0.20.6"
2866,'s1monw',"BooleanFieldMapper should set `IndexOptions.DOCS_ONLY` by default.\ntoday and it seems like 0.20 as well uses the default which is `IndexOptions.DOCS_AND_FREQS_AND_POSITIONS` - this stores extra term frequency and position information for each boolean which seems unnecessary here. I flagged this as breaking since its a new default that might case different behaviour under certain circumstances but I'd guess almost all users are not affected. Anybody doing phrase queries on boolean fields? "
2848,'s1monw','Uncaught exception in javascript\nWhen using the JS plugin in 0.20.6, the following request triggers an uncaught exception, which never returns:\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/_all/_search?pretty=1\'  -d \'\r\n    {\r\n       "script_fields" : {\r\n          "num" : {\r\n             "script" : "1/doc[\\u0027num\\u0027].value",\r\n             "lang" : "js"\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    Exception in thread "elasticsearch[Stunner][search][T#4]" java.lang.AbstractMethodError\r\n\t    at org.elasticsearch.search.fetch.script.ScriptFieldsFetchSubPhase.hitExecute(ScriptFieldsFetchSubPhase.java:70)\r\n\t    at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:250)\r\n\t    at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:438)\r\n\t    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:345)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.executeFetch(TransportSearchQueryThenFetchAction.java:149)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction$2.run(TransportSearchQueryThenFetchAction.java:136)\r\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t    at java.lang.Thread.run(Thread.java:722)\r\n\r\n\r\n\r\n'
2847,'spinscale',"Allow contributors add to documentation with comments ....\nor something similar, maybe the way apidock.com does it?\r\nThere are a bunch of gaps in the elasticsearch documentation (necessarily so, the writer can't think of every possible need of a reader), places where there isn't a lot of detail, or a corner case is overlooked.\r\n\r\nIt would be great if people could help others out if they've spent hours dealing with something. \r\n\r\nI'm happy to help out with this if need be."
2834,'s1monw','Scripts not casting integers to doubles\nWhen running a calculation on an integer which results in a double, v0.20.6 returns the double correctly, while v0.90.0.RC1 leaves it as an integer, and returns the incorrect value:\r\n\r\n    curl -XPUT \'http://127.0.0.1:9200/test/?pretty=1\'  -d \'\r\n    {\r\n       "mappings" : {\r\n          "test" : {\r\n             "properties" : {\r\n                "num" : {\r\n                   "type" : "integer"\r\n                }\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/test?pretty=1\'  -d \'\r\n    {\r\n       "num" : 8\r\n    }\r\n    \'\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/_all/_search?pretty=1\'  -d \'\r\n    {\r\n       "script_fields" : {\r\n          "num" : {\r\n             "script" : "1 / doc[\\u0027num\\u0027].value "\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\nIn version 0.20.6:\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_score" : 1,\r\n    #             "fields" : {\r\n    #                "num" : 0.125\r\n    #             },\r\n    #             "_index" : "test",\r\n    #             "_id" : "CyF_z9ZJT1-NyYRAxjNJxw",\r\n    #             "_type" : "test"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 1\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 2\r\n    # }\r\n\r\nIn version 0.90.0.RC1:\r\n\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_score" : 1,\r\n    #             "fields" : {\r\n    #                "num" : 0\r\n    #             },\r\n    #             "_index" : "test",\r\n    #             "_id" : "csZwbG85TqyqOKfg8A5j2g",\r\n    #             "_type" : "test"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 1\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "took" : 150\r\n    # }\r\n'
2828,'spinscale','Make highlight function pluggable so that customized highlight function ...\n'
2825,'spinscale','Shard replica differs from origin\nShards in cluster became inconsistent and they return different results, as they contain different data. How can i fix it? What is the reason for that?\r\n\r\nVersion 0.20.4'
2820,'imotov','Least used store distributor allocates all data to the last directory on the list\nSee https://groups.google.com/d/msg/elasticsearch/UaaCsk7xwFs/vHpB9TOl5n8J'
2819,'javanna','Date Math does not work in filtered query\nnewbie to ES.\r\nThis does not return any docs:\r\n```json\r\n  "query": {\r\n    "filtered": {\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "range": {\r\n          "screened": {\r\n            "from": "now-3d",\r\n            "to": "now+3d"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n```\r\nThis does:\r\n```json\r\n      "query": {\r\n        "match_all": {}\r\n      },\r\n      "filter": {\r\n        "range": {\r\n          "screened": {\r\n            "from": "now-3d",\r\n            "to": "now+3d"\r\n          }\r\n        }\r\n      }\r\n```\r\n'
2817,'s1monw','Phrase suggester throws ArrayIndexOutOfBoundsException when stopwords are the only text values\n-Version 0.90.0.RC1\r\nWhen searching for phrase suggestions and all suggestions are stop words, then I would expect empty results instead of an ArrayIndexOutOfBoundsException.\r\n\r\n```bash\r\ncurl -XPUT http://localhost:9200/test -d \'{ "number_of_shards":1, "number_of_replicas":0 }\'\r\ncurl -XPUT http://localhost:9200/test/test/1 -d \'{ "subject": "a test subject" }\'\r\ncurl -XPOST http://localhost:9200/test/_suggest?pretty=true -d \'{\r\n  "text": "a an the",\r\n  "sug2": {\r\n    "phrase": {\r\n      "field": "subject",\r\n      "size": 1,\r\n      "real_word_error_likelihood": 0.95,\r\n      "max_errors": 0.5,\r\n      "gram_size": 2,\r\n      "direct_generator": [\r\n        {\r\n          "field": "subject",\r\n          "suggest_mode": "always",\r\n          "min_word_len": 1\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\'\r\n\r\n# Response\r\n{\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 0,\r\n    "failed" : 1,\r\n    "failures" : [ {\r\n      "index" : "test",\r\n      "shard" : 0,\r\n      "reason" : "BroadcastShardOperationFailedException[[test][0] ]; nested: ElasticSearchException[failed to execute suggest]; nested: ArrayIndexOutOfBoundsException[0]; "\r\n    } ]\r\n  }\r\n}\r\n```'
2816,'imotov','Alias filter is ignored if a sort field is specified\nRepro: https://gist.github.com/imotov/5242738'
2814,'s1monw','0.90.0.RC1 Percolator, Not Matching Properly on Custom Analyzer\nI found this bug in 0.90.0.RC1. (Works properly in 0.20.6 and earlier). It seems to have something to do with the custom analyzer/tokenizer being specified.\r\n\r\n1) PUT http://localhost:9200/foo\r\n{\r\n  "settings": {\r\n    "number_of_shards": 1,\r\n    "number_of_replicas": 0,\r\n    "index": {\r\n      "analysis": {\r\n        "analyzer": {\r\n          "lwhitespacecomma": {\r\n            "tokenizer": "whitespacecomma",\r\n            "filter": [\r\n              "lowercase"\r\n            ]\r\n          }\r\n        },\r\n        "tokenizer": {\r\n          "whitespacecomma": {\r\n            "pattern": "(,|\\\\s+)",\r\n            "type": "pattern"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "mappings": {\r\n    "doc": {\r\n      "properties": {\r\n        "filingcategory": {\r\n            "type": "string",\r\n            "analyzer": "lwhitespacecomma"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n2) POST http://localhost:9200/_percolator/foo/1\r\n{\r\n  "source": "productizer",\r\n  "query": {\r\n    "constant_score": {\r\n      "query": {\r\n        "query_string": {\r\n          "query": "filingcategory:s"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n3) POST http://localhost:9200/foo/doc/_percolate\r\n{\r\n  "query": {\r\n    "term": {\r\n      "source": "productizer"\r\n    }\r\n  },\r\n  "doc": {\r\n    "filingcategory": "s"\r\n  }\r\n}\r\n\r\nReturns (in 0.90.0.RC1): {"ok":true,"matches":[]}\r\n\r\nReturns (in 0.20.6 or below): {"ok":true,"matches":["1"]}\r\n\r\nThanks!'
2812,'s1monw','TooManyOpenFiles might cause data-loss in ElasticSearch (Lucene)\nUnder certain circumstances a TooManyOpenFiles exception (in Java thrown as FileNotFoundException) might cause data loss where entire shards (lucene indices) are deleted. This is mainly caused by [Lucene-4870](https://issues.apache.org/jira/browse/LUCENE-4870) - currently all Elasticsearch releases are affected by this.'
2807,'s1monw','When faceting on integer field using TERMS , 0.9 fails to count records where the facet field is missing\nreproduceable example:\r\nhttps://gist.github.com/roytmana/5215441\r\n\r\nsorry my curl commands could be somewhat wrong - I am on windows and not using it much'
2806,'s1monw','0.90.0.RC1 Percolator Query, NPE\nI think this is a bug in the latest build but I\'m not sure how to open a ticket. This is using 0.90.0.RC1. We\'ve been using ES from 0.19.x through 0.20.5 and this worked fine up to this point. Looks like it is broken in the newest build.\r\n\r\n1) POST http://localhost:9200/_percolator/foo/1\r\n{\r\n  "source": "foo",\r\n  "query": {\r\n    "term": {\r\n      "foo": "bar"\r\n    }\r\n  }\r\n}\r\n\r\n2) PUT http://localhost:9200/foo\r\n\r\n3) POST http://localhost:9200/foo/doc/_percolate\r\n{\r\n  "query": {\r\n    "term": {\r\n      "source": "foo"\r\n    }\r\n  },\r\n  "doc": {\r\n    "foo": "bar"\r\n  }\r\n}\r\n\r\nES 0.90.0.RC1 returns:\r\n\r\n{\r\nerror: NullPointerException[null]\r\nstatus: 500\r\n}\r\n\r\nThanks!'
2802,'spinscale',"Support for partial document updates in the Bulk API\nI added support for partial document updates in the bulk API. We are using this functionality for one month now in our development environment. Script functionality is not supported via this change and i'm not planning to make it either.\r\n"
2800,'s1monw',"Stemmer override with large rule set causing java.lang.OutOfMemoryError: Java heap space\nHello,\r\n\r\nI was trying to create czech lemmatisation analyzer using [stemmer override filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-override-tokenfilter.html) and czech dictionary from [aspell](ftp://ftp.gnu.org/gnu/aspell/dict/cs).\r\n\r\nThis dictionary contains around `300 000` words in base form and some suffix/prefix rules. After expansion format file looks like this\r\n\r\n    Aakjaer Aakjaerech Aakjaery Aakjaerům Aakjaerů Aakjaerem Aakjaere Aakjaerovi Aakjaeru Aakjaera Aakjaerové\r\n    Aakjaerová Aakjaerovými Aakjaerovým Aakjaerových Aakjaerovou Aakjaerové\r\n    Aakjaerův Aakjaerovýma Aakjaerovými Aakjaerových Aakjaerovou Aakjaerovo Aakjaerovy Aakjaerovi Aakjaerovým \r\n\r\neach line is one word with its forms. \r\n\r\nBecause of rules format `form => lemma` the final rule set is expanded from `300 000` to `4 364 674` lines. \r\n\r\nWhen I was trying on my local machine to index czech wikipedia pages (around `400 000` documents) `java.lang.OutOfMemoryError: Java heap space` error occured after approx 10 minutes of indexing (log file [here](https://gist.github.com/vhyza/9f88b921a1d0f650ccd8#file-elasticsearch-log))\r\n\r\nI'm using snapshot build of elasticsearch ([54e7e309a5d407b2fb1123a79e6af9d62e41ea1e](https://github.com/elasticsearch/elasticsearch/commit/54e7e309a5d407b2fb1123a79e6af9d62e41ea1e)), `JAVA_OPTS -Xss200000 -Xms2g -Xmx2g` with no other indices.\r\n\r\nIndex settings/mapping and river settings are in separate [gist](https://gist.github.com/vhyza/9f88b921a1d0f650ccd8#file-stemmer_override_test-sh)\r\n\r\nI was trying to achieve this functionality using [synonym token filter](http://www.elasticsearch.org/guide/reference/index-modules/analysis/synonym-tokenfilter.html), because of better format of synonym rules - `form1, form2, form3, form4 => lemma` (so number of rules are only about `300 000`).\r\n\r\nBut it's not the same. In the case of using `stemmer override filter`, when token was not found in rule set, stemmer was used. I probably can do the same by adding [keyword marker](http://www.elasticsearch.org/guide/reference/index-modules/analysis/keyword-marker-tokenfilter.html) and [stemmer](http://www.elasticsearch.org/guide/reference/index-modules/analysis/stemmer-tokenfilter.html) in the filter chain, but I don't think it is the right way to do that.\r\n\r\nPlease, is there some better 'compressed' format of `stemmer override filter` rules? Any thoughts how to avoid `java.lang.OutOfMemoryError: Java heap space` error?\r\n"
2799,'s1monw',"Specialise the default codec to reuse Lucene41 files in the default case \ntoday we use a BloomFilter format for out uid field which almosts doubles the # of files per segment since it's a dedicated PostingsFormat. Yet, since this is the default for I'd say almost all people (I don't expect many folks to no use the BloomFilter on the uid field) we should optimise for the common case"
2798,'jpountz','Filter strategy comments and code inconsistent\nThe comments in XFilteredQuery claim that the strategy will fall back to LEAP_FROG_FILTER FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L186 \r\nbut the code refers to:\r\n\r\n*  LEAP_FROG_QUERY_FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L208 and \r\n* QUERY_FIRST https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/common/lucene/search/XFilteredQuery.java#L216\r\n\r\nis this logic correct?'
2796,'colings86','GeoJSONShapParser can parse MultiLineString and GeometryCollection\nHere is an update of the GeoJSON Shape Parser so it can parse MultiLineString and GeometryCollection.\r\n\r\nAs far as I know, this was the only reason why elasticsearch geo_shape type was not supporting MultiLineString and GeometryCollection.'
2792,'dadoonet',"Move all generated files during tests to maven target dir\nReopening issue #2310:\r\n\r\nBy now when launching tests, some dir are created under `/data` or `/work` dirs.\r\nIMHO, it's best to use `/target` dir to hold test dirs.\r\n\r\nSo, here is the change proposal:\r\n- all tests creates data files under `/target/es/data` and work files under `/target/es/work`\r\n- `src/test/resources/es-test.properties` is created and contain `path.data` and `path.work` settings\r\n- `pom.xml` is modified to parse test resources and modify `${project.build.directory}` to maven target dir\r\n- some TestNG tests are modified to use theses settings\r\n- some *main()* tests (stress tests) are modified to use theses settings\r\n"
2791,'s1monw','Expose zero_terms_query to the mutl_match API\nThe MatchQuery already exposes the zero_terms_query property via the match API, this exposes the same to the multi_match API by adding parsing to the MultiMatchQueryParser and building to the MultiMatchQueryBuilder.'
2785,'s1monw',"StoreStats's throttleTimeInNanos overflows causing serialization issues\nIt appears that the new SimpleRateLimiter that was added to Lucene 4.x returns the targetNS (meaning the time when the method should exit) instead of the duration that was actually paused.  This then leads to StoreStats overflowing the throttleTimeInNanos which then causes serialization issues with writeVLong (arithmetic exceptions, etc)"
2784,'dadoonet','Add version to plugins\nRelative to #2668 \r\n\r\nPlugin developpers can now add a version number to their es-plugin.properties file:\r\n\r\n```properties\r\nplugin=org.elasticsearch.test.integration.nodesinfo.TestPlugin\r\nversion=0.0.7-SNAPSHOT\r\n```\r\n\r\nAlso, for site plugins, it\'s recommended to add a `es-plugin.properties` file in root site directory with `description` and `version` properties:\r\n```properties\r\ndescription=This is a description for a dummy test site plugin.\r\nversion=0.0.7-BOND-SITE\r\n```\r\n\r\nWhen running Nodes Info API, you will get information on versions:\r\n\r\n```sh\r\n$ curl \'http://localhost:9200/_nodes?plugin=true&pretty\'\r\n```\r\n\r\n```javascript\r\n{\r\n  "ok" : true,\r\n  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",\r\n  "nodes" : {\r\n    "RHMsToxiRcCXwHiS6mEaFw" : {\r\n      "name" : "node2",\r\n      "transport_address" : "inet[/192.168.0.15:9301]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9201]",\r\n      "plugins" : [ {\r\n        "name" : "dummy",\r\n        "version" : "0.0.7-BOND-SITE",\r\n        "description" : "This is a description for a dummy test site plugin.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : true,\r\n        "jvm" : false\r\n      } ]\r\n    },\r\n    "IKiUOo-LSCq1Km1GUhBwPg" : {\r\n      "name" : "node3",\r\n      "transport_address" : "inet[/192.168.0.15:9302]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9202]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "version" : "0.0.7-SNAPSHOT",\r\n        "description" : "test-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      } ]\r\n    },\r\n    "H64dcSF2R_GNWh6XRCYZJA" : {\r\n      "name" : "node1",\r\n      "transport_address" : "inet[/192.168.0.15:9300]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9200]",\r\n      "plugins" : [ ]\r\n    },\r\n    "mGEZcYl8Tye0Rm5AACBhPA" : {\r\n      "name" : "node4",\r\n      "transport_address" : "inet[/192.168.0.15:9303]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/192.168.0.15:9203]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "version" : "0.0.7-SNAPSHOT",\r\n        "description" : "test-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      }, {\r\n        "name" : "test-no-version-plugin",\r\n        "version" : "NA",\r\n        "description" : "test-no-version-plugin description",\r\n        "site" : false,\r\n        "jvm" : true\r\n      }, {\r\n        "name" : "dummy",\r\n        "version" : "NA",\r\n        "description" : "No description found for dummy.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : true,\r\n        "jvm" : false\r\n      } ]\r\n    }\r\n  }\r\n}\r\n```'
2782,'s1monw',"Remove alternative keys in suggest request\nsimilar to #2781 we have alternative names in the suggest requests where we support `shard_size` and `shardSize`. This API is pretty new and I don't think we should support any alternative names here. This was introduced in 0.90Beta1 so I think we should be free to remove those.\r\n\r\n"
2781,'s1monw',"Remove unnecessary sort_order, sort_mode keys from sort object\nthis came up due to #2767 since we had documentation issues that used `sort_mode` instead of `mode` in the sort object. I tried to make this consistent and added `sort_order` next to `order` since I though this was introduced before 0.90Beta1 but it wasn't so I'd rather drop those entirely. Any objections?"
2780,'s1monw','Make StupidBackoff the default model for phrase suggester\ngiven the feedback on #2709 I think using stupid backoff by default make most sense.'
2779,'dadoonet','Rename artifact when building with CI\nWhen building with CI tool, we want to define a specific name for our atifacts.\r\n\r\n```sh\r\n$ mvn install -PCI\r\n```\r\nWill build `elasticsearch-0.90.0.Beta2-SNAPSHOT-20130314-1021.jar` artifact.\r\n\r\nWhen using default build, artifact naming is preserved:\r\n\r\n```sh\r\n$ mvn install\r\n```\r\nWill build `elasticsearch-0.90.0.Beta2-SNAPSHOT.jar` artifact.\r\n'
2778,'chilling','Doing Geo Bounding Box filtering using a single geohash value\nI\'m writing an application which handles geohashes explicitly, as a way to define a grid in the space (a cell in the grid is defined by a geohash).\r\n\r\nIn this use case I would find handy to be able to do a geo bounding box filter using a single geohash. The bounding box to filter upon would be the geohash itself. My idea for the syntax is something like below:\r\n\r\n```json\r\n{\r\n    "filtered" : {\r\n        "query" : {\r\n            "match_all" : {}\r\n        },\r\n        "filter" : {\r\n            "geo_bounding_box" : {\r\n                "pin.location" : {\r\n                    "cell" : "drm3btev3"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nCould this use case be of common interest?'
2774,'s1monw','Suggest Feature should have a dedicated REST endpoint\n# REST Suggester API\r\nThe REST Suggester API binds the \'Suggest API\' to the REST Layer directly. Hence there is no need to touch the query layer for requesting suggestions.\r\nThis API extracts the Phrase Suggester API and makes \'suggestion request\' top-level objects in suggestion requests. The complete API can be found in the\r\nunderlying ["Suggest Feature API"](http://www.elasticsearch.org/guide/reference/api/search/suggest.html).\r\n\r\n# API Example\r\nThe following examples show how Suggest Actions work on the REST layer. According to this a simple request and its response will be shown.\r\n\r\n## Suggestion Request\r\n```json\r\ncurl -s -XPOST \'localhost:9200/_suggest -d {\r\n    "text" : "Xor the Got-Jewel",\r\n    "simple_phrase" : {\r\n        "phrase" : {\r\n            "analyzer" : "bigram",\r\n            "field" : "bigram",\r\n            "size" : 1,\r\n            "real_word_error_likelihood" : 0.95,\r\n            "max_errors" : 0.5,\r\n            "gram_size" : 2\r\n        }\r\n    }\r\n}\r\n```\r\nThis example shows how to query a suggestion for the global text \'Xor the Got-Jewel\'. A \'simple phrase\' suggestion is requested and\r\na \'direct generator\' is configured to generate the candidates.\r\n\r\n## Suggestion Response\r\nOn success the request above will reply with a response like the following:\r\n```json\r\n{\r\n    "simple_phrase" : [ {\r\n        "text" : "Xor the Got-Jewel",\r\n        "offset" : 0,\r\n        "length" : 17,\r\n        "options" : [ {\r\n            "text" : "xorr the the got got jewel",\r\n            "score" : 3.5283546E-4\r\n        } ]\r\n    } ]\r\n}\r\n```\r\nThe \'suggest\'-response contains a single \'simple phrase\' which contains an \'option\' in turn. This option represents a suggestion of the\r\nqueried text. It contains the corrected text and a score indicating the probability of this option to be meant.'
2773,'s1monw','Sort Fails with AIOOB exception if field has rarely a value.\nI ran into this playing with kibana-dashboard this morning and I am very suprised that our tests don\'t catch that. It seems we using numDocs as an upperBound rather than numOrds in the sorting code....\r\n\r\n```\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [twitter][1]: query[filtered(place.country:united _all:states)->cache(created-at:[1363171743730 TO 1363175343730])],from[0],size[100],sort[<custom:"place.country": org.elasticsearch.index.fielddata.fieldcomparator.BytesRefFieldComparatorSource@38aa2a95>!]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:139)\r\n\tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:242)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 2538\r\n\tat org.apache.lucene.util.packed.Direct8.get(Direct8.java:52)\r\n\tat org.elasticsearch.index.fielddata.plain.PagedBytesAtomicFieldData$BytesValues.getValueByOrd(PagedBytesAtomicFieldData.java:143)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.binarySearch(BytesRefOrdValComparator.java:459)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.binarySearch(BytesRefOrdValComparator.java:452)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.setBottom(BytesRefOrdValComparator.java:431)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator$PerSegmentComparator.setBottom(BytesRefOrdValComparator.java:165)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator.setNextReader(BytesRefOrdValComparator.java:410)\r\n\tat org.elasticsearch.index.fielddata.fieldcomparator.BytesRefOrdValComparator$PerSegmentComparator.setNextReader(BytesRefOrdValComparator.java:155)\r\n\tat org.apache.lucene.search.TopFieldCollector$OneComparatorNonScoringCollector.setNextReader(TopFieldCollector.java:97)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:602)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:192)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:572)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:524)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:501)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:345)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:128)\r\n\t... 9 more\r\n```'
2771,'s1monw','Documentation Error: Geo Distance Filter does not have distance_unit, use "unit" instead\nTested on 0.20.2 and 0.20.5 \r\n\r\nThe documentation on  http://www.elasticsearch.org/guide/reference/query-dsl/geo-distance-filter.html\r\nstates that one can use a "distance_unit" parameter to set the distance unit when the distance parameter is a numeric value. but the correct name of the parameter is "unit".\r\nexample:\r\n\r\n "filter": {\r\n            "geo_distance": {\r\n              "distance": "2",\r\n              "unit": "km",\r\n              "distance_type": "plane",\r\n              "optimize_bbox": "indexed",\r\n              "related.location": {\r\n                "lat": 51,\r\n                "lon": 9\r\n              }\r\n            }\r\n          }\r\n'
2769,'dadoonet','Index Mapping with Specific Field\nHello All,\r\n\r\nI have created template for mapping index. I want to map specific filed to index. I dont want all filed. Please find below command.\r\n\r\ncurl -XPUT http://localhost:9200/fact/Activity/_mapping -d " {\\"Activity\\" : { \r\n\r\n\\"_all\\" : { \\"enabled\\" : false}, \\"_source\\": { \\"compress\\": false }, \r\n\r\n\\"properties\\" : {\\"ActiveUser\\" : {\\"type\\" : \\"string\\", \\"store\\" : \\"yes\\"},\r\n\r\n\\"ActiveOrganization\\" : {\\"type\\" : \\"string\\", \\"store\\" : \\"yes\\"}}}}\r\n\r\nIn this i have added two field like ActiveUser and Activeorganization into fact Index. But when I push data form coucbase then it mapped all fileds with same index.I have set _all filed to false but its not working. I have also attched image please check it.\r\n\r\nBefore push data from coucbase it looks as below. I want only two fields like as below\r\n\r\n![ElasticSearch-part1](https://f.cloud.github.com/assets/3797406/252534/9e28df90-8ba4-11e2-92bb-ced7b2dfe74e.png)\r\n\r\nJSON format for mapping which is as below.\r\n\r\n![ElasticSearch-part2](https://f.cloud.github.com/assets/3797406/252536/d974a2a0-8ba4-11e2-80cb-fd47d3aa3b02.png)\r\n\r\nAfter push data from couchbase it mapped all fields with index which is as below. I dont want this? How to disable all fileds? \r\n\r\n![ElasticSearch-part3](https://f.cloud.github.com/assets/3797406/252538/090f0e92-8ba5-11e2-83d3-8fd3de704afb.png)\r\n\r\nPlease help me for same.\r\nQuick response will be appreciable\r\n\r\nThanks \r\nSuraj Bhansali\r\n'
2767,'s1monw','Multi-valued field sorting: "sort_order" is actually "order" ?\nThe syntax used in the new [multi-valued fields](https://github.com/elasticsearch/elasticsearch/issues/2634) (and on the [Sort documentation](http://www.elasticsearch.org/guide/reference/api/search/sort.html)) specifies "sort_order", but it appears that the property is actually "order".\r\n\r\nJust looking for a clarification if "order" or "sort_order" is the intended parameter name. \r\n\r\nRecreation, on 0.90.0.Beta1:\r\n\r\n```bash\r\n#add some test data\r\n$ curl -X POST "http://localhost:9200/index/document/2" -d \'{"id":2,"sortingfield":["g","h","i"]}\'\r\n$ curl -X POST "http://localhost:9200/index/document/3" -d \'{"id":3,"sortingfield":["e","f"]}\'\r\n$ curl -X POST "http://localhost:9200/index/document/1" -d \'{"id":1,"sortingfield":["a", "b", "c"]}\'\r\n```\r\n\r\n```bash\r\n# "sort_order" examples\r\n$ curl -XGET localhost:9200/index/document/_search?pretty -d \'{\r\n   "query": {\r\n     "match_all": {}\r\n   },\r\n   "sort": [\r\n     {"sortingfield" : {"sort_order" : "asc", "sort_mode" : "avg"}}\r\n   ]\r\n }\' | grep "_source"\r\n\r\n      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},\r\n      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},\r\n      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},\r\n\r\n\r\n$ curl -XGET localhost:9200/index/document/_search?pretty -d \'{\r\n   "query": {\r\n     "match_all": {}\r\n   },\r\n   "sort": [\r\n     {"sortingfield" : {"sort_order" : "desc", "sort_mode" : "avg"}}\r\n   ]\r\n }\' | grep "_source"\r\n\r\n      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},\r\n      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},\r\n      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},\r\n```\r\n\r\nAnd now using just "order", which shows correct sorting:\r\n\r\n```bash\r\n# "order" examples\r\n$ curl -XGET localhost:9200/index/document/_search?pretty -d \'{\r\n   "query": {\r\n     "match_all": {}\r\n   },\r\n   "sort": [\r\n     {"sortingfield" : {"order" : "asc", "sort_mode" : "avg"}}\r\n   ]\r\n }\' | grep "_source"\r\n\r\n      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},\r\n      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},\r\n      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},\r\n\r\n$ curl -XGET localhost:9200/index/document/_search?pretty -d \'{\r\n   "query": {\r\n     "match_all": {}\r\n   },\r\n   "sort": [\r\n     {"sortingfield" : {"order" : "desc", "sort_mode" : "avg"}}\r\n   ]\r\n }\' | grep "_source"\r\n\r\n      "_score" : null, "_source" : {"id":2,"sortingfield":["g","h","i"]},\r\n      "_score" : null, "_source" : {"id":3,"sortingfield":["e","f"]},\r\n      "_score" : null, "_source" : {"id":1,"sortingfield":["a", "b", "c"]},\r\n```\r\n'
2762,'s1monw','Alias filters cached/type inferenced incorrectly\nA while ago we started playing with alias filters, but initially ran into a confusing bug.\r\n\r\nOur environment creates our index along with its type mappings and aliases all at once, then proceeds to index documents into it.\r\n\r\nThe problem lies with using a numeric filter in an alias on a field that does not have an explicit type via its mapping. After indexing documents that match the alias\' filter, no results are returned. This appears to be due to the alias being cached.\r\n\r\nThe following is a minimal reproduction of the issue:\r\n\r\n```\r\ncurl -XPUT \'http://localhost:9200/testindex/\'\r\n\r\necho create alias that filters by numericField:1\r\ncurl -XPOST \'http://localhost:9200/_aliases\' -d \'{\r\n  "actions": [\r\n    {\r\n      "add": {\r\n        "index": "testindex",\r\n        "alias": "testalias",\r\n        "filter": { "term": { "numericField": 1 } }\r\n      }\r\n    }\r\n  ]\r\n}\'\r\n\r\necho index a case that should match our filter\r\ncurl -XPUT \'http://localhost:9200/testindex/testtype/1\' -d \'{\r\n  "name": "bob",\r\n  "numericField": 1,\r\n  "message": "testing numeric alias filters"\r\n}\'\r\n\r\necho confirm we can find the document via term filter\r\ncurl -XPOST \'http://localhost:9200/testindex/_search\' -d \'{\r\n  "filter": {\r\n    "term": { "numericField": 1 }\r\n  }\r\n}\'\r\n\r\necho document not matched when filtering via the alias\r\ncurl -XPOST \'http://localhost:9200/testalias/_search\' -d \'{\r\n  "query": {\r\n    "match_all": {}\r\n  }\r\n}\'\r\n```\r\n\r\nInspecting the alias via the _aliases endpoint doesn\'t reveal any information that would explain the lack of matching documents.\r\n\r\nIf you explicitly type the filtered field in the mapping before creating the alias, the alias filter works as expected.\r\n```\r\ncurl -XPOST \'http://localhost:9200/testindex/\' -d \'{\r\n  "mappings": {\r\n    "testtype": {\r\n      "properties": {\r\n        "numericField": { "type": "long" }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\nGiven that this was relatively tricky to figure out (nothing is mentioned about type inferencing in relation to alias filters in the docs), this struck us as being a bug in how alias filters are being stored/cached.'
2761,'imotov','ES 0.20.5 stuck in RED after node loss\n0.19.x series seemed to have fixed random file deletion issues for me but now 0.20.x series seems to have reverted.  \r\n\r\nRecently upgraded from 0.19.12 to 0.20.5.  A node went down over the weekend in my 30 node cluster.  I use ES as a DB so there were constant writes while the node was down.  Restarted the node and went RED.\r\n\r\nI get the following exception over and over.\r\n\r\n\r\n[2013-03-10 15:28:22,209][WARN ][indices.cluster          ] [moloches-m13b] [stats][0] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [stats][0] shard allocated for local recovery (post api), should exists, but doesn\'t\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:122)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\n\r\n\r\nWhat is the correct way to recover?\r\n\r\n\r\nI have full replication on for this index, and it is a tiny index (8 documents for total store size of 10k)\r\n\r\n{\r\n  "stats" : {\r\n    "settings" : {\r\n      "index.number_of_replicas" : "29",\r\n      "index.auto_expand_replicas" : "0-all",\r\n      "index.number_of_shards" : "1",\r\n      "index.version.created" : "200599"\r\n    }\r\n  }\r\n}\r\n\r\n\r\nMy desire is that ES should just take care of this, I have 29 other copies why do I need to do anything? :-)  In a dream world ES would delete the broken index and copy from somewhere else.  What am I missing?\r\n\r\n\r\nI don\'t have any other logs any more, so was hoping the exception will be enough.'
2757,'spinscale',"Use repository and signatures for .deb packages\nCurrently I´m checking once in a while if there are new packages available for Debian systems, but this is cumbersome. Ideally I could add your repository and import public keys into my apt keyring and get updates whenever they're ready. It doesn't really scale on an admin level to verify all packages manually ;-)"
2756,'s1monw',"change geo_shape default configuration\nCurrently geo_shape defaults to the geohash implementation and a rather high tree_levels setting. I've been testing with some open street map data consisting of about 120K points, polygons, and linestrings in the Brandenburg area on Simon Willnauer's lucene 4.2 branch which is about to be merged in the next few days. The pre lucene 4.2 geo_shape seems to have severe accuracy problems currently.\r\n\r\nWhen I index this data with the default settings (geohash, tree_levels=24), I end up with nearly a GB of index data (the raw data is 35MB). Indexing takes nearly ten minutes using the bulk api, batches of 500, and with six threads Reducing the tree_levels to 9 reduces the index size substantially. However with this setting, the accuracy drops significantly and such indiceses are useless for searching unless you find margins of errors measured in kilometers acceptable.\r\n\r\nI've also tested with the quadtree implementation. The default setting for this offers very poor accuracy and returns tens of thousands of results for queries that should return only a few dozen results. However, increasing the tree_levels to 20 results in pretty good accuracy (a few tens of meters margin of error) and an index size of only 72MB, which is very reasonable and it indexes in seconds. Increasing the tree_levels to 25 increases the index size to half a GB and the accuracy improves to being comparable with geohash at its default settings.\r\n\r\nBased on this, I would like the defaults for geo_shape to be changed as follows:\r\n1) make quadtree the default implementation. I think that overall it offers better index size, indexing speed than geo_hash. A smaller index size also means it probably performs better on queries and uses less memory.\r\n2) increase default tree_levels for quadtree to 20. This seems to give a reasonable tradeoff between index size, indexing speed, and accuracy. The user can increase it for better accuracy. The current default does not provide anywhere near acceptable levels of accuracy."
2753,'s1monw','Add KeywordRepeatFilter from Lucene to Beta2\nI just added a new TokenFilter in [https://issues.apache.org/jira/browse/LUCENE-4817](LUCENE-4817) that allows to index stemmed and unstemmed versions of a token into the same field. I find that pretty useful and some users on the list asked for it as well. I think we should add it to ES as a copy until Lucene 4.3 is released.'
2752,'s1monw',"PhraseSuggest CandidateGenerator doesn't respect `size` parameter\nthe size parameter on CandidateGenerator is ignored and always set to 5."
2749,'imotov','Invalid string for refresh_interval is successfully accepted(but internally fails), log spews heavily after restart\ncurl -X PUT localhost:9200/mytestindex\r\ncurl -X PUT \'localhost:9200/mytestindex/_settings?pretty\' -d \' { "index.refresh_interval" : "" } \'\r\n{\r\n  "ok" : true\r\n}\r\n\r\nHowever in the logs - \r\n    [2013-03-08 10:13:44,021][WARN ][index.settings           ] [Lyja] [mytestindex] failed to refresh settings for [org.elasticsearch.index.shard.service.InternalIndexShard$ApplyRefreshSettings@78fc8ee1]\r\n        org.elasticsearch.ElasticSearchParseException: Failed to parse []\r\n            at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:253)\r\n            at org.elasticsearch.common.settings.ImmutableSettings.getAsTime(ImmutableSettings.java:191)\r\n            at org.elasticsearch.index.shard.service.InternalIndexShard$ApplyRefreshSettings.onRefreshSettings(InternalIndexShard.java:700)\r\n            at org.elasticsearch.index.settings.IndexSettingsService.refreshSettings(IndexSettingsService.java:54)\r\n            at org.elasticsearch.indices.cluster.IndicesClusterStateService.applySettings(IndicesClusterStateService.java:323)\r\n            at org.elasticsearch.indices.cluster.IndicesClusterStateService.clusterChanged(IndicesClusterStateService.java:175)\r\n            at org.elasticsearch.cluster.service.InternalClusterService$2.run(InternalClusterService.java:315)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n            at java.lang.Thread.run(Thread.java:619)\r\n    Caused by: java.lang.NumberFormatException: For input string: ""\r\n            at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\r\n            at java.lang.Long.parseLong(Long.java:431)\r\n            at java.lang.Long.parseLong(Long.java:468)\r\n            at org.elasticsearch.common.unit.TimeValue.parseTimeValue(TimeValue.java:249)\r\n            ... 9 more\r\n\r\nIf the issue is missed, and ES restarted, then there is heavy spew in the logs with the above trace repeated continuously. I can pass it garbage like "asfad" and it still accepts successfuly but prints error in the log. I am using version 0.20.1.'
2740,'martijnvg','Percolating based on More Like This queries returns "incorrect" results\nWhen a more like this query is added to the percolator for a given index the results of percolating a document through the query are inconsistent with what you would get if you re-ran the MLT query against the index. The document being percolated may match the MLT query in the percolator, but may not be contained in the results of the MLT being ran against the index.\r\n\r\nThe reason for this is that the MLT query uses the document frequency of the terms, and number of documents in the index, to weight the term frequency in the MLT like text in order to generate the score for the given term, i.e.:\r\n      float idf = similarity.idf(docFreq, numDocs);\r\n      float score = tf * idf;\r\n\r\nSince the index that is being used to generate the document frequency and number of documents contains just the singular document being percolated this causes the score to be vastly different (i.e. common terms in the index are not discounted).\r\n\r\nTherefore when the rewrite is performed on the MoreLikeThisQuery from within the PercolatorExecutor (via search / createNormalizedWeight / rewrite(query) - in IndexSearcher) it would be more correct to use the IndexReader defined for the index associated with this percolated query to calculate the document frequency and number of documents.\r\n\r\nNot sure if using the "correct" index to rewrite other queries would make similar improvements with other query types.\r\n\r\n(Note: this is based on my understanding of this code which might be slightly off and is based on 0.19.10)'
2739,'javanna','Index template aliases\nAdded support for aliases in index templates.'
2735,'s1monw','Move Smoothing Model into its own sub-object in the Phrase suggest request\n\r\nrather than having\r\n```json\r\ncurl -s -XPOST \'localhost:9200/_search\' -d \'{\r\n  "suggest" : {\r\n    "text" : "Xor the Got-Jewel",\r\n    "simple_phrase" : {\r\n      "phrase" : {\r\n      ...\r\n        "stupid_backoff" : {\r\n          "discount" : 0.4\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n we should have: \r\n```json\r\ncurl -s -XPOST \'localhost:9200/_search\' -d \'{\r\n  "suggest" : {\r\n    "text" : "Xor the Got-Jewel",\r\n    "simple_phrase" : {\r\n      "phrase" : {\r\n      ...\r\n        "smoothing" : {\r\n          "stupid_backoff" : {\r\n            "discount" : 0.4\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```'
2734,'s1monw','Add support for ignore_indices to delete by query\nAs the doc says all multi indices API support the ignore_indices option, and the delete by query API is a multi indices API I think it makes sense to support it.'
2729,'s1monw','Phrase Suggester "size" should override "shard_size" when searching on index with 1 shard\nWhen using the phrase suggester against and index with 1 shard, setting the "size" parameter > 5 will not return the correct number of suggestions unless you also set "shard_size" to the same number.   The minimum "shard_size" should be set to something like ceil(size/num_shards) to ensure the correct number of suggestions are returned.'
2726,'imotov','Implement search shards API\nAn "admin.cluster" based API, which for a given a search request URI returns back the shards and nodes that the search is going to execute on.\r\n\r\nThe search shards API is using the same index as search API, except it uses the `_search_shards` end point instead of `_search`. For example, the following command will return a list of nodes and shards that would be used to execute a search request on index `twitter` with routing `march`:\r\n\r\n```\r\n$ curl -XGET \'http://localhost:9200/twitter/tweet/_search_shards?routing=march\'\r\n```\r\n\r\nShards that this command returns are grouped by `index` and `shard_id`. '
2724,'s1monw','Interchanged values in field_data stats\nIn version `0.9.0.Beta1` there is an interchange between `field_data.memory_size` and `field_data.memory_size_in_bytes`.\r\n\r\nTo reproduce:\r\n1. curl `http://localhost:9200/_nodes/_local/stats?pretty=true`\r\n\r\nActual result:\r\n```\r\n        "field_data" : {\r\n          "memory_size" : 257199478,\r\n          "memory_size_in_bytes" : "245.2mb"\r\n        },\r\n```\r\nExpected result:\r\n```\r\n        "field_data" : {\r\n          "memory_size" : "245.2mb",\r\n          "memory_size_in_bytes" : 257199478\r\n        },\r\n```\r\n\r\nHere is a pull request: https://github.com/elasticsearch/elasticsearch/pull/2725'
2717,'s1monw','Beta: Min_prefix Option does not work\nAccording to the Suggest API, "min_prefix" is available as an option to increase performance or try spellcheck from the start of word.\r\n\r\nhttps://github.com/elasticsearch/elasticsearch/issues/2585\r\n\r\nAdding the option results in this exception:\r\nElasticSearchIllegalArgumentException[suggester[fuzzy] doesn\'t support [min_prefix]]; '
2714,'s1monw','ES have bugs in 0.20.4 and 0.20.5 which cause creating some indices failure.\nThese days I found ES maybe have bugs in 0.20.4 and 0.20.5 which cause creating some indices failure.\r\n  The test steps are following:\r\n  1) I setup 20 nodes with 0.20.4, and bring a fresh cluster up. 3 nodes are master nodes, 2 nodes are load balancer, 15 nodes are data nodes \r\n  2) After the cluster is up, I tried to create some empty indices for example index-2013-02-25, index-2013-02-26, index-2013-02-27, index-2013-03-01, etc\r\n  But some shards stuck in initializing status for long time.\r\n{\r\n  "cluster_name" : "es-test",\r\n  "status" : "yellow",\r\n  "timed_out" : false,\r\n  "number_of_nodes" : 20,\r\n  "number_of_data_nodes" : 15,\r\n  "active_primary_shards" : 105,\r\n  "active_shards" : 201,\r\n  "relocating_shards" : 0,\r\n  "initializing_shards" : 9,\r\n  "unassigned_shards" : 0\r\n}\r\n  Moreover when I created index-2013-03-02, the cluster became to red.\r\n{\r\n  "cluster_name" : "es-test",\r\n  "status" : "red",\r\n  "timed_out" : false,\r\n  "number_of_nodes" : 20,\r\n  "number_of_data_nodes" : 15,\r\n  "active_primary_shards" : 119,\r\n  "active_shards" : 228,\r\n  "relocating_shards" : 0,\r\n  "initializing_shards" : 11,\r\n  "unassigned_shards" : 1\r\n}\r\n \r\n  I set the log level to trace, and checked the logs, no error logs are shown. but from the logs, I can know some shards are initializing and unassigned. I will attach the logs later.\r\n  And I also tried 0.20.5, the same problem happened.  \r\n  But for 0.19.11, the problem disappeared. All the empty indices can be created successfully instantly even for some strange index names.\r\n  So I guess ES have some bugs in 0.20.4 and 0.20.5.\r\n  By the way, I also tested 0.20.4 on single node and two nodes with multiple ES instances, but such problem doesn\'t happen.'
2711,'s1monw','NullPointerException when applying a sort and using ignoreMapped(true) and the field does not exist (0.20.5)\ncode:\r\nSortBuilder sortBuilder = SortBuilders.fieldSort(query.getSortField()).ignoreUnmapped(true);\r\nsortBuilder.order(query.getSortOrder());\r\n\r\nResult:\r\norg.elasticsearch.search.builder.SearchSourceBuilderException: Failed to build search source\r\n\tat org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:559)\r\n\tat org.elasticsearch.action.search.SearchRequest.source(SearchRequest.java:251)\r\n\tat org.elasticsearch.action.search.SearchRequestBuilder.doExecute(SearchRequestBuilder.java:814)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:62)\r\n\tat org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:57)\r\n\tat \r\n...........\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator._writeFieldName(SmileGenerator.java:580)\r\n\tat org.elasticsearch.common.jackson.dataformat.smile.SmileGenerator.writeFieldName(SmileGenerator.java:453)\r\n\tat org.elasticsearch.common.xcontent.json.JsonXContentGenerator.writeFieldName(JsonXContentGenerator.java:74)\r\n\tat org.elasticsearch.common.xcontent.XContentBuilder.field(XContentBuilder.java:267)\r\n\tat org.elasticsearch.common.xcontent.XContentBuilder.startObject(XContentBuilder.java:136)\r\n\tat org.elasticsearch.search.sort.FieldSortBuilder.toXContent(FieldSortBuilder.java:78)\r\n\tat org.elasticsearch.search.builder.SearchSourceBuilder.toXContent(SearchSourceBuilder.java:673)\r\n\tat org.elasticsearch.search.builder.SearchSourceBuilder.buildAsBytes(SearchSourceBuilder.java:556)\r\n\t... 100 more\r\n'
2707,'s1monw','Custom SimilarityProvider does not work 0.90.0-Beta1\nOn index creation i define own custom similarity provider (index.similarity.index.type andindex.similarity.search.type).\r\n\r\nSearch scoring is calculated from default similarity and not from my similarity provider'
2705,'martijnvg',"Parent/child queries don't work with via the delete by query api\nReported via ML:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/Wsv3ziKdeTk"
2703,'spinscale',"NodesStatsResponse isn't fully populated if search wasn't made\nUsing 0.90\r\n\r\nIf a data node hasn't been searched on, the getOs().getMem() in the NodesStatsResponse for that node will be null. It appears as if an actual search request has to be issued against a node for that to be populated."
2695,'s1monw','Ids filter without a type throws IndexOutOfBounds\nIn 0.90.0.Beta1, an `ids` filter without a `type` throws an IndexOutOfBoundsException. Used to work correctly in 0.20\r\n\r\n\r\n    Failure [Failed to parse source [\r\n    {\r\n       "query" : {\r\n          "constant_score" : {\r\n             "filter" : {\r\n                "ids" : {\r\n                   "values" : [\r\n                      1\r\n                   ]\r\n                }\r\n             }\r\n          }\r\n       }\r\n    }\r\n    ]]\r\n\t    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:566)\r\n\t    at org.elasticsearch.search.SearchService.createContext(SearchService.java:481)\r\n\t    at org.elasticsearch.search.SearchService.createContext(SearchService.java:466)\r\n\t    at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:236)\r\n\t    at org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\r\n\t    at org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\r\n\t    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t    at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1\r\n\t    at java.util.ArrayList.rangeCheck(ArrayList.java:604)\r\n\t    at java.util.ArrayList.get(ArrayList.java:382)\r\n\t    at org.elasticsearch.index.mapper.Uid.createTypeUids(Uid.java:149)\r\n\t    at org.elasticsearch.index.query.IdsFilterParser.parse(IdsFilterParser.java:111)\r\n\t    at org.elasticsearch.index.query.QueryParseContext.parseInnerFilter(QueryParseContext.java:230)\r\n\t    at org.elasticsearch.index.query.ConstantScoreQueryParser.parse(ConstantScoreQueryParser.java:68)\r\n\t    at org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:193)\r\n\t    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:272)\r\n\t    at org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:250)\r\n\t    at org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n\t    at org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)\r\n\t    ... 11 more\r\n'
2694,'s1monw','"-4" parsed as a date\n    $ curl -s -XPOST localhost:9200/test/1 \\\r\n           -d \'{"foo":[{"bar": "-4"},{"bar": "::1"}]}\' | \\\r\n           jq .\r\n    {\r\n      "status": 400,\r\n      "error": "MapperParsingException[Failed to parse [foo.bar]]; nested: MapperParsingException[failed to parse date field [::1], tried both date format [dateOptionalTime], and timestamp number]; nested: IllegalArgumentException[Invalid format: \\"::1\\"]; "\r\n    }\r\n'
2692,'s1monw','Using Java node client and deleting all indexes cause system hungs\nUsing elasticsearch 0.20.5 in Java 1.6 with Ubuntu 12.04 (but also happens with Windows), the system is frozen when you try to delete all indexes when there are no indexes yet into the elasticsearch. This error is a bit strange but I can reproduce it without any problem.\r\n\r\nFirst of all create a new project in Eclipse (in my case but of course in command line or Intellij should do the same). Then add elasticsearch 0.20.5 as dependency and write one simple test:\r\n\r\n```java\r\nNode node = nodeBuilder().local(true).node();\r\nClient client = node.client();\r\n\t\t\r\nDeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);\r\n\r\ndeleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());\r\ndeleteByQueryRequestBuilder.execute().actionGet();\r\n\t\t\r\nnode.close();\r\n```\r\n\r\nThen I run the test and the output is:\r\n\r\n```\r\n26-feb-2013 10:17:31 org.elasticsearch.node\r\nINFO: [Assassin] {0.20.5}[4352]: initializing ...\r\n26-feb-2013 10:17:31 org.elasticsearch.plugins\r\nINFO: [Assassin] loaded [], sites []\r\n26-feb-2013 10:17:33 org.elasticsearch.node\r\nINFO: [Assassin] {0.20.5}[4352]: initialized\r\n26-feb-2013 10:17:33 org.elasticsearch.node\r\nINFO: [Assassin] {0.20.5}[4352]: starting ...\r\n26-feb-2013 10:17:33 org.elasticsearch.transport\r\nINFO: [Assassin] bound_address {local[1]}, publish_address {local[1]}\r\n26-feb-2013 10:17:33 org.elasticsearch.cluster.service\r\nINFO: [Assassin] new_master [Assassin][1][local[1]]{local=true}, reason: local-disco-initial_connect(master)\r\n26-feb-2013 10:17:33 org.elasticsearch.discovery\r\nINFO: [Assassin] elasticsearch/1\r\n26-feb-2013 10:17:33 org.elasticsearch.http\r\nINFO: [Assassin] bound_address {inet[/0.0.0.0:9200]}, publish_address {inet[/<ipaddress>:9200]}\r\n26-feb-2013 10:17:33 org.elasticsearch.node\r\nINFO: [Assassin] {0.20.5}[4352]: started\r\n26-feb-2013 10:17:33 org.elasticsearch.gateway\r\nINFO: [Assassin] recovered [0] indices into cluster_state\r\n```\r\nBut the test never ends, the actionGet() call does not return any result it is hung there.\r\n\r\nThen I abort the test and I do the next modification in test:\r\n\r\n```java\r\nNode node = nodeBuilder().local(true).node();\r\nClient client = node.client();\r\n\t\t\r\nString json = "{" +\r\n\t        "\\"user\\":\\"kimchy\\"," +\r\n\t        "\\"postDate\\":\\"2013-01-30\\"," +\r\n\t        "\\"message\\":\\"trying out Elastic Search\\"" +\r\n\t    "}";\r\n\r\nIndexResponse response = client.prepareIndex("twitter", "tweet")\r\n\t\t        .setSource(json)\r\n\t\t        .execute()\r\n\t\t        .actionGet();\r\n\t\t\r\nDeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);\r\n\r\ndeleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());\r\ndeleteByQueryRequestBuilder.execute().actionGet();\r\n\t\t\r\nnode.close();\r\n```\r\n\r\nAnd then it works as expected, but now comes the "paranoic" part, I rerun the first test again without preparing an index before deleting:\r\n\r\n```java\r\nNode node = nodeBuilder().local(true).node();\r\nClient client = node.client();\r\n\t\t\r\nDeleteByQueryRequestBuilder deleteByQueryRequestBuilder = new DeleteByQueryRequestBuilder(client);\r\n\r\ndeleteByQueryRequestBuilder.setQuery(QueryBuilders.matchAllQuery());\r\ndeleteByQueryRequestBuilder.execute().actionGet();\r\n\t\t\r\nnode.close();\r\n```\r\n\r\nBut now also it works as expected, mostly because there is one index. (data directory is there already created so can work perfectly).\r\n\r\n```\r\nINFO: [Cold War] recovered [1] indices into cluster_state\r\n```\r\nSo it seems that delete all command does not work correctly when there is no indexes to remove (first time you start the node). Maybe I am doing wrong about how to remove all indexes, but anyway there is still an error on operation which makes all application (test in my case) hungs.\r\n\r\nAlex.'
2690,'s1monw','NPE with Fuzzy Like This on non existing field\nConsider the following curl recreation:\r\n\r\n```sh\r\ncurl -XDELETE http://localhost:9200/npefuzzy\r\n \r\ncurl -XPUT http://localhost:9200/npefuzzy?pretty -d \'{\r\n  "settings" : { "index" : { "number_of_shards" : 1, "number_of_replicas" : 0 }}\r\n}\'\r\n \r\ncurl -XPUT http://localhost:9200/npefuzzy/beer/1?pretty -d \'{\r\n   "brand":"Grimbergen",\r\n   "colour":"PALE",\r\n   "size":1.3476184461445255,\r\n   "price":7.318647685097387,\r\n   "date":1320090719440\r\n}\'\r\n \r\ncurl -XPOST "localhost:9200/npefuzzy/_refresh?pretty"\r\n \r\n# This one is OK\r\ncurl -XPOST http://localhost:9200/npefuzzy/beer/_search?pretty -d \'\r\n{"query":{\r\n  "flt" : {\r\n    "fields" : [ "brand", "colour" ],\r\n    "like_text" : "heineken is a pale beer",\r\n    "max_query_terms" : 12\r\n  }\r\n}}\'\r\n \r\n# This one was ok (no error) in 0.20 (Lucene 3) but fails with 0.21 (Lucene 4)\r\ncurl -XPOST http://localhost:9200/npefuzzy/beer/_search?pretty -d \'\r\n{"query":{\r\n  "flt" : {\r\n    "fields" : [ "brand", "nonexistingfield" ],\r\n    "like_text" : "heineken is a pale beer",\r\n    "max_query_terms" : 12\r\n  }\r\n}}\'\r\n```\r\n\r\nIn 0.20.x (Lucene 3), we get no result:\r\n\r\n```javascript\r\n{\r\n "took": 5,\r\n "timed_out": false,\r\n "_shards": {\r\n  "total": 1,\r\n  "successful": 1,\r\n  "failed": 0\r\n },\r\n "hits": {\r\n  "total": 0,\r\n  "max_score": null,\r\n  "hits": []\r\n }\r\n}\r\n```\r\n\r\nWith 0.21 (Lucene 4):\r\n\r\n```javascript\r\n{\r\n  "error" : "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[5d_VkN-ARky1L_LUVoKJEQ][npefuzzy][0]: QueryPhaseExecutionException[[npefuzzy][0]: query[filtered(null)->cache(_type:beer)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: NullPointerException; }]",\r\n  "status" : 500\r\n}\r\n```\r\n\r\nLogs are:\r\n\r\n```\r\n[2013-02-25 21:10:21,141][DEBUG][action.search.type       ] [Umar] [npefuzzy][0], node[5d_VkN-ARky1L_LUVoKJEQ], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@313e36eb]\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [npefuzzy][0]: query[filtered(null)->cache(_type:beer)],from[0],size[10]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:139)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:680)\r\nCaused by: java.lang.NullPointerException\r\n\tat org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum$LinearFuzzyTermsEnum.<init>(SlowFuzzyTermsEnum.java:89)\r\n\tat org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum.maxEditDistanceChanged(SlowFuzzyTermsEnum.java:58)\r\n\tat org.apache.lucene.search.FuzzyTermsEnum.bottomChanged(FuzzyTermsEnum.java:211)\r\n\tat org.apache.lucene.search.FuzzyTermsEnum.<init>(FuzzyTermsEnum.java:144)\r\n\tat org.apache.lucene.sandbox.queries.SlowFuzzyTermsEnum.<init>(SlowFuzzyTermsEnum.java:48)\r\n\tat org.apache.lucene.sandbox.queries.FuzzyLikeThisQuery.addTerms(FuzzyLikeThisQuery.java:209)\r\n\tat org.apache.lucene.sandbox.queries.FuzzyLikeThisQuery.rewrite(FuzzyLikeThisQuery.java:262)\r\n\tat org.elasticsearch.common.lucene.search.XFilteredQuery.rewrite(XFilteredQuery.java:93)\r\n\tat org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:616)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.rewrite(ContextIndexSearcher.java:112)\r\n\tat org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:663)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:126)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:281)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:155)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:269)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:135)\r\n\t... 9 more\r\n```\r\n\r\nFull curl recreation is: https://gist.github.com/dadoonet/5031671\r\n'
2683,'s1monw','Fix bug when searching concrete and routing aliased indices fix #2682\n'
2682,'s1monw','Bug when searching concrete and routing aliased indices\nTo reproduce:\r\n\r\n* let\'s create 2 concrete indices `foo` and `foo_2`\r\n\r\n```bash\r\n$ curl -XPOST \'http://localhost:9200/foo\'\r\n$ curl -XPOST \'http://localhost:9200/foo_2\'\r\n```\r\n\r\n* let\'s create a routing alias `foo_1` for index `foo` and routing value 1\r\n\r\n```bash\r\n$ curl -XPOST \'http://localhost:9200/_aliases\' -d \'\r\n{\r\n    "actions" : [\r\n        {\r\n            "add" : {\r\n                 "index" : "foo",\r\n                 "alias" : "foo_1",\r\n                 "routing" : "1"\r\n            }\r\n        }\r\n    ]\r\n}\'\r\n```\r\n\r\n* let\'s index 2 docs one in `foo_1` and the other in `foo_2`\r\n\r\n```bash\r\n$ curl -XPOST \'http://localhost:9200/foo_1/type/1\' -d \'{"foo1":"bar1"}\'\r\n$ curl -XPOST \'http://localhost:9200/foo_2/type/2\' -d \'{"foo2":"bar2"}\'\r\n```\r\n\r\n* Now this search gives 1 result instead of the 2 I expected\r\n\r\n```bash\r\n$ curl -XGET \'http://localhost:9200/foo_*/_search\' \r\n```'
2681,'dadoonet','Fix exception typo\n'
2677,'spinscale','Maven Surefire Issue\nMy integration test starts within the setUp() method a new elasticsearch "master" + "data" node and in the test method using client node i want to perform some operations on that master node. Everything runs without any problem if i run the test case within the Eclipse but as soon as i try to package my application and the tests are being run, i get following exception. The problem is that our CI server can not run these tests, too.  Here is how i configure the server node:\r\n\t\tmap.put("cluster.name", clusterName);\r\n\t\tmap.put("node.name", "search-engine-test");\r\n\t\tmap.put("node.master", true);\r\n\t\tmap.put("node.data", true);\r\n\t\tmap.put("node.client", false);\r\n\t\tmap.put("node.gateway.type", "none");\r\n\t\tmap.put("node.http.enabled", true);\r\n\t\tmap.put("path.data", "target/elasticsearch");\r\n\t\tmap.put("index.number_of_shards", 1);\r\n\t\tmap.put("index.number_of_replicas", 0);\r\n\t\tmap.put("index.store.type", "memory");\r\n\r\nand here is how the client node configured:\r\n\t\tmap.put("cluster.name", clusterName);\r\n\t\tmap.put("node.name", DEFAULT_NODE_NAME);\r\n\t\tmap.put("node.master", false);\r\n\t\tmap.put("node.data", false);\r\n\t\tmap.put("node.client", true);\r\n\t\tmap.put("discovery.zen.ping.multicast.enabled", false);\r\n\t\tmap.put("discovery.zen.ping.unicast.hosts", "127.0.0.1");\r\n\r\ni appreciate your help !\r\n\r\n\r\nTests run: 49, Failures: 0, Errors: 24, Skipped: 0, Time elapsed: 6.648 sec <<< FAILURE!\r\nall(de.guj.searchengine.domain.search.ElasticSearchQueryIntegrationTest)  Time elapsed: 0.126 sec  <<< ERROR!\r\norg.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized];[SERVICE_UNAVAILABLE/2/no master];\r\n        at org.elasticsearch.cluster.block.ClusterBlocks.globalBlockedException(ClusterBlocks.java:138)\r\n        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:103)\r\n        at org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.checkGlobalBlock(TransportIndicesStatusAction.java:59)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction.<init>(TransportBroadcastOperationAction.java:136)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:73)\r\n        at org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction.doExecute(TransportBroadcastOperationAction.java:44)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:47)\r\n        at org.elasticsearch.client.node.NodeIndicesAdminClient.execute(NodeIndicesAdminClient.java:64)\r\n        at org.elasticsearch.client.support.AbstractIndicesAdminClient.status(AbstractIndicesAdminClient.java:333)\r\n'
2668,'dadoonet','List of existing plugins with Node Info API\nWe want to display information about loaded plugins in Node Info API using plugin option:\r\n\r\n```sh\r\ncurl http://localhost:9200/_nodes?plugin=true\r\n```\r\n\r\nFor example, on a 4 nodes cluster, it could provide the following output:\r\n\r\n```javascript\r\n{\r\n  "ok" : true,\r\n  "cluster_name" : "test-cluster-MacBook-Air-de-David.local",\r\n  "nodes" : {\r\n    "lodYfbFTRnmwE6rjWGGyQQ" : {\r\n      "name" : "node1",\r\n      "transport_address" : "inet[/172.18.58.139:9300]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/172.18.58.139:9200]",\r\n      "plugins" : [ ]\r\n    },\r\n    "hJLXmY_NTrCytiIMbX4_1g" : {\r\n      "name" : "node4",\r\n      "transport_address" : "inet[/172.18.58.139:9303]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/172.18.58.139:9203]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "description" : "test-plugin description",\r\n        "site" : true,\r\n        "jvm" : false\r\n      }, {\r\n        "name" : "test-no-version-plugin",\r\n        "description" : "test-no-version-plugin description",\r\n        "site" : true,\r\n        "jvm" : false\r\n      }, {\r\n        "name" : "dummy",\r\n        "description" : "No description found for dummy.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : false,\r\n        "jvm" : true\r\n      } ]\r\n    },\r\n    "bnoySsBfTrSzbDRZ0BFHvg" : {\r\n      "name" : "node2",\r\n      "transport_address" : "inet[/172.18.58.139:9301]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/172.18.58.139:9201]",\r\n      "plugins" : [ {\r\n        "name" : "dummy",\r\n        "description" : "This is a description for a dummy test site plugin.",\r\n        "url" : "/_plugin/dummy/",\r\n        "site" : false,\r\n        "jvm" : true\r\n      } ]\r\n    },\r\n    "0Vwil01LSfK9YgRrMce3Ug" : {\r\n      "name" : "node3",\r\n      "transport_address" : "inet[/172.18.58.139:9302]",\r\n      "hostname" : "MacBook-Air-de-David.local",\r\n      "version" : "0.90.0.Beta2-SNAPSHOT",\r\n      "http_address" : "inet[/172.18.58.139:9202]",\r\n      "plugins" : [ {\r\n        "name" : "test-plugin",\r\n        "description" : "test-plugin description",\r\n        "site" : true,\r\n        "jvm" : false\r\n      } ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nInformation are cached for 10 seconds by default. Modify `plugins.info_refresh_interval` property if needed.\r\nSetting `plugins.info_refresh_interval` to `-1` will cause infinite caching.\r\nSetting `plugins.info_refresh_interval` to `0` will disable caching.\r\n '
2665,'spinscale','HTTP Pipelining causes responses to mixed up.\nElasticSearch seems to advertise HTTP pipelining support by using HTTP 1/1 and not supplying a Connection-header in the response, but fails to deliver on the promises of responding to the requests in the same order they are sent, which means that clients might get the response from an unexpected request.\r\n\r\nExample reproduction:\r\n\r\nIt sometimes works es expected\r\n\r\n    $ printf "GET /_nodes HTTP/1.1\\r\\n\\r\\nGET / HTTP/1.1\\r\\n\\r\\n" | nc -i 1 127.0.0.1 9200\r\n    HTTP/1.1 200 OK\r\n    Content-Type: application/json; charset=UTF-8\r\n    Content-Length: 222\r\n    \r\n    {"ok":true,"cluster_name":"elasticsearch","nodes":{"MVf7UrJJRyaOJj35MAdODg":{"name":"Caiera","transport_address":"inet[/10.0.0.6:9300]    ","hostname":"machine.local","version":"0.20.4","http_address":"inet[/10.0.0.6:9200]"}}}HTTP/1.1 200 OK\r\n    Content-Type: application/json; charset=UTF-8\r\n    Content-Length: 169\r\n    \r\n    {\r\n      "ok" : true,\r\n      "status" : 200,\r\n      "name" : "Caiera",\r\n      "version" : {\r\n        "number" : "0.20.4",\r\n        "snapshot_build" : false\r\n      },\r\n      "tagline" : "You Know, for Search"\r\n    }\r\n\r\nBut sometimes, given the exact same request, changes the order of the responses:\r\n    \r\n    $ printf "GET /_nodes HTTP/1.1\\r\\n\\r\\nGET / HTTP/1.1\\r\\n\\r\\n" | nc -i 1 127.0.0.1 9200\r\n    HTTP/1.1 200 OK\r\n    Content-Type: application/json; charset=UTF-8\r\n    Content-Length: 169\r\n    \r\n    {\r\n      "ok" : true,\r\n      "status" : 200,\r\n      "name" : "Caiera",\r\n      "version" : {\r\n        "number" : "0.20.4",\r\n        "snapshot_build" : false\r\n      },\r\n      "tagline" : "You Know, for Search"\r\n    }HTTP/1.1 200 OK\r\n    Content-Type: application/json; charset=UTF-8\r\n    Content-Length: 222\r\n    \r\n    {"ok":true,"cluster_name":"elasticsearch","nodes":{"MVf7UrJJRyaOJj35MAdODg":{"name":"Caiera","transport_address":"inet[/10.0.0.6:9300]    ","hostname":"machine.local","version":"0.20.4","http_address":"inet[/10.0.0.6:9200]"}}}    '
2664,'dadoonet','Display list of all available site plugins on /_plugin/ end point\nWhen you need to know all available site plugins, you can send:\r\n\r\n\r\n```sh\r\n$ curl localhost:9200/_plugin/\r\n```\r\n\r\nAnd get the following result:\r\n\r\n```javascript\r\n{\r\n  "sites" : [ {\r\n    "name" : "anotherplugin",\r\n    "url" : "/_plugin/anotherplugin/"\r\n  }, {\r\n    "name" : "dummy",\r\n    "url" : "/_plugin/dummy/"\r\n  } ]\r\n}\r\n```'
2660,'s1monw','Expose CJKBigram and Width TokenFilters in ElasticSearch\nCJKBigram especially can be used together with ICU / StandartTokenizer to for Bigrams for CJK lanugages only and leave other scripts untouched. We should expose those filters directly.'
2657,'dadoonet','Refactoring accessors using only getters and setters\nWe want to clean up the code and use only *standard* accessors (gettters and setters).\r\nSo we remove old non standard accessors.\r\n\r\nCurrent request builders to build request with elasticsearch stay the same, and its the recommended way to execute APIs with elasticsearch. The pure "request" APIs have changed, and now have setters methods instead of the non setters options, code will need to change if pure Request objects/api are used.\r\n\r\nLast, all response levels objects now only expose the getter API variant. We have had both variants (getter and non getter) for some time, and its time to clean it up as it creates both confusion and overhead on our end to maintain it.\r\n'
2654,'dadoonet','Support trailing slashes on plugin _site URLs\nFor common plugins like head and paramedic, I often attempt to visit the plugin with this URL:\r\n\r\nhttp://localhost:9200/_plugin/paramedic\r\n\r\nbut get a blank screen.  The correct URL has a trailing slash like so:\r\n\r\nhttp://localhost:9200/_plugin/paramedic/\r\n\r\nI\'m assuming this is an elasticsearch issue rather than an individual plugin, but can we redirect to the URL with trailing slash to prevent the blank screen "why is $PLUGIN not working?" questions?\r\n\r\nThanks!\r\nAndrew'
2650,'imotov','Add a way to randomly select which data directory to store on when using multiple data directories.\nThe store distributor settings `index.store.distributor` allows specifying how index files should be distributed among multiple store directories. Two types of distributors are supported. The `least_used` distributor always selects the directory with the most available space. The `random` distributor selects directories at random. The probability of selecting a particular directory is proportional to amount of available space in this directory.\r\n\r\n## Original Request\r\n\r\nWhen using multiple data directories all on different hard disks, if a new disk is added later on with 0% capacity used, then all ElasticSearch insertions will using this new disk until it is even with the others. This causes a large I/O load on the single disk and leaves the others untouched. Even worse, other processes that also share that new disk, such as Hadoop processes, end up being I/O blocked whenever they need to access it.\r\n\r\nIf a way to configure ElasticSearch to randomly select a data directory to write new data to was created, then the I/O load could still be distributed across all disks when new disks are added in.\r\n\r\nHadoop does random disk selection until a disk becomes almost full in which case it is excluded from the random selection. I think this is a good strategy but even just random all the time would prevent the new disk I/O problem.'
2640,'s1monw','Add ability to re-score Top-K query results with a secondary query,\n# Rescore Feature\r\n\r\nThe rescore feature allows te rescore a document returned by a query based\r\non a secondary algorithm. Rescoring is commonly used if a scoring algorithm\r\nis too costly to be executed across the entire document set but efficient enough\r\nto be executed on the Top-K documents scored by a faster retrieval method. Rescoring\r\ncan help to improve precision by reordering a larger Top-K window than actually\r\nreturned to the user. Typically is it executed on a window between 100 and 500 documents\r\nwhile the actual result window requested by the user remains the same. \r\n\r\n# Query Rescorer\r\n\r\nThe `query` rescorer executes a secondary query only on the Top-K results of the actual\r\nuser query and rescores the documents based on a linear combination of the user query\'s score\r\nand the score of the `rescore_query`. This allows to execute any exposed query as a\r\n`rescore_query` and supports a `query_weight` as well as a `rescore_query_weight` to weight the\r\nfactors of the linear combination. \r\n\r\n# Rescore API\r\n\r\nThe `rescore` request is defined along side the query part in the json request:\r\n\r\n```json\r\ncurl -s -XPOST \'localhost:9200/_search\' -d {\r\n  "query" : {\r\n    "match" : {\r\n      "field1" : {\r\n        "query" : "the quick brown",\r\n        "type" : "boolean",\r\n        "operator" : "OR"\r\n      }\r\n    }\r\n  },\r\n  "rescore" : {\r\n    "window_size" : 50,\r\n    "query" : {\r\n      "rescore_query" : {\r\n        "match" : {\r\n          "field1" : {\r\n            "query" : "the quick brown",\r\n            "type" : "phrase",\r\n            "slop" : 2\r\n          }\r\n        }\r\n      },\r\n      "query_weight" : 0.7,\r\n      "rescore_query_weight" : 1.2\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nEach `rescore` request is executed on a per-shard basis within the same roundtrip. Currently the rescore API\r\nhas only one implementation (the `query` rescorer) which modifies the result set in-place. Future developments\r\ncould include dedicated rescore results if needed by the implemenation ie. a pair-wise reranker.\r\n*Note:* Only regualr queries are rescored, if the search type is set to `scan` or `count` rescorers are not executed.'
2626,'s1monw','GeoShape intersects filter omits matching docs.\nThere are some cases where a geo_shape "intersects" (and presumably "within", though I did not check) filter fails to find a matching document.\r\n\r\nTo reproduce, use a simple, standard mapping:\r\n\r\n```javascript\r\n    {\r\n        "location": {\r\n            "type": "geo_shape",\r\n            "tree": "quadtree",\r\n            "distance_error_pct": 0.0\r\n        }\r\n    }\r\n```\r\n\r\nIndex a document:\r\n```\r\ncurl -XPUT \'localhost:9200/test/type1/b\' -d \'\r\n{"id": "b",\r\n "location": {\r\n    "type": "Polygon",\r\n    "coordinates": [[[-122.83, 48.57],\r\n                     [-122.77, 48.56],\r\n                     [-122.79, 48.53],\r\n                     [-122.83, 48.57]]]\r\n }\r\n}\r\n```\r\n\r\nThe following query intersects the document and therefore should match it:\r\n```\r\ncurl -XGET \'localhost:9200/test/type1/_search\' -d \'\r\n{\r\n  "query": {\r\n    "constant_score": {\r\n      "boost": 1,\r\n      "filter": {\r\n        "geo_shape": {\r\n          "location": {\r\n            "relation": "intersects",\r\n            "shape": {\r\n              "type": "Polygon",\r\n              "coordinates": [[[-122.88, 48.62],\r\n                               [-122.88, 48.54],\r\n                               [-122.82 ,48.54],\r\n                               [-122.82, 48.62],\r\n                               [-122.88, 48.62]]]\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nHowever, the query returns 0 matches.\r\n\r\nI have tracked the cause of this down to an attempted optimization in `org.elasticsearch.common.lucene.spatial.prefix.tree.SpatialPrefixTree#recursiveGetNodes`.  Specifically, the optimization prevents recursion into the deepest tree level if a parent node in the penultimate level covers all its children.\r\n\r\nThe bug is that this optimization can be invoked at both indexing and query time:  If the optimization is invoked for the indexed doc\'s shape, then a query shape that doesn\'t invoke the optimization will not match.  The converse is also true: if the optimization is invoked for a query shape, it will not match intersecting documents for whose shape the optimization was not invoked.  (I can provide illustrations if this remains unclear.)\r\n\r\nOne possible bugfix to ensure the indexing and query paths do not _both_ invoke the optimization is to simply disable the optimization at indexing time, at the one-time cost of more nodes being generated and stored at indexing time.  Or, the optimization could be disabled at query time, but at a cost of more nodes being generated at query time.'
2624,'s1monw','Analyze API returns in YAML format if analyzed string begins with ---\n    $ curl -XGET \'http://127.0.0.1:9200/_analyze?analyzer=standard\' -d \'this is a test\'\r\n    {"tokens":[{"token":"test","start_offset":10,"end_offset":14,"type":"<ALPHANUM>","position":4}]}\r\n\r\n    $ curl -XGET \'http://127.0.0.1:9200/_analyze?analyzer=standard\' -d \'---this is a test\'\r\n    ---\r\n    tokens:\r\n    - token: "test"\r\n      start_offset: 13\r\n      end_offset: 17\r\n      type: "<ALPHANUM>"\r\n      position: 4\r\n\r\nVersion:\r\n\r\n    $ curl -XGET \'http://127.0.0.1:9200/\'\r\n    {\r\n      "ok" : true,\r\n      "status" : 200,\r\n      "name" : "Multiple Man",\r\n      "version" : {\r\n        "number" : "0.20.4",\r\n        "snapshot_build" : false\r\n      },\r\n      "tagline" : "You Know, for Search"\r\n    }'
2621,'s1monw','Throw a more meaningful message when no document is specified for indexing\nCurrently the a user forgets to send a document when indexing, they get a cryptic error that doesn\'t point to the actual cause:\r\n\r\n```\r\n∴ curl -s -XPUT localhost:9200/test/doc/1 | python -mjson.tool\r\n{\r\n    "error": "ElasticSearchParseException[Failed to derive xcontent from (offset=0, length=0): []]",\r\n    "status": 400\r\n}\r\n```\r\n\r\nThis change makes the error message a bit more readable for someone who runs into this issue:\r\n\r\n```\r\n∴ curl -s -XPUT localhost:9200/test/doc/1 | python -mjson.tool\r\n{\r\n    "error": "MapperParsingException[failed to parse, document is empty]",\r\n    "status": 400\r\n}\r\n```\r\n\r\nThe Exception originates from https://github.com/elasticsearch/elasticsearch/blob/ed09ba0a18ebe720db32b78e5146d4383525d014/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java#L456\r\n\r\nI chose to make this a MapperParsingException to match the Exception that would normally get thrown in the case that a document didn\'t map correctly: https://github.com/elasticsearch/elasticsearch/blob/ed09ba0a18ebe720db32b78e5146d4383525d014/src/main/java/org/elasticsearch/index/mapper/DocumentMapper.java#L540'
2615,'spinscale',"Better FieldDataStats\nFieldDataStats was added in commit e8c1180 , but there's no public getter for it in NodeIndicesStats\r\n\r\nAlso, the only thing that is exposed through the FDS class is the actual memory used by it. It will be extremely useful to have that broken down by field names, so we can actually tell which FieldData is a bottleneck culprit.\r\n\r\nAre there any plans to support that, or would you accept a PR enabling that?"
2613,'dadoonet','When termsFilter is empty, use missingFilter instead\nAs discussed in the mailing list: https://groups.google.com/d/topic/elasticsearch/4Foz1VD0vt8/discussion\r\n\r\nHere is a pull request that perform a `missing` filter when terms array is empty. '
2608,'imotov','NullPointerException\nI\'m using version 0.20.2. When I perform the following query (obviously flawed syntactically):\r\n\r\n```console\r\n$ curl -XGET localhost:9200/foo/bar/_search -d \'{"query": {"terms": {"query": "foo"}}}\'\r\n```\r\n\r\nI get the following null pointer exception:\r\n\r\n    [2013-01-31 21:52:32,071][DEBUG][action.search.type       ] [Ent] [idents][0], node[mU366vK5T42xL01gFekyvw], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@1f2f0ce9]\r\n    org.elasticsearch.search.SearchParseException: [idents][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{"query": {"terms": {"query": "foo"}}}]]\r\n      at org.elasticsearch.search.SearchService.parseSource(SearchService.java:566)\r\n    \tat org.elasticsearch.search.SearchService.createContext(SearchService.java:481)\r\n    \tat org.elasticsearch.search.SearchService.createContext(SearchService.java:466)\r\n    \tat org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:236)\r\n    \tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:141)\r\n    \tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n    \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n    \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\r\n    \tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\r\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n    \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n    \tat java.lang.Thread.run(Thread.java:680)\r\n    Caused by: java.lang.NullPointerException\r\n    \tat org.elasticsearch.index.mapper.MapperService.smartName(MapperService.java:697)\r\n    \tat org.elasticsearch.index.query.QueryParseContext.smartFieldMappers(QueryParseContext.java:264)\r\n    \tat org.elasticsearch.index.query.TermsQueryParser.parse(TermsQueryParser.java:102)\r\n    \tat org.elasticsearch.index.query.QueryParseContext.parseInnerQuery(QueryParseContext.java:188)\r\n    \tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:268)\r\n    \tat org.elasticsearch.index.query.IndexQueryParserService.parse(IndexQueryParserService.java:246)\r\n    \tat org.elasticsearch.search.query.QueryParseElement.parse(QueryParseElement.java:33)\r\n    \tat org.elasticsearch.search.SearchService.parseSource(SearchService.java:554)\r\n    \t... 11 more\r\n'
2596,'s1monw','Support MultiPhrasePrefix (MatchQuery.Type.PHRASE_PREFIX) in Highlighters\nthis is related to this PR (https://github.com/elasticsearch/elasticsearch/pull/1065) closed 2 years ago. It seems this never really worked. But given the popularity of this query we should really fix that'
2589,'spinscale','ArithmeticException during `stats?all`\nSaw this while helping a user tonight.\r\n\r\n        # curl -s $(hostname):9200/_nodes/stats\\?all=1\r\n        {"error":"ArithmeticException[Value cannot fit in an int: -2562047788015]","status":500}'
2585,'martijnvg','Add suggest api\n# Suggest feature\r\nThe suggest feature suggests similar looking terms based on a provided text by using a suggester. At the moment there the only supported suggester is `fuzzy`. The suggest feature is available from version `0.21.0`.\r\n\r\n# Fuzzy suggester\r\nThe `fuzzy` suggester suggests terms based on edit distance. The provided suggest text is analyzed before terms are suggested. The suggested terms are provided per analyzed suggest text token. The `fuzzy` suggester doesn\'t take the query into account that is part of request. \r\n\r\n# Suggest API\r\nThe suggest request part is defined along side the query part as top field in the json request.\r\n\r\n```\r\ncurl -s -XPOST \'localhost:9200/_search\' -d \'{\r\n  "query" : {\r\n    ...\r\n  },\r\n  "suggest" : {\r\n    ...\r\n  }\r\n}\' \r\n```\r\n\r\nSeveral suggestions can be specified per request. Each suggestion is identified with an arbitary name. In the example below two suggestions are requested. Both `my-suggest-1` and `my-suggest-2` suggestions use the `fuzzy` suggester, but have a different `text`. \r\n\r\n```\r\n"suggest" : {\r\n  "my-suggest-1" : {\r\n    "text" : "the amsterdma meetpu",\r\n    "fuzzy" : {\r\n      "field" : "body"\r\n    }\r\n  },\r\n  "my-suggest-2" : {\r\n    "text" : "the rottredam meetpu",\r\n    "fuzzy" : {\r\n      "field" : "title",\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe below suggest response example includes the suggestion response for `my-suggest-1` and `my-suggest-2`. Each suggestion part contains entries. Each entry is effectively a token from the suggest text and contains the suggestion entry text, the original start offset and length in the suggest text and if found an arbitary number of options.\r\n\r\n```\r\n{\r\n  ...\r\n  "suggest": {\r\n    "my-suggest-1": [\r\n      {\r\n        "text" : "amsterdma",\r\n        "offset": 4,\r\n        "length": 9,\r\n        "options": [\r\n           ...\r\n        ]\r\n      },     \r\n      ...       \r\n    ],\r\n    "my-suggest-2" : [\r\n      ... \r\n    ]\r\n  }\r\n  ...\r\n}\r\n```\r\n \r\nEach options array contains a option object that includes the suggested text, its document frequency and score compared to the suggest entry text. The meaning of the score depends on the used suggester. The fuzzy suggester\'s score is based on the edit distance.\r\n\r\n```\r\n"options": [\r\n  {\r\n    "text": "amsterdam",\r\n    "freq": 77,\r\n    "score": 0.8888889\r\n  },\r\n  ...\r\n]  \r\n```\r\n\r\n# Global suggest text\r\n\r\nTo avoid repitition of the suggest text, it is possible to define a global text. In the example below the suggest text is defined globally and applies to the `my-suggest-1` and `my-suggest-2` suggestions.\r\n\r\n```\r\n"suggest" : {\r\n  "text" : "the amsterdma meetpu"\r\n  "my-suggest-1" : {\r\n    "fuzzy" : {\r\n      "field" : "title"\r\n    }\r\n  },\r\n  "my-suggest-2" : {\r\n    "fuzzy" : {\r\n      "field" : "body"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe suggest text can in the above example also be specied as suggestion specific option. The suggest text specified on suggestion level override the suggest text on the global level.\r\n\r\n# Other suggest example.\r\n\r\nIn the below example we request suggestions for the following suggest text: `devloping distibutd saerch engies` on the `title` field with a maximum of 3 suggestions per term inside the suggest text. Note that in this example we use the `count` search type. This isn\'t required, but a nice optimalization. The suggestions are gather in the `query` phase and in the case that we only care about suggestions (so no hits) we don\'t need to execute the `fetch` phase.\r\n\r\n```\r\ncurl -s -XPOST \'localhost:9200/_search?search_type=count\' -d \'{\r\n  "suggest" : {\r\n    "my-title-suggestions-1" : {\r\n      "text" : "devloping distibutd saerch engies",\r\n      "fuzzy" : {\r\n        "size" : 3,\r\n        "field" : "title"  \r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nThe above request could yield the response as stated in the code example below. As you can see if we take the first suggested options of each suggestion entry we get `developing distributed search engines` as result.\r\n\r\n```\r\n{\r\n  ...\r\n  "suggest": {\r\n    "my-title-suggestions-1": [\r\n      {\r\n        "text": "devloping",\r\n        "offset": 0,\r\n        "length": 9,\r\n        "options": [\r\n          {\r\n            "text": "developing",\r\n            "freq": 77,\r\n            "score": 0.8888889\r\n          },\r\n          {\r\n            "text": "deloping",\r\n            "freq": 1,\r\n            "score": 0.875\r\n          },\r\n          {\r\n            "text": "deploying",\r\n            "freq": 2,\r\n            "score": 0.7777778\r\n          }\r\n        ]\r\n      },\r\n      {\r\n        "text": "distibutd",\r\n        "offset": 10,\r\n        "length": 9,\r\n        "options": [\r\n          {\r\n            "text": "distributed",\r\n            "freq": 217,\r\n            "score": 0.7777778\r\n          },\r\n          {\r\n            "text": "disributed",\r\n            "freq": 1,\r\n            "score": 0.7777778\r\n          },\r\n          {\r\n            "text": "distribute",\r\n            "freq": 1,\r\n            "score": 0.7777778\r\n          }\r\n        ]\r\n      },\r\n      {\r\n        "text": "saerch",\r\n        "offset": 20,\r\n        "length": 6,\r\n        "options": [\r\n          {\r\n            "text": "search",\r\n            "freq": 1038,\r\n            "score": 0.8333333\r\n          },\r\n          {\r\n            "text": "smerch",\r\n            "freq": 3,\r\n            "score": 0.8333333\r\n          },\r\n          {\r\n            "text": "serch",\r\n            "freq": 2,\r\n            "score": 0.8\r\n          }\r\n        ]\r\n      },\r\n      {\r\n        "text": "engies",\r\n        "offset": 27,\r\n        "length": 6,\r\n        "options": [\r\n          {\r\n            "text": "engines",\r\n            "freq": 568,\r\n            "score": 0.8333333\r\n          },\r\n          {\r\n            "text": "engles",\r\n            "freq": 3,\r\n            "score": 0.8333333\r\n          },\r\n          {\r\n            "text": "eggies",\r\n            "freq": 1,\r\n            "score": 0.8333333\r\n          }\r\n        ]\r\n      }\r\n    ]\r\n  }\r\n  ...\r\n}\r\n```\r\n\r\n# Common suggest options:\r\n* `text` - The suggest text. The suggest text is a required option that needs to be set globally or per suggestion.\r\n\r\n# Common fuzzy suggest options\r\n* `field` - The field to fetch the candidate suggestions from. This is an required option that either needs to be set globally or per suggestion.\r\n* `analyzer` - The analyzer to analyse the suggest text with. Defaults to the search analyzer of the suggest field.\r\n* `size` - The maximum corrections to be returned per suggest text token.\r\n* `sort` - Defines how suggestions should be sorted per suggest text term. Two possible value:\r\n** `score` - Sort by sore first, then document frequency and then the term itself.\r\n** `frequency` - Sort by document frequency first, then simlarity score and then the term itself.\r\n* `suggest_mode` - The suggest mode controls what suggestions are included or controls for what suggest text terms, suggestions should be suggested. Three possible values can be specified:\r\n** `missing` - Only suggest terms in the suggest text that aren\'t in the index. This is the default.\r\n** `popular` - Only suggest suggestions that occur in more docs then the original suggest text term.\r\n** `always` - Suggest any matching suggestions based on terms in the suggest text.\r\n\r\n# Other fuzzy suggest options:\r\n* `lowercase_terms` - Lower cases the suggest text terms after text analyzation.\r\n* `max_edits` - The maximum edit distance candidate suggestions can have in order to be considered as a suggestion. Can only be a value between 1 and 2. Any other value result in an bad request error being thrown. Defaults to 2.\r\n* `min_prefix` - The number of minimal prefix characters that must match in order be a candidate suggestions. Defaults to 1. Increasing this number improves spellcheck performance. Usually misspellings don\'t occur in the beginning of terms.\r\n* `min_query_length` -  The minimum length a suggest text term must have in order to be included. Defaults to 4.\r\n* `shard_size` - Sets the maximum number of suggestions to be retrieved from each individual shard. During the reduce phase only the top N suggestions are returned based on the `size` option. Defaults to the `size` option. Setting this to a value higher than the `size` can be useful in order to get a more accurate document frequency for spelling corrections at the cost of performance. Due to the fact that terms are partitioned amongst shards, the shard level document frequencies of spelling corrections may not be precise. Increasing this will make these document frequencies more precise.\r\n* `max_inspections` - A factor that is used to multiply with the `shards_size` in order to inspect more candidate spell corrections on the shard level. Can improve accuracy at the cost of performance. Defaults to 5.\r\n* `threshold_frequency` - The minimal threshold in number of documents a suggestion should appear in. This can be specified as an absolute number or as a relative percentage of number of documents. This can improve quality by only suggesting high frequency terms. Defaults to 0f and is not enabled. If a value higher than 1 is specified then the number cannot be fractional. The shard level document frequencies are used for this option.\r\n* `max_query_frequency` - The maximum threshold in number of documents a sugges text token can exist in order to be included. Can be a relative percentage number (e.g 0.4) or an absolute number to represent document frequencies. If an value higher than 1 is specified then fractional can not be specified. Defaults to 0.01f. This can be used to exclude high frequency terms from being spellchecked. High frequency terms are usually spelled correctly on top of this this also improves the spellcheck performance.  The shard level document frequencies are used for this option.'
2583,'s1monw','Expose CommonTermsQuery in ES\nLucene 4.1 has a handy CommonTermsQuery. We should expose it'
2581,'s1monw','Reuse MemoryIndex across requests\nWith Lucene 4.1 memory index can reuse its internal memory structures. We should take advantage of this and reuse MemoryIndex in Percolator requests'
2579,'s1monw','A forgotten Lucene 4.1 version reference\n'
2576,'s1monw','Upgrade to Lucene 4.1\nLucene 4.1 has been release... we should upgrade'
2573,'dadoonet','Add support for externalValues in GeoMapper\nHere is the support of externalValue(Point) for GeoPoint Field Mapper.\r\nFix #2553'
2569,'uboness','Allow facet "queries" to run against unmapped fields\nAn optional parameter on the facet, like sort\'s "ignore_unmapped" parameter would be great.\r\n\r\nContext : We use "dynamic_templates" in our index mapping and allow clients to search, sort, and "facet" on any field that is valid by the rules of the mapping.\r\n\r\nThe problem arises when facets are run on fields that do not exist in the data, and thus are not "actually" in the mapping.\r\n\r\nExample : With a "dynamic_template" for "rating-*" fields, we can search, filter, and sort\r\n   (with "ignore_unmapped" = true), against a "rating-Tuna" field, even though there are no\r\n   documents in the index with that field.\r\n\r\nSee Gist https://gist.github.com/4568689 for a script that will quickly demonstrate the problem.\r\n'
2566,'s1monw','Make lowercase_expanded_terms apply to fuzzy words in query_string\nIn the query_string query, `lowercase_expanded_terms` applies to wildcards, but not to fuzzy terms:\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&explain=true\'  -d \'\r\n    {\r\n       "field" : {\r\n          "t" : {\r\n             "query" : "full text Saerch~2 Wild*",\r\n             "default_operator" : "AND"\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # {\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 1,\r\n    #       "total" : 1\r\n    #    },\r\n    #    "explanations" : [\r\n    #       {\r\n    #          "index" : "test",\r\n    #          "explanation" : "+t:full +t:text +t:Saerch~2 +t:wild*",\r\n    #          "valid" : true\r\n    #       }\r\n    #    ],\r\n    #    "valid" : true\r\n    # }\r\n\r\n\r\nNote: it isn\'t just validate that shows this - it is borne out in tests'
2565,'s1monw','Rename min_similarity to fuzziness\nNow that fuzziness is really an edit_distance of 1 or 2, we should rename `min_similarity` in the fuzzy, flt and flt_field queries, and `fuzzy_min_sim` in the query_string and field queries to `fuzziness`, which is consistent with the `match` query.'
2557,'s1monw','Allow ShardAllocators to be configured via node level settings. \nsince #2555 we have multiple options for ShardAllocators. We should give users the ability to select their implementation as well as one of the defaults impls.'
2555,'s1monw','Add new ShardsAllocator that takes indices into account when allocating and balancing shards\nThe current EvenShardCountAllocator balances shards in a global fashion across all eligible nodes in the cluster. Under certain circumstances this algorithm can overload certain nodes with multiple shards for a single index while still gaining global balance. \r\nThe shard allocation algorithm should be more flexible to take into account more information like indexes or primaries and eventually things like the size of a shard or request patterns of an index. '
2550,'dadoonet','Handles URLs with basic authentication for plugin downloader\nThe plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.\r\n\r\n**Use Case**\r\n\r\nIn the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.\r\n\r\n**Workaround**\r\n\r\nDownload using wget passing auth details and use the elasticsearch plugin command line passing the *file* url instead of *http* url\r\n\r\n**Good to have**\r\n\r\nPlugin command line to have options to take in basic auth credentials and pass it on to the downloader.'
2547,'s1monw','null pointer exception in 19.8. same code path exists in master branch\nUpon restarting an elasticsearch server, I received a "failed to recover commit_point" message from Elasticsearch. The cause was a null pointer being passed into a concurrent hashmap. The null comes from PercolatorExecutor.parseQuery, which seems to always parse a query or raise an exception. If there is no "query" token, however, the parseQuery method appears to return null.\r\n\r\nI patched code in PercolatorService.QueryiesLoaderCollector.collect to check the return value of parseQuery for null before inserting it into the map, and that seemed to fix it.\r\n\r\nI\'m not familiar enough with ES to know if that\'s actually the fix. Maybe parseQuery should raise an exception, or maybe the _percolator index needs better sanitization. But in 19.8, at least, it seems possible to get the _percolator index into a state that makes the data unrecoverable without edits to the ES source.\r\n\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [_percolator][0] failed to recover commit_point [commit-19]/[45]\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:424)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.lang.NullPointerException\r\n        at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)\r\n        at java.util.concurrent.ConcurrentHashMap.putAll(ConcurrentHashMap.java:909)\r\n        at org.elasticsearch.index.percolator.PercolatorExecutor.addQueries(PercolatorExecutor.java:259)\r\n        at org.elasticsearch.index.percolator.PercolatorService.loadQueries(PercolatorService.java:128)\r\n        at org.elasticsearch.index.percolator.PercolatorService.access$700(PercolatorService.java:52)\r\n        at org.elasticsearch.index.percolator.PercolatorService$ShardLifecycleListener.afterIndexShardStarted(PercolatorService.java:220)\r\n        at org.elasticsearch.indices.InternalIndicesLifecycle.afterIndexShardStarted(InternalIndicesLifecycle.java:86)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.start(InternalIndexShard.java:277)\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recoverTranslog(BlobStoreIndexShardGateway.java:435)\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:421)\r\n'
2546,'s1monw','null pointer exception in 19.8. same code path exists in master branch\nUpon restarting an elasticsearch server, I received a "failed to recover commit_point" message from Elasticsearch. The cause was a null pointer being passed into a concurrent hashmap. The null comes from PercolatorExecutor.parseQuery, which seems to always parse a query or raise an exception. If there is no "query" token, however, the parseQuery method appears to return null.\r\n\r\nI patched code in PercolatorService.QueryiesLoaderCollector.collect to check the return value of parseQuery for null before inserting it into the map, and that seemed to fix it. Here\'s the pull request: \r\n\r\nhttps://github.com/elasticsearch/elasticsearch/pull/2547\r\n\r\nI\'m not familiar enough with ES to know if that\'s actually the fix. Maybe parseQuery should raise an exception, or maybe the _percolator index needs better sanitization. But in 19.8, at least, it seems possible to get the _percolator index into a state that makes the data unrecoverable without edits to the ES source.\r\n\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [_percolator][0] failed to recover commit_point [commit-19]/[45]\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:424)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:177)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n        at java.lang.Thread.run(Thread.java:662)\r\nCaused by: java.lang.NullPointerException\r\n        at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)\r\n        at java.util.concurrent.ConcurrentHashMap.putAll(ConcurrentHashMap.java:909)\r\n        at org.elasticsearch.index.percolator.PercolatorExecutor.addQueries(PercolatorExecutor.java:259)\r\n        at org.elasticsearch.index.percolator.PercolatorService.loadQueries(PercolatorService.java:128)\r\n        at org.elasticsearch.index.percolator.PercolatorService.access$700(PercolatorService.java:52)\r\n        at org.elasticsearch.index.percolator.PercolatorService$ShardLifecycleListener.afterIndexShardStarted(PercolatorService.java:220)\r\n        at org.elasticsearch.indices.InternalIndicesLifecycle.afterIndexShardStarted(InternalIndicesLifecycle.java:86)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.start(InternalIndexShard.java:277)\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recoverTranslog(BlobStoreIndexShardGateway.java:435)\r\n        at org.elasticsearch.index.gateway.blobstore.BlobStoreIndexShardGateway.recover(BlobStoreIndexShardGateway.java:421)\r\n'
2542,'martijnvg','Issue with boost settings\nHi, \r\nI think this is an issue. I posted in the user group but got no response. \r\nWhen the following query is created on an Index with lot of fields...\r\n\r\n    "bool" : {\r\n        "must" : {\r\n          "bool" : {\r\n            "should" : [ {\r\n              "bool" : {\r\n                "must" : {\r\n                  "query_string" : {\r\n                    "query" : "\\"microsoft\\"",\r\n                    "fields" : [ "nameTokens" ],\r\n                    "analyzer" : "analyzerWithoutStop"\r\n                  }\r\n                },\r\n                "boost" : 5.0\r\n              }\r\n            }, {\r\n              "bool" : {\r\n                "should" : {\r\n                  "query_string" : {\r\n                    "query" : "\\"insideview.com\\"",\r\n                    "fields" : [ "companyDomain" ],\r\n                    "analyzer" : "analyzerWithoutStop"\r\n                  }\r\n                },\r\n                "boost" : 10.0\r\n              }\r\n            } ]\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nOn the "nameTokens" filed the boost parameter keeps increasing with the number of queries. Thats is\r\n1. When the query is fired for the first time. The explanation i got for the two results, one matched the "nameToken" clause and the other matched the "companyDomain" clause:\r\n\r\n1st result\r\n\r\n    3.1991732 = (MATCH) product of:\r\n      6.3983464 = (MATCH) sum of:\r\n        6.3983464 = (MATCH) weight(companyDomain:insideview.com^10.0 in 223), product of:\r\n          0.93694496 = queryWeight(companyDomain:insideview.com^10.0), product of:\r\n            10.0 = boost\r\n            6.8289456 = idf(docFreq=2, maxDocs=1020)\r\n            0.013720199 = queryNorm\r\n          6.8289456 = (MATCH) fieldWeight(companyDomain:insideview.com in 223), product of:\r\n            1.0 = tf(termFreq(companyDomain:insideview.com)=1)\r\n            6.8289456 = idf(docFreq=2, maxDocs=1020)\r\n            1.0 = fieldNorm(field=companyDomain, doc=223)\r\n      0.5 = coord(1/2)\r\n\r\n2nd result\r\n\r\n    0.89017844 = (MATCH) product of:\r\n      1.7803569 = (MATCH) sum of:\r\n        1.7803569 = (MATCH) weight(nameTokens:microsoft^5.0 in 209), product of:\r\n          0.3494771 = queryWeight(nameTokens:microsoft^5.0), product of:\r\n            5.0 = boost\r\n            5.0943446 = idf(docFreq=16, maxDocs=1020)\r\n            0.013720199 = queryNorm\r\n          5.0943446 = (MATCH) fieldWeight(nameTokens:microsoft in 209), product of:\r\n            1.0 = tf(termFreq(nameTokens:microsoft)=1)\r\n            5.0943446 = idf(docFreq=16, maxDocs=1020)\r\n            1.0 = fieldNorm(field=nameTokens, doc=209)\r\n      0.5 = coord(1/2)\r\n\r\n\r\n2nd time the same search result is fired:\r\n\r\n    2.5471714 = (MATCH) product of:\r\n      5.0943427 = (MATCH) sum of:\r\n        5.0943427 = (MATCH) weight(nameTokens:microsoft^15625.0 in 209), product of:\r\n          0.99999964 = queryWeight(nameTokens:microsoft^15625.0), product of:\r\n            15625.0 = boost\r\n            5.0943446 = idf(docFreq=16, maxDocs=1020)\r\n            1.2562947E-5 = queryNorm\r\n          5.0943446 = (MATCH) fieldWeight(nameTokens:microsoft in 209), product of:\r\n            1.0 = tf(termFreq(nameTokens:microsoft)=1)\r\n            5.0943446 = idf(docFreq=16, maxDocs=1020)\r\n            1.0 = fieldNorm(field=nameTokens, doc=209)\r\n      0.5 = coord(1/2)\r\n\r\n2nd result\r\n\r\n     0.0029293338 = (MATCH) product of:\r\n      0.0058586677 = (MATCH) sum of:\r\n        0.0058586677 = (MATCH) weight(companyDomain:insideview.com^10.0 in 223), product of:\r\n          8.5791684E-4 = queryWeight(companyDomain:insideview.com^10.0), product of:\r\n            10.0 = boost\r\n            6.8289456 = idf(docFreq=2, maxDocs=1020)\r\n            1.2562947E-5 = queryNorm\r\n          6.8289456 = (MATCH) fieldWeight(companyDomain:insideview.com in 223), product of:\r\n            1.0 = tf(termFreq(companyDomain:insideview.com)=1)\r\n            6.8289456 = idf(docFreq=2, maxDocs=1020)\r\n            1.0 = fieldNorm(field=companyDomain, doc=223)\r\n      0.5 = coord(1/2)\r\n\r\n\r\nIf you look at the boost parameter in the query explanation, it is increasing with every query of one clause. \r\n\r\n\r\n'
2538,'spinscale','Disable startup by default after package installation\nHi there,\r\n\r\nHappy new year first.\r\n\r\nThis patch add envar used by init.d script to prevent from starting by default.\r\nOften before you have to work on config files to fit your needs (manually or with provisioning tool like chef or puppet) and then only start the service.\r\nThis can be used also for tier programs that depends on ES libs but does not need for the service running on the same host.\r\n\r\nCheers,\r\n'
2534,'dadoonet','Support for REST get ALL templates. Fix #2532\nHeya,\r\n\r\nHere is a simple way to support a GET /_template to show all templates:\r\n\r\n`curl localhost:9200/_template/`\r\n\r\nwill return all existing templates'
2532,'dadoonet','enable GET /_template to show all templates\n/_template shows:\r\nNo handler found for uri [/_template] and method [GET]\r\n\r\nIt would make sense to list the templates as they are listed in the /_cluster/state call.\r\n\r\nSee also:\r\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wnGOnT-JTQo'
2531,'martijnvg','Percolator responses become inconsistent with cluster after forceful reboot of a node\nWe have seen this issue in a production environment (Running Elastic Search 0.19.12) on more than one occasion - the most recent occasion being after one of the servers in the cluster was unexpectedly rebooted by Amazon.\r\n\r\nWe have been able to reproduce this locally using multiple Vagrant boxes - although not every time (2/3 attempts will reproduce this)\r\n\r\nThis bug seems very similar / the same as https://groups.google.com/forum/?fromgroups=#!searchin/elasticsearch/percolator/elasticsearch/VZjslNupBNY/Q3VFfXDqt6sJ\r\n\r\n## Steps to reproduce:\r\n\r\n- Have a cluster of Elastic Search servers\r\n- Index some Percolator queries\r\n- Test that each node returns the same response to a percolation request\r\n- Forcefully reboot one of the servers (e.g. sudo reboot -f)\r\n- Wait until the rebooted node rejoins the cluster\r\n- Test each node returns the same response\r\n\r\n## Expected Result:\r\n\r\n- Every node returns the same (correct) response\r\n\r\n## Actual Result:\r\n\r\n- The restarted node returns no matched percolation queries, despite the cluster being in the healthy (green) state and the expected number of docs showing in the _percolator index.\r\n- Sometimes the correct result is returned by the restarted node for a short time, and _then_, after a few seconds, starts returning no matches.\r\n- Restarting Elastic Search on the node seems to fix this.\r\n- Nothing unusual appears in the log files.'
2522,'spinscale','JAVA API: Get API GetField.getValue() returns inconsistent result type for binary fields\nAfter a Get request, Java API GetField.getValue() of a binary field may return different type of value depending on whether fetching the document was done from the transaction log or from the index.\r\n\r\nWhen fetching from the index, the result is a very convenient byte[], while when fetching from the transaction log, getValue() returns the base64 encoded representation as a String.\r\n\r\nBoth ways should return a byte[].\r\n\r\nThis issue was already opened and fixed (see #1476), but the code was removed in 0.20 causing a regression. The provided fix caused dates to be re-encoded to longs, leaving the inconsistency with binary fields.'
2515,'spinscale','"The package is of bad quality"\nOpening the .deb provided for elasticsearch 0.20.2 on ubuntu 12.10 gives me the following message:\r\n\r\nThe package is of bad quality\r\n\r\nThe installation of a package which violates the quality standards isn\'t allowed. This could cause serious problems on your computer. Please contact the person or organisation who provided this package file and include the details beneath.\r\n\r\n    Lintian check results for /home/francois/Downloads/elasticsearch-0.20.2.deb:\r\n    BFD: usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so: warning: sh_link not set for section `.IA_64.unwind\'\r\n    E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so\r\n    E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so\r\n    E: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so\r\n    E: elasticsearch: control-file-has-bad-permissions conffiles 0755 != 0644\r\n'
2510,'spinscale','NullPointerException on get with fresh index\n# Environment\r\n\r\nElasticSearch Server Version: 0.19.11\r\nElasticSearch Configuration: \r\n```cluster.name: sstarkey\r\n   index.number_of_shards: 1\r\n   index.number_of_replicas: 0\r\n   gateway.type: none```\r\n\r\nUsing Node Client in Java API\r\n\r\n# Steps\r\n\r\n 1. Create a new index on a previously running ElasticSearch node\r\n 2. Do something unrelated to the index that takes the "right" amount of time\r\n 3. Try to get something that doesn\'t exist\r\n\r\n# Expected\r\n\r\nA SearchResponse where isExists() returns false\r\n\r\n# Actual\r\n\r\n```Caused by: java.lang.NullPointerException\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.perform(TransportShardSingleOperationAction.java:150)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction$AsyncSingleAction.start(TransportShardSingleOperationAction.java:125)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:72)\r\n        at org.elasticsearch.action.support.single.shard.TransportShardSingleOperationAction.doExecute(TransportShardSingleOperationAction.java:47)\r\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:61)\r\n        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:90)\r\n        at org.elasticsearch.client.support.AbstractClient.get(AbstractClient.java:175)\r\n        at org.elasticsearch.action.get.GetRequestBuilder.doExecute(GetRequestBuilder.java:135)\r\n        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)\r\n        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)\r\n        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)\r\n        ... 58 more```\r\n\r\n# Unfortunately...\r\n\r\nThis exception only occurs at the integration test layer (i.e. when executing REST calls against our application which wraps ElasticSearch as opposed to when running unit tests directly against the library which integrates with ElasticSearch), and only intermittently.  We have not been able to figure out how to write a standalone unit test which reproduces this problem consistently.\r\n\r\n# Thanks!\r\n\r\nHelp is appreciated, as we\'d really rather not catch this exception in our get service, and it smells of a kind of weird race condition.'
2505,'s1monw','custom_filters_score causes UnsupportedOperationException \nHi,\r\n\r\nI am very new to ES. I am struggling with a strange issue with ES.\r\ncustom_filters_score causing UnsupportedOperationException when fuzzy query used for some partial entries.\r\n\r\nMy index contains the term "express". The following query fails to execute when I search "exp" and "expr". Surprisingly other variations(e,ex,expre,expres,express) works properly.\r\n\r\nthanks.\r\n\r\n{\r\n    "query": {\r\n        "custom_filters_score": {\r\n            "query": {\r\n                "fuzzy": {\r\n                    "_all": {\r\n                        "term": "exp",\r\n                        "boost": 1,\r\n                        "min_similarity": 0.5,\r\n                        "prefix_length": 0,\r\n                        "rewrite": "constant_score_auto"\r\n                    }\r\n                }\r\n            },\r\n            "boost": 1,\r\n            "filters": [{\r\n                "filter": {\r\n                    "ids": {\r\n                        "type": "productModel",\r\n                        "values": ["1234"]\r\n                    }\r\n                },\r\n                "boost": 3\r\n            }]\r\n        }\r\n    }\r\n}\r\n\r\n\r\n[2012-12-26 22:04:16,854][DEBUG][action.search.type       ] [node1] [kang1_11][0], node[vmCjUzdBTI-SxvwU_fUEvg], [P], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@18515bf]\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [kang1_11][0]: query[filtered(custom score (_all:exp~1, functions: [{filter(_uid:productModel#1234), function [boost[3.0]]}]))->cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@953b6c6c)],from[0],size[10]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:183)\r\n\tat org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:316)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteFetch(SearchServiceTransportAction.java:243)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryAndFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryAndFetchAction.java:75)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:205)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:192)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:178)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:619)\r\nCaused by: java.lang.UnsupportedOperationException\r\n\tat org.apache.lucene.search.BooleanScorer.nextDoc(BooleanScorer.java:316)\r\n\tat org.elasticsearch.common.lucene.search.function.FiltersFunctionScoreQuery$CustomBoostFactorScorer.nextDoc(FiltersFunctionScoreQuery.java:283)\r\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:61)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:588)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:199)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:465)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:421)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:264)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:164)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:252)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:179)'
2492,'spinscale','Tiny request: Create Java static variables (or enums) for missing sort values "_last" and "_first"\nI like nice clean code without hardcoding arbitrary values that might change. The ElasticSearch codebase is excellent and makes good use of Java static variables. Do not want to nitpick, but I would love to see something like:\r\n\r\npublic class FieldSortBuilder extends SortBuilder {\r\n    public static final String LAST = "_last"; \r\n    public static final String FIRST = "_first"; \r\n\r\nThis way I can use the statics in my client-side code. On the server side, these Strings are used in places such as FloatFieldDataType and IntFieldDataType.'
2488,'kimchy',"minimum_master_nodes does not prevent split-brain if splits are intersecting\nG'day,\r\n\r\nI'm using ElasticSearch 0.19.11 with the unicast Zen discovery protocol.\r\n\r\nWith this setup, I can easily split a 3-node cluster into two 'hemispheres' (continuing with the brain metaphor) with one node acting as a participant in both hemispheres.  I believe this to be a significant problem, because now `minimum_master_nodes` is incapable of preventing certain split-brain scenarios.\r\n\r\nHere's what my 3-node test cluster looked like before I broke it:\r\n\r\n![](https://saj.beta.anchortrove.com/es-splitbrain-1.png)\r\n\r\nHere's what the cluster looked like after simulating a communications failure between nodes (2) and (3):\r\n\r\n![](https://saj.beta.anchortrove.com/es-splitbrain-2.png)\r\n\r\nHere's what seems to have happened immediately after the split:\r\n\r\n 1. Node (2) and (3) lose contact with one another.  (`zen-disco-node_failed` ... `reason failed to ping`)\r\n 1. Node (2), still master of the left hemisphere, notes the disappearance of node (3) and broadcasts an advisory message to all of its followers.  Node (1) takes note of the advisory.\r\n 1. Node (3) has now lost contact with its old master and decides to hold an election.  It declares itself winner of the election.  On declaring itself, it assumes master role of the right hemisphere, then broadcasts an advisory message to all of its followers.  Node (1) takes note of this advisory, too.\r\n\r\nAt this point, I can't say I know what to expect to find on node (1).  If I query both masters for a list of nodes, I see node (1) in both clusters.\r\n\r\nLet's look at `minimum_master_nodes` as it applies to this test cluster.  Assume I had set `minimum_master_nodes` to 2.  Had node (3) been completely isolated from nodes (1) and (2), I would not have run into this problem.  The left hemisphere would have enough nodes to satisfy the constraint; the right hemisphere would not.  This would continue to work for larger clusters (with an appropriately larger value for `minimum_master_nodes`).\r\n\r\nThe problem with `minimum_master_nodes` is that it does not work when the split brains are intersecting, as in my example above.  Even on a larger cluster of, say, 7 nodes with `minimum_master_nodes` set to 4, all that needs to happen is for the 'right' two nodes to lose contact with one another (a master election has to take place) for the cluster to split.\r\n\r\nIs there anything that can be done to detect the intersecting split on node (1)?\r\n\r\nWould #1057 help?\r\n\r\nAm I missing something obvious? :)"
2483,'dakrone',"Use Explain in AllocationDecider's Decisions\nCurrently the Decision class already supports an explain parameter. It would be helpful for development, debugging and for certain error messages like in MoveAllocationCommand to log the actual reasoning behind the decision."
2480,'s1monw',"Inconsistency\nHi,\r\n\r\nI get sometimes some inconsistency when I delete and reload my entries right after.\r\n\r\nIt looks like elasticsearch delete the index asynchronously, and i'm reloading the indexes before the index is actually removed.\r\n\r\nHow can I avoid this behavior ?\r\n\r\nThanks in advance\r\n"
2478,'s1monw','TermsQueryBuilder ambiguous constructor\nOne can not use TermsQueryBuilder("test", 1, 2) because of float and double versions variable argument constructors make them ambiguous in nature.\r\n\r\nI would suggest to use array or List instead of variable arguments in constructor.\r\n\r\n'
2475,'javanna',"recursive path match on dynamic templates\nWe have a custom field where users can enter any data.  Within our database we store all keys/values of this custom dict as strings.  However, it's not possible to specify a dynamic_template which supports recursive/nested mappings.  This is a problem because custom.A may be a string for one customer, but a number for another customer.\r\n\r\nPossibly add path_match_nested?\r\n\r\n```\r\n        # Index all custom fields as strings for now.                                                                                                              \r\n        'dynamic_templates': [\r\n            {   \r\n                'custom_as_string': {                                                                                                                              \r\n                    'path_match': 'custom.*',  \r\n                    'path_match_nested': true,                                                                                                                    \r\n                    'mapping': {                                                                                                                                   \r\n                        'type': 'string',                                                                                                                          \r\n                    }\r\n                }\r\n            }\r\n        ], \r\n```"
2473,'dadoonet','[Feature Request] Add century interval in Date Histogram facet\nHeya\r\n\r\nI would like to have a century interval option in the Date Histogram Facet.\r\n\r\n```javascript\r\n{\r\n    "query" : {\r\n        "match_all" : {}\r\n    },\r\n    "facets" : {\r\n        "histo" : {\r\n            "date_histogram" : {\r\n                "field" : "field_name",\r\n                "interval" : "century"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n'
2467,'karmi','In which cases I should wait for yellow/green status?\nHello.\n\nWe\'re using elasticsearch in our integration tests (We are using "tire" gem for Rails.)\nTypical workflow is:\n\n(I)\n\n1) delete index\n2) add items to index\n3) immediately search in that index\n\nOR (II)\n\n1) delete index\n2) add items\n3) change or remove items\n4) immediately search.\n\nAs I told we use it in unit tests. So, search step executed immediately after updating index.. And we need search to return always same, correct result.\n\nWe are not getting correct results now. Sometimes there are 2 items in index, while we expected one. Sometimes there are errors like HTTP 500 No active shards. Failures are random.\n\nI think you mention here\nhttps://groups.google.com/forum/?fromgroups=#!topic/elasticsearch/wPKUJXiG2mw\nthat wait for yellow/green status should be used in this case.\n\nalso another link\nhttp://grokbase.com/t/gg/elasticsearch/12ag6njpyz/no-active-shards-in-tests-using-the-java-api\n\nIs there any documentation which will explain usage of health request in our workflow?\n'
2463,'javanna','Added options to DynamicTemplate\nDynamic templates are a great feature because they make it possible to control the dynamic mapping capabilities of ElasticSearch. One of the issues with dynamic templates is that it is necessary to have several templates to handle even very small differences in mapping objectives. \nFor example, four templates are needed to handle the cases \'analyzed\', \'not_analyzed\',\n\'analyzed+stored\' and \'not_analyzed+stored\'. \n\nThis change overcomes this limitation by adding three new features to dynamic templates\n1. Options that are merged into the mapping based on keys specified in the field name\n2. The ability to capture substrings from the field name and use these to parameterize the mapping and options \n3. The ContentPath is made available to the DynamicTemplate for use in parameterization \n\nConsider the following example:\n\n{\n  "template_1":{\n    "match": "*__*",\n    "capture":"(([a-zA-Z0-9]+?)(?:_[a-zA-Z0-9]+)*)__[a-zA-Z0-9]+", \n    "options" :{\n      "s":{"type":"string"},\n      "l":{"type":"long"},\n      "m":{"index_name":"{1}"},\n      "n":{"index_name":"{path}.{2}"},\n      "i":{"index":"not_analyzed"},\n      "j":{"index":"no"},\n      "t":{"store":true},\n      "a":{"include_in_all":false},\n      "1":{"boost":1.5},\n      "d":{"dynamic":false}\n    },\n    "mapping": {\n      "index_name":"{path}.{1}"\n    }\n  }\n}\n\nThe match, path_match, unmatch, path_unmatch are unchanged.\nTwo new optional fields: \n"capture" : This specifies a regular expression with capture groups that are used to\ncapture substrings from the field name that are made available as paramaeters ({1},{2}, etc)\nfor substitution into option and mapping key/value pairs in the same way that {name} and\n{dynamic_type} are currently.\n\n"options" : A list of key/value pairs where keys are single character identifiers\nand values are objects containing mapping options that are merged into the "mapping".\n\nIf the field name has a double underscore "__" then the DynamicTemplate class treats any characters following it as option identifiers. The following example "person" document and resulting mapping illustrate how this one options enabled dynamic template does what would otherwise require 5 dynamic templates without options. \n\nNotes \n1. The {path} parameter strips out the option specifiers. A {raw_path} is also available.\n2. Fields \'name_first_sn\' and \'name_last_sn1\' both map to the index field \'details.name\'\n\nindex/person/id1\n{\n  "details__d":{\n    "salutation":"Mr",\n    "name_first__sn":"",\n    "name_last__sn1":"",\n    "marital_status__sit":"married",\n    "num_children__l":3\n  }\n}\n\nmapping for type person\n{\n  "person" :{\n    "properties" : {\n      "details__d" : {\n        "type":"object", \n        "dynamic":false,                           // option \'d\'\n        "properties" {\n          "salutation" : {\n\t    "type":"string";                       // Not picked up by match, default mapping\n          },\n          "name_first__sn": {\n            "type":"string",                       // option \'s\' \n            "index_name":"details.name"            // option \'n\'\n          },\n          "name_last__sn1": {\n            "type":"string",                       // option \'s\'\n            "index_name":"details.name",           // option \'n\'\n            "boost":1.5                            // option \'1\'\n          },\n          "marital_status__sit": {\n            "type":"string",                       // option \'s\'\n            "index_name":"details.marital_status", // mapping\n\t    "index":"not_analyzed",                // option \'i\'\n            "store":true                           // option \'t\'\n          },\n          "num_children__l":{\n            "type":"long",                         // option l  \n            "index_name":"details.num_children",   // mapping\n          }\n        }\n      }\n    }\n  }   \n}\n      \n'
2462,'s1monw','Facet value of boolean mapped fields should be "true" or "false"\nWhen performing the following index and search operations I would expect the facet values of boolean fields to be "true" or "false" and not "T" and "F", since "T" and "F" are implementation detail of the BooleanFieldMapper.\n\nTo reproduce (using any version I tested, including current master):\n\n    curl -XPUT http://localhost:9200/boolfacet\n    curl -XPUT http://localhost:9200/boolfacet/auto/1 -d \'{"foo":true}\'\n    curl -XPOST http://localhost:9200/boolfacet/auto/_search?pretty=true -d \'{"query":{"match_all":{}}, "facets":{"bar":{"terms":{"field":"foo"}}}}\' \n```json\n{\n   "took" : 1,\n   "timed_out" : false,\n   "_shards" : {\n     "total" : 5,\n    "successful" : 5,\n     "failed" : 0\n   },\n   "hits" : {\n     "total" : 1,\n     "max_score" : 1.0,\n     "hits" : [ {\n       "_index" : "boolfacet",\n       "_type" : "auto",\n       "_id" : "1",\n       "_score" : 1.0, "_source" : {"foo":true}\n     } ]\n   },\n   "facets" : {\n     "bar" : {\n       "_type" : "terms",\n       "missing" : 0,\n       "total" : 1,\n       "other" : 0,\n       "terms" : [ {\n         "term" : "T",\n         "count" : 1\n       } ]\n     }\n   }\n }\n```\n\nNote the "term": "T" line in the facets.\nA preliminary patch follows shortly.'
2449,'clintongormley','Query DSL: Improved explanation for match_phrase_prefix\nThe explanation provided by `/_validate` for the `match_phrase_prefix` is missing the prefix part - it just shows the same explanation as for `match_phrase` \n\n    curl -XGET \'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&explain=true\'  -d \'\n    {\n       "match_phrase" : {\n          "text" : "one tw"\n       }\n    }\n    \'\n\n    # {\n    #    "_shards" : {\n    #       "failed" : 0,\n    #       "successful" : 1,\n    #       "total" : 1\n    #    },\n    #    "explanations" : [\n    #       {\n    #          "index" : "test",\n    #          "explanation" : "text:\\"one tw\\"",\n    #          "valid" : true\n    #       }\n    #    ],\n    #    "valid" : true\n    # }\n\n\n    curl -XGET \'http://127.0.0.1:9200/test/test/_validate/query?pretty=1&explain=true\'  -d \'\n    {\n       "match_phrase_prefix" : {\n          "text" : "one tw"\n       }\n    }\n    \'\n\n    # {\n    #    "_shards" : {\n    #       "failed" : 0,\n    #       "successful" : 1,\n    #       "total" : 1\n    #    },\n    #    "explanations" : [\n    #       {\n    #          "index" : "test",\n    #          "explanation" : "text:\\"one tw\\"",\n    #          "valid" : true\n    #       }\n    #    ],\n    #    "valid" : true\n    # }\n\n\n'
2446,'spinscale','StringIndexOutOfBoundsException when performing has_child filter (0.20.0RC1)\nWhen performing a search using a `has_child` filter, I\'m running into a StringOutOfBoundsException.\n\n**Parent Mapping "resource":**\n\n```javascript\n{\n  "resource": {\n    "properties": {\n      "access": {\n        "type": "string",\n        "index": "not_analyzed"\n      },\n      "general": {\n        "type": "string"\n      },\n      "resourceType": {\n        "type": "string",\n        "index": "not_analyzed",\n        "store": "yes"\n      },\n      "sort": {\n        "type": "string",\n        "index": "not_analyzed"\n      },\n      "tenantId": {\n        "type": "string",\n        "index": "not_analyzed",\n        "store": "yes"\n      },\n      "thumbnailUrl": {\n        "type": "string",\n        "index": "no",\n        "store": "yes"\n      },\n      "title": {\n        "type": "string",\n        "index": "no",\n        "store": "yes"\n      },\n      "visibility": {\n        "type": "string",\n        "index": "not_analyzed",\n        "store": "yes"\n      }\n    }\n  }\n}\n```\n\n**Child Mapping (resource_memberships):**\n\n```javascript\n{\n  "resource_memberships": {\n    "_parent": {\n      "type": "resource"\n    },\n    "_routing": {\n      "required": true\n    },\n    "properties": {\n      "direct_memberships": {\n        "type": "string",\n        "index": "not_analyzed"\n      }\n    }\n  }\n}\n```\n\n**Query:**\n\n```javascript\n     "query": {\n        "filtered": {\n          "query": {\n            "match": {\n              "general": {\n                "query": "Content",\n                "operator": "and"\n              }\n            }\n          },\n          "filter": {\n            "and": [\n              {\n                "and": [\n                  {\n                    "term": {\n                      "_type": "resource"\n                    }\n                  }\n                ]\n              },\n              {\n                "or": [\n                  {\n                    "has_child": {\n                      "type": "resource_memberships",\n                      "query": {\n                        "terms": {\n                          "direct_memberships": [\n                            "u:gttest:TV2Ek7T1JM"\n                          ]\n                        }\n                      }\n                    }\n                  },\n                  {\n                    "and": [\n                      {\n                        "or": [\n                          {\n                            "term": {\n                              "visibility": "public"\n                            }\n                          },\n                          {\n                            "terms": {\n                              "joinable": [\n                                "yes",\n                                "request"\n                              ]\n                            }\n                          },\n                          {\n                            "and": [\n                              {\n                                "term": {\n                                  "tenantId": "gttest"\n                                }\n                              },\n                              {\n                                "term": {\n                                  "visibility": "loggedin"\n                                }\n                              }\n                            ]\n                          }\n                        ]\n                      }\n                    ]\n                  }\n                ]\n              }\n            ]\n          }\n        }\n      },\n      "size": "1",\n      "sort": {\n        "sort": "asc"\n      },\n      "fields": [\n        "*",\n        "_source.extra"\n      ]\n    }\n```\n\n**Error:**\n\n```\n"error": "SearchPhaseExecutionException[Failed to execute phase [query_fetch], total failure; shardFailures {[W_ljaTkKQSqrhuZ_IK1Ugg][oaetest][0]: QueryPhaseExecutionException[[oaetest][0]: query[filtered(general:content)->++cache(_type:resource) +child_filter[resource_memberships/resource](filtered(direct_memberships:u:gttest:TV2Ek7T1JM)->cache(_type:resource_memberships)) +cache(visibility:public) cache(joinable:request joinable:yes) +cache(tenantId:gttest) +cache(visibility:loggedin)],from[0],size[1],sort[<custom:\\"sort\\": org.elasticsearch.index.field.data.strings.StringFieldDataType$1@698f0a46>]: Query Failed [Failed to refresh id cache for child queries]]; nested: StringIndexOutOfBoundsException[String index out of range: -1]; }]",\n      "status": 500\n    }\n```\n\n**Other possibly useful info:**\n\n* There is another mapping that is also a child of the `resource` document, it\'s mapping name is `resource_members`\n* The `resource` document\'s child may not exist / have not been indexed, at the time of running that query'
2440,'kimchy','Use specified interface address, despite available subinterfaces\nThis fixes issue #2437'
2436,'s1monw','Expose Lucenes KeepWordTokenFilter in ElasticSearch\nLucene 4 comes with a handy KeepWordTokenFilter that is basically an inverse stop filter. ES should expose this filter by default.'
2427,'martijnvg',"Using percolate API for new document classification\nHi,\n\nThis comes out of a conversationat the Barcelona training. (it was suggested that i should comment here with this)\n\nThe percolate API takes queries against an index and when a new document is added, it flags which queries in an index match against it (e.g. a simple boolean yes/no).\n\nWould it be possible to extend percolate so that it could be used as a form of index classification e.g. I have x percolate queries registered in index A and y percolate queries in index B, tell me which index i match the new content on based on percolation.\n\nI'm guessing this is a logical extension of some form of exposing the term frequency (or other) already in each index to classify a new document. The idea would be that you could work out what terms are 'good training classification data' for an index and then use that to figure out where a document should go?\n\nDerry"
2421,'spinscale','@Required annotation has description for @Nullable one\nCopy-paste issue. Would be nice to fix the description for @Required annotation.'
2416,'javanna','Indices filter parsed for indices to which it should not apply\nA used in the forum is trying to apply a nested filter to just one index, while querying more than one index, some of which don\'t have the nested mapping.\n\nThe query is complaining about a missing nested mapping, but on the indices to which the query should not apply.\n\nCreate two indices: `test_1` with a nested mapping, and `test_2` without a nested mapping:\n\n    curl -XPUT \'http://127.0.0.1:9200/test_1/?pretty=1\'  -d \'\n    {\n       "mappings" : {\n          "foo" : {\n             "properties" : {\n                "authors" : {\n                   "type" : "nested",\n                   "properties" : {\n                      "name" : {\n                         "type" : "string"\n                      }\n                   }\n                },\n                "title" : {\n                   "type" : "string"\n                }\n             }\n          }\n       }\n    }\n    \'\n\n    curl -XPUT \'http://127.0.0.1:9200/test_2/?pretty=1\'  -d \'\n    {\n       "mappings" : {\n          "bar" : {\n             "properties" : {\n                "title" : {\n                   "type" : "string"\n                }\n             }\n          }\n       }\n    }\n    \'\n\nIndex some data:\n\n    curl -XPOST \'http://127.0.0.1:9200/test_1/foo?pretty=1\'  -d \'\n    {\n       "author" : {\n          "name" : "john smith"\n       },\n       "title" : "test 1 doc"\n    }\n    \'\n\n    curl -XPOST \'http://127.0.0.1:9200/test_2/bar?pretty=1\'  -d \'\n    {\n       "title" : "test 2 doc"\n    }\n    \'\n\nQuery `test_1` and `test_2` but limit the nested filter to just `test_1`:\n\n    curl -XGET \'http://127.0.0.1:9200/test_1%2Ctest_2/_search?pretty=1\'  -d \'\n    {\n       "query" : {\n          "filtered" : {\n             "filter" : {\n                "indices" : {\n                   "no_match_filter" : "none",\n                   "filter" : {\n                      "nested" : {\n                         "filter" : {\n                            "term" : {\n                               "author.name" : "john"\n                            }\n                         },\n                         "path" : "author"\n                      }\n                   },\n                   "indices" : [\n                      "test_1"\n                   ]\n                }\n             },\n             "query" : {\n                "match" : {\n                   "title" : "test"\n                }\n             }\n          }\n       }\n    }\n    \'\n\nThrows this error:\n\n    SearchPhaseExecutionException[Failed to execute phase [query], total failure; shardFailures {[Yit05d94RgiUwMg9vzMOgw][test_1][1]: SearchParseException[[test_1][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\n       "query" : {\n          "filtered" : {\n             "filter" : {\n                "indices" : {\n                   "no_match_filter" : "none",\n                   "filter" : {\n                      "nested" : {\n                         "filter" : {\n                            "term" : {\n                               "author.name" : "john"\n                            }\n                         },\n                         "path" : "author"\n                      }\n                   },\n                   "indices" : [\n                      "test_1"\n                   ]\n                }\n             },\n             "query" : {\n                "match" : {\n                   "title" : "test"\n                }\n             }\n          }\n       }\n    }\n'
2415,'spinscale',"Parse and validate mappings in index templates when they are put\nCurrently, putting a template will succeed if the JSON is valid even if the mapping couldn't be parsed. An error will be thrown when the index will be created and try to use the mapping, but that may be too late.\n\nAn example for such an error is described in https://github.com/elasticsearch/elasticsearch/issues/2414\n\nThe probable solution is to validate the mapping defined in the template when putting it."
2414,'spinscale','Support built-in named date formats in index mapping\nAt the moment, having this in a mapping throws a parsing error:\n\n"topic_date" : { "type" : "date", "format": "date_optional_time||yyyy-MM-dd\'T\'HH:mm" }'
2407,'spinscale','Retrieve content upto wordsAround  the search keyword\nIs there a way to highlight   only a part of a string field .\r\nSuppose I have a field named Content and its very lengthy. \r\nfor example I have \r\nContent  : After a contentious three-week patent trial between Apple and Samsung, jurors awarded Apple $1.05 billion and concluded that Samsung "willfully" infringed several Apple patents. The legal battle was significant for the normally clandestine company. Lawyers managed to get Apple talking in ways it never had, from telling emails between executives to weird and wonderful iPhone prototypes. Here are the juiciest revelations.\r\n\r\nI want to search say "apple " in the content and I set wordsAround =5\r\nthen my query should return  "trial between Apple and Samsung" \r\nI found fragment_size can be set on highlighted fields but I didn\'t understand it fully .\r\nis  fragment_size do the same ??'
2403,'spinscale','Stats in Delete By Query API\nHow to get the stats for  deleted records using Delete By Query API?'
2379,'martijnvg','Percolate query ignores the index name when querystring is specified\nI wrote some code to index/percolate documents, and I have a unit test that creates an index (with a random name) then runs my indexations / search code, then deletes the index. This means that I have percolators with identical IDs in different indexes.\r\n\r\nWhen I percolate my documents, I specify a query string for the percolators :\r\n\r\n    testindex-2b086e73/doc/a889db29-2409-4174-9425-818eb7293b57?percolate=SourceId:1\r\n\r\nAnd then the result contains duplicate percolator IDs, as if the index name was ignored :\r\n\r\n```javascript\r\n{\r\n\t"ok" : true,\r\n\t"_index" : "testindex-17eecc11",\r\n\t"_type" : "doc",\r\n\t"_id" : "824f26e4-76a6-4b66-9cb2-f3adc0103586",\r\n\t"_version" : 1,\r\n\t"matches" : ["1.789", "1.789", "1.789"]\r\n}\r\n```\r\nBut when I don\'t specify a query for the percolators (?percolate=*) the result only contains one ID as expected.\r\n\r\n**Edit:**\r\nI feel like it\'s an intended behavior in (https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/index/percolator/PercolatorExecutor.java#L345) is it? If yes, how do I filter percolators only on one documenttype/index?\r\n\r\n'
2335,'spinscale',"Yellow State: JsonParseException[Numeric value (9223372036854775807) out of range of int] \nI post this here since i isn't an tire problem.\r\nhttps://github.com/karmi/tire/issues/474"
2334,'spinscale','Update src/main/java/org/elasticsearch/search/facet/AbstractFacetBuilder...\nImprove handling of global in AbstractFacetBuilder.global(boolean).\r\n\r\nCurrently argument is ignored and global scope is always enabled.'
2331,'dadoonet','Add ElasticSearch Cloud Azure plugin\nDevelop an ElasticSearch Cloud plugin for Windows Azure to support the discovery of ElasticSearch nodes. Similar reasoning to that of AWS EC2 whereas the discovery of nodes via multicast is not possible on Azure either.'
2321,'spinscale',"System.err.println causes NullPointerException when using ElasticSearch as a java library\nAfter upgrade of Elastic Search java library from version 0.19.4 to 0.19.9 there are random NullPointerExceptions when using System.err.\r\nThey are caused by code in org.elasticsearch.common.compress.snappy.xerial.XerialSnappy.\r\nBest way to repair this would be to patch snappy-java library and remove broken code from XerialSnappy.\r\nIf it's impossible then maybe at least replace System.setErr(null) to System.setErr(nullPrintStream) where nullPrintStream is an instance of PrintStream that does nothing when invoked from Snappy?\r\n"
2320,'spinscale',"Lintian errors on deb package\nWhen I run lintian to check the package I'm getting some errors.\r\nThis is the lintian command: lintian -EvI --color=auto elasticsearch-0.19.10.deb\r\n\r\nAnd these are the errors:\r\nE: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so\r\nE: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so\r\nE: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so\r\nE: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so\r\nE: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so\r\nE: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-ia64-linux.so\r\nE: elasticsearch: arch-independent-package-contains-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so\r\nE: elasticsearch: arch-dependent-file-in-usr-share usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so\r\nE: elasticsearch: unstripped-binary-or-object usr/share/elasticsearch/lib/sigar/libsigar-x86-linux.so\r\nE: elasticsearch: missing-dependency-on-libc needed by usr/share/elasticsearch/lib/sigar/libsigar-amd64-linux.so and 2 others\r\nE: elasticsearch: control-file-has-bad-permissions conffiles 0755 != 0644\r\nE: elasticsearch: no-copyright-file\r\n\r\nCan someone fix that?\r\nThanks."
2316,'javanna','Better handling of missing index failures\nWe see the following during normal operation, when trying to add documents. Auto-create indexes is set to true.\r\n\r\n[2012-10-10 14:00:11,259][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2011-11-21-0000] creating index, cause [auto(index api)], shards [1]/[0], mappings [thread]\r\n[2012-10-10 14:00:11,294][DEBUG][action.admin.indices.stats] [Iron Man 2020] [el-2011-11-21-0000][0], node[LxU0GSjRQtmZM0h8vBxuBg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.stats.IndicesStatsRequest@41201158]\r\norg.elasticsearch.transport.RemoteTransportException: [Crimson Cowl][inet[/192.168.1.11:9301]][indices/stats/s]\r\nCaused by: org.elasticsearch.indices.IndexMissingException: [el-2011-11-21-0000] missing\r\n\tat org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)\r\n\tat org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:144)\r\n\tat org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction.shardOperation(TransportIndicesStatsAction.java:53)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2012-10-10 14:00:11,295][DEBUG][action.admin.indices.status] [Iron Man 2020] [el-2011-11-21-0000][0], node[LxU0GSjRQtmZM0h8vBxuBg], [P], s[INITIALIZING]: Failed to execute [org.elasticsearch.action.admin.indices.status.IndicesStatusRequest@1b3f459d]\r\norg.elasticsearch.transport.RemoteTransportException: [Crimson Cowl][inet[/192.168.1.11:9301]][indices/status/s]\r\nCaused by: org.elasticsearch.indices.IndexMissingException: [el-2011-11-21-0000] missing\r\n\tat org.elasticsearch.indices.InternalIndicesService.indexServiceSafe(InternalIndicesService.java:244)\r\n\tat org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:152)\r\n\tat org.elasticsearch.action.admin.indices.status.TransportIndicesStatusAction.shardOperation(TransportIndicesStatusAction.java:59)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:398)\r\n\tat org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$ShardTransportHandler.messageReceived(TransportBroadcastOperationAction.java:384)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)'
2313,'spinscale','NullPointerException on a running node\nWhen connecting using the Java API and doing a mass import. All documents made it in as far as I can tell, but I noticed the following error in the console (using 0.9.10):\r\n\r\n[2012-10-10 13:58:50,451][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2012-10-09-0000] creating index, cause [auto(index api)], shards [1]/[0], mappings [thread]\r\n[2012-10-10 13:58:50,619][INFO ][cluster.metadata         ] [Iron Man 2020] [el-2012-10-09-0000] update_mapping [thread] (dynamic)\r\n[2012-10-10 13:58:50,938][INFO ][cluster.service          ] [Iron Man 2020] removed {[Rosenberg, Marsha][GlUDrCclSweRcWq2hNjbtw][inet[/192.168.1.11:9302]]{client=true, data=false},}, reason: zen-disco-node_failed([Rosenberg, Marsha][GlUDrCclSweRcWq2hNjbtw][inet[/192.168.1.11:9302]]{client=true, data=false}), reason transport disconnected (with verified connect)\r\n[2012-10-10 13:58:50,939][DEBUG][action.admin.cluster.node.stats] [Iron Man 2020] failed to execute on node [GlUDrCclSweRcWq2hNjbtw]\r\norg.elasticsearch.transport.NodeDisconnectedException: [Rosenberg, Marsha][inet[/192.168.1.11:9302]][cluster/nodes/stats/n] disconnected\r\n[2012-10-10 14:00:03,466][INFO ][cluster.service          ] [Iron Man 2020] added {[Modred the Mystic][D1OyVHOBTGex3kQ29PU6ow][inet[/192.168.1.11:9302]]{client=true, data=false},}, reason: zen-disco-receive(join from node[[Modred the Mystic][D1OyVHOBTGex3kQ29PU6ow][inet[/192.168.1.11:9302]]{client=true, data=false}])\r\n[2012-10-10 14:00:03,509][DEBUG][action.admin.cluster.node.stats] [Iron Man 2020] failed to execute on node [D1OyVHOBTGex3kQ29PU6ow]\r\norg.elasticsearch.transport.RemoteTransportException: [Modred the Mystic][inet[/192.168.1.11:9302]][cluster/nodes/stats/n]\r\nCaused by: java.lang.NullPointerException\r\n\tat org.elasticsearch.action.support.nodes.NodeOperationResponse.writeTo(NodeOperationResponse.java:66)\r\n\tat org.elasticsearch.action.admin.cluster.node.stats.NodeStats.writeTo(NodeStats.java:290)\r\n\tat org.elasticsearch.transport.support.TransportStreams.buildResponse(TransportStreams.java:139)\r\n\tat org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:76)\r\n\tat org.elasticsearch.transport.netty.NettyTransportChannel.sendResponse(NettyTransportChannel.java:67)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:276)\r\n\tat org.elasticsearch.action.support.nodes.TransportNodesOperationAction$NodeTransportHandler.messageReceived(TransportNodesOperationAction.java:267)\r\n\tat org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:268)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n'
2286,'spinscale','Some mapping conflicts do not throw errors\nFor instance, mapping a field as type `nested` then trying to change it to type `object` should throw a conflict error, but this change gets silently ignored.\n\nNot sure what other mapping conflicts exist which are also being ignored.\n\nMap field to type `nested`:\n\n    curl -XPUT \'http://127.0.0.1:9200/test/?pretty=1\'  -d \'\n    {\n       "mappings" : {\n          "test" : {\n             "properties" : {\n                "foo" : {\n                   "type" : "nested",\n                   "properties" : {\n                      "bar" : {\n                         "type" : "string"\n                      }\n                   }\n                }\n             }\n          }\n       }\n    }\n    \'\n\n    # {\n    #    "ok" : true,\n    #    "acknowledged" : true\n    # }\n\nTry to change to type `object`:\n\n    curl -XPUT \'http://127.0.0.1:9200/test/test/_mapping?pretty=1\'  -d \'\n    {\n       "test" : {\n          "properties" : {\n             "foo" : {\n                "type" : "object",\n                "properties" : {\n                   "bar" : {\n                      "type" : "string"\n                   }\n                }\n             }\n          }\n       }\n    }\n    \'\n\n    # {\n    #    "ok" : true,\n    #    "acknowledged" : true\n    # }\n\n\nMapping hasn\'t changed, but no conflict error was thrown above:\n\n    curl -XGET \'http://127.0.0.1:9200/test/test/_mapping?pretty=1\' \n\n    # {\n    #    "test" : {\n    #       "properties" : {\n    #          "foo" : {\n    #             "type" : "nested",\n    #             "properties" : {\n    #                "bar" : {\n    #                   "type" : "string"\n    #                }\n    #             }\n    #          }\n    #       }\n    #    }\n    # }\n\n\n'
2284,'spinscale','Fail on sorting in empty result\nHi, I had used elasticsearch in my ruby on rails app through tire. It works fine in development mode, but yesterday I add sorting and my test is fail with error\n\nParse Failure [No mapping found for [published_at] in order to sort on]\n\nAfter reading docs I found answer -  add "ignoring unmapped fields" and it works. But I really dont understand why sorting without "ignoring unmapped fields" must fail on empty results. Can you fix that?'
2275,'s1monw','The `_id` path should not allow arrays\n\n    curl -XPUT \'http://127.0.0.1:9200/test/?pretty=1\'  -d \'\n    {\n       "mappings" : {\n          "test" : {\n             "_id" : {\n                "path" : "foo.bar"\n             }\n          }\n       }\n    }\n    \'\n\nThis works correctly:\n\n    curl -XPOST \'http://127.0.0.1:9200/test/test?pretty=1\'  -d \'\n    {\n       "foo" : {\n          "bar" : 1,\n          "baz" : "xx"\n       }\n    }\n    \'\n\n    # [Fri Sep 21 11:14:21 2012] Response:\n    # {\n    #    "ok" : true,\n    #    "_index" : "test",\n    #    "_id" : "1",\n    #    "_type" : "test",\n    #    "_version" : 1\n    # }\n\nThis should throw an error, not set the `_id` to `[`:\n\n    curl -XPOST \'http://127.0.0.1:9200/test/test?pretty=1\'  -d \'\n    {\n       "foo" : {\n          "bar" : [\n             2\n          ],\n          "baz" : "xx"\n       }\n    }\n    \'\n\n    # [Fri Sep 21 11:14:23 2012] Response:\n    # {\n    #    "ok" : true,\n    #    "_index" : "test",\n    #    "_id" : "[",\n    #    "_type" : "test",\n    #    "_version" : 1\n    # }\n\n'
2272,'kimchy','Allow passing additional parameters with query request.\nA small addition to ElasticSearch allowing to pass additional, not parsed parameters along with the query, for example like the following:\n\n```\n{\n "from" : 0,\n "size" : 60,\n "query" : {\n  "term" : { \n   "name" : "value" \n  }\n },\n "additional" : { \n  "token" : "nabsy26dsqnbu1276", \n  "userId" : "12345" \n }\n}\n```'
2260,'spinscale','0.19\n'
2255,'jpountz','Shard failures when searching multiple indices with a type filter and sort field\nWhen executing a search with both a type filter and a sort across multiple indices any shards that do not contain the target type mapping fail. While these failures can be masked by using ignore_unmapped, it seems as if the type filter should make the flag unnecessary.\n\nFor example, let there be an index aaa with a type mapping Y and another index bbb without mapping Y. A search across all indices for all records with type Y (i.e. GET /_all/Y) and sorted by a field F in Y will fail on all shards containing data for index bbb. The failed shards all throw SearchParseExceptions because there was no mapping Y found for the field F (the sort field).\n\nThis failure arises specifically because the sort field is resolved (for type information, etc) before the type filter (or any other filter) is applied. After the filters are applied there are no matching records and so no sort is needed. There does not appear to be any downside to deferring this field resolution until after the number of matched records is known to be greater than zero.\n\nSimple test case for scenario above:\n```\ncurl -XPUT \'localhost:9200/aaa\' -d \'{\n    "settings" : {\n        "number_of_shards" : 5\n    },\n    "mappings" : {\n        "Y" : {\n            "properties" : {\n                "F" : { "type" : "integer" }\n            }\n        }\n    }\n}\'\n\ncurl -XPUT \'localhost:9200/bbb\' -d \'{\n    "settings" : {\n        "number_of_shards" : 5\n    }\n}\'\n\ncurl -XPUT \'localhost:9200/aaa/Y/1\' -d \'{\n\t"F" : 1\n}\'\n\ncurl -XPOST \'localhost:9200/_all/Y/_search\' -d \'{\n\t "sort" : [\n        { "F" : {"order" : "asc"} }\n\t]\n}\'\n```'
2235,'spinscale',"Hanging during bulk inserts\nI have a cluster that we have one index / day, 7 nodes and 7 shards 1 replica.  Currently we insert using bulk operations of ~10k docs.  Each doc is very small (<1k).  I've been getting inserts requests hanging across all nodes, and it looks as though the bulk threads are getting blocked on a single node, causing all inserts to hang.  When this happens the node that appears to be gummed up has no IO, and no CPU utilization and still responds to _cluster health requests.\n\nI created a fork and uploaded the jstack for the process in question as it is large:\nhttps://github.com/bradvoth/elasticsearch/blob/master/jstacks/jstack.lockedinBulk\n"
2215,'spinscale','every node for itself\nSetup : \n\n\t5 similar nodes : \n\n\tbtrainer-1.182\t(192.168.1.182)\t(Current Master before incident) \n\tbtrainer-1.186 (192.168.1.186)\n\tbtrainer-1.136\t(192.168.1.136)\n\tbtrainer-13.137\t(192.168.13.137)\n\tbtrainer-1.138\t(192.168.1.138)\n\nES Configs : (version : 0.19.8)\n\n\tcluster.name: btrainer\n\tdiscovery.zen.ping.multicast.enabled: false\n\tdiscovery.zen.ping.unicast.hosts: [ "192.168.1.182:10300", "192.168.1.186:10300", "192.168.1.136:10300", "192.168.13.137:10300", "192.168.1.138:10300" ]\n\thttp.port: 10200\n\tindex.number_of_replicas: 4\n\ttransport.tcp.port: 10300\n\nJava Options : \n\n\t-Des-foreground=yes \n\t-Des.path.home=/elasticsearch \n\t-Xms4096m \n\t-Xmx20480m \n\t-Djline.enabled=true \n\t-XX:+UseParNewGC \n\t-XX:+UseConcMarkSweepGC \n\t-XX:+CMSParallelRemarkEnabled \n\t-XX:SurvivorRatio=8 \n\t-XX:MaxTenuringThreshold=1 \n\t-XX:CMSInitiatingOccupancyFraction=75 \n\t-XX:+UseCMSInitiatingOccupancyOnly \n\t-cp /elasticsearch/lib/*:/elasticsearch/lib/sigar/* \n\torg.elasticsearch.bootstrap.ElasticSearch\n\nProblem :\n\nThis problem repeats itself every 5-12 hours period. When everything running smoothly (cluster is green) 1 node goes down and everynode creates its own cluster (not 1/4 split, 1/1/1/1/1 split). The sample problem happened exactly at 22:06, we have a job checking cluster state every minute. This cluster mainly used for training so we have heavy traffic spikes on both reads and writes when jobs are triggered (also some continious small reads). \n\n1) What happened to btrainer-1.138 ?\n2) Even if 1 node (btrainer-1.138) behaves irrationally why didn\'t the cluster split by 1/4; why did other nodes lose the master btrainer-1.182 ?\n\nLogs :\n\nyou can check the logs from the nodes : https://gist.github.com/3510448'
2182,'spinscale',"Org. Elasticsearch. ElasticSearchInterruptedException  custom-filters-score-query\nHello, I use in the application of custom - filters - score - query achieve business requirements, similarity calculation. But I found, two machines running for a period of time, of which one will search performance fell sharply, \nObviously see IO, speaking, reading and writing down, finally the following anomaly, but found that stop off one of the machine and returned to normal. \n \n \nThe 2012-08-16 10:46:37, 048 ERROR filter. ExceptionFilter - [DUBBO] Got unchecked and undeclare service method invoke exception: null, DUBBO version: 2.0.14, current host: 172.22.28.88 \nOrg. Elasticsearch. ElasticSearchInterruptedException \nAt org. Elasticsearch. Action. Support. AdapterActionFuture. ActionGet (AdapterActionFuture. Java: 47) \n \n \nUnfortunately I don't have specific anomaly information, only org. Elasticsearch. ElasticSearchInterruptedException. Please instruct me. \n \n \n2  *  （ 16 core 24 g physical machine ）"
2178,'s1monw',"Use TermQuery In _all field if positions are omitted\nCurrently the _all field is always using a specialized SpanTermQuery as a leave query. This might cause unexpected scoring effects and might cost performance if we don't have positions / payloads. We should default to TermQuery if positions are omitted."
2171,'spinscale','replace init.d with upstart scripts\nThis patch replaces init.d with upstart.\n\nIt probably needs review.'
2160,'s1monw',"Expose omitPositions via FieldMapping to omit positions only if frequencies are required\nLucene has the ability to omit positions only and keep frequencies. This is one of the most common usecases when folks don't need proximity information. Yet, if you need freqs for scoring you might pay a huge price for indexing and merging positions like wasting CPU, Memory and diskspace. This PR exposes omit_positions just like omit_norms etc."
2148,'spinscale','shardFailures in SearchResponse showing from previous query\nSteps to repro issue with the shardFailures array showing msgs from a previous (not current) query:\n\n1) create something\ncurl -XPUT localhost:9200/acme/blog/1111 -d \'{"message":"foo"}\'\n\n2) execute this query - it should succeed\ncurl -XGET localhost:9200/acme/blog/_search -d \'{"query":{"field":{"message":"foo"}}}\'\n\n3) this is an invalid query and is expected to fail\ncurl -XGET localhost:9200/acme/blog/_search -d \'{"foobar":{"message":"foo"}}\'\n\n4) now rerun the query from step 2. This query succeeds, and returns the hits, but also shows the shardFailures from the failed query in step 3\ncurl -XGET localhost:9200/acme/blog/_search -d \'{"query":{"field":{"message":"foo"}}}\'\n\n\nHere is the output I get from step 4)  :\n\n{"took":3,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0,"failures":[{"index":"acme","shard":1,"status":400,"reason":"SearchParseException[[acme][1]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"foobar\\":{\\"message\\":\\"foo\\"}}]]]; nested: SearchParseException[[acme][1]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "},{"index":"acme","shard":0,"status":400,"reason":"SearchParseException[[acme][0]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"foobar\\":{\\"message\\":\\"foo\\"}}]]]; nested: SearchParseException[[acme][0]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "},{"index":"acme","shard":4,"status":400,"reason":"SearchParseException[[acme][4]: from[-1],size[-1]: Parse Failure [Failed to parse source [{\\"foobar\\":{\\"message\\":\\"foo\\"}}]]]; nested: SearchParseException[[acme][4]: from[-1],size[-1]: Parse Failure [No parser for element [foobar]]]; "}]},"hits":{"total":1,"max_score":1.6931472,"hits":[{"_index":"acme","_type":"blog","_id":"1111","_score":1.6931472, "_source" : {"message" : "foo" }}]}}\n'
2144,'spinscale','Elastic Search Client library fails when dealing with small score values\nrepro: set score script to 0.00000000000000001, execute query, get results, and then try to do getHits() on the result and see it break.\n\nWork around: if your score script will produce really small values, add 1.0 to your score script.'
2117,'kimchy','split brain condition after second network disconnect - even with minimum_master_nodes set\n## Summary:\n\nSplit brain can occur on the second network disconnect of a node, when the minimum_master_nodes is configured correctly(n/2+1).  The split brain occurs if the nodeId(UUID) of the disconnected node is such that the disconnected node picks itself as the next logical master while pinging the other nodes(NodeFaultDetection).  The split brain only occurs on the second time that the node is disconnected/isolated.\n\n## Detail:\n\nUsing ZenDiscovery, Node Id\'s are randomly generated(A UUID):  ZenDiscovery:169.\n\nWhen the node is disconnected/isolated it the ElectMasterService uses an ordered list of the Nodes (Ordered by nodeId) to determine a new potential master.  It picks the first of the ordered list: ElectMasterService:95\n\nBecause the nodeId\'s are random, it\'s possible for the disconnected/isolated node to be first in the ordered list, electing itself as a possible master.\n\nThe first time network is disconnected, the minimum_master_nodes property is honored and the disconnected/isolated node goes into a "ping" mode, where it simply tries to ping for other nodes.  Once the network is re-connected, the node re-joins the cluster successfully.\n\nThe Second time the network is disconnected, the minimum_master_nodes intent is not honored.  The disconnected/isolated node fails to realise that it\'s not connected to the remaining node in the 3 node cluster and elects itself as master, still thinking it\'s connected.\n\nIt feels like there is a failure in the transition between MasterFaultDetection and NodeFaultDetection, because it works the first time!\n\nThe fault only occurs if the nodeId is ordered such that the disconnected node picks itself as the master while isolated.  If the nodeId\'s are ordered such that it picks one of the other 2 nodes to be potential master then the isolated node honors the minimum_master_nodes intent every time.\n\nBecause the nodeId\'s are randomly(UUID) generated, the probability of this occuring drops as the number of nodes in the cluster goes up.  For our 3 node cluster it\'s ~50% (with one node detected as gone, it\'s up to the ordering of the remaining two nodeId\'s)\n\n\nNote, While we were trying track this down we found that the cluster.service TRACE level logging (which outputs the cluster state) does not list the nodes in election order.  IE, the first node in that printed list is not necessarily going to elected as master by the isolated node.\n\n## Detail Steps to reproduce:\n\nBecause the ordering of the nodeId\'s is random(UUID) we were having trouble getting a consitantly reproducable test case.  To fix the ordering, we made a patch to ZenDiscovery to allow us to optionally configure a nodeId.  This allowed us to set the nodeId of the disconnected/isolated node to guarantee it\'s ordering, allowing us to consistently reproduce.\n\nWe\'ve tested this scenario on the 0.19.4, 0.19.7, 0.19.8 distributions and see the error when the nodeId\'s were ordered just right.\n\nWe also tested this scenario on the current git master with the supplied patch.\n\nIn this scenario, node3 will the be the node we disconnect/isolate.  So we start the nodes up in numerical order to ensure node3 doesn\'t _start_ as master.\n\n1. Configure nodes with attached configs (one is provided for each node)\n2. Start up nodes 1 and 2.  After they are attached and one is master, start node 3\n3. Create a blank index with default shard/replica(5/1) settings\n4. Pull network cable from node 3\n5. Node 3 detects master has gone (MasterFaultDetection)\n6. Node 3 elects itself as master (Because the nodeId\'s are ordered just right)\n7. Node 3 detects the remaining node has gone, enters ZenDiscovery minimum_master_nodes mode, prints a message indicating not enough nodes\n8. Node 3 goes into a ping state looking for nodes\n9. At this point, node 1 and node 2 report a valid cluster, they know about each other but not about node 3.\n10. Reconnect network to node 3\n11. Node 3 rejoins the cluster correctly, seeing that there is already a master in the cluster.\n\nAt this point, everything is working as expected.\n\n12.  Pull network cable from node 3 again\n13.  Node 3 detects master has gone (MasterFaultDetection)\n14.  Node 3 elects as itself as master (Because the nodeId\'s are ordered just right)\n15.  Node 3 now fails to detect that the remaining node in the cluster is not accessible.  It starts throwing a number of Netty NoRouteToHostExceptions about the remaining node.\n16.  According to node 3, cluster health is yellow and cluster state shows 2 data nodes\n17.  Reconnect network to node 3\n18.  Node 3 appears to connect to the node that it thinks it\'s still connected to.  (can see that via the cluster state api).  The other nodes log nothing and do not show the disconnected node as connected in any way.\n19.  Node 3 at this point accepts indexing and search requests, a classic split brain.\n\nHere\'s a gist with the patch to ZenDiscovery and the 3 node configs.\n\nhttps://gist.github.com/3174651'
2116,'s1monw','ShingleTokenFilterFactory doesn\'t expose all relevant settings\nCurrently you are only able to set "max_shingle_size" & "output_unigrams". For efficient use of this filter we should expose "min_shingle_size", "output_unigrams_if_no_shingles" & "token_separator" as well.\n\n'
2114,'uboness','Allow setting ttl at index level\nAllow setting ttl to index, so the whole index gets deleted when expired'
2111,'spinscale',"Add regular expression query\nAdd support for Lucene's RegexQuery to ElasticSearch.\n\nRegexQuery is really slow on Lucene 3.x, we are planning to use this query only with Percolate API"
2109,'dadoonet','Add regex filter on terms_stats facet\nRelative to #2063, here come the pull request...\nBTW, documentation update proposal is here : https://github.com/elasticsearch/elasticsearch.github.com/pull/212'
2100,'spinscale','Highlighting breaks when using Fuzzy Like This Query\nAlso posted this in the google group, but appears to be an issue according to people in the IRC Chat.\n\nWe are switching from using Suery String to Fuzzy Like This Query. Doing so breaks the highlighting. Here is the query, result and mapping: https://gist.github.com/3097781\n\nWe are using Elasticsearch 0.19.8\n\n'
2057,'spinscale','Plugin info REST endpoint in node info\nAnother version of pull request #2007, now integrated into node info mechanism.\n\nOne minor flaw is that the "plugins" message in the logs appears twice. I am not sure how to initialize the PluginsService correctly without getting the message printed.'
2050,'spinscale','Request - Published hashes of elasticsearch download artifacts\nHi,\n\nI\'m writing an elasticsearch juju charm (see https://juju.ubuntu.com).  During the charm review process I was asked if there were published hashes for the elasticsearch tarball.  I was not able to find anything like that in the download area.  Specifically the reviewer said:\n\n"charms that are part of the charmstore need to cryptographically\nverify downloads.  Can you please look to see if there\'re published\nhashes of the elasticsearch tarball that can be used?"\n\nWould it be possible to provide that ?  It seems its a pre-requisite for charms that can be published in the juju charm store.\n\nAnother nice to have would be a way to download the latest version without knowing what the version number is, maybe with a symlinked /latest in the url. Otherwise it becomes quite a chore to update the default version number in the charm given the rapid release cycles practiced here.\n\nThanks!\nLuis '
2034,'spinscale','org.elasticsearch.index.mapper.MapperParsingException\ntry to use jdbc river, but from logs I got a lot exceptions repeated:\norg.elasticsearch.index.mapper.MapperParsingException: Failed to parse [geoLong]\n        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:325)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.serializeValue(ObjectMapper.java:585)\n        at org.elasticsearch.index.mapper.object.ObjectMapper.parse(ObjectMapper.java:449)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:493)\n        at org.elasticsearch.index.mapper.DocumentMapper.parse(DocumentMapper.java:437)\n        at org.elasticsearch.index.shard.service.InternalIndexShard.prepareIndex(InternalIndexShard.java:311)\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnPrimary(TransportShardBulkAction.java:156)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction.performOnPrimary(TransportShardReplicationOperationAction.java:532)\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$AsyncShardOperationAction$1.run(TransportShardReplicationOperationAction.java:430)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:722)\nCaused by: org.elasticsearch.index.mapper.MapperParsingException: failed to parse date field [100.516667], tried both date format [dateOptionalTime], and timestamp number        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:412)\n        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseCreateField(DateFieldMapper.java:341)        at org.elasticsearch.index.mapper.core.AbstractFieldMapper.parse(AbstractFieldMapper.java:312)\n        ... 11 moreCaused by: java.lang.IllegalArgumentException: Invalid format: "100.516667" is malformed at ".516667"\n        at org.elasticsearch.common.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:644)        at org.elasticsearch.index.mapper.core.DateFieldMapper.parseStringValue(DateFieldMapper.java:406)\n        ... 13 more\n\nwhile actually geolong is a simple nvarchar(50) type, an example value is geoLong":"100.516667", what\'s the problem? thanks\n'
2027,'spinscale',"Catch LinkageError when executing scripts (most likely recoverable).\nSometimes we experience LinkageErrors in 3rd party libraries (mvel) when executing scripts. LinkageErrors (in scripts) are most likely recoverable and could be handled by wrapped into a RuntimeException (and printed as an error in the 'failures' property) instead of just being unhandled and causing the thread to terminate without responding or closing the connection properly."
2010,'spinscale','Enhancement: Add INFO level log for gateway recovery\nHi,\n\nI had configured my cluster to start initial recovery after five minutes. Some weeks later I wondered, why all my nodes returned a HTTP/503 after startup (forgot that I had configured that initial recovery time... ).\n\nIt would be nice to have some INFO level log messages on startup when a node will return a HTTP/503 for some time.\nFor example, if "gateway.recover_after_time" if configured, print out something like:\n"Suspending operation for XX seconds - resume node operation at YY:YY:YY"\n\nRegards,\nRobert\n\n'
1992,'spinscale',"mlockall in 0.19.4 does not appear to be working\nelasticsearch.yml : bootstrap.mlockall\n\nelasticsearch.conf: min & max heap both set to 1.9gb\n\nuser limits: user memlock - unlimited\n\ncore file size          (blocks, -c) 0\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 31971\nmax locked memory       (kbytes, -l) unlimited\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 32768\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 2047\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n\n\n\nOn node restart, I get no memlock messages in the log indicating success or failure to lock the memory. Debug log below.\nBigdesk shows a 1.9gb heap commit and ~100mb used on restart (same as nodes that do no have bootstrap.mlockall : true set).\n\n\n[2012-05-31 15:15:03,561][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: stopping ...\n[2012-05-31 15:15:03,750][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: stopped\n[2012-05-31 15:15:03,751][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: closing ...\n[2012-05-31 15:15:03,941][INFO ][node                     ] [prod-es-r01] {0.19.4}[30362]: closed\n[2012-05-31 15:15:16,002][INFO ][bootstrap                ] max_open_files [65517]\n[2012-05-31 15:15:18,833][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: initializing ...\n[2012-05-31 15:15:18,893][INFO ][plugins                  ] [prod-es-r01] loaded [transport-thrift, hashing-analyzer], sites [bigdesk]\n[2012-05-31 15:15:18,938][DEBUG][env                      ] [prod-es-r01] using node location [[/opt/elastic_search/data/brewster/nodes/0]], local_node_id [0]\n[2012-05-31 15:15:20,768][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [generic], type [cached], keep_alive [30s]\n[2012-05-31 15:15:20,773][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [index], type [fixed], size [20], queue_size [null], reject_policy [caller]\n[2012-05-31 15:15:20,775][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [bulk], type [cached], keep_alive [5m]\n[2012-05-31 15:15:20,775][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [get], type [cached], keep_alive [5m]\n[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [search], type [fixed], size [20], queue_size [null], reject_policy [abort]\n[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [percolate], type [cached], keep_alive [5m]\n[2012-05-31 15:15:20,776][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [management], type [scaling], min [1], size [5], keep_alive [5m]\n[2012-05-31 15:15:20,777][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [flush], type [scaling], min [1], size [10], keep_alive [5m]\n[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [merge], type [scaling], min [1], size [20], keep_alive [5m]\n[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [refresh], type [cached], keep_alive [1m]\n[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [cache], type [scaling], min [1], size [4], keep_alive [5m]\n[2012-05-31 15:15:20,778][DEBUG][threadpool               ] [prod-es-r01] creating thread_pool [snapshot], type [scaling], min [1], size [5], keep_alive [5m]\n[2012-05-31 15:15:20,792][DEBUG][transport.netty          ] [prod-es-r01] using worker_count[8], port[9300-9400], bind_host[null], publish_host[null], compress[false], connect_timeout[30s], connections_per_node[2/6/1]\n[2012-05-31 15:15:20,810][DEBUG][discovery.zen.ping.unicast] [prod-es-r01] using initial hosts [10.180.48.178:9300, 10.183.69.144:9300, 10.182.14.97:9300, 10.180.35.110:9300, 10.180.39.14:9300, 10.180.46.203:9300, 10.180.48.216:9300, 10.180.48.255:9300], with concurrent_connects [10]\n[2012-05-31 15:15:20,812][DEBUG][discovery.zen            ] [prod-es-r01] using ping.timeout [15s]\n[2012-05-31 15:15:20,818][DEBUG][discovery.zen.elect      ] [prod-es-r01] using minimum_master_nodes [3]\n[2012-05-31 15:15:20,819][DEBUG][discovery.zen.fd         ] [prod-es-r01] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n[2012-05-31 15:15:20,822][DEBUG][discovery.zen.fd         ] [prod-es-r01] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\n[2012-05-31 15:15:20,848][DEBUG][monitor.jvm              ] [prod-es-r01] enabled [true], last_gc_enabled [false], interval [1s], gc_threshold [{default=GcThreshold{name='default', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}, ParNew=GcThreshold{name='ParNew', warnThreshold=1000, infoThreshold=700, debugThreshold=400}, ConcurrentMarkSweep=GcThreshold{name='ConcurrentMarkSweep', warnThreshold=10000, infoThreshold=5000, debugThreshold=2000}}]\n[2012-05-31 15:15:21,358][DEBUG][monitor.os               ] [prod-es-r01] Using probe [org.elasticsearch.monitor.os.SigarOsProbe@35e5ebbf] with refresh_interval [1s]\n[2012-05-31 15:15:21,364][DEBUG][monitor.process          ] [prod-es-r01] Using probe [org.elasticsearch.monitor.process.SigarProcessProbe@3fdb8a73] with refresh_interval [1s]\n[2012-05-31 15:15:21,367][DEBUG][monitor.jvm              ] [prod-es-r01] Using refresh_interval [1s]\n[2012-05-31 15:15:21,368][DEBUG][monitor.network          ] [prod-es-r01] Using probe [org.elasticsearch.monitor.network.SigarNetworkProbe@3d6a2c7b] with refresh_interval [5s]\n[2012-05-31 15:15:21,375][DEBUG][monitor.network          ] [prod-es-r01] net_info\nhost [prod-es-r01.ihost.brewster.com]\neth1\tdisplay_name [eth1]\n\t\taddress [/fe80:0:0:0:4240:75ff:feca:294d%3] [/10.180.48.178] \n\t\tmtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]\neth0\tdisplay_name [eth0]\n\t\taddress [/fe80:0:0:0:4240:4fff:feb3:e842%2] [/184.106.133.96] \n\t\tmtu [1500] multicast [true] ptp [false] loopback [false] up [true] virtual [false]\nlo\tdisplay_name [lo]\n\t\taddress [/0:0:0:0:0:0:0:1%1] [/127.0.0.1] \n\t\tmtu [16436] multicast [false] ptp [false] loopback [true] up [true] virtual [false]\n\n[2012-05-31 15:15:21,381][DEBUG][monitor.fs               ] [prod-es-r01] Using probe [org.elasticsearch.monitor.fs.SigarFsProbe@78214f6b] with refresh_interval [1s]\n[2012-05-31 15:15:21,696][DEBUG][cache.memory             ] [prod-es-r01] using bytebuffer cache with small_buffer_size [1kb], large_buffer_size [1mb], small_cache_size [10mb], large_cache_size [500mb], direct [true]\n[2012-05-31 15:15:21,744][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using node_concurrent_recoveries [2], node_initial_primaries_recoveries [2]\n[2012-05-31 15:15:21,745][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using [cluster.routing.allocation.allow_rebalance] with [indices_all_active]\n[2012-05-31 15:15:21,745][DEBUG][cluster.routing.allocation.decider] [prod-es-r01] using [cluster_concurrent_rebalance] with [2]\n[2012-05-31 15:15:21,749][DEBUG][gateway.local            ] [prod-es-r01] using initial_shards [quorum], list_timeout [30s]\n[2012-05-31 15:15:21,834][DEBUG][indices.recovery         ] [prod-es-r01] using max_size_per_sec[10mb], concurrent_streams [2], file_chunk_size [100kb], translog_size [100kb], translog_ops [1000], and compress [true]\n[2012-05-31 15:15:21,869][DEBUG][http.netty               ] [prod-es-r01] using max_chunk_size[8kb], max_header_size[8kb], max_initial_line_length[4kb], max_content_length[100mb]\n[2012-05-31 15:15:21,878][DEBUG][indices.memory           ] [prod-es-r01] using index_buffer_size [203.9mb], with min_shard_index_buffer_size [4mb], max_shard_index_buffer_size [512mb], shard_inactive_time [30m]\n[2012-05-31 15:15:21,883][DEBUG][indices.cache.filter     ] [prod-es-r01] using [node] filter cache with size [20%], actual_size [407.9mb]\n[2012-05-31 15:15:22,327][DEBUG][gateway.local.state.shards] [prod-es-r01] took 370ms to load started shards state\n[2012-05-31 15:15:24,192][DEBUG][gateway.local.state.meta ] [prod-es-r01] took 1.8s to load state\n[2012-05-31 15:15:24,195][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: initialized\n[2012-05-31 15:15:24,195][INFO ][node                     ] [prod-es-r01] {0.19.4}[1579]: starting ...\n[2012-05-31 15:15:24,263][INFO ][thrift                   ] [prod-es-r01] bound on port [9500]\n[2012-05-31 15:15:24,297][DEBUG][netty.channel.socket.nio.NioProviderMetadata] Using the autodetected NIO constraint level: 0\n[2012-05-31 15:15:24,410][DEBUG][transport.netty          ] [prod-es-r01] Bound to address [/10.180.48.178:9300]\n[2012-05-31 15:15:24,411][INFO ][transport                ] [prod-es-r01] bound_address {inet[/10.180.48.178:9300]}, publish_address {inet[/10.180.48.178:9300]}\n[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_8#][inet[/10.180.48.255:9300]]]\n[2012-05-31 15:15:24,499][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[prod-es-r01][BkIy7zTyQJqXfhQ6vUlaKQ][inet[/10.180.48.178:9300]]{subnet=180.48, datacenter=ORD1}]\n[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_2#][inet[/10.183.69.144:9300]]]\n[2012-05-31 15:15:24,498][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_6#][inet[/10.180.46.203:9300]]]\n[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_5#][inet[/10.180.39.14:9300]]]\n[2012-05-31 15:15:24,498][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_7#][inet[/10.180.48.216:9300]]]\n[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_4#][inet[/10.180.35.110:9300]]]\n[2012-05-31 15:15:24,492][DEBUG][transport.netty          ] [prod-es-r01] connected to node [[#zen_unicast_3#][inet[/10.182.14.97:9300]]]\n[2012-05-31 15:15:39,440][DEBUG][transport.netty          ] [prod-es-r01] disconnected from [[#zen_unicast_7#][inet[/10.180.48.216:9300]]]\n[2012-05-31 15:15:39,438][DEBUG][discovery.zen            ] [prod-es-r01] ping responses:\n\t--> target [[prod-es-r07][MAWcm4KOSpCkfgdf4oMQ_g][inet[/10.180.48.216:9300]]{subnet=180.48, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r04][V0_bFMKDRYWn90hLCKJBVw][inet[/10.180.35.110:9300]]{subnet=180.35, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r05][SoTwCd1aTrCQaiwdnvTl4Q][inet[/10.180.39.14:9300]]{subnet=180.39, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r08][WzZIAFrIRMO2VaR-68UvMw][inet[/10.180.48.255:9300]]{subnet=180.48, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r03][aV17ANXvSPmxbd8wonM4-Q][inet[/10.182.14.97:9300]]{subnet=182.14, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n\t--> target [[prod-es-r06][sAFQAI6XQsu50I3LNdpcvA][inet[/10.180.46.203:9300]]{subnet=180.46, datacenter=ORD1}], master [[prod-es-r02][6tepGHE8Q7GHwfAvrBkhJA][inet[/10.183.69.144:9300]]{subnet=183.69, datacenter=ORD1}]\n"
1990,'dakrone','Add a file watcher which trigger the reload of the logging configuration\nWhen log4j is in use, the default, the configuration is not reloaded on change on the configuration file. The suggested patch makes it check every 10s for change and reload the log config if necessary.\n'
1968,'dakrone','Support for shard balancing based on index size\nIt would be very useful to have support for shard balancing based on index size or number of documents.\n\nThe current balance algorithm can lead to some strange shard distributions.\n\nI currently have two indices: \n - History. Empty 0 documents (0B);\n - Active. 60M documents (79GB).\n\nI have 4 nodes:\n - Node 1: 1 shard of history; 19 shards of active;\n - Node 2: 19 shards of history: 2 shards of active;\n - Node 3: 1 shard 19 shards of active;\n - Node 4: 19 shards of history; 0 shards of active;\n\nSo basically i have two machines sleeping and two working =).\n\nAlso if the distribution was made at the index level it might be easier?'
1962,'spinscale',"Default params from /etc/default/elasticsearch ignored by startup scripts\nES installed on Ubuntu 11.04 from deb package. Works but params from 'default file' seems to be ignored.\n\nAfter setting some params in /etc/default/elasticsearch file, they're visible in /etc/init.d/elasticsearch script, but after calling start-stop-daemon they seems to be lost in script /usr/share/elasticsearch/bin/elasticsearch"
1957,'spinscale','Terms facet on string field with "-" problem/bug\nHi, \nI have a problem with terms facet on a field that has "-" in its value. My facet is:\n```json\n"facets" : {\n   "my_facet" : { "terms" : {"field" : "my_field"} }\n}\n```\nAnd the result of the facet:\n```json\n"terms": [\n    {\n          "term": "kwidzyn.pl"\n          "count": 1\n    },\n    {\n          "term": "forum.e"\n          "count": 1\n    }\n]\n```\nwhile the value of my_field in document is "forum.e-kwidzyn.pl" and no other document matches the query. That field is defined in schema like this:\n```json\n"my_field": {\n    "include_in_all": false,\n    "index": "not_analyzed",\n    "type": "string"\n}\n```\nIs this a bug or am I missing something?'
1951,'spinscale',"added statisticalFacet to the testFacets test to show example\nIt wasn't really obvious to get correct results of a Statistical Facet with nested documents and a facet filter applied.\nupdated testFacets test to demonstrate."
1944,'spinscale',"Geo query keeps failing on a specific type/table\nUsing v0.18.7. Problem is that for a given type/table in an index geo search keeps failing. I can load the same data in another type/table and all is fine but even if I delete the type/table and then reload the query fails. Here's the stack trace\n\n<pre>\n... node[FA1mzqRITkCwHB_hw6Faxw], [R], s[STARTED]: Failed to execute [org.elasticsearch.action.search.SearchRequest@3c02e8fe]\norg.elasticsearch.transport.RemoteTransportException: [Death][inet[/193.34.146.144:9300]][search/phase/query]\nCaused by: org.elasticsearch.search.query.QueryPhaseExecutionException: [...][0]: query[filtered(ConstantScore(org.elasticsearch.index.search.geo.GeoDistanceFilter@d40d7f57))->FilterCacheFilterWrapper(_type:30d6d9aa-0f07-4db3-9ddd-ba615d00ec21)],from[0],size[10]: Query Failed [Failed to execute main query]\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:221)\n        at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:238)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:447)\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryTransportHandler.messageReceived(SearchServiceTransportAction.java:438)\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:358)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)\n        at java.lang.Thread.run(Thread.java:636)\nCaused by: java.lang.StringIndexOutOfBoundsException: String index out of range: -1\n        at java.lang.String.substring(String.java:1949)\n        at org.elasticsearch.index.mapper.geo.GeoPointFieldData$StringTypeLoader.collectTerm(GeoPointFieldData.java:177)\n        at org.elasticsearch.index.field.data.support.FieldDataLoader.load(FieldDataLoader.java:55)\n        at org.elasticsearch.index.mapper.geo.GeoPointFieldData.load(GeoPointFieldData.java:160)\n        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:51)\n        at org.elasticsearch.index.mapper.geo.GeoPointFieldDataType.load(GeoPointFieldDataType.java:34)\n        at org.elasticsearch.index.field.data.FieldData.load(FieldData.java:110)\n        at org.elasticsearch.index.cache.field.data.support.AbstractConcurrentMapFieldDataCache.cache(AbstractConcurrentMapFieldDataCache.java:119)\n        at org.elasticsearch.index.search.geo.GeoDistanceFilter.getDocIdSet(GeoDistanceFilter.java:114)\n        at org.apache.lucene.search.DeletionAwareConstantScoreQuery$DeletionConstantWeight.scorer(DeletionAwareConstantScoreQuery.java:53)\n        at org.apache.lucene.search.FilteredQuery.getFilteredScorer(FilteredQuery.java:149)\n        at org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:117)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:577)\n        at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:199)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:445)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:426)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:342)\n        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:330)\n        at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:217)\n        ... 7 more\n</pre>\n\nAny ideas what causes this problem and how it could be fixed (it's a real pain for us to renames types/tables ...)"
1930,'spinscale','StackOverflowError in TransportClientNodesService\nI observed a StackOverflowError lately\r\n\r\n\tException in thread "elasticsearch[generic]-pool-1-thread-24" java.lang.StackOverflowError\r\n\t        at java.net.Inet4Address.getHostAddress(Inet4Address.java:322)\r\n\t        at java.net.InetAddress.toString(InetAddress.java:663)\r\n\t        at java.net.InetSocketAddress.toString(InetSocketAddress.java:276)\r\n\t        at java.lang.String.valueOf(String.java:2902)\r\n\t        at java.lang.StringBuilder.append(StringBuilder.java:128)\r\n\t        at org.elasticsearch.common.transport.InetSocketTransportAddress.toString(InetSocketTransportAddress.java:150)\r\n\t        at java.lang.String.valueOf(String.java:2902)\r\n\t        at java.lang.StringBuilder.append(StringBuilder.java:128)\r\n\t        at org.elasticsearch.transport.ActionTransportException.buildMessage(ActionTransportException.java:71)\r\n\t        at org.elasticsearch.transport.ActionTransportException.<init>(ActionTransportException.java:46)\r\n\t        at org.elasticsearch.transport.ConnectTransportException.<init>(ConnectTransportException.java:44)\r\n\t        at org.elasticsearch.transport.ConnectTransportException.<init>(ConnectTransportException.java:32)\r\n\t        at org.elasticsearch.transport.NodeNotConnectedException.<init>(NodeNotConnectedException.java:32)\r\n\t        at org.elasticsearch.transport.netty.NettyTransport.nodeChannel(NettyTransport.java:637)\r\n\t        at org.elasticsearch.transport.netty.NettyTransport.sendRequest(NettyTransport.java:445)\r\n\t        at org.elasticsearch.transport.TransportService.sendRequest(TransportService.java:185)\r\n\t        at org.elasticsearch.action.TransportActionNodeProxy.execute(TransportActionNodeProxy.java:63)\r\n\t        at org.elasticsearch.client.transport.support.InternalTransportClient$2.doWithNode(InternalTransportClient.java:100)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:217)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        at org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\t        [...endlessly repeated... ]'
1911,'spinscale','API to pin a shard to a node\nAs a stopgap to index-based allocation, could offer an API to move a shard to a specific node.\r\n\r\nThis can be used to move large shards to specific nodes, then let smaller shards reallocate around them.\r\n\r\nA plugin like elasticsearch-head could let you drag & drop a shard to a node to manually rebalance.'
1893,'javanna','require_field_match works incorrect in a query which has nested query\nI have the following mapping and data:\r\n\r\n\r\n--Mapping:\r\n\r\n```\r\ncurl -XPUT "http://localhost:9200/test?pretty=1" -d \'\r\n{\r\n  "mappings": {\r\n    "programlanguage": {\r\n      "properties": {\r\n        "name": {\r\n          "type": "string"\r\n        },\r\n        "components": {\r\n          "properties": {\r\n            "name": {\r\n              "type": "string"\r\n            }\r\n          },\r\n          "type": "nested"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\'\r\n```\r\n\r\n--Index data:\r\n\r\n```\r\ncurl -XPUT \'http://localhost:9200/test/programlanguage/2\' -d \'{\r\n    "name" : "java",\r\n    "components" : [{"name":"java core"},{"name":"abc"},{"name": "bbb"}]\r\n}\'\r\n```\r\n\r\n-----query\r\n\r\n```\r\n{\r\n  "query": {\r\n    "bool": {\r\n      "should": [\r\n        {\r\n          "text": {\r\n            "name": {\r\n              "query": "java core",\r\n              "operator": "and"\r\n            }\r\n          }\r\n        },\r\n        {\r\n          "nested": {\r\n            "path": "components",\r\n            "query": {\r\n              "text": {\r\n                "components.name": {\r\n                  "query": "java core",\r\n                  "operator": "and"\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      ],\r\n      "minimum_number_should_match": 1,\r\n      "boost": 1\r\n    }\r\n  },\r\n  "highlight": {\r\n    "tags_schema": "styled",\r\n    "fields": {\r\n      "name": {      }\r\n    },\r\n    "require_field_match": true\r\n  }\r\n}\r\n```\r\n\r\nWith the query, I expected the highligh list is only "java" from "programlanguage" field which is not my expectation.\r\nPlease help me verify it'
1888,'spinscale','Prevent shard-relocation of existing index upon creation of a new index\nHi,\r\n\r\nI have a cluster of 4 nodes with an existing index (consisting of 32 shards, 1 replica).\r\n\r\nWhen I add another new index to the cluster, the cluster begins to relocate shards of the "old" index.\r\nI do not think that this is necessary.\r\nHint: During relocation of the "old" shards, there might be unallocated shards of the new index.\r\n\r\nIt seems that shard relocation of old index(es) triggers while the new index has not been fully created yet.\r\n\r\nRobert.\r\n'
1868,'spinscale',"NoNodeAvailableException after 2 hours of bulk indexing\nHi,\r\n\r\nI continuously receive the following exceptions after my bulk indexer runs for appox 2 hours.\r\n\r\nI'm using a cluster with 4 elasticsearch nodes and all nodes were always running. One process issues bulk request with 100 index requests each with a throughput of about 1000~2000 docs per second.\r\n\r\nThe elasticsearch server log files say nothing.\r\n\r\n\r\norg.elasticsearch.client.transport.NoNodeAvailableException: No node available\r\n\tat org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:214)\r\n\tat org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\tat org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\tat org.elasticsearch.client.transport.TransportClientNodesService$RetryListener.onFailure(TransportClientNodesService.java:220)\r\n\tat org.elasticsearch.client.transport.TransportClientNodesService.execute(TransportClientNodesService.java:182)\r\n\tat org.elasticsearch.client.transport.support.InternalTransportClient.execute(InternalTransportClient.java:97)\r\n\tat org.elasticsearch.client.support.AbstractClient.bulk(AbstractClient.java:141)\r\n\tat org.elasticsearch.client.transport.TransportClient.bulk(TransportClient.java:295)\r\n\tat org.elasticsearch.action.bulk.BulkRequestBuilder.doExecute(BulkRequestBuilder.java:128)\r\n\tat org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)\r\n\tat org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)\r\n"
1843,'martijnvg','Stop iterating on sub readers if the parent is found in TopChildrenQuery\nThere are some comment saying "we found a match, add it and break", but there\'s no actual break.\r\n\r\nIn the suggested patch, it rewrote the loop to make the loop ending more explicit. I could have just added a "break", but I don\'t like much "break". Well, actually the main reason is this patch avoid too much diff with fetch-children branch :).\r\n'
1840,'spinscale',"PhoneticFilter.incrementToken wasn't defined with final signal\nHi,\r\n\r\nI met the same issue with https://github.com/elasticsearch/elasticsearch/issues/1397 when working with PhoneticFilter in version 0.19.1, the issue is  PhoneticFilter.incrementToken wasn't defined with final signal\r\n\r\nPlease check it this is a bug."
1836,'spinscale',"Race condition / unusable index with illegal index setting\nHi,\r\n\r\nI've accidently corrupted my index by simply setting the value \r\nindex.merge.policy.merge_factor\r\nin ES 0.19.1 to an empty string.\r\n\r\nIt is not possible to reopen the index after the setting has been applied. It is even not possible to close it because the open index action loops infinitely.\r\n\r\nAll I wanted to do is to remove the setting (and apply the default value) - is this possible?\r\n\r\nRobert\r\n"
1827,'javanna',"Feature Request: search and highlight on all fields\nCurrently if we don't specify which fields to search and/or highlight, _all field is used.\r\nThere are some limitations using _all field however. For example, it doesn't let you specify different analyzer for different field. Also, highlighting on _all field is difficult and expensive.\r\n\r\nWe'd like to have a mechanism to search and/or highlight on all fields without specifying all of them."
1825,'javanna','Allow setting aliases on index template\nAllow setting aliases on index template so the newly created index is automatically assigned to the alias.'
1810,'dakrone','QueryBuilders should be Serializable\nSo that queries can be passed around especially in map reduce applications, it would be convenient if the queryBuilders were serializable. '
1809,'spinscale','ElasticSearch goes down periodically\nHi there, \r\nWe use ElasticSearch in one of our projects. The system goes down periodically, without any reason. And than we need to restart everything from command line. \r\n\r\nHow can we understand the reason? \r\n\r\nThanks\r\nTolga'
1792,'spinscale','Failed to install cloud-aws, reason: failed to download v0.19.0\nFollowing the [install instructions](http://www.elasticsearch.org/tutorials/2011/08/22/elasticsearch-on-ec2.html) and it looks like version 19 is unable to install cloud-aws because of an incorrect url.  \r\n\r\n```bash\r\nwget https://github.com/downloads/elasticsearch/elasticsearch/elasticsearch-0.19.0.zip\r\n...snip...\r\n[ec2-user@x elasticsearch]$ cd elasticsearch-0.19.0/\r\n[ec2-user@x elasticsearch-0.19.0]$ sudo bin/plugin -install cloud-aws\r\n-> Installing cloud-aws...\r\nTrying http://elasticsearch.googlecode.com/svn/plugins/cloud-aws/elasticsearch-cloud-aws-0.19.0.zip...\r\nFailed to install cloud-aws, reason: failed to download\r\n```\r\n\r\nI was able to install cloud-aws by using v0.18.7.\r\n\r\nThanks\r\n'
1785,'spinscale','Faceting field caching issue\nI am opening a bug report for the issue described here:\r\n\r\nhttp://groups.google.com/group/elasticsearch/browse_thread/thread/f666209e0095f6e1/cba28b4a143baf15'
1782,'s1monw','[Code refactoring] IndicesStats -> IndicesStatsResponse\nRename (org.elasticsearch.action.admin.indices.stats) IndicesStats to IndicesStatsResponse.\r\nAll classes extending BroadcastOperationResponse follow *Response pattern. May be IndicesStats could too?'
1760,'dadoonet','Add Maven javadoc jar\nAs seen here : https://groups.google.com/forum/?hl=fr&fromgroups#!topic/elasticsearch/m0RjOrnwUB0\r\n\r\nWe can add javadoc to be deployed as a jar in maven repositories.\r\n\r\nHTH\r\nDavid.'
1745,'spinscale','wrong exit value in startup script\nThe startup script returns 1 when started with no "-p" option.\r\n\r\n```bash \r\n  else\r\n        exec "$JAVA" $JAVA_OPTS $ES_JAVA_OPTS $es_parms -cp $ES_CLASSPATH $props \\\r\n                    org.elasticsearch.bootstrap.ElasticSearch <&- &\r\n\r\n        [ ! -z "$pidpath" ] && printf \'%d\' $! > "$pidpath"\r\n    fi\r\n\r\n    return $?\r\n```\r\n\r\nThe return value of exec should be stored. Then checked.\r\nOnly then optional writing of the pid should be done.\r\n\r\n- Tom'
1728,'spinscale','debian package lib dir not found in target\n[ERROR] Failed to create debian package /tmp/es/elasticsearch/target/releases/elasticsearch-0.19.0.RC4-SNAPSHOT.deb\r\norg.vafer.jdeb.PackagingException: Failed to create debian package /tmp/es/elasticsearch/target/releases/elasticsearch-0.19.0.RC4-SNAPSHOT.deb\r\n\r\nCaused by: java.io.FileNotFoundException: Data source not found : /tmp/es/elasticsearch/target/lib\r\n\r\ncreated a symbolic link to ../lib to solve my issue.\r\n\r\nthank you,\r\nPierre.\r\n'
1726,'spinscale','debian package violates naming convention\nCheck out 7.3 here: http://www.debian.org/doc/manuals/debian-faq/ch-pkg_basics.html\r\n\r\ntl;dr the packages should be named elasticsearch_version-revision_arch.deb. Those underscores are dashes at the moment.'
1720,'javanna','SearchRequestBuilder.toString() prints only the body of the request\nAs the title says the toString() is not giving the complete picture of the request like the document type and index that is set in the requestbuilder. Right now its only printing the body of the request. '
1712,'martijnvg',"Percolators: Support delete-by-query\nThe error doesn't happen when I delete the queries one by one.\r\n\r\nIt only happens when you delete the percolator index itself for a index.\r\n\r\nGist for reproducing the error:\r\n\r\nhttps://gist.github.com/1678150\r\n\r\n"
1697,'spinscale','Split debian package in libelasticsearch and elasticsearch\nHello,\r\n\r\nWhen you have dependency on elasticsearch in java-app to instantiate client node for example, you rarely need to have an elasticssearch server running aside.\r\n\r\nMy idea is to package all the jars and lib dependencies in a "libelasticsearch" package to be deployed with java-app, and all the executables and init.d script in an "elasticsearch" package (depending on libelasticsearch) to be deployed on active cluster nodes.\r\n\r\nCheers.'
1695,'dadoonet','Creating an index and alias in the same request\nHi,\r\n\r\nAs discussed in the mailing list, here is a feature request.\r\n\r\nI would like to be able to define an alias for an index while I\'m creating the index.\r\n\r\n```\r\ncurl -XPUT http://localhost:9200/twitter/ -d \' \r\n{ \r\n  "settings": { \r\n    "number_of_shards": 3, \r\n    "number_of_replicas": 2 \r\n  }, \r\n  "alias": [ \r\n    "myalias" \r\n    ] \r\n} \r\n\'\r\n```\r\n\r\nIt would be nice to have in the Java API also :\r\n\r\n```java\r\nCreateIndexRequestBuilder cirb = client.admin().indices().prepareCreate(index);\r\ncirb.setSettings(source);\r\n// New method here\r\ncirb.setAlias("myalias");\r\ncirb.execute().actionGet();\r\n```\r\n\r\nHere is the signature expected for setAlias method : ```setAlias(String... alias)```\r\n\r\nDiscussion here : https://groups.google.com/group/elasticsearch/browse_thread/thread/f60bff3fda0f93dc?hl=fr'
1678,'dakrone','A content decompressor that throws a human readable message when compression is disabled and the user sends compressed content\nCurrently the user gets an obscure error message about content that cannot be decoded because ES handles the compressed content as uncompressed content.\r\nI personally think that we should not care about broken clients and respond to requests according to the HTTP specs.\r\nCompression is very useful when you use the bulk-API btw....'
1661,'spinscale','Bounding Box Query is slow\nHi there ,\r\n i have been writing about this issue in the elastic search google groups , but unfortunately no one address the issue.\r\nSo i am creating the Ticket here .\r\nProblem : \r\nSetup  4 core cpu with 24gb of ram .\r\nIndex size is 10gb , Using ES latest version \r\nNow when i have no where clause in the query its gives results in no time say 20 ms. but when i added BoundingBox filter its started giving result in > 200 ms .\r\nPlease correct me if i am wrong , i have used the same example you guys have mentioned in the Elastic Search (GeoBoundingBoxTests.java).\r\nDo i have to do something else or is this the limitation of ES.\r\nRegards\r\nPrashant \r\n\r\n '
1646,'spinscale','JSON REF\nSupport indexing and query traversal through inter-document references according to the JSON-REF ietf proposal \r\n* http://tools.ietf.org/html/draft-pbryan-zyp-json-ref-00\r\n* http://json-schema.org/json-ref\r\n* http://livedocs.dojotoolkit.org/dojox/json/ref\r\n* http://www.sitepen.com/blog/2008/06/17/json-referencing-in-dojo/\r\n\r\n'
1642,'jpountz','Facets incorrect when scrolling\nWhen requesting facets on a scrolled search, the counts just keep rising, incorrectly.\r\n\r\nI\'d say it makes more sense to only return the facets on the first request, not on subsequent scroll requests\r\n\r\n\r\n    curl -XPUT \'http://127.0.0.1:9200/test/?pretty=1\' \r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 111 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 112 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 113 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 114 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 115 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 116 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 117 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 118 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 119 }\'\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \' { "num" : 120 }\'\r\n\r\nThen run facets:\r\n\r\n    curl -XGET \'http://127.0.0.1:9200/test/foo/_search?scroll=1m&pretty=1\'  -d \'\r\n    {\r\n       "facets" : {\r\n          "num" : {\r\n             "terms" : {\r\n                "field" : "num"\r\n             }\r\n          }\r\n       },\r\n       "size" : 1\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 19:58:12 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 113\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "ApvhoFAnTTayEyB4NHdMGw",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 111\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 10\r\n    #       }\r\n    #    },\r\n    #    "took" : 4\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 116\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "bJUOIFylTBqyTS4kSzeIcw",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 16\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 117\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "QVlLmSz6QUm7zi3WthUntQ",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 22\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 119\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "OdDkJXgRTPOsvbYA4PjXtg",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 28\r\n    #       }\r\n    #    },\r\n    #    "took" : 0\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 114\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "f24bgaz7ROC1jTldU6UGiQ",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 34\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 111\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "B0u6QEKZSKCwaV5-uqkoFQ",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 6,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 40\r\n    #       }\r\n    #    },\r\n    #    "took" : 0\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 112\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "3Gho6ITESYmIDdqCIpWWPg",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 7,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 46\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 118\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "gwtal_yVRhqx8ACAOU2Dog",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 8,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 52\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 120\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "_tcnUkiFR2mxw6Puzll14A",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 9,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 58\r\n    #       }\r\n    #    },\r\n    #    "took" : 0\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 115\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "G0Te7qRFRWSq9XzyLNb74g",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 10,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 64\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 19:58:21 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OTpXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7\' \r\n\r\n    # [Thu Jan 26 19:58:21 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [],\r\n    #       "max_score" : 1,\r\n    #       "total" : 10\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTs5NjpXSWkxTzRiVVFIS2EtUz\r\n    # >    JlVnFXdXFnOzk3OldJaTFPNGJVUUhLYS1TMmVWcVd1cWc7OTg6V0lpMU80Yl\r\n    # >    VRSEthLVMyZVZxV3VxZzsxMDA6V0lpMU80YlVRSEthLVMyZVZxV3VxZzs5OT\r\n    # >    pXSWkxTzRiVVFIS2EtUzJlVnFXdXFnOzA7",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 120\r\n    #             },\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 118\r\n    #             },\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 115\r\n    #             },\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 114\r\n    #             },\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 112\r\n    #             },\r\n    #             {\r\n    #                "count" : 11,\r\n    #                "term" : 111\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 119\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 117\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 116\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 113\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 70\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:12 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XPUT \'http://127.0.0.1:9200/test/?pretty=1\' \r\n\r\n    # [Thu Jan 26 20:02:12 2012] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "acknowledged" : true\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \'\r\n    {\r\n       "num" : 1\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "_index" : "test",\r\n    #    "_id" : "4kjF8ezXQUSTdm0wqHCDmg",\r\n    #    "_type" : "foo",\r\n    #    "_version" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \'\r\n    {\r\n       "num" : 2\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "_index" : "test",\r\n    #    "_id" : "0AmRRD58QnWE-hyeRF11Kw",\r\n    #    "_type" : "foo",\r\n    #    "_version" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \'\r\n    {\r\n       "num" : 3\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "_index" : "test",\r\n    #    "_id" : "whjOB26XQFq5eiYiThNk-Q",\r\n    #    "_type" : "foo",\r\n    #    "_version" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XPOST \'http://127.0.0.1:9200/test/foo?pretty=1\'  -d \'\r\n    {\r\n       "num" : 4\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 20:02:15 2012] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "_index" : "test",\r\n    #    "_id" : "Ax7l0JEWTgWZUYsUcl4oog",\r\n    #    "_type" : "foo",\r\n    #    "_version" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:18 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/test/foo/_search?scroll=1m&pretty=1\'  -d \'\r\n    {\r\n       "facets" : {\r\n          "num" : {\r\n             "terms" : {\r\n                "field" : "num"\r\n             }\r\n          }\r\n       },\r\n       "size" : 1\r\n    }\r\n    \'\r\n\r\n    # [Thu Jan 26 20:02:18 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 3\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "whjOB26XQFq5eiYiThNk-Q",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 4\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV\r\n    # >    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU\r\n    # >    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz\r\n    # >    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 4\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 3\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 2\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 1\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 4\r\n    #       }\r\n    #    },\r\n    #    "took" : 2\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D\' \r\n\r\n    # [Thu Jan 26 20:02:20 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 2\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "0AmRRD58QnWE-hyeRF11Kw",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 4\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV\r\n    # >    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU\r\n    # >    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz\r\n    # >    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 4\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 2\r\n    #             },\r\n    #             {\r\n    #                "count" : 2,\r\n    #                "term" : 1\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 3\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 7\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D\' \r\n\r\n    # [Thu Jan 26 20:02:20 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 4\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "Ax7l0JEWTgWZUYsUcl4oog",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 4\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV\r\n    # >    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU\r\n    # >    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz\r\n    # >    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 4\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 2\r\n    #             },\r\n    #             {\r\n    #                "count" : 3,\r\n    #                "term" : 1\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 3\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 10\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D\' \r\n\r\n    # [Thu Jan 26 20:02:20 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [\r\n    #          {\r\n    #             "_source" : {\r\n    #                "num" : 1\r\n    #             },\r\n    #             "_score" : 1,\r\n    #             "_index" : "test",\r\n    #             "_id" : "4kjF8ezXQUSTdm0wqHCDmg",\r\n    #             "_type" : "foo"\r\n    #          }\r\n    #       ],\r\n    #       "max_score" : 1,\r\n    #       "total" : 4\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV\r\n    # >    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU\r\n    # >    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz\r\n    # >    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 4\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 2\r\n    #             },\r\n    #             {\r\n    #                "count" : 4,\r\n    #                "term" : 1\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 3\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 13\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n\r\n    # [Thu Jan 26 20:02:20 2012] Protocol: http, Server: 192.168.5.10:9200\r\n    curl -XGET \'http://127.0.0.1:9200/_search/scroll?scroll=1m&pretty=1&scroll_id=cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw%3D%3D\' \r\n\r\n    # [Thu Jan 26 20:02:20 2012] Response:\r\n    # {\r\n    #    "hits" : {\r\n    #       "hits" : [],\r\n    #       "max_score" : 1,\r\n    #       "total" : 4\r\n    #    },\r\n    #    "timed_out" : false,\r\n    #    "_shards" : {\r\n    #       "failed" : 0,\r\n    #       "successful" : 5,\r\n    #       "total" : 5\r\n    #    },\r\n    #    "_scroll_id" : "cXVlcnlUaGVuRmV0Y2g7NTsxMTE6V0lpMU80YlVRSEthLV\r\n    # >    MyZVZxV3VxZzsxMTI6V0lpMU80YlVRSEthLVMyZVZxV3VxZzsxMTM6V0lpMU\r\n    # >    80YlVRSEthLVMyZVZxV3VxZzsxMTU6V0lpMU80YlVRSEthLVMyZVZxV3VxZz\r\n    # >    sxMTQ6V0lpMU80YlVRSEthLVMyZVZxV3VxZzswOw==",\r\n    #    "facets" : {\r\n    #       "num" : {\r\n    #          "other" : 0,\r\n    #          "terms" : [\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 4\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 2\r\n    #             },\r\n    #             {\r\n    #                "count" : 5,\r\n    #                "term" : 1\r\n    #             },\r\n    #             {\r\n    #                "count" : 1,\r\n    #                "term" : 3\r\n    #             }\r\n    #          ],\r\n    #          "missing" : 0,\r\n    #          "_type" : "terms",\r\n    #          "total" : 16\r\n    #       }\r\n    #    },\r\n    #    "took" : 1\r\n    # }\r\n'
1494,'imotov','Infinite loop in DynamicChannelBuffer.ensureWritableBytes\nFrames larger than 1GB can trigger an infinite loop in [DynamicChannelBuffer.ensureWritableBytes](\r\nhttps://github.com/netty/netty/blob/3.2/src/main/java/org/jboss/netty/buffer/DynamicChannelBuffer.java#L80). It happens because of integer overflow in newCapacity calculation for any minNewCapacity > 1073741824.\r\n\r\nTo reproduce:\r\n\r\n1) Start ES with enough memory to accomodate 1GB of data.\r\n\r\n    $ ES_MAX_MEM=2048m bin/elasticsearch -f\r\n\r\n2) Execute the following python script \r\n\r\n```Python\r\nimport socket\r\nimport struct\r\n\r\nsize = 1073741824\r\nchunk = 1024\r\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\r\ns.connect(("localhost", 9300))\r\n# Send an integer equal to or larger than 1073741824 or large \r\n# so SizeHeaderFrameDecoder would start collecting data\r\ns.send(struct.pack("!l",size))\r\n# followed by at least 1GB of data to trigger overflow\r\nfor x in range(size / chunk):\r\n\ts.send(" " * chunk)\r\ns.close()\r\n```\r\n\r\n3) Run jstack on elasticsearch pid\r\n\r\n```\r\n$ jstack 5475 | grep -A 20 -B 2 ensureWritableBytes \r\n"New I/O server worker #1-8" daemon prio=5 tid=112092000 nid=0x10d921000 runnable [10d920000]\r\n   java.lang.Thread.State: RUNNABLE\r\n\tat org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.ensureWritableBytes(DynamicChannelBuffer.java:81)\r\n\tat org.elasticsearch.common.netty.buffer.DynamicChannelBuffer.writeBytes(DynamicChannelBuffer.java:239)\r\n\tat org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.writeBytes(AbstractChannelBuffer.java:457)\r\n\tat org.elasticsearch.common.netty.buffer.AbstractChannelBuffer.writeBytes(AbstractChannelBuffer.java:450)\r\n\tat org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:213)\r\n\tat org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:80)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:783)\r\n\tat org.elasticsearch.common.netty.OpenChannelsHandler.handleUpstream(OpenChannelsHandler.java:81)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564)\r\n\tat org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:274)\r\n\tat org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:261)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)\r\n\tat org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)\r\n\tat org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)\r\n\tat org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:44)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)\r\n\tat java.lang.Thread.run(Thread.java:680)\r\n```\r\n\r\nThis issue was observed on elasticsearch 0.17.7 during shard relocation.\r\n'
1441,'dadoonet','deleting CouchDB river can leave some docs behind\nI\'m running into a problem trying to atomically reindex a couchdb river. It seems that deleting the couchdb river can occasionally leave behind the `_seq` document. I believe this is happening because the river shutdown process isn\'t waiting for the couchdb\'s river indexer thread to exit before deleting all the couchdb river docs.\r\n\r\nHere\'s the code to close the river:\r\n\r\n    @Override public void close() {\r\n        if (closed) {\r\n            return;\r\n        }\r\n        logger.info("closing couchdb stream river");\r\n        slurperThread.interrupt();\r\n        indexerThread.interrupt();\r\n        closed = true;\r\n    }\r\n\r\nAfter this function exits, ES goes off to delete all the river documents. However, because the plugin isn\'t joining on the subthreads, it\'s possible for the indexer thread to miss the interrupt until after it\'s updated the `_seq` doc;\r\n\r\n               ....\r\n               } catch (InterruptedException e) {\r\n                    if (closed) {\r\n                        return;\r\n                    }\r\n                }\r\n\r\n                // thread halts here.\r\n\r\n                if (lastSeq != null) {\r\n                    try {\r\n                        if (logger.isTraceEnabled()) {\r\n                            logger.trace("processing [_seq  ]: [{}]/[{}]/[{}], last_seq [{}]", riverIndexName, riverName.name(), "_seq", lastSeq);re\r\n                        }\r\n                        bulk.add(indexRequest(riverIndexName).type(riverName.name()).id("_seq")\r\n                                .source(jsonBuilder().startObject().startObject("couchdb").field("last_seq", lastSeq).endObject().endObject()));\r\n                    } catch (IOException e) {\r\n                        logger.warn("failed to add last_seq entry to bulk indexing");\r\n                    }\r\n                }\r\n                ...\r\n\r\nIf everything happens just right, ES will delete the plugin subdocuments, and this thread will recreate the `_seq` file.\r\n\r\nIt looks like the other river plugins use threads, but I haven\'t checked if they\'re also susceptible to this bug.'
1314,'s1monw','Highlighting - does not work with custom_score or boosting\nreported by: https://github.com/nkvoll\r\n\r\nHighlighting does not seem to work with custom_score or boosting (and maybe others, but these were the one I just tested).\r\n\r\nExample (from https://gist.github.com/9f003179c02ebe9b69ae ):\r\n\r\n    curl -XDELETE http://localhost:9200/test?pretty\r\n    curl -XPOST \'http://localhost:9200/test/my_type/1?pretty=1&refresh=true\' -d \'\r\n    {\r\n        "foo": {\r\n            "bar": "this is bar"\r\n        }\r\n    }\'\r\n\r\n    # works as expected:\r\n    curl http://localhost:9200/test/_search?pretty -XPOST -d \'{\r\n        "query": {"query_string": {"query":"bar"}},\r\n        "highlight": {"fields": {"foo.bar":{}}}\r\n    }\'\r\n\r\n\r\n    # does not highlight:\r\n    curl http://localhost:9200/test/_search?pretty -XPOST -d \'{\r\n        "query": {\r\n            "boosting": {\r\n                "positive": {\r\n                    "query_string": {\r\n                        "query":"bar"\r\n                    }\r\n                },\r\n                "negative": {"term": {"foo.bar": "baz"}},\r\n                "negative_boost": 0.9\r\n            }\r\n        },\r\n        "highlight": {"fields": {"foo.bar":{}}}\r\n    }\'\r\n\r\n\r\n    # does not highlight either:\r\n    curl http://localhost:9200/test/_search?pretty -XPOST -d \'{\r\n        "query": {\r\n            "custom_score": {\r\n                "query": {\r\n                    "query_string": {\r\n                        "query":"bar"\r\n                    }\r\n                },\r\n                "script": "_score"\r\n            }\r\n        },    \r\n        "highlight": {"fields": {"foo.bar":{}}}\r\n    }\'    '
1275,'spinscale','Geofacet with boundingbox\nCreate a geo facet that uses a boundingbox instead of distance. \r\n\r\n```javascript\r\n{\r\n  "geo_bbox": {\r\n       "bbox": [-180,90,180,-90],\r\n       "spacing": {\r\n           "lat": 0.0006993,\r\n           "lon": 0.0006993\r\n       },       \r\n       "key_field": "location",\r\n       "value_field": "propery_value"\r\n  }\r\n}\r\n```\r\nThe result is expanded to:\r\n\r\n```javascript\r\n"geotest" : {\r\n    "_type" : "geo_bbox",\r\n    "missing" : 0,\r\n    "bbox": [-180,90,180,-90],\r\n    "rows": 200,\r\n    "cols": 200,\r\n    "data" : [\r\n        {\r\n          "bbox": [ 72, 18, 72.5, 17.5 ],\r\n          "center": { \r\n               "lat":  17.75,\r\n               "lon": 72.25\r\n          },\r\n         "count": 15168,\r\n         "total_count": 14897,\r\n         "min": 6500,\r\n         "max": 225000000,\r\n         "total": 37108379266,\r\n         "mean": 2490996.795730684\r\n    } \r\n    ]\r\n  }\r\n\r\n```\r\n\r\nThe idea is to provide a boundingbox and a gridsize/spacing. The spacing and outer boundingbox is expanded to {row} x {column} number of boundingboxes which is used to test the key against. The value_field (script_field etc) is used for calculations in the same manner as in term/term_stat facet.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'
1171,'javanna','Highlighting returning an excerpt even with no highlights\nHighlighting is great as providing an *highlighted excerpt*, but if you run no query against one field, you will have no output at all.\r\nHighlighting could provide a non highlighted excerpt (some number of characters from the beginning of the value) as a fallback.\r\nThis would be yield shorted answers than always asking for the whole field content (that may be quite long).\r\n\r\n```json\r\n{\r\n  "highlight" : {\r\n    "fields" : {\r\n      "text" : {\r\n        "number_of_fragments" : 3,\r\n        "size_of_fragments" : 150,\r\n        "no_results" : VALUE\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWhere *VALUE* could take: `hide` (default), `show`, or a character count.\r\nWhere `show` would be equivalent to setting the character count to `number_of_fragments * size_of_fragments`.'
1109,'spinscale','JMS River\nElasticSearch has a river for [RabbitMQ](http://www.elasticsearch.org/guide/reference/river/rabbitmq.html). However, it would be nice to have a generic river that leverages JMS. This way JMS providers such as MQ, TIBCO EMS etc can be indexed. \r\n\r\nIdeally, it should support the same format as the bulk api. However, ideally an XML representation would be great. Alternatively just being able to index the JMS message directly is another option. Support for TOPICS and QUEUES will be needed. \r\n\r\nBe glad to help develop this solution.'
1072,'s1monw','fragment_size doesn\'t work with quoted phrase\nI nave query with highlight like this: (Here is curl recreation https://gist.github.com/1032233 )\r\nLook at issued result ( item4.description ) (you can see all doc also in  _source ), and mapping.\r\n\r\nif i supply query:\r\ntwo words\r\nitem.description returns only 128 chars, as expected. But if\r\n"two words"\r\nit returns the same 128 chars and all remainder of that field. Bit strange.\r\n\r\nES (v 0.16.0)\r\n\r\nPlease tell, can I do the trick with highlight fields like ["item*.title", "item*.description"]  in query fields?\r\n\r\nI understand, it looks bit strange. I’m try to explain:\r\nAll my docs contains 10 items with title and description (and so on, not important). I need to have 1 match per field, so my first version with just array of items wont work. If i setup number_of_fragments to 1 it returns _only_ 1 result for all 10 items. If i setup number_of_fragments to 0 it returns all description concatenated in 1 highlight. So solution is to make item1, item2 etc.\r\nIs it best solution?\r\n\r\nPS No any reaction on my 3 messages after first at mailing list. I believe this is a bug, so i created this issue.'
988,'uboness','why no RegexQueryBuilder?\nI need to do some regex search but can not find a QueryBuilder like \r\nRegexQueryBuilder. \r\nhow should i do regex search under ES?'
898,'spinscale','Silent failure when heap larger than available memory\nSimple issue...\n\nIf I set the heap min greater than available memory (eg, 8GB on a 7.5GB instance) elasticsearch will quietly fail to start.\n\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ "/var/assistly/elasticsearch/bin/service/exec/elasticsearch-linux-x86-64" "/var/assistly/elasticsearch/bin/service/elasticsearch.conf" wrapper.syslog.ident="elasticsearch" wrapper.pidfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.pid" wrapper.name="elasticsearch" wrapper.displayname="ElasticSearch" wrapper.daemonize=TRUE   wrapper.statusfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.status" wrapper.java.statusfile="/var/assistly/elasticsearch-0.16.0-SNAPSHOT/bin/../../elasticsearch-servicewrapper/service/./elasticsearch.java.status"\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ echo $?\n    0\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ ps auxw | grep elastic | grep -v grep\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ ls /mnt/elasticsearch/profiling/\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ tail elasticsearch.log\n    [2011-05-03 04:18:25,593][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: stopping ...\n    [2011-05-03 04:18:25,631][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: stopped\n    [2011-05-03 04:18:25,632][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: closing ...\n    [2011-05-03 04:18:26,047][INFO ][node                     ] [Anvil] {elasticsearch/0.16.0-SNAPSHOT/2011-03-12T19:52:17}[8622]: closed\n    PROFILING: Search 4:ubuntu@ip-10-72-65-201:/var/assistly/elasticsearch/logs$ date\n    Tue May  3 05:14:56 UTC 2011\n\nThe command line I used is the same command line generated by the service wrapper.\n\nNotice the exit status is 0, there are no errors logged, etc.  I assume this is to do with the fact that the JVM itself is the one taking care of the allocation?\n\nIf that\'s the case, is it possible to pass additional options to the JVM to tell it to complain loudly so that the service wrapper will pick it up?\n\nCheers,\nDavid.'
896,'jpountz','Sort: Support "missing" specific handling, include _last, _first, and custom value (for string values)\nThis is a follow up to #772 that supports special sorting for \'null\' values for numeric fields. The same would be very useful for (not analyzed) string fields as well. If feasible, special handling for empty and blank strings would be useful. The latter is however optional as it is possible to achieve uniform handling by filtering out blank values at indexing time.'
777,'brwe','Conflicts in put mapping only reported for properties field\nWhen putting a mapping, if there is a conflict in `properties`, an exception is thrown.  However, any conflicts in the other parameters ( eg `_all`, `dynamic` etc) are silently ignored.\r\n\r\nThese should also throw errors.\r\n\r\n    # [Sat Mar 12 19:38:18 2011] Protocol: http, Server: 127.0.0.1:9200\r\n    curl -XPUT \'http://127.0.0.1:9200/foo/\' \r\n\r\n    # [Sat Mar 12 19:38:18 2011] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "acknowledged" : true\r\n    # }\r\n\r\n    # [Sat Mar 12 19:38:21 2011] Protocol: http, Server: 127.0.0.1:9200\r\n    curl -XPUT \'http://127.0.0.1:9200/foo/bar/_mapping\'  -d \'\r\n    {\r\n       "bar" : {\r\n          "_all" : {\r\n             "enabled" : 0\r\n          },\r\n          "dynamic" : 0,\r\n          "properties" : {\r\n             "text" : {\r\n                "type" : "string"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # [Sat Mar 12 19:38:21 2011] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "acknowledged" : true\r\n    # }\r\n\r\n    # [Sat Mar 12 19:38:24 2011] Protocol: http, Server: 127.0.0.1:9200\r\n    curl -XPUT \'http://127.0.0.1:9200/foo/bar/_mapping\'  -d \'\r\n    {\r\n       "bar" : {\r\n          "_all" : {\r\n             "enabled" : 1\r\n          },\r\n          "dynamic" : 1,\r\n          "properties" : {\r\n             "text" : {\r\n                "type" : "string"\r\n             }\r\n          }\r\n       }\r\n    }\r\n    \'\r\n\r\n    # [Sat Mar 12 19:38:24 2011] Response:\r\n    # {\r\n    #    "ok" : true,\r\n    #    "acknowledged" : true\r\n    # }\r\n\r\n'
761,'martijnvg','Feature Request:  The ability to "join" parent and children\nThere are many times I would like both the parent and children of a record.  Currently to find the children of a query (even a has_child query) requires an individual GET for each returned record.\n\n1)  The simplest solution may be to enhance the has_child query, which already specifies parent and children types, allowing the actual children to be returned along with the parents.\n\n2)  Enhance the query DSL to allow the children/parents of any query results to be joined and returned.  Perhaps even allowing additional filtering.\n\n3)  Add a join API call.'
692,'spinscale','Inconsistent response behavior between _search and _count\nWhen a REST call with an invalid query is made to a \\_search URL in ElasticSearch, it returns a response with a status of "500 Internal Server Error". When the same invalid query is made on "\\_count", it returns a response with a status of "200 OK", with a simillar error message in the body. I believe the \\_count call should return exactly the same 500 status response as the \\_search call, if possible, to keep their behavior consistent and make the display of errors in the client side easier to implement.'
504,'spinscale','Add a Maximum size for attachements (reject on failure)\nFor an attachment field, specify a maximum allowed attachment size.  Attachments bigger than the size can result in error, or silent ignoring of the attachment (option?  if not just go with error)'
8611,'dakrone','[TRANSLOG] Cut over to Path API\nThis commit also refactors `RAFReference` to work on top of a Channel instead of fetching the channel from the RandomAccessFile. The channel is created to truncate the file it opens too. This likely needs some documentation or we should allow the caller to pass the open options but I wanted to get this change out for review.'
8609,'rmuir','[GATEWAY] Cut over MetaDataStateFormat to Path API\n'
8588,'mikemccand','Core: upgrade to current Lucene 5.0.0 snapshot\nA few recent lucene changes to fold in:\r\n\r\nWe\'ve removed IndexWriter.unLock in Lucene, so I also removed "clear\r\nlock on startup" in Elasticsearch.  A couple test cases were missing\r\nengine.close() before opening a new engine.\r\n\r\nI exposed the new SerbianNormalizationFilterFactory.\r\n\r\nI noticed that we silently use NoLockFactory if you have a typo in\r\nyour index.store.fs.fs_lock; I changed this to throw\r\nStoreException instead.  I think it\'s scary to be lenient\r\nhere?\r\n\r\nThe trickiest change was DistributorDirectory, and how it tracks the\r\nfile created by the lock factory.  For the writeFiles boolean, I had\r\nto remove the check that "lockFactory instanceof FSLockFactory"; maybe\r\nwe could use reflection to get this, if it\'s really necessary?  (Do we\r\nreally support using NoLockFactory w/ FSDirectory?).  Or maybe instead\r\nwe could check if the class name of the lock/factory is NoLock, but\r\nMockDirWrapper wraps in AssertingLock...\r\n\r\nI added some harmless "synchronized" to methods (all callers of these\r\nprivate methods are already sync\'d) just to make it clear on quick\r\nlook that all access/changes to nameDirMapping are in fact sync\'d.\r\n\r\nI also fixed ConcurrentMergeSchedulerProvider to disable Lucene\'s CMS\r\nstalling (override w/ an empty maybeStall method).  We can simplify\r\nthings here: remove EnableMergeScheduler, the separate MERGE thread\r\npool, let IndexWriter launch merges when they are needed (not\r\nseparately, once every second), etc., since Lucene won\'t stall\r\nindexing threads anymore ... I\'ll open a separate issue for this.\r\n'
8576,'jpountz','ScriptDocValues.getValues() returns an reused list\nUsing ElasticSearch 1.4.0 from your official .deb, given an index with:\r\n\r\n    "plays": {\r\n        "mappings": {\r\n            "play": {\r\n                "_source": {\r\n                    "enabled": false\r\n                },\r\n                "_timestamp": {\r\n                    "default": null,\r\n                    "enabled": true\r\n                },\r\n                "dynamic": "strict",\r\n                "properties": {\r\n                    "artist": {\r\n                        "index": "not_analyzed",\r\n                        "type": "string"\r\n                    },\r\n\r\nAnd:\r\n    \r\n    "settings": {\r\n        "index": {\r\n            "creation_date": "1416491787644",\r\n            "merge": {\r\n                "scheduler": {\r\n                    "max_thread_count": "1"\r\n                }\r\n            },\r\n            "number_of_replicas": "1",\r\n            "number_of_shards": "1",\r\n            "refresh_interval": "-1",\r\n            "uuid": "XDRbDOoLSR-cMFjQAm9TjQ",\r\n            "version": {\r\n                "created": "1040099"\r\n            }\r\n        }\r\n    }\r\n\r\nA search like:\r\n\r\n    {\r\n        "script_values": {\r\n            "artist": {\r\n                "script": "_doc[\'artist\'].values"\r\n            }\r\n        }\r\n    }\r\n\r\nWill return a result set whose hits contain an `artist` array whose elements correspond to the elements for the last search result, i.e. all previous hits `artist` arrays assume the same contents as that of the last hit. It looks like an object is being reused somehow, although casting a glance at ScriptDocValues.java I can\'t see how. Is it possible `listLoaded` is not being reset somehow?\r\n\r\nA trivial workaround is:\r\n\r\n    {\r\n        "script_values": {\r\n            "artist": {\r\n                "script": "_doc[\'artist\'].values.take(100)"\r\n            }\r\n        }\r\n    }\r\n'
8545,'s1monw',"Snapshot/Restore: restore with wait_for_completion=true should wait for succesfully restored shards to get started\nThis commit ensures that restore operation with wait_for_completion=true doesn't return until all successfully restored shards are started. Before it was returning as soon as restore operation was over, which cause some shards to be unavailable immediately after restore completion.\r\n\r\nFixes #8340"
8538,'martijnvg','disk.watermark.high relocates all shards creating a relocation storm\nI have a relatively large cluster running ES 1.3.5 and we recently started to get low on disk space. The cluster has roughly 110 TB usable space of which about 86TB is used. We have 93 indices of which 90 are rotating daily indices and the other 3 are permanent. With this setup we end up with shards that range in size from a few MB to over 160GB and we\'re finding the shard count based allocation strategy results in some nodes starting to run out of space earlier than expected which trips the cluster.routing.allocation.disk.watermark.high setting.\r\n\r\nIt appears when this happens that "all" of the shards from that node are relocated. This then puts more pressure on the other nodes and at some point another node will trip cluster.routing.allocation.disk.watermark.high and relocate all of its shards too. This then goes on and on and on and never stops until we intercede and manually cancel the allocations. At times we\'ve seen more than 200 relocations in progress at the same time.\r\n\r\nIs the relocation of "all" shards really the intended behavior? In looking at the source code it appears to do a reroute in this scenario but this seems like the last thing you would want to do. You\'re almost guaranteed that if one node is tripping that setting then others are going to be close as well. In our cluster we have room on many of the other nodes and if it selectively moved a shard or two everything would be fine but moving all of them is really problematic.\r\n\r\nIt also seems that when all these relocations occur that cluster.routing.allocation.disk.watermark.low doesn\'t prevent the overallocation of other nodes. I believe this is occurring because of the number of relocations that are started at once so the node gets asked multiple times to accept shards within a short time period and all requests succeed because no pending relocations are considered in respect to disk utilization. This pretty much guarantees that node will eventually trip cluster.routing.allocation.disk.watermark.high and continue the cycle even though there are other nodes in the cluster with hundreds of GB free. \r\n\r\nWe\'re in the process of adding additional capacity to our cluster so we\'re not at risk of bumping into these limits but otherwise this behavior seems quite problematic.'
8521,'colings86','[GEO] Fix for geo_shape query with polygon from -180/90 to 180/-90\nThis fix adds a simple consistency check that intersection edges appear pairwise. Polygonal boundary tests were passing (false positive) on the Eastern side of the dateline simply due to the initial order (edge direction) of the intersection edges.  Polygons in the Eastern hemispehere (which were not being tested) were correctly failing inside of JTS due to an attempt to connect incorrect intersection edges (that is, edges that were not even intersections). While this patch fixes issue/8467 (and adds broader test coverage) it is not intented as a long term solution.  The mid term fix (in work) will refactor all geospatial computational geometry to use ENU / ECF coordinate systems for higher accuracy and eliminate brute force mercator checks and conversions.\r\n\r\nCloses #8467'
8517,'s1monw','[CI Failure] CorruptedFileTest.testCorruptFileAndRecover\nOccurred in same test run as #8516 and #8515\r\n\r\nhttp://build-us-1.elasticsearch.org/job/es_core_13_debian/2379/\r\n```java\r\njava.lang.NullPointerException\r\n\tat org.elasticsearch.test.ElasticsearchIntegrationTest.client(ElasticsearchIntegrationTest.java:603)\r\n\tat org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:541)\r\n\tat org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1566)\r\n\tat sun.reflect.GeneratedMethodAccessor44.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:483)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1617)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:884)\r\n\tat org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\r\n\tat org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\r\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n\tat org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\r\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\r\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:783)\r\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:443)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:835)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:737)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:771)\r\n\tat com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:782)\r\n\tat org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n\tat org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\r\n\tat com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n\tat com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n\tat org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\r\n\tat org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n\tat org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\r\n\tat org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\r\n\tat com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n\tat com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:359)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```'
8497,'javanna','[CI Failure] IndicesRequestTests.testIndicesStats failure\nhttp://build-us-1.elasticsearch.org/job/es_core_14_window-2012/304/\r\n\r\n```\r\njava.lang.AssertionError: org.elasticsearch.action.admin.indices.stats.TransportIndicesStatsAction$IndexShardStatsRequest\r\nExpected: ["test1", "test0", "test0"]\r\n     got: []\r\n\r\n\tat __randomizedtesting.SeedInfo.seed([C97289D5AE27CFBB:D82FFC40EC13878D]:0)\r\n\tat org.junit.Assert.assertThat(Assert.java:780)\r\n\tat org.elasticsearch.action.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:816)\r\n\tat org.elasticsearch.action.IndicesRequestTests.assertSameIndices(IndicesRequestTests.java:801)\r\n\tat org.elasticsearch.action.IndicesRequestTests.testIndicesStats(IndicesRequestTests.java:485)\r\n```'
8493,'spinscale','Lack of systemd init files in deb package\nHello,\r\n\r\nIs it possible to add files which are needed to start the service with systemd in elasticsearch deb package?\r\nThis will allow to start "normally" elasticsearch under latest Debian-based Linux distributions which switched recently from system-V to systemd init system. As I understand such files (mostly elasticsearch.service) are included already in RPM packages and they have to be just slightly modified.\r\n\r\nThanks,\r\n  Alexandr '
8491,'clintongormley','Reword note regarding _source for accuracy\nPreviously it suggested _source was always present, when that is not the case.'
8490,'clintongormley','Date Range Filter unclear rounding behavior\nIf I run the query :\r\n```\r\n{\r\n  "size": 14,\r\n  "_source": "context.data.eventTime",\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "bool": {\r\n          "must": [\r\n            {\r\n              "range": {\r\n                "context.data.eventTime": {\r\n                  "gte": "2014-11-12T14:54:59.000Z",\r\n                  "lte": "2014-11-12T14:55:00.000Z"\r\n                }\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI get :\r\n```\r\n{\r\n    "took": 0,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 1,\r\n        "successful": 1,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 3,\r\n        "max_score": 1,\r\n        "hits": [\r\n            {\r\n                "_index": "test",\r\n                "_type": "request",\r\n                "_id": "4a5acb28-e6e8-4dfc-b86f-80926ed82fd7",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "context": {\r\n                        "data": {\r\n                            "eventTime": "2014-11-12T14:55:00.1458607Z"\r\n                        }\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "_index": "test",\r\n                "_type": "request",\r\n                "_id": "22a6d539-06a7-4dd9-860c-2ea5ed0956cf",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "context": {\r\n                        "data": {\r\n                            "eventTime": "2014-11-12T14:55:00.5976447Z"\r\n                        }\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "_index": "test",\r\n                "_type": "request",\r\n                "_id": "554b25f3-36a9-4f84-b2a3-69f257cf1ac0",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "context": {\r\n                        "data": {\r\n                            "eventTime": "2014-11-12T14:55:00.9979586Z"\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n```\r\n\r\neventTime has mappings :\r\n```\r\n"eventTime": {\r\n                "type": "date",\r\n                "format": "dateOptionalTime"\r\n              }\r\n```\r\n\r\nShouldn\'t no records match ?\r\n\r\nInterestingly if I run the same query with the range filter using \'lt\' instead of a \'lte\' I get no records matched.\r\n'
8487,'bleskes','ElasticSearch 1.3.4 recovery slow on larger clusters (50+ total nodes)\nWe are seeing a situation on clusters running 1.3.4 with greater than 50 total nodes where shard recovery/allocation is either failing or is VERY slow.\r\n\r\nFull details:\r\nI currently have 6 clusters, 4 with 24 total nodes and 2 larger clusters with 53 & 63 nodes respectively.  Everything is run on VMs running Windows Server 2012 R2 within Azure.\r\n\r\nAll clusters (except the 63 node) were upgraded from 1.3.2 to 1.3.4 using an offline method of shutting down the cluster clean, swapping out the x64 Windows Service, then restarting all of the nodes.  The 63 node cluster was built last week clean with 1.3.4.\r\n\r\nFor the 24 node clusters, the clusters returned to a green status after the upgrade in less than 30 minutes and are performing very well.  I also did a rolling reboot of all of the machines to apply OS updates where I have automation that sets allocation to new_primaries, shuts down the service, reboots the machine, waits for the node to rejoin the cluster, sets allocation back to all, then waits for the cluster to return to green before proceeding to the next node.  Again for each of the 24 node clusters, it completed the entire process in just shy of 2 hours.\r\n\r\nThe 53 node cluster is our oldest cluster (with 144 indices & 3330 shards) and has under gone several upgrades using the offline method and in each scenario, the cluster returned to yellow within 15 minutes and green within 40 minutes.  Monday night we upgraded this cluster and it took 3 hours to get to yellow and 6 hours to get to green.\r\n\r\nThe 63 node cluster is our newest cluster (currently has 12 indices & 272 shards).  Last night & today while performing a rolling reboot, each node has a max of 3 shards on it, after the machine rebooted, it was taking > 10 minutes for the node to rejoin the cluster and randomly some shards would never finish initialization.\r\n\r\nWhen I query _cat/recovery?pretty=true&v&active_only=true, no shards are listed however _cat/shards would show 1 or 2 shards as INITIALIZING.  If I issued the command for reroute cancel on the initializing shards, they would almost immediately allocation and the cluster would turn green.\r\n'
8486,'colings86','Order by scripted_metric sub aggregation\nSince there is a new Scripted metric aggregation (scripted_metric) in 1.4, it is possible to do a lot of amazing stuff. \r\nFor example it is possible to implement Weighted Average aggregation, which we were missing before.\r\n\r\nNow we are really missing a **possibility to sort by scripted_metric results**.\r\n\r\n**Live example**:\r\n\r\nWe calculate **weightedAvgVis** with scripted_metric and want to get ids with **TOP 5 values of weightedAvgVis**. Since script returns double, it looks logically possible. \r\n\r\n```json\r\n{  \r\n   "from":0,\r\n   "size":0,\r\n   "query":{  \r\n      "match_all":{   }\r\n   },\r\n   "aggregations":{  \r\n      "idNodes":{  \r\n         "terms":{  \r\n            "field":"id",\r\n            "size":5,\r\n            "order":{  \r\n               "weightedAvgVis":"asc"\r\n            }\r\n         },\r\n         "aggregations":{  \r\n            "weightedAvgVis":{  \r\n               "scripted_metric":{  \r\n                  "init_script":"_agg[\'weightedSum\'] = 0d; _agg[\'countSum\'] = 0L;",\r\n                  "map_script":"_agg[\'weightedSum\'] = _agg.weightedSum + _source[\'avgVis\'] * _source[\'count\']; _agg[\'countSum\'] = _agg.countSum + _source[\'count\'];",\r\n                  "reduce_script":"weightedSum = 0d; countSum = 0L; for(a in _aggs) {weightedSum += a.weightedSum; countSum += a.countSum;};if(countSum == 0L) {return 0d;} else {return weightedSum / countSum}"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n'
8481,'pickypg',"Add CBOR-friendly bulk format\nThe current bulk format in JSON uses the `\\n` as end-of-line markers, allowing the coordinating node to parse just the metadata line, then skip over the body line.\r\n\r\nCBOR doesn't have any disallowed characters or character sequences, so this mechanism will not work.  Instead we should have a CBOR specific format which looks like this:\r\n\r\n    { metadata line }\r\n    int_length_of_body_line\r\n    { body line }\r\n\r\nThe `int_length_of_body_line` would allow us to skip to the next metadata line.  \r\n\r\nFor the `delete` action, no `int_length_of_body_line` would be required, as we already know that it can't have a body.\r\n\r\nDepends on #7640"
8473,'martijnvg','filtered aliases in templates do not inherit mappings from aliased index\nRelated to #6110 and #8431. This behavior is new since 1.4.0.\r\n\r\n```\r\n#reset logstash templates\r\nDELETE /_template/logstash\r\nDELETE /_template/filtered_logstash\r\n\r\n# ensure index does not exist\r\nDELETE logstash-2014.11.11\r\n\r\n#create template for logstash indexes\r\nPUT /_template/logstash\r\n{\r\n  "template": "logstash-*",\r\n  "mappings": {\r\n    "_default_": {\r\n      "dynamic_templates": [\r\n        {\r\n          "string_fields": {\r\n            "mapping": {\r\n              "index": "not_analyzed",\r\n              "type": "string"\r\n            },\r\n            "match_mapping_type": "string",\r\n            "match": "*"\r\n          }\r\n        }\r\n      ],\r\n      "properties": {\r\n        "@timestamp": {\r\n          "type": "date"\r\n        },\r\n        "clientip": {\r\n          "index": "not_analyzed",\r\n          "type": "string"\r\n        }\r\n      },\r\n      "_all": {\r\n        "enabled": true\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nPUT _template/filtered_logstash\r\n{\r\n  "template": "logstash-*",\r\n  "order": 1,\r\n  "aliases": {\r\n    "filtered-{index}": {\r\n      "filter": {\r\n        "bool": {\r\n          "must_not": [\r\n            {\r\n              "terms": {\r\n                "clientip": [\r\n                  "1.2.3.4",\r\n                  "2.3.4.5"\r\n                ]\r\n              }\r\n            }\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n# (FAILS) insert doc to dynamically create index\r\nPOST /logstash-2014.11.11/event/1\r\n{\r\n  "@timestamp": "2014-11-11T00:02:14.000Z",\r\n  "clientip": "3.4.5.6"\r\n}\r\n\r\n```\r\n\r\nI would expect this to work, but instead I get `Strict field resolution and no field mapping can be found for the field with name [clientip]` when I try to `POST` that event.'
8469,'clintongormley','Update terms-aggregation.asciidoc\nIt looks like the reference editor forgot to remove some draft notes. These two sentences look more like a version history or like TODOs than the reference info.'
8467,'nknize','geo_shape query with polygon from -180/90 to 180/-90\nHi,\r\nSince 1.4.0 i have issues running the following query:\r\n```\r\n{\r\n  "size" : 0,\r\n  "query" : {\r\n    "bool" : {\r\n      "must" : [ \r\n {\r\n        "geo_shape" : {\r\n          "loc" : {\r\n            "shape" : {\r\n              "type" : "polygon",\r\n              "coordinates" : [ [ \r\n[ -180, 90 ], \r\n[ 180, 90 ], \r\n[ 180, -90 ], \r\n[ -180, -90 ], \r\n[ -180, 90 ] \r\n] ]\r\n            }\r\n          }\r\n        }\r\n      } ]\r\n    }\r\n  },\r\n  "aggregations" : {\r\n    "ship" : {\r\n      "terms" : {\r\n        "field" : "entity_id",\r\n        "size" : 0\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nThe error is :\r\n```\r\n TopologyException[found non-noded intersection between LINESTRING ( -180.0 -90.0, -180.0 90.0 ) and LINESTRING ( Infinity Infinity, -180.0 -90.0 ) [ (-180.0, -90.0, NaN) ]]; }]",\r\n```\r\nThe same query was running with 1.3.4\r\nIf i change the points to : 179/89 this works.\r\n\r\nI am reading the polygon from PostGIS DB as :\r\n0103000000010000000500000000000000008066C0000000000080564000000000008066400000000000805640000000000080664000000000008056C000000000008066C000000000008056C000000000008066C00000000000805640\r\n\r\nst_asgeojson:\r\n{"type":"Polygon","coordinates":[[[-180,90],[180,90],[180,-90],[-180,-90],[-180,90]]]}'
8465,'clintongormley','Documents missing after adding a new node\n1. Create a new Elasticsearch cluster with 2 nodes and 5000 documents.\r\n2. Add  a new node with zone awareness in a different zone. \r\n3. Exclude IPs and delete the first 2 nodes.\r\n4. Delete 20 documents.\r\n5. Add 2 new nodes with zone awareness in a different zone.\r\n6. Exclude IPs and delete the old single node.\r\n7. Add back the same 20 documents.\r\n8. Add a new node with zone awareness in a different zone.\r\n\r\nWe can now see inconsistent document counts returned from each of the nodes. If we flush we see the correct 5000 documents. But if we exclude IPs and delete the 2 old nodes we see data loss and only 4980 documents. The newly added 20 documents go missing.'
8453,'tlrx','Remove the "all-trusting" TrustManager\nCloses #8444'
8451,'clintongormley',"Client nodes don't join cluster back post OOM exception\nIn our setup we are always seeing when client nodes throw OOM, it is booted out of cluster. It is never able to join cluster back post OOM exception.\r\n\r\nWe saw the following from logs\r\n\r\njava.lang.OutOfMemoryError: Java heap space\r\n\tat java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)\r\n\tat java.nio.ByteBuffer.allocate(ByteBuffer.java:331)\r\n\tat sun.nio.cs.StreamEncoder.<init>(StreamEncoder.java:195)\r\n\tat sun.nio.cs.StreamEncoder.<init>(StreamEncoder.java:175)\r\n\tat sun.nio.cs.StreamEncoder.forOutputStreamWriter(StreamEncoder.java:68)\r\n\tat java.io.OutputStreamWriter.<init>(OutputStreamWriter.java:133)\r\n\tat java.io.PrintStream.<init>(PrintStream.java:111)\r\n\tat java.io.PrintStream.<init>(PrintStream.java:175)\r\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:384)\r\n\tat sun.net.www.http.HttpClient.openServer(HttpClient.java:473)\r\n\tat sun.net.www.http.HttpClient.<init>(HttpClient.java:203)\r\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:290)\r\n\tat sun.net.www.http.HttpClient.New(HttpClient.java:306)\r\n\tat sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:995)\r\n\tat sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:931)\r\n\tat sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:849)\r\n\tat org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:325)\r\n\tat org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:182)\r\n\tat org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:248)\r\n\tat org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:130)\r\n\tat org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:349)\r\n\tat org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:236)\r\n\tat java.lang.Thread.run(Thread.java:724)\r\n\r\n...............\r\n[2014-11-04 09:34:50,844][DEBUG][action.search.type       ] [ES-NODE-Q02] [3312] Failed to execute fetch phase\r\norg.elasticsearch.transport.NodeDisconnectedException: [ES-NODE-D02][inet[/10.0.0.8:9300]][search/phase/fetch/id] disconnected\r\n...\r\n\r\n[2014-11-04 09:41:35,957][DEBUG][action.admin.indices.alias.get] [ES-NODE-Q02] connection exception while trying to forward request to master node [[ES-NODE-M01][K13SBxlOQ06-axx0_UL1bg][es-node-m01][inet[/10.0.0.10:9300]]{updateDomain=0, tag=masternode, data=false, faultDomain=0, master=true}], scheduling a retry. Error: [org.elasticsearch.transport.NodeDisconnectedException: [ES-NODE-M01][inet[/10.0.0.10:9300]][indices/get/aliases] disconnected]\r\n\r\n\r\nPost OOM exception client node is never able to join cluster back. We needed to restart es service on client node to get this node join back to cluster.\r\n\r\nIs it a known issue? Is it expected behavior?\r\n"
8449,'jpountz','Search: Track most commonly used filters and cache them\nWhether or not a filter should be cached is currently up to the filter parsers. For instance, we cache `terms` filters because they are costly to build and produce a result that is already cacheable, and do not cache geo-distance filters because they are usually expected to be applied to the position of a user of the system that keeps changing all the time.\r\n\r\nWe could make the default better by tracking usage statistics of the filters and decide to cache based on the cost of the filter and how often the filter is used.'
8448,'clintongormley',"elasticsearch 1.4.0 hanging with rivers starting up\nThe elasticsearch-river-mongodb river tries to query elasticsearch whether a specific index already exists in its `#start()` method. If there are many of those rivers elasticsearch's startup essentially hangs: all threads in the 'listener' pool are used by the `RiversService#createRiver()` method, and the listener for the 'indexExists' call never gets executed.\r\n\r\nA full thread is available at https://gist.github.com/ankon/e000198100ce3d52cb3b#file-thread-dump-txt, and the issue is filed also in the elasticsearch-river-mongodb project: https://github.com/richardwilly98/elasticsearch-river-mongodb/issues/406\r\n\r\nSome questions:\r\n1. What is the proper behavior of a river inside `#start()`, in particular: is it allowed to use the elasticsearch client? Should it do anything special when invoking methods on it?\r\n2. Where is the best place for documenting these requirements?\r\n"
8434,'colings86',"Use JSONPath syntax for aggregation paths\n@jpountz  pointed out that we could use JSONPath to define paths to properties in aggregations. This would be a new syntax and would replace the old `a>b.c` style syntax. We could add a config option to enable the older syntax for bwc. The new syntax would allow for complex aggregation paths for use in reducers to, for example, get properties from a particular bucket in an aggregation, or retrieve all the values of the property `c` in all the buckets even if c is three levels down the aggregation tree. It would also give a standard way to access the root of the agg tree (with '$').\r\n\r\nfor reference see: http://goessner.net/articles/JsonPath/"
8421,'colings86','Aggregations: Added getProperty method to Aggregations\nThis allows arbitrary properties to be retrieved from an aggregation tree. The property is specified using the same syntax as the\r\norder parameter in the terms aggregation. If a property path contians a multi-bucket aggregation the property values from each bucket will be returned in an array.'
8419,'spinscale',"The elasticsearch user's home directory belongs to root for RPM installs\nThe home directory for the elasticsearch user is not created with the user, it's created later on during the install and hence gets created as root:root with everything under `/usr/share/elasticsearch` belonging to root as a result. This is from a fresh install on a CentOS 6.5 box:\r\n```\r\n$ cat /etc/issue\r\nCentOS release 6.5 (Final)\r\nKernel \\r on an \\m\r\n\r\n$ ls -ald /usr/share/elasticsearch/\r\ndrwxr-xr-x 4 root root 4096 Nov 10 05:42 /usr/share/elasticsearch/\r\n\r\n$ ls -al /usr/share/elasticsearch/\r\ntotal 44\r\ndrwxr-xr-x   4 root root  4096 Nov 10 05:42 .\r\ndrwxr-xr-x. 67 root root  4096 Nov 10 05:42 ..\r\ndrwxr-xr-x   2 root root  4096 Nov 10 05:42 bin\r\ndrwxr-xr-x   3 root root  4096 Nov 10 05:42 lib\r\n-rw-r--r--   1 root root 11358 Jul 28 14:53 LICENSE.txt\r\n-rw-r--r--   1 root root   150 Jul 28 14:53 NOTICE.txt\r\n-rw-r--r--   1 root root  8421 Jul 28 14:53 README.textile\r\n```\r\nWas there a valid reason for this?"
8409,'clintongormley',"java api issue\nin 1.4.0, if an es client is obtained via node builder and then all of the indexes are dropped and recreated via this client, subsequent data indexing calls appear to fail. \r\n\r\nIf you follow this at trace level, each index call results in a message 'primary shard [xxx] is not yet active or we do not know the node it is assigned to [null], scheduling a retry. ' (originating from TransportShardReplicationOperationAction). \r\n\r\nThis is not the behaviour in 1.3.4. \r\n\r\nThe issue also doesn't manifest itself if a transport client is used instead.\r\n"
8406,'jpountz','Terms agg with zero-result filtered query searches whole index\nWe\'re using a filtered query followed by a terms agg, as so:\r\n\r\n```\r\n{\r\n  "query":{\r\n    "filtered":{\r\n      "query":{ \r\n        "query_string":{\r\n          "query": "type:web_info_log AND log_namespace:FOO AND message:\\"bar\\""\r\n        }\r\n      },\r\n      "filter":{\r\n        "bool":{\r\n          "must":[{\r\n            "range":{\r\n              "@timestamp":{\r\n                "from":1415464349189,"to":1415465564077\r\n              }\r\n            }\r\n          }]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "aggs":{\r\n    "histogram_aggregation":{\r\n      "histogram":{\r\n        "interval":10,"field":"@timestamp"\r\n      },\r\n      "aggs":{\r\n        "terms_bucket":{\r\n          "terms":{\r\n            "field":"log_message","execution_hint":"map"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nNormally the filtered query returns some small number of requests (under 100) and everything is fine.\r\nHowever, when it returns zero results, the terms aggregations runs on the entire index (in our case, ~4bn docs), causing repeated GCs which take many hours and make the cluster unusable.\r\nIt should, in fact, not run at all because it got zero results from the filtered query.'
8396,'jpountz',"ArrayIndexOutOfBoundsException in murmur hash with has_child\nRunning 1.4.0 it is possible to hang ES.\r\n\r\nSteps to reproduce:\r\nThree processes in a while(true) loop.  One is using the _bulk API to insert (and update) a small random number of documents. The other two processes are executing fairly complex filteredQuery's.\r\n\r\nI can run any of the two processes concurrently without problems, but when a the third one starts, ES fails with the exception below.\r\n\r\nOther times, the exception doesn't occur but instead ES just hangs and I have to kill -9 its jvm.  \r\n\r\n```\r\n[2014-11-07 16:46:36,175][DEBUG][action.search.type       ] [Hardcore] [2848] Failed to execute query phase\r\norg.elasticsearch.search.query.QueryPhaseExecutionException: [xxx.public.test.idxtest][2]: query[filtered(ConstantScore(++cache(_xmin:[2882514 TO 2882514]) +cache(_cmin:[* TO 0}) +cache(_xmax:[0 TO 0]) +cache(_xmax:[2882514 TO 2882514]) +cache(_cmax:[0 TO *]) +cache(_xmin_is_committed:T) +cache(_xmax:[0 TO 0]) +cache(_xmax:[2882514 TO 2882514]) +cache(_cmax:[0 TO *]) +NotFilter(cache(_xmax:[2882514 TO 2882514])) +cache(_xmax_is_committed:F) +CustomQueryWrappingFilter(child_filter[data/xact](filtered(ConstantScore(cache(BooleanFilter(_field_names:id))))->cache(_type:data)))))->cache(+_type:xact +org.elasticsearch.index.search.nested.NonNestedDocsFilter@38f048bd)],from[0],size[32768]: Query Failed [Failed to execute main query]\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:163)\r\n\tat org.elasticsearch.search.SearchService.executeScan(SearchService.java:245)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:520)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$21.call(SearchServiceTransportAction.java:517)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction$23.run(SearchServiceTransportAction.java:559)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 13417\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:136)\r\n\tat org.elasticsearch.index.search.child.CustomQueryWrappingFilter.getDocIdSet(CustomQueryWrappingFilter.java:72)\r\n\tat org.elasticsearch.common.lucene.search.AndFilter.getDocIdSet(AndFilter.java:54)\r\n\tat org.elasticsearch.common.lucene.search.ApplyAcceptedDocsFilter.getDocIdSet(ApplyAcceptedDocsFilter.java:46)\r\n\tat org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:157)\r\n\tat org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:542)\r\n\tat org.apache.lucene.search.FilteredQuery$1.scorer(FilteredQuery.java:136)\r\n\tat org.apache.lucene.search.FilteredQuery$RandomAccessFilterStrategy.filteredScorer(FilteredQuery.java:542)\r\n\tat org.apache.lucene.search.FilteredQuery$FilterStrategy.filteredBulkScorer(FilteredQuery.java:504)\r\n\tat org.apache.lucene.search.FilteredQuery$1.bulkScorer(FilteredQuery.java:150)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:618)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:191)\r\n\tat org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:309)\r\n\tat org.elasticsearch.search.scan.ScanContext.execute(ScanContext.java:52)\r\n\tat org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:120)\r\n\t... 7 more\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException: 13417\r\n\tat org.apache.lucene.util.StringHelper.murmurhash3_x86_32(StringHelper.java:205)\r\n\tat org.apache.lucene.util.StringHelper.murmurhash3_x86_32(StringHelper.java:229)\r\n\tat org.apache.lucene.util.BytesRef.hashCode(BytesRef.java:143)\r\n\tat org.elasticsearch.common.util.BytesRefHash.add(BytesRefHash.java:151)\r\n\tat org.elasticsearch.index.search.child.ParentIdsFilter.createShortCircuitFilter(ParentIdsFilter.java:67)\r\n\tat org.elasticsearch.index.search.child.ChildrenConstantScoreQuery.createWeight(ChildrenConstantScoreQuery.java:127)\r\n\tat org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:684)\r\n\tat org.elasticsearch.search.internal.ContextIndexSearcher.createNormalizedWeight(ContextIndexSearcher.java:133)\r\n\t... 21 more\r\n```"
8394,'martijnvg',"Large index no longer initialises under 1.4.0 and 1.4.0 Beta 1 due to OutOfMemoryException\nWe have one particularly large index in our cluster - it contains 10s of millions of documents and has quite a lot of nesteds too. Prior to 1.4.0 Beta 1 (including 1.2.x and 1.3.x) the index re-initialised on a node with 8GB allocated to ElasticSearch (16GB+ available in OS). Since 1.4.0 Beta 1 (and still on 1.4.0) we're getting an OOM exception (startup log and exception stack below). At this point, the node ceases recovery (expected, I guess) and becomes unresponsive. All data nodes suffer the same fate and the entire cluster becomes unresponsive.\r\n\r\n    [2014-11-07 17:12:39,895][WARN ][common.jna               ] unable to link C library. native methods (mlockall) will be disabled.\r\n    [2014-11-07 17:12:40,077][INFO ][node                     ] [dvlp_FRONTEND2] version[1.4.0], pid[9052], build[bc94bd8/2014-11-05T14:26:12Z]\r\n    [2014-11-07 17:12:40,077][INFO ][node                     ] [dvlp_FRONTEND2] initializing ...\r\n    [2014-11-07 17:12:40,129][INFO ][plugins                  ] [dvlp_FRONTEND2] loaded [cloud-aws], sites [bigdesk, head, inquisitor, kopf]\r\n    [2014-11-07 17:12:45,220][INFO ][node                     ] [dvlp_FRONTEND2] initialized\r\n    [2014-11-07 17:12:45,220][INFO ][node                     ] [dvlp_FRONTEND2] starting ...\r\n    [2014-11-07 17:12:45,438][INFO ][transport                ] [dvlp_FRONTEND2] bound_address {inet[/0:0:0:0:0:0:0:0:50882]}, publish_address {inet[FRONTEND2/192.168.10.73:50882]}\r\n    [2014-11-07 17:12:45,452][INFO ][discovery                ] [dvlp_FRONTEND2] dvlp/C2f-euXcRc-cEv3dnsBnXw\r\n    [2014-11-07 17:13:15,451][WARN ][discovery                ] [dvlp_FRONTEND2] waited for 30s and no initial state was set by the discovery\r\n    [2014-11-07 17:13:15,468][INFO ][http                     ] [dvlp_FRONTEND2] bound_address {inet[/0:0:0:0:0:0:0:0:50881]}, publish_address {inet[frontend2/192.168.10.73:50881]}\r\n    [2014-11-07 17:13:15,468][INFO ][node                     ] [dvlp_FRONTEND2] started\r\n    [2014-11-07 17:13:48,552][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:14:51,597][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:15:54,633][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:16:57,647][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:18:00,664][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:19:03,675][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:20:06,684][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [ElasticsearchTimeoutException[Timeout waiting for task.]]\r\n    [2014-11-07 17:20:36,950][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][jwhGk5NyTx-E1HInKTLDkg][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [NodeDisconnectedException[[dvlp_FRONTEND2_coordinator][inet[/192.168.10.73:55591]][internal:discovery/zen/join] disconnected]]\r\n    [2014-11-07 17:20:41,171][WARN ][transport.netty          ] [dvlp_FRONTEND2] Message not fully read (response) for [85] handler future(org.elasticsearch.transport.EmptyTransportResponseHandler@2060e2c8), error [true], resetting\r\n    [2014-11-07 17:20:41,171][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND1_coordinator][4y8Hh5kAQPK2Ie3gzc58Ww][FRONTEND1][inet[/192.168.10.70:55858]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_coordinator, master=true}], reason [RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: StreamCorruptedException[unexpected end of block data]; ]\r\n    [2014-11-07 17:20:45,520][INFO ][discovery.zen            ] [dvlp_FRONTEND2] failed to send join request to master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}], reason [RemoteTransportException[[dvlp_FRONTEND2_coordinator][inet[/192.168.10.73:55591]][internal:discovery/zen/join]]; nested: ElasticsearchIllegalStateException[Node [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[FRONTEND2/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}] not master for join request from [[dvlp_FRONTEND2][C2f-euXcRc-cEv3dnsBnXw][FRONTEND2][inet[/192.168.10.73:50882]]{datacentrename=site2, nodename=dvlp_FRONTEND2, master=false}]]; ], tried [3] times\r\n    [2014-11-07 17:20:48,831][INFO ][cluster.service          ] [dvlp_FRONTEND2] detected_master [dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}, added {[dvlp_DEVBATH01.exabre.co.uk_loadbalancer][8i4izXAUQiWeS2arwV9LeA][DEVBATH01][inet[/192.168.10.65:12184]]{datacentrename=site1, data=false, nodename=dvlp_DEVBATH01.exabre.co.uk_loadbalancer, master=true},[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true},[dvlp_FRONTEND2_loadbalancer][joVXc_fGTx-SC_YwJ2YBmQ][FRONTEND2][inet[/192.168.10.73:65341]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_loadbalancer, master=false},[dvlp_FRONTEND1_loadbalancer][snDHwo0YTR6VsAFV9nBcxw][FRONTEND1][inet[/192.168.10.70:55054]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_loadbalancer, master=false},}, reason: zen-disco-receive(from master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}])\r\n    [2014-11-07 17:21:01,937][INFO ][cluster.service          ] [dvlp_FRONTEND2] added {[dvlp_FRONTEND1_coordinator][4y8Hh5kAQPK2Ie3gzc58Ww][FRONTEND1][inet[/192.168.10.70:55858]]{datacentrename=site1, data=false, nodename=dvlp_FRONTEND1_coordinator, master=true},}, reason: zen-disco-receive(from master [[dvlp_FRONTEND2_coordinator][-O87CxU3RRSTHZkuC985Yw][FRONTEND2][inet[/192.168.10.73:55591]]{datacentrename=site2, data=false, nodename=dvlp_FRONTEND2_coordinator, master=true}])\r\n    [2014-11-07 17:25:25,598][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][739][27] duration [8s], collections [1]/[9s], total [8s]/[8.8s], memory [7.8gb]->[7.7gb]/[7.9gb], all_pools {[young] [172.4mb]->[46.5mb]/[199.6mb]}{[survivor] [24.9mb]->[0b]/[24.9mb]}{[old] [7.6gb]->[7.7gb]/[7.7gb]}\r\n    [2014-11-07 17:25:46,387][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][746][32] duration [5s], collections [1]/[6s], total [5s]/[23.6s], memory [7.9gb]->[7.9gb]/[7.9gb], all_pools {[young] [195mb]->[199.6mb]/[199.6mb]}{[survivor] [0b]->[10.9mb]/[24.9mb]}{[old] [7.7gb]->[7.7gb]/[7.7gb]}\r\n    [2014-11-07 17:28:16,136][WARN ][index.warmer             ] [dvlp_FRONTEND2] [dvlp_13_67_item_20140410][7] failed to load fixed bitset for [org.elasticsearch.index.search.nested.NonNestedDocsFilter@fd00879d]\r\n    org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)\r\n        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)\r\n        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:139)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:287)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\n    Caused by: java.lang.OutOfMemoryError: Java heap space\r\n        at org.apache.lucene.util.FixedBitSet.<init>(FixedBitSet.java:187)\r\n        at org.apache.lucene.search.MultiTermQueryWrapperFilter.getDocIdSet(MultiTermQueryWrapperFilter.java:104)\r\n        at org.elasticsearch.common.lucene.search.NotFilter.getDocIdSet(NotFilter.java:49)\r\n        at org.elasticsearch.index.search.nested.NonNestedDocsFilter.getDocIdSet(NonNestedDocsFilter.java:46)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:142)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:139)\r\n        at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)\r\n        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)\r\n        ... 8 more\r\n    [2014-11-07 17:28:29,215][INFO ][monitor.jvm              ] [dvlp_FRONTEND2] [gc][old][749][40] duration [22.9s], collections [4]/[2.3m], total [22.9s]/[1m], memory [7.9gb]->[7.9gb]/[7.9gb], all_pools {[young] [199.5mb]->[199.6mb]/[199.6mb]}{[survivor] [22.9mb]->[23.1mb]/[24.9mb]}{[old] [7.7gb]->[7.7gb]/[7.7gb]}\r\n    [2014-11-07 17:28:23,797][WARN ][index.warmer             ] [dvlp_FRONTEND2] [dvlp_13_67_item_20140410][7] failed to load fixed bitset for [org.elasticsearch.index.search.nested.NestedDocsFilter@fd00879d]\r\n    org.elasticsearch.common.util.concurrent.ExecutionError: java.lang.OutOfMemoryError: Java heap space\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2201)\r\n        at org.elasticsearch.common.cache.LocalCache.get(LocalCache.java:3937)\r\n        at org.elasticsearch.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4739)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.getAndLoadIfNotPresent(FixedBitSetFilterCache.java:139)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache.access$100(FixedBitSetFilterCache.java:75)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$FixedBitSetFilterWarmer$1.run(FixedBitSetFilterCache.java:287)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\n    Caused by: java.lang.OutOfMemoryError: Java heap space\r\n        at org.apache.lucene.util.FixedBitSet.<init>(FixedBitSet.java:187)\r\n        at org.apache.lucene.search.MultiTermQueryWrapperFilter.getDocIdSet(MultiTermQueryWrapperFilter.java:104)\r\n        at org.elasticsearch.index.search.nested.NestedDocsFilter.getDocIdSet(NestedDocsFilter.java:50)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:142)\r\n        at org.elasticsearch.index.cache.fixedbitset.FixedBitSetFilterCache$2.call(FixedBitSetFilterCache.java:139)\r\n        at org.elasticsearch.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4742)\r\n        at org.elasticsearch.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282)\r\n        at org.elasticsearch.common.cache.LocalCache$Segment.get(LocalCache.java:2197)\r\n        ... 8 more"
8393,'MaineC','Search Template - parse template if it is provided as a single escaped string.\nI was trying to implement fix for #8308\r\n\r\nI think I nailed it down. It seems it is combination of two related issues:\r\n\r\n1) The `TemplateQueryParser.java` was not correctly parsing the request when the `template` contained a single `VALUE_STRING` token. I think this could not have been working before (obviously there were no tests for this use case). I improved the main `parse` method to detect string token. **Please review** namely the complex `if` conditions - I am sure there is a way how to express them in more elegant way.\r\n\r\n2) The second issue is with `SearchService.java` in `parseTemplate` method where it tries to parse the template for second time. When I commented this part out then all the tests that I added to `TemplateQueryTest.java` started to pass. **Please review** validity of commenting this out. I did not notice any tests that would be broken due to this but the chance is that there were no tests covering the logic behind second parsing pass.\r\n\r\nLooking for the feedback.'
8376,'MaineC','Search Template - conditional clauses not rendering correctly\n   - implemented the conditional parsing capabilities\r\n   - attached few junit test cases to test it\r\n\r\nCloses #8308'
8374,'palecur',"Documentation doesn't explain overall structure or composition of document components (queries, filters, etc.)\nhttps://twitter.com/mausch/status/530504035540160513\r\n\r\nhttps://twitter.com/LeviNotik/status/530505312777023488\r\n\r\nhttps://twitter.com/mwotton/status/442775637648891904\r\n\r\nhttps://twitter.com/melvinmt/status/530474427306479616\r\n\r\nhttps://twitter.com/kosmikko/status/502340971527671808\r\n\r\nhttps://twitter.com/kenperkins/status/494173990152175617 (navigation, I've had this problem and I'm a relatively experienced user of the site and ES itself)\r\n\r\nhttps://twitter.com/icyliquid/status/477500839020752896\r\n\r\nhttps://twitter.com/werg/status/466733972324880384 (another suggestion to bypass official ES documentation)\r\n\r\nI wrote https://github.com/bitemyapp/bloodhound partly because I was having a hell of a time generating valid Elasticsearch requests without lots of copy-paste and templating.\r\n\r\nThe docs have not been improved much in the last couple years other than when you did the big site change.\r\n\r\nIt's extremely hard for a new user of Elasticsearch to get a bird's eye view.\r\n\r\nI brought up Bloodhound when I first released it as a way to possibly start a proper spec of what Elasticsearch accepts as a valid request and what it does not. I'd like to revive the idea of having a spec so that users and client implementors don't have to keep rediscovering how Elasticsearch works through haphazard experimentation.\r\n\r\nBack when I was implementing Bloodhound I asked if there was a spec. I was told there wasn't one. I was briefly hopeful when I saw there was a Thrift spec...but it passes off the actual document structure as a JSON blob, lending no useful information.\r\n\r\nPlease at least consider having a spec for the API."
8340,'imotov','Several recoveries cause IndexShardGatewayRecoveryException\nI have tests environment that restore the index from snapshot before every test.\r\nAfter a few successful restorings, it fails on:\r\n\r\n*[2014-11-03 16:03:54,957][WARN ][indices.cluster ] [Baron Von Blitzschlag] [qs_rm_3][0] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [qs_rm_3][0] failed recovery\r\nat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\nat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] restore failed\r\nat org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:130)\r\nat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:127)\r\n... 3 more\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] failed to restore snapshot [qs_rm_alias]\r\nat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:159)\r\nat org.elasticsearch.index.snapshots.IndexShardSnapshotAndRestoreService.restore(IndexShardSnapshotAndRestoreService.java:124)\r\n... 4 more\r\nCaused by: org.elasticsearch.index.snapshots.IndexShardRestoreFailedException: [qs_rm_3][0] Failed to recover index\r\nat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:840)\r\nat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository.restore(BlobStoreIndexShardRepository.java:156)\r\n... 5 more\r\nCaused by: java.io.FileNotFoundException: C:\\TestResults\\QuickSearch\\data\\elasticsearch\\nodes\\0\\indices\\qs_rm_3\\0\\index_8.si (Access is denied)\r\nat java.io.FileOutputStream.open(Native Method)\r\nat java.io.FileOutputStream.(Unknown Source)\r\nat java.io.FileOutputStream.(Unknown Source)\r\nat org.apache.lucene.store.FSDirectory$FSIndexOutput.(FSDirectory.java:389)\r\nat org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:282)\r\nat org.apache.lucene.store.RateLimitedFSDirectory.createOutput(RateLimitedFSDirectory.java:40)\r\nat org.elasticsearch.index.store.DistributorDirectory.createOutput(DistributorDirectory.java:118)\r\nat org.apache.lucene.store.FilterDirectory.createOutput(FilterDirectory.java:69)\r\nat org.elasticsearch.index.store.Store.createVerifyingOutput(Store.java:298)\r\nat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restoreFile(BlobStoreIndexShardRepository.java:887)\r\nat org.elasticsearch.index.snapshots.blobstore.BlobStoreIndexShardRepository$RestoreContext.restore(BlobStoreIndexShardRepository.java:830)\r\n... 6 more*'
8339,'jpountz','date_histogram aggregation DST bug\nHi,\r\n\r\nSince it is the time of year where we adjust our clocks from daylight savings time to normal time again a bug in the date histogram struck us.\r\n\r\nWhen we run a date_histogram in a timezone other than UTC you will see some buckets being wrongfully combined. In the first example you see a `date_histogram` aggregate in UTC followed by the same aggregate in CET;\r\n\r\nUTC query:\r\n\r\n    {\r\n        "size": 0,\r\n        "query": {\r\n            "range": {\r\n               "published": {\r\n                  "gte": 1414274400000,\r\n                  "lt": 1414292400000\r\n               }\r\n            }\r\n        },\r\n        "aggs": {\r\n            "vot": {\r\n                "date_histogram": {\r\n                    "field": "published",\r\n                    "interval": "hour",\r\n                    "min_doc_count": 0,\r\n                    "time_zone": "UTC"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nUTC result:\r\n\r\n    {\r\n       "took": 12,\r\n       "timed_out": false,\r\n       "_shards": {\r\n          "total": 1,\r\n          "successful": 1,\r\n          "failed": 0\r\n       },\r\n       "hits": {\r\n          "total": 1130,\r\n          "max_score": 0,\r\n          "hits": []\r\n       },\r\n       "aggregations": {\r\n          "vot": {\r\n             "buckets": [\r\n                {\r\n                   "key_as_string": "2014-10-25T22:00:00.000Z",\r\n                   "key": 1414274400000,\r\n                   "doc_count": 260\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-25T23:00:00.000Z",\r\n                   "key": 1414278000000,\r\n                   "doc_count": 216\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T00:00:00.000Z",\r\n                   "key": 1414281600000,\r\n                   "doc_count": 222\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T01:00:00.000Z",\r\n                   "key": 1414285200000,\r\n                   "doc_count": 200\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T02:00:00.000Z",\r\n                   "key": 1414288800000,\r\n                   "doc_count": 232\r\n                }\r\n             ]\r\n          }\r\n       }\r\n    }\r\n\r\nCET query:\r\n\r\n    {\r\n        "size": 0,\r\n        "query": {\r\n            "range": {\r\n               "published": {\r\n                  "gte": 1414274400000,\r\n                  "lt": 1414292400000\r\n               }\r\n            }\r\n        },\r\n        "aggs": {\r\n            "vot": {\r\n                "date_histogram": {\r\n                    "field": "published",\r\n                    "interval": "hour",\r\n                    "min_doc_count": 0,\r\n                    "time_zone": "CET"\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nUTC result:\r\n\r\n    {\r\n       "took": 9,\r\n       "timed_out": false,\r\n       "_shards": {\r\n          "total": 1,\r\n          "successful": 1,\r\n          "failed": 0\r\n       },\r\n       "hits": {\r\n          "total": 1130,\r\n          "max_score": 0,\r\n          "hits": []\r\n       },\r\n       "aggregations": {\r\n          "vot": {\r\n             "buckets": [\r\n                {\r\n                   "key_as_string": "2014-10-25T22:00:00.000Z",\r\n                   "key": 1414274400000,\r\n                   "doc_count": 260\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-25T23:00:00.000Z",\r\n                   "key": 1414278000000,\r\n                   "doc_count": 0\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T00:00:00.000Z",\r\n                   "key": 1414281600000,\r\n                   "doc_count": 216\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T01:00:00.000Z",\r\n                   "key": 1414285200000,\r\n                   "doc_count": 422\r\n                },\r\n                {\r\n                   "key_as_string": "2014-10-26T02:00:00.000Z",\r\n                   "key": 1414288800000,\r\n                   "doc_count": 232\r\n                }\r\n             ]\r\n          }\r\n       }\r\n    }\r\n\r\nPoints of interest in these queries are the result buckets for key: `1414278000000`. When ran in UTC it give a `doc_count` of `216`. When ran in CET it has a `doc_count` of `0`.\r\n\r\nFurther more, you will find a `doc_count` of `216` in the CET bucket of `1414281600000` (an hour to late). Last to point out is the next CET bucket, it contains the value of `422` which is the sum of `222` + `200`. As you can see these values can be found in the corresponding bucket and the previous bucket in UTC.\r\n\r\nAttached you will find a patch file adding some basic tests to the `org.elasticsearch.common.rounding` package. Here we test key rounding for the `CET` and `America/Chicago` timezones on the days of the DST switch.\r\n\r\nI tried diggin into the issue and found that it has to do with the recalculation of `preTz.getOffset` on the following lines [TimeZoneRounding.java:156](https://github.com/elasticsearch/elasticsearch/blob/5797682bd0b87e8efeffb046258287d480435395/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java#L156) and [TimeZoneRounding.java:163](https://github.com/elasticsearch/elasticsearch/blob/5797682bd0b87e8efeffb046258287d480435395/src/main/java/org/elasticsearch/common/rounding/TimeZoneRounding.java#L163).\r\n\r\nRunning this step by step on my first CET test case results the first time (line 156) in 7200000 (2 hours) and the second time in 3600000 (1 hour). This difference is causeing `2014-10-26T01:01:01 GMT+0200` to resolve to `2014-10-26T02:00:00 GMT+0200`. Explaining why the `1414278000000` (2014-10-26T01:01:01 GMT+0200) bucket to be empty, the next bucket containing the contents of this bucket, and the bucket after that a sum of two buckets.\r\n\r\n0001-Add-tests-for-timezone-problems.patch:\r\n\r\n    From 4e022035b25a141027be6aaae78c8ad2454e673f Mon Sep 17 00:00:00 2001\r\n    From: Nils Dijk <me@thanod.nl>\r\n    Date: Tue, 4 Nov 2014 07:56:05 -0600\r\n    Subject: [PATCH 1/1] Add tests for timezone problems.\r\n    \r\n    ---\r\n     .../common/rounding/TimeZoneRoundingTests.java     | 39 ++++++++++++++++++++++\r\n     1 file changed, 39 insertions(+)\r\n\r\n    diff --git a/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java b/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java\r\n    index a3d70c7..e79ad1e 100644\r\n    --- a/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java\r\n    +++ b/src/test/java/org/elasticsearch/common/rounding/TimeZoneRoundingTests.java\r\n    @@ -83,6 +83,45 @@ public class TimeZoneRoundingTests extends ElasticsearchTestCase {\r\n             assertThat(tzRounding.round(utc("2009-02-03T01:01:01")), equalTo(time("2009-02-03T01:00:00", DateTimeZone.forOffsetHours(+2))));\r\n             assertThat(tzRounding.nextRoundingValue(time("2009-02-03T01:00:00", DateTimeZone.forOffsetHours(+2))), equalTo(time("2009-02-03T02:00:00", DateTimeZone.forOffsetHours(+2))));\r\n         }\r\n    +    \r\n    +    @Test\r\n    +    public void testTimeTimeZoneRoundingDST() {\r\n    +        Rounding tzRounding;\r\n    +        // testing savings to non savings switch \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();\r\n    +        assertThat(tzRounding.round(time("2014-10-26T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-10-26T01:00:00", DateTimeZone.forID("CET"))));\r\n    +        \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("CET")).build();\r\n    +        assertThat(tzRounding.round(time("2014-10-26T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-10-26T01:00:00", DateTimeZone.forID("CET"))));\r\n    +        \r\n    +        // testing non savings to savings switch\r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();\r\n    +        assertThat(tzRounding.round(time("2014-03-30T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-03-30T01:00:00", DateTimeZone.forID("CET"))));\r\n    +        \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("CET")).build();\r\n    +        assertThat(tzRounding.round(time("2014-03-30T01:01:01", DateTimeZone.forID("CET"))), equalTo(time("2014-03-30T01:00:00", DateTimeZone.forID("CET"))));\r\n    +        \r\n    +        // testing non savings to savings switch (America/Chicago)\r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();\r\n    +        assertThat(tzRounding.round(time("2014-03-09T03:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-03-09T03:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +        \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();\r\n    +        assertThat(tzRounding.round(time("2014-03-09T03:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-03-09T03:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +        \r\n    +        // testing savings to non savings switch 2013 (America/Chicago)\r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();\r\n    +        assertThat(tzRounding.round(time("2013-11-03T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2013-11-03T06:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +        \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();\r\n    +        assertThat(tzRounding.round(time("2013-11-03T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2013-11-03T06:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +        \r\n    +        // testing savings to non savings switch 2014 (America/Chicago)\r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("UTC")).build();\r\n    +        assertThat(tzRounding.round(time("2014-11-02T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-11-02T06:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +        \r\n    +        tzRounding = TimeZoneRounding.builder(DateTimeUnit.HOUR_OF_DAY).preZone(DateTimeZone.forID("America/Chicago")).build();\r\n    +        assertThat(tzRounding.round(time("2014-11-02T06:01:01", DateTimeZone.forID("America/Chicago"))), equalTo(time("2014-11-02T06:00:00", DateTimeZone.forID("America/Chicago"))));\r\n    +    }\r\n     \r\n         private long utc(String time) {\r\n             return time(time, DateTimeZone.UTC);\r\n    -- \r\n    1.9.3 (Apple Git-50)\r\n\r\n'
8333,'bleskes','Possible race condition: Cluster with no master, Indexing call can timeout/fail, but document can still make it into the index\nBoaz, this is from the issue we were working on.\r\n\r\nIn a single master cluster, when an indexing request is in-flight and the master node leaves, it is possible that there is small race condition between the time the index request is timed-out by ES, and a shard started cluster event happens - leading to successful indexing of the document (even though the client was notified of a failed/time-out error).'
8308,'MaineC','Search Template - conditional clauses not rendering correctly\nIs the example of filtered query template [1] working? It seems to contain extra `</6>` string which seems as an issue to me. I am unable to test this example even after I remove this extra string.\r\n\r\n[1] http://www.elasticsearch.org/guide/en/elasticsearch/reference/1.3/search-template.html#_conditional_clauses'
8285,'dakrone','Immediately clear the filter cache when requested\nCurrently, a `/_cache/clear?filter` request marks entries to be invalidated, but they only get cleared after a maximum delay of `cleanedInterval`.\r\n\r\nWe should force the cache to be cleaned in these conditions, as was done for the fielddata cache in https://github.com/elasticsearch/elasticsearch/commit/65ce5acfb41b4abd0c527aa0e870c2a1076d76cd'
8255,'GaelTadh','Make template params take arrays\nCurrently the templateParams in SearchRequest is a `Map<String, String>` - this means that it could not take Arrays as part of the parameters, which is not the case when accessing it through the REST api, and also as documented in the docs for this feature.\r\n\r\nThis changes it to `Map<String, Object>` so that it can take other data-types.'
8254,'s1monw',"Unicode index names causing DELETE index to hang\nThe test in https://github.com/elasticsearch/elasticsearch/blob/master/rest-api-spec/test/index/10_with_id.yaml uses Unicode characters in the index name: `test-weird-index-中文`.  After running the test, the test runner tries to clear up the created indices with a `DELETE /_all`.\r\n\r\nAt least on OSX, the name of the index is being changed to `test-weird-index-??`. \r\n\r\nIn 1.4, the test succeeds but Elasticsearch logs errors about not being able to delete the index, but DELETE all at least returns.  In 1.x, the DELETE all just hangs.\r\n\r\nBoth versions leave the `test-weird-index-??` in place.\r\n\r\nRelates to #6736 \r\n\r\nLogs from 1.x:\r\n\r\n    [2014-10-28 14:07:54,695][INFO ][node                     ] [Turbo] version[1.5.0-SNAPSHOT], pid[60278], build[${build/NA]\r\n    [2014-10-28 14:07:54,696][INFO ][node                     ] [Turbo] initializing ...\r\n    [2014-10-28 14:07:54,701][INFO ][plugins                  ] [Turbo] loaded [], sites []\r\n    [2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] initialized\r\n    [2014-10-28 14:07:57,345][INFO ][node                     ] [Turbo] starting ...\r\n    [2014-10-28 14:07:57,457][INFO ][transport                ] [Turbo] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.5.103:9300]}\r\n    [2014-10-28 14:07:57,479][INFO ][discovery                ] [Turbo] elasticsearch/aprLOBxNRXKldBazmm8VTg\r\n    [2014-10-28 14:08:01,253][INFO ][cluster.service          ] [Turbo] new_master [Turbo][aprLOBxNRXKldBazmm8VTg][Slim.local][inet[/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)\r\n    [2014-10-28 14:08:01,270][INFO ][http                     ] [Turbo] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.5.103:9200]}\r\n    [2014-10-28 14:08:01,270][INFO ][node                     ] [Turbo] started\r\n    [2014-10-28 14:08:01,283][INFO ][gateway                  ] [Turbo] recovered [0] indices into cluster_state\r\n    [2014-10-28 14:10:37,337][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]/[1], mappings []\r\n    [2014-10-28 14:10:37,586][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\r\n    java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp\r\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\r\n        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:287)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:334)\r\n        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\r\n        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\r\n        at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:10:37,624][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\r\n    java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp\r\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\r\n        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:287)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:334)\r\n        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\r\n        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\r\n        at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:10:37,633][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\r\n    java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-1.st.tmp\r\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\r\n        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:287)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:334)\r\n        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\r\n        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\r\n        at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:10:37,702][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\r\n    [2014-10-28 14:10:37,725][WARN ][index.mapper             ] [Turbo] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\r\n    [2014-10-28 14:10:37,727][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] update_mapping [weird.type] (dynamic)\r\n    [2014-10-28 14:10:37,732][WARN ][gateway.local.state.meta ] [Turbo] [test-weird-index-中文]: failed to write index state\r\n    java.nio.file.NoSuchFileException: /Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-中文/_state/state-2.st.tmp\r\n        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)\r\n        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)\r\n        at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:176)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:287)\r\n        at java.nio.channels.FileChannel.open(FileChannel.java:334)\r\n        at org.apache.lucene.util.IOUtils.fsync(IOUtils.java:313)\r\n        at org.elasticsearch.gateway.local.state.meta.MetaDataStateFormat.write(MetaDataStateFormat.java:117)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.writeIndex(LocalGatewayMetaState.java:371)\r\n        at org.elasticsearch.gateway.local.state.meta.LocalGatewayMetaState.clusterChanged(LocalGatewayMetaState.java:218)\r\n        at org.elasticsearch.gateway.local.LocalGateway.clusterChanged(LocalGateway.java:207)\r\n        at org.elasticsearch.cluster.service.InternalClusterService$UpdateTask.run(InternalClusterService.java:437)\r\n        at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:153)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:10:37,791][INFO ][cluster.metadata         ] [Turbo] [test-weird-index-中文] deleting index\r\n\r\nLogs from 1.4:\r\n\r\n    [2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] version[1.4.0-SNAPSHOT], pid[60351], build[${build/NA]\r\n    [2014-10-28 14:12:34,362][INFO ][node                     ] [Burstarr] initializing ...\r\n    [2014-10-28 14:12:34,367][INFO ][plugins                  ] [Burstarr] loaded [], sites []\r\n    [2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] initialized\r\n    [2014-10-28 14:12:36,930][INFO ][node                     ] [Burstarr] starting ...\r\n    [2014-10-28 14:12:37,065][INFO ][transport                ] [Burstarr] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/192.168.5.103:9300]}\r\n    [2014-10-28 14:12:37,080][INFO ][discovery                ] [Burstarr] elasticsearch/M_KcoO3qQCOBp9VuHZ31AA\r\n    [2014-10-28 14:12:40,854][INFO ][cluster.service          ] [Burstarr] new_master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]], reason: zen-disco-join (elected_as_master)\r\n    [2014-10-28 14:12:40,869][INFO ][http                     ] [Burstarr] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/192.168.5.103:9200]}\r\n    [2014-10-28 14:12:40,869][INFO ][node                     ] [Burstarr] started\r\n    [2014-10-28 14:12:40,883][INFO ][gateway                  ] [Burstarr] recovered [0] indices into cluster_state\r\n    [2014-10-28 14:12:50,301][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] creating index, cause [auto(index api)], shards [5]/[1], mappings []\r\n    [2014-10-28 14:12:50,580][INFO ][gateway.local.state.meta ] [Burstarr] [test-weird-index-??] dangling index, exists on local file system, but not in cluster metadata, scheduling to delete in [2h], auto import to cluster state [YES]\r\n    [2014-10-28 14:12:50,581][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\r\n    [2014-10-28 14:12:50,611][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\r\n    [2014-10-28 14:12:50,617][INFO ][gateway.local.state.meta ] [Burstarr] dangled index directory name is [test-weird-index-??], state name is [test-weird-index-中文], renaming to directory name\r\n    [2014-10-28 14:12:50,624][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]]]\r\n    [2014-10-28 14:12:50,682][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\r\n    [2014-10-28 14:12:50,709][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] shard is locked, releasing lock\r\n    [2014-10-28 14:12:50,733][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] shard is locked, releasing lock\r\n    [2014-10-28 14:12:50,760][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] shard is locked, releasing lock\r\n    [2014-10-28 14:12:50,782][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] shard is locked, releasing lock\r\n    [2014-10-28 14:12:50,828][WARN ][index.mapper             ] [Burstarr] [test-weird-index-中文] Type [weird.type] contains a '.', it is recommended not to include it within a type name\r\n    [2014-10-28 14:12:50,831][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] update_mapping [weird.type] (dynamic)\r\n    [2014-10-28 14:12:50,837][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-??] deleting index\r\n    [2014-10-28 14:12:55,730][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][2] Could not lock IndexWriter isLocked [true]\r\n    org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,732][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed recovery\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][2] failed to create engine\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        ... 3 more\r\n    Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        ... 6 more\r\n    [2014-10-28 14:12:55,749][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][3] Could not lock IndexWriter isLocked [true]\r\n    org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,750][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed recovery\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.engine.EngineCreationFailureException: [test-weird-index-??][3] failed to create engine\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:288)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        ... 3 more\r\n    Caused by: org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        ... 6 more\r\n    [2014-10-28 14:12:55,778][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][1] Could not lock IndexWriter isLocked [true]\r\n    org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,795][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-??][4] Could not lock IndexWriter isLocked [true]\r\n    org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index/write.lock\r\n        at org.apache.lucene.store.Lock.obtain(Lock.java:89)\r\n        at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:753)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1448)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:273)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:732)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:231)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock]; ]]\r\n    [2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][3] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index/write.lock]; ]]\r\n    [2014-10-28 14:12:55,800][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock]; ]]\r\n    [2014-10-28 14:12:55,801][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed recovery]; nested: EngineCreationFailureException[[test-weird-index-??][2] failed to create engine]; nested: LockObtainFailedException[Lock obtain timed out: NativeFSLock@/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index/write.lock]; ]]\r\n    [2014-10-28 14:12:55,806][INFO ][cluster.metadata         ] [Burstarr] [test-weird-index-中文] deleting index\r\n    [2014-10-28 14:12:55,808][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][0] failed to rollback writer on close\r\n    org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index' does not exist\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\r\n        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\r\n        at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\r\n        at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\r\n        at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,813][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][2] failed to rollback writer on close\r\n    org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index' does not exist\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\r\n        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\r\n        at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\r\n        at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\r\n        at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,815][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][1] failed to rollback writer on close\r\n    org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index' does not exist\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\r\n        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\r\n        at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\r\n        at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\r\n        at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,817][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][4] failed to rollback writer on close\r\n    org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index' does not exist\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\r\n        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\r\n        at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\r\n        at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\r\n        at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,819][WARN ][index.engine.internal    ] [Burstarr] [test-weird-index-中文][3] failed to rollback writer on close\r\n    org.apache.lucene.store.NoSuchDirectoryException: directory '/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index' does not exist\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:218)\r\n        at org.apache.lucene.store.FSDirectory.listAll(FSDirectory.java:242)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.elasticsearch.index.store.DistributorDirectory.listAll(DistributorDirectory.java:88)\r\n        at org.apache.lucene.store.FilterDirectory.listAll(FilterDirectory.java:48)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:424)\r\n        at org.apache.lucene.index.IndexFileDeleter.refresh(IndexFileDeleter.java:460)\r\n        at org.apache.lucene.index.IndexWriter.rollbackInternal(IndexWriter.java:2134)\r\n        at org.apache.lucene.index.IndexWriter.rollback(IndexWriter.java:2074)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.close(InternalEngine.java:1295)\r\n        at org.elasticsearch.index.service.InternalIndexService.removeShard(InternalIndexService.java:419)\r\n        at org.elasticsearch.index.service.InternalIndexService$1.run(InternalIndexService.java:279)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    [2014-10-28 14:12:55,822][INFO ][gateway.local.state.meta ] [Burstarr] auto importing dangled indices [test-weird-index-??/OPEN] from [[Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]]]\r\n    [2014-10-28 14:12:55,881][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][2] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] failed to fetch index version after copying it over\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\r\n        ... 4 more\r\n    Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\r\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\r\n        ... 4 more\r\n    [2014-10-28 14:12:55,900][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][0] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] failed to fetch index version after copying it over\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\r\n        ... 4 more\r\n    Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\r\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\r\n        ... 4 more\r\n    [2014-10-28 14:12:55,913][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][3] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] failed to fetch index version after copying it over\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\r\n        ... 4 more\r\n    Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\r\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\r\n        ... 4 more\r\n    [2014-10-28 14:12:55,929][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][4] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] failed to fetch index version after copying it over\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\r\n        ... 4 more\r\n    Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\r\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\r\n        ... 4 more\r\n    [2014-10-28 14:12:55,929][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] sending failed shard for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][3] received shard failed for [test-weird-index-??][3], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][3] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][3] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/3/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,930][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] sending failed shard for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][0] received shard failed for [test-weird-index-??][0], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][0] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][0] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/0/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] sending failed shard for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,931][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][2] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][2] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/2/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] sending failed shard for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,932][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][4] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][4] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/4/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,934][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][4] received shard failed for [test-weird-index-??][4], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\r\n    [2014-10-28 14:12:55,951][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][2] received shard failed for [test-weird-index-??][2], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [master [Burstarr][M_KcoO3qQCOBp9VuHZ31AA][Slim.local][inet[/192.168.5.103:9300]] marked shard as initializing, but shard is marked as failed, resend shard failure]\r\n    [2014-10-28 14:12:55,953][WARN ][indices.cluster          ] [Burstarr] [test-weird-index-??][1] failed to start shard\r\n    org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] failed to fetch index version after copying it over\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:158)\r\n        at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:722)\r\n    Caused by: org.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:132)\r\n        ... 4 more\r\n    Caused by: org.apache.lucene.index.IndexNotFoundException: no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:864)\r\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:752)\r\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:450)\r\n        at org.elasticsearch.common.lucene.Lucene.readSegmentInfos(Lucene.java:85)\r\n        at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:123)\r\n        ... 4 more\r\n    [2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] sending failed shard for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n    [2014-10-28 14:12:55,958][WARN ][cluster.action.shard     ] [Burstarr] [test-weird-index-??][1] received shard failed for [test-weird-index-??][1], node[M_KcoO3qQCOBp9VuHZ31AA], [P], s[INITIALIZING], indexUUID [FsbgmgXZTY6ksoOUAIBgOQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[test-weird-index-??][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[test-weird-index-??][1] shard allocated for local recovery (post api), should exist, but doesn't, current files: []]; nested: IndexNotFoundException[no segments* file found in store(least_used[rate_limited(niofs(/Users/clinton/workspace/elasticsearch/data/elasticsearch/nodes/0/indices/test-weird-index-??/1/index), type=MERGE, rate=20.0)]): files: []]; ]]\r\n"
8242,'bleskes',"Received a join request for an existing node\nWe had an outage this morning caused by 10/18 of our nodes being unable to clear any of their old generation.  The cause of this isn't yet clear to me so this bug isn't about that.  After restarting the full nodes I believe the cluster handshaking code had some trouble:\r\n```\r\n[2014-10-27 14:50:49,000][WARN ][discovery.zen            ] [elastic1002] received join request from node [[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}], but found existing node [elastic1017][ua7AQNuDQfu78lHZUz8G6A][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false} with same address, removing existing node\r\n[2014-10-27 14:50:49,000][INFO ][cluster.service          ] [elastic1002] removed {[elastic1017][ua7AQNuDQfu78lHZUz8G6A][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false},}, added {[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false},}, reason: zen-disco-receive(join from node[[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}])\r\n...\r\n[2014-10-27 14:50:52,047][INFO ][discovery.zen            ] [elastic1002] received a join request for an existing node [[elastic1017][iiL0v4nPSy6dwAo-Ro5e1Q][elastic1017][inet[/10.64.48.39:9300]]{rack=D3, row=D, master=false}]\r\n```\r\nIn the end I had to restart the cluster master to get it to seed control to another node.  The new node worked as expected and added the other nodes back.\r\nThis is Elasticsearch 1.3.4.  I know 1.4 contains lots of work on the zen code.  Any chance this is something it'd handle?"
8209,'jpountz','Extended bounds create misaligned empty buckets in date histogram aggregation.\nUsing a timezone in combination with \'pre_zone_adjust_large_interval\' set to true leads to the creation of buckets which are not correctly aligned. \r\n\r\nExample:\r\nTimezone set to CET, interval set to month (march and april are off here):\r\n````\r\n"aggregations" : {\r\n    "histo" : {\r\n      "buckets" : [ {\r\n        "key_as_string" : "2013-12-31T23:00:00.000Z",\r\n        "key" : 1388530800000,\r\n        "doc_count" : 1\r\n      }, {\r\n        "key_as_string" : "2014-01-31T23:00:00.000Z",\r\n        "key" : 1391209200000,\r\n        "doc_count" : 0\r\n      }, {\r\n        "key_as_string" : "2014-02-28T23:00:00.000Z",\r\n        "key" : 1393628400000,\r\n        "doc_count" : 0\r\n      }, {\r\n        "key_as_string" : "2014-03-28T23:00:00.000Z",\r\n        "key" : 1396047600000,\r\n        "doc_count" : 0\r\n      }, {\r\n        "key_as_string" : "2014-04-28T23:00:00.000Z",\r\n        "key" : 1398726000000,\r\n        "doc_count" : 0\r\n      } ]\r\n    }\r\n  }\r\n````\r\n\r\nReproduction:\r\nhttps://gist.github.com/miccon/4eecfeafc3a66a9d8b24'
8208,'clintongormley','Update pathhierarchy-tokenizer.asciidoc\nThe docs seem to contain a bug regarding the type of the path hierarchy tokenizer.'
8201,'pickypg','Enable indy (invokedynamic) compile flag for Groovy scripts by default\nWith Java 7 and the `groovy-all-x.y.z-indy` jar, Groovy scripts can be compiled with `invokedynamic` instruction support. Even with the "indy" jar, Groovy does not enable this by default so that Groovy code compiled with these features are compatible with JVMs prior to Java 7.\r\n\r\nWith this commit, it is turned on by default because Elasticsearch requires Java 7. However, Java 7\'s invokedynamic support was buggy until Java 7u60 (and later), which means that users of this should update their JVMs to at least that version.\r\n\r\nThis does provide an unadvertised property (`"script.groovy.indy"`) to disable the feature in the unlikely event that user\'s do not want it enabled (e.g., future, non-existent JVM bug). Set it to `false` to disable this feature.\r\n\r\nCloses #8184'
8200,'jpountz','Date histogram aggregation Milliseconds problem using minutes in date_range\nWhen using minutes in the date range the returned keys are wrong (minutes and milliseconds).\r\n\r\nUsing the query according to:\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-aggregations-bucket-daterange-aggregation.html\r\n\r\nI\'m using ElasticSearch Version 1.3.2.\r\n\r\nExample query:\r\n"date_range":{\r\n"field":"doc_published","\r\nformat":"dateOptionalTime",\r\n"ranges":[\r\n{"from":"20140101T00:10:00","to":"20140101T00:15:00"},{"from":"20140101T00:15:00","to":"20140101T00:20:00"}]\r\n}\r\n\r\n\r\nExample response:\r\n{"key":"20140101-01-01T00:10:00.064Z-20140101-01-01T00:14:59.968Z",\r\n"from":6.3549803335740006E17,\r\n"from_as_string":"20140101-01-01T00:10:00.064Z",\r\n"to":6.3549803335769997E17,\r\n"to_as_string":"20140101-01-01T00:14:59.968Z"},\r\n\r\n{"key":"20140101-01-01T00:14:59.968Z-20140101-01-01T00:20:00.000Z",\r\n"from":6.3549803335769997E17,\r\n"from_as_string":"20140101-01-01T00:14:59.968Z",\r\n"to":6.35498033358E17,\r\n"to_as_string":"20140101-01-01T00:20:00.000Z"}\r\n\r\n\r\nThe returning keys are invalid:\r\nRECEIVED: 20140101-01-01T00:10:00.064Z-20140101-01-01T00:14:59.968Z\r\nShould be: \r\n20140101-01-01T00:10:00.000Z-20140101-01-01T00:14:59.968Z \r\n\r\nRECEIVED: 20140101-01-01T00:14:59.968Z-20140101-01-01T00:20:00.000Z\r\nShould be: \r\n20140101-01-01T00:15:00.000Z-20140101-01-01T00:20:00.000Z  \r\n\r\n'
8184,'pickypg','enable indy compilation in GroovyScriptEngineService\nThis should just be a one liner like:\r\n    compilerConfiguration.setOptimizationOptions(Collections.singletonMap("indy"), Boolean.TRUE);\r\nin the ctor \r\n\r\nThanks @uschindler for tracking this down'
8155,'martijnvg','Nested: Change structure of the nested query and filter\nCurrently the structure of the `nested` query is:\r\n```json\r\n{\r\n   "nested" : {\r\n      "path" : "a-nested-object-path",\r\n      "query ": { <query> } \r\n   }\r\n}\r\n```\r\n\r\nIf the structure were to be changed to:\r\n\r\n```json\r\n{\r\n   "nested" : {\r\n      "a-nested-object-path" : {\r\n         "query" : { <query> } \r\n      }  \r\n   }\r\n}\r\n```\r\n\r\nThen the structure is streaming parsing friendly and the nested object path is always known when the `query` is parsed. Also the `NestedQueryParser.LateBindingParentFilter` workaround can then be removed.\r\n\r\nLike in #8154 nested path validation can then also be added, so that when a `nested` query is wrapped in another `nested` query a parser error can be thrown if the supplied paths don\'t match with each other.'
8154,'martijnvg','Parent/child: Change structure of has_child, has_parent, top_children queries and filters.\nCurrently the structure of the `has_child` query is:\r\n```json\r\n{\r\n   "has_child" : {\r\n      "type" : "a-child-type",\r\n      "query ": { <query> } \r\n   }\r\n}\r\n```\r\n\r\nIf the structure were to be changed to:\r\n\r\n```json\r\n{\r\n   "has_child" : {\r\n      "a-child-type" : {\r\n         "query" : { <query> } \r\n      }  \r\n   }\r\n}\r\n```\r\n\r\nThen the structure is streaming parsing friendly and the child type is always known when the `query` is parsed. \r\n\r\nAt the moment if the `query` field is encountered before the `type` field by the parser then the query json object is read into memory, but not parsed. It is parsed after the `type` field has been read. This work around can then also be removed.\r\n\r\nAlso validation should be added whether the used type is valid to use. This becomes useful in cases where a query contains multiple hierarchal `has_child` or `has_parent` queries. For example if the following types are defined grand_parent -> parent -> child. Then the only valid type when wrapping a has_child query in a has_child query that has type `parent` is `child`. This validation is already possible, but it is a nice opportunity to add this as well with this change.'
8141,'tlrx','PluginManager: Add an --update command\nAdds a simple --update command that combines a remove + install commands.\r\n\r\nCloses #5064'
8129,'areek','Completion Suggest: Omitting results even output is different\nI have a case where the completion suggester is omitting results, below are two example documents. For a search like "Samsung", only Document 1 will be returned. Actually its the only document returned, so the `size=10` is not reached. The only major difference i can is see is the weight, will the completion suggester omitt result if there\'s a huge difference in weight?\r\n\r\nDocument 1:\r\n`{\r\n      "_index" : "suggester",\r\n      "_type" : "suggestions",\r\n      "_id" : "b089f7c6a85f39fb882398b6c8ba34c1",\r\n      "_score" : 1.0,\r\n      "_source":{"suggest":{"input":["Sm","Samsung Mobiles Tablets","Mobiles \\u0026 Tablets","(Export) Galaxy Note 4 Sm","Galaxy Note 4 Sm","Note 4 Sm","4 Sm"], "output":62020, "payload":{"br":"Samsung", "ca":"Mobiles \\u0026 Tablets", "cl":"N910C LTE 32GB White","co":"product","li":"http://one", "pr":"(Export) Galaxy Note 4 Sm","th":"thumb1.jpg"},"weight":63}}\r\n    }`\r\n\r\nDocument 2:\r\n`{\r\n      "_index" : "suggester",\r\n      "_type" : "suggestions",\r\n      "_id" : "22480c43af984689f7254bda8d16f99a",\r\n      "_score" : 1.0,\r\n      "_source":{"suggest":{"input":["Samsung Mobiles Tablets","Mobiles \\u0026 Tablets","Galaxy Note 4 Sm","Note 4 Sm","4 Sm","Sm"],"output":21970, "payload":{"br":"Samsung","ca":"Mobiles \\u0026 Tablets","cl":"N910S LTE Black (Local warranty)","co":"product","li":"http://two", "pr":"Galaxy Note 4 Sm", "th":"thumb2.jpg"}, "weight":27}}\r\n    }`'
8122,'pickypg','Some ActionRequests require constructor args\nThe vast majority of `ActionRequest`s provide both getters and setters for every parameter, which  behave eerily similar to their `*Builder` versions.\r\n\r\nHowever, some implementations do not provide no-arg constructors and, then, they require those arguments to be known at construction time. In each case, these seem in contrast with the majority of other requests.\r\n\r\nOf note: `MoreLikeThisRequest` (index), `AnalyzeRequest` (text), and `DeleteIndexTemplateRequest` (name) lack setters and require those parameters in the constructor.'
8120,'pickypg',"SearchRequestBuilder passes aggregation bytes as facet bytes\nThis issue does not exist on master because facets were removed, but it persists on 1.x where they're only deprecated. Chances are that this method is rarely used in this form.\r\n\r\n```java\r\nsourceBuilder().facets(aggregations, aggregationsOffset, aggregationsLength);\r\n```\r\n\r\n`facets` should be `aggregations`."
8110,'colings86','Reducers - Post processing of aggregation results\n# Overview\r\n\r\nThis feature would introduce a new \'reducers\' module, allowing for the processing and transformation of aggregation results. The current aggregation module is very powerful and can compute varied and complex analytics but is limited when calculating analytics which depend on numerous independently calculated aggregated metrics. Currently, the only way to calculate these types of complex metrics is to write client side logic.  The reducer module will add a new phase in the search request where the coordinating node can pass the reduced aggregations for further processing.\r\n\r\n# Key Concepts & Terminology\r\n\r\n* *Reducer* - A reducer is the computation unit which receives a `bucket aggregation` and uses it\'s set of buckets to perform calculations and generate a new `aggregation` which represents either a single answer (a new `metric aggregation`) or a new set of buckets (a new `bucket aggregation`). Reducers run after aggregations and act on the result tree returned by aggregations or the result tree of previous reducers. There are 2 types of `reducer`:\r\n    * *Bucket reducer* - Take all specified buckets and divide them into groups (`selections`). A `bucket reducer` may create new buckets to add to the `selections` but is not able to modify existing buckets.\r\n    * *Metric reducer* - Performs calculations on the input aggregation and outputs a single result (i.e. cannot create buckets)\r\n\r\n## Reducer Structure\r\n\r\nThe following snippet captures the basic structure of aggregations:\r\n\r\n```javascript\r\n"reducers" : [\r\n    {\r\n        "<reducer_name>" : {\r\n            "<reducer_type>" : {\r\n                <reducer_body>\r\n            },\r\n            "reducers" : [ \r\n                {\r\n                    <sub_reducer_1>\r\n                },\r\n                {\r\n                    <sub_reducer_2>\r\n                },\r\n                ...\r\n           ] \r\n        }\r\n    },\r\n    {\r\n        "<reducer_name_2>" : { ... }\r\n    },    ...\r\n]\r\n```\r\n\r\nSome rules for reducers requests:\r\n* reducers and aggregations are separate in the request structure\r\n* reducers are specified in order because they can process the output of other reducers\r\n* reducers can be nested, in which case they only see the data for the bucket they\'re in, although they can refer to any value in the tree with `_root.path.to.value`\r\n\r\n# Use Cases\r\nSome possible implementations of reducers are detailed below:\r\n## Bucket Reducers\r\n* Sliding window - Can be used with the results of a histogram aggregation to produce selections representing a sliding time window of buckets. The reducer would create a selection for each bucket in the histogram aggregation which contains the bucket and the N subsequent buckets (where N is a configurable window size). \r\n\r\n\r\n## Metric Reducers\r\n* Avg/Min/Max/Sum/Stats - Same as their aggregation equivalent except they calculate the result for a particular field in their input aggregation\'s buckets.\r\n* Delta - Calculates the range of the values of a field between the input buckets\r\n* Gradient - Calculates the range of values of two fields between the input buckets and returns the result of the first divided by the second (e.g. change in price divided by change in time)\r\n\r\n## Example\r\n\r\nAs an example, imagine that you have sales data in an index and you would like to calculate the derivative of the price field over time. This is useful for determining trends in the data, such as whether sales are increasing linearly over time.\r\n\r\nYou might start with the following aggregation:\r\n\r\n```javascript\r\n{\r\n  "aggs": {\r\n    "months": {\r\n      "date_histogram": {\r\n        "field": "date",\r\n        "interval": "month"\r\n      },\r\n      "aggs": {\r\n        "avg_price": {\r\n          "avg": {\r\n            "field": "price"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\nThe results of these aggregations might look something like the following:\r\n\r\n```javascript\r\n"aggregations": {\r\n  "months": {\r\n     "buckets": [\r\n        {\r\n           "key_as_string": "2014/02/01 00:00:00",\r\n           "key": 1391212800000,\r\n           "doc_count": 6,\r\n           "avg_price": {\r\n              "value": 11\r\n           }\r\n        },\r\n        {\r\n           "key_as_string": "2014/03/01 00:00:00",\r\n           "key": 1391212800000,\r\n           "doc_count": 4,\r\n           "avg_price": {\r\n              "value": 19\r\n           }\r\n        },\r\n        {\r\n           "key_as_string": "2014/04/01 00:00:00",\r\n           "key": 1391212800000,\r\n           "doc_count": 8,\r\n           "avg_price": {\r\n              "value": 31\r\n           }\r\n        }\r\n     ]\r\n  }\r\n}\r\n```\r\n\r\nThe sliding window reducer could then be used to request time windows of two months. A nested delta reducer inside the sliding window reducer could be used to calculate the rate of change of price for each two month selection.\r\n```javascript\r\n"reducers": [\r\n  {\r\n    "two_months": {\r\n      "sliding_window": {\r\n        "buckets": "months",\r\n        "window": 2\r\n      },\r\n      "reducers": [\r\n        {\r\n          "price_derivative": {\r\n            "gradient": {\r\n              "field": "avg_price.value"\r\n            }\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n]\r\n\r\n```\r\n\r\nWhich would produce the following response (derivative value is shown in price per month):\r\n```javascript\r\n{\r\n  "reducers": {\r\n    "two_months": {\r\n      "selections": [\r\n        {\r\n          "buckets": [\r\n            {\r\n              "key_as_string": "2014/02/01 00:00:00",\r\n              "key": 1391212800000,\r\n              "doc_count": 6,\r\n              "avg_price": {\r\n                "value": 11\r\n              }\r\n            },\r\n            {\r\n              "key_as_string": "2014/03/01 00:00:00",\r\n              "key": 1391212800000,\r\n              "doc_count": 4,\r\n              "avg_price": {\r\n                "value": 19\r\n              }\r\n            }\r\n          ],\r\n          "price_derivative": {\r\n            "value": 8\r\n          }\r\n        },\r\n        {\r\n          "buckets": [\r\n            {\r\n              "key_as_string": "2014/03/01 00:00:00",\r\n              "key": 1391212800000,\r\n               "doc_count": 4,\r\n               "avg_price": {\r\n                "value": 19\r\n               }\r\n            },\r\n            {\r\n              "key_as_string": "2014/04/01 00:00:00",\r\n              "key": 1391212800000,\r\n              "doc_count": 8,\r\n              "avg_price": {\r\n                "value": 31\r\n              }\r\n            }\r\n          ],\r\n          "price_derivative": {\r\n            "value": 12\r\n          }\r\n        }\r\n      ]\r\n    }\r\n  }\r\n}\r\n```'
8100,'martijnvg','Children aggregation for recursive parent-child relation leads to incorrect results\nHi,\r\n\r\nI tried to create recursive parent-child relation:\r\n\r\nfamily-mapping.json:\r\n```javascript\r\n{\r\n  "mappings": {\r\n    "member": {\r\n      "_parent": {\r\n        "type": "member" \r\n      },\r\n      "properties" : { \r\n        "name" : {\r\n          "type" : "string",\r\n          "index" : "not_analyzed"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nfamily.json:\r\n```javascript\r\n{ "index" : { "_id" : "1", "parent" : "0" } }\r\n{ "name" : "Abraham Simpson" }\r\n{ "index" : { "_id" : "2", "parent" : "1" } }\r\n{ "name" : "Homer J Simpson" }\r\n{ "index" : { "_id" : "3", "parent" : "2" } }\r\n{ "name" : "Bart Simpson" }\r\n```\r\n\r\naggregation.json:\r\n````javascript\r\n{\r\n  "aggs" : {\r\n    "parent" : {\r\n       "terms" : { "field" : "name" },\r\n       "aggs" : {\r\n          "child-members" : {\r\n             "children" : { "type" : "member" },\r\n             "aggs" : {\r\n               "child-names" : {\r\n                 "terms" : { "field" : "name" }\r\n               }\r\n             }\r\n          }\r\n       }\r\n    }\r\n  }\r\n}\r\n````\r\n\r\nWhen I\'m trying to aggregate this data I\'m getting child buckets inside parent ones with same name:\r\n```javascript\r\n.....\r\n  "aggregations" : {\r\n    "parent" : {\r\n      "doc_count_error_upper_bound" : 0,\r\n      "buckets" : [ {\r\n        "key" : "Abraham Simpson",\r\n        "doc_count" : 1,\r\n        "child-members" : {\r\n          "doc_count" : 1,\r\n          "child-names" : {\r\n            "doc_count_error_upper_bound" : 0,\r\n            "buckets" : [ {\r\n              "key" : "Abraham Simpson",\r\n              "doc_count" : 1\r\n            } ]\r\n          }\r\n        }\r\n      }, {\r\n        "key" : "Bart Simpson",\r\n        "doc_count" : 1,\r\n        "child-members" : {\r\n          "doc_count" : 1,\r\n          "child-names" : {\r\n            "doc_count_error_upper_bound" : 0,\r\n            "buckets" : [ {\r\n              "key" : "Bart Simpson",\r\n              "doc_count" : 1\r\n            } ]\r\n          }\r\n        }\r\n      }, {\r\n        "key" : "Homer J Simpson",\r\n        "doc_count" : 1,\r\n        "child-members" : {\r\n          "doc_count" : 1,\r\n          "child-names" : {\r\n            "doc_count_error_upper_bound" : 0,\r\n            "buckets" : [ {\r\n              "key" : "Homer J Simpson",\r\n              "doc_count" : 1\r\n            } ]\r\n          }\r\n        }\r\n      } ]\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI\'m using 1.4.0-beta1 version (downloaded from elasticsearch.org). '
8096,'areek','Completion suggester ignores filters set on index alias\nTo reproduce:\r\n1) Create an index\r\n2) Create an alias with a filter on the index that restricts the set of document returned\r\n3) Use the completion suggester on a field of your choosing targeting the alias created\r\n4) Notice that results are returned that are filtered out on a normal search query on the alias\r\n\r\nI think this problem may generalize to all suggesters, not just the completion suggester.\r\n\r\n'
8095,'markharwood','New feature - add lookup by Lucene doc ID to ShardTermVectorService.getTermVector\nAs part of breaking up https://github.com/elasticsearch/elasticsearch/pull/6796 into more digestible pieces we need a service to fetch the tokens in a field.\r\nThe existing ShardTermVectorService.getTermVector(..) handles much of the complexity of reading stored TermVectors or using Analyzers on source fields to provide terms but is predicated on the caller providing an es ID (primary key) to identify the required document. In the context of analyzing query results, the caller has a collection of Lucene document IDs and not primary keys and so it would be useful/faster if the ShardTermvectorService could cope with looking up TermVectors given a Lucene document ID.'
8089,'areek','Completion Suggester Highlighting\n## Highlighting\r\nMany search-as-you-type implementations highlight the portion of the term which has already been entered. We plan to add this to the response as we well.\r\n[You complete me (August 22, 2013)](http://www.elasticsearch.org/blog/you-complete-me/)\r\n\r\nAre there any plans when these feature will be implemented?'
8082,'jpountz','Histogram not working as advertised?\nI\'ve been trying to make sense of Elasticssearch histogram aggregations the last couple of days. And I\'ve found that they don\'t work as expected, or even advertised.\r\n\r\nLets say i want to aggregate like so:\r\n\r\n    "aggregations": {\r\n      "sea_water_temperature": {\r\n        "histogram": {\r\n          "field": "sea_water_temperature",\r\n          "interval": 3\r\n        }\r\n      }\r\n    }\r\n\r\nResponse buckets looks fine at first glance, but when trying to query for documents within the bounds of a bucket I don\'t get the same document count as the bucket suggested. E.g.\r\n\r\n    "filter": {\r\n      "range": {\r\n        "sea_water_temperature": {\r\n          "lt": 0,\r\n          "gte": -3\r\n        }\r\n      }\r\n    }\r\n\r\nThis could give x results while the bucket "-3" had a doc_count of y. This seems to only be an issue for negative bucket keys.\r\n\r\nIn the docs for histogram it states that the bucket key for a given value is:\r\n\r\n    rem = value % interval\r\n    if (rem < 0) {\r\n      rem += interval\r\n    }\r\n    bucket_key = value - rem\r\n\r\nHowever I tried a term aggregation with that as a value script:\r\n\r\n    "aggregations": {\r\n      "sea_water_temperature": {\r\n        "terms": {\r\n          "field": "sea_water_temperature",\r\n          "script": "rem = _value % interval; rem = rem < 0 ? rem + interval : rem; _value - rem",\r\n          "params": {\r\n            "interval": 3\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nThat gives me the same kind of bucketing as histogram does but now my filter queries actually match the doc_counts of the buckets(!). Why isn\'t histogram working as described? or am I missing something?'
8072,'colings86','add params properly when using wildcard\nfix #8071'
8071,'colings86','PathTrie wrongly adds params\nFor instance, PathTrie builds two tries: "/a/{x}/b", "/{y}/c/d".  While retrieving "/a/c/d", it breaks for the params are {x=c, y=a} instead of {y=a}. We should push params before retrieving next token and pop them if we have to use wildcard to retrieve again.'
8049,'rmuir','[Analysis] Parse synonyms with the same analysis chain\nChange behavior synonym filter factory.\r\nNow, synonyms are tokenized with the whitespace tokenizer or a tokenizer specified "tokenizer" parameter.\r\nThis PR is to tokenize synonyms with whatever tokenizer and token filters appear before it in the chain.\r\n* change parsing synonyms from constructor to building custom analyzer\r\n* clone SynonymTokenFilterFactory with the analysis chain in CustomAnalyzerProvider\r\n\r\n\r\nClose #7199'
8041,'tlrx','Add Node Name to _cat/recovery\nWe have multiple data nodes, on a single server and when using _cat/recovery it could be useful to not only display the Host Name, but also the Node Name, so we can identify which node is in recovery process.'
8009,'s1monw','Shard initialization fails with DocValues exception\nHi,\r\n\r\n  we have been running into strange errors lately. We get a lot of exceptions of the type: \r\n\r\n```\r\n[2014-10-07 06:41:26,235][WARN ][cluster.action.shard     ] [Madcap] [my_index][1] sending failed shard for [my_index][1], node[-QGTVk8RRcuKuBwdqD8l1A], [P], s[INITIALIZING], indexUUID [u68VqfHsRii16gYXtPj1cQ], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[my_index][1] failed recovery]; nested: IllegalArgumentException[cannot change DocValues type from BINARY to SORTED_SET for field "custom.my_field"]; ]]\r\n[2014-10-07 06:41:26,587][WARN ][indices.cluster          ] [Madcap] [my_index][1] failed to start shard\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [my_index][1] failed recovery\r\n  at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\n  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n  at java.lang.Thread.run(Thread.java:745)\r\nCaused by: java.lang.IllegalArgumentException: cannot change DocValues type from BINARY to SORTED_SET for field "custom.my_field"\r\n  at org.apache.lucene.index.FieldInfos$FieldNumbers.addOrGet(FieldInfos.java:198)\r\n  at org.apache.lucene.index.IndexWriter.getFieldNumberMap(IndexWriter.java:868)\r\n  at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:819)\r\n  at org.elasticsearch.index.engine.internal.InternalEngine.createWriter(InternalEngine.java:1420)\r\n  at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:271)\r\n  at org.elasticsearch.index.shard.service.InternalIndexShard.postRecovery(InternalIndexShard.java:692)\r\n  at org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:217)\r\n  at org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n  ... 3 more\r\n```\r\n\r\n  We are in a daily index situation so the index is quite new. It contains 10 to 20 millions of documents spread over 10 shards and 5 nodes. At some point (after hours of the index being green), one of the shards becomes INITALIZING and can never start (because of the aforementioned exception). The index is then in red state and we cannot set it back on track... In this case the only solution we have found is to scroll over the whole index and reindex the data into a new index (but we most likely have lost the data from the failing shard). The field that causes the issue has the following definition `{"type":"long","doc_values":true,"include_in_all":false}`. This mapping is inferred from a dynamic template `{"mapping":{"index":"not_analyzed","include_in_all":false,"doc_values":true,"type":"{dynamic_type}"},"match":"*"}`.\r\n  One important note is that this part of the data is free-form (ie user input) and it is possible that some documents have conflicting types (one document having the field as string, the other as long); that\'s why the index has the setting `index.mapping.ignore_malformed` set to `true`.\r\n  Also, it might not be relevant, but this only happened on days when we had at least one node that was restarted.\r\n  We have noticed this issue since running ElasticSearch 1.3.4 (but can\'t be 100% sure that it did not happen before).\r\n\r\n  We cannot isolate and reproduce the issue but have faced it several times over the past few days. Feel free to suggest actions we can undertake should it happen again, to get more details to help fix it. Also, if you have suggestions to help bypass the issue when it happens (so that we can avoid reindexing the data), that\'d be great.\r\n\r\nThanks,\r\nEmmanuel'
8003,'jpountz','Duplicate document found while searching with parent/child setup - ES 1.0\nelasticsearchserver_node1:7104/metadata/asset/_search?q=_id:641804  returns two documents\r\nBut\r\nelasticsearchserver_node1:7104/metadata/asset/641804?parent=80018039 returns only one document\r\n\r\nThere was not update to the document after initial index.\r\n\r\nelasticsearchserver_node1:7104/metadata/asset/_search?q=_id:641804 returns following results\r\n\r\n=========\r\n\r\n{\r\n    "took": 2,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 10,\r\n        "successful": 10,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 2,\r\n        "max_score": 1,\r\n        "hits": [\r\n            {\r\n                "_index": "metadata",\r\n                "_type": "asset",\r\n                "_id": "641804",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "createdDate": 1410901296000,\r\n                    "lastUpdatedDate": 1410901295989,\r\n                    "id": 641804,\r\n                    "status": "NEW",\r\n                    "parentAssetId": 0,\r\n                    "masterAssetId": null,\r\n                    "source": "AUTOMATION",\r\n                    "spaceCode": "creative-services",\r\n                    "originatingAssetId": null,\r\n                    "spaceShareCount": null,\r\n                    "childCount": null,\r\n                    "isAllMainFilesPresent": false,\r\n                    "assetPath": [],\r\n                    "assetExtension": {\r\n                        "assetId": 641804,\r\n                        "territoryCode": "LATAM",\r\n                        "languageCode": null,\r\n                        "partnerProvidedLocalizedName": null,\r\n                        "personId": 0,\r\n                        "characterId": 0\r\n                    },\r\n                    "assetFiles": [],\r\n                    "assetAttributes": [],\r\n                    "sharedSpaceCodes": [],\r\n                    "futureEffectiveDate": null,\r\n                    "assetWindows": [\r\n                        {\r\n                            "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",\r\n                            "windowStartDate": 1416016800000,\r\n                            "windowEndDate": 1463281200000\r\n                        }\r\n                    ],\r\n                    "assetSchema": "asset",\r\n                    "slaveAsset": false,\r\n                    "publishedDerivativePresent": false,\r\n                    "sharedAcrossSpace": true,\r\n                    "mainFiles": [],\r\n                    "folderTypeAsset": false,\r\n                    "genericAsset": false,\r\n                    "metaTypeAsset": true,\r\n                    "metaId": 80018039,\r\n                    "sourceFiles": [],\r\n                    "futurePublishedDerivativePresent": false,\r\n                    "characterAsset": false,\r\n                    "personAsset": false,\r\n                    "masterAsset": false,\r\n                    "timestamp": 1410901295989,\r\n                    "_parent": "80018039"\r\n                }\r\n            },\r\n            {\r\n                "_index": "movie_metadata",\r\n                "_type": "asset",\r\n                "_id": "641804",\r\n                "_score": 1,\r\n                "_source": {\r\n                    "_index": "metadata",\r\n                    "_type": "asset",\r\n                    "_id": "641804",\r\n                    "_score": 1,\r\n                    "_source": {\r\n                        "createdDate": 1410901296000,\r\n                        "lastUpdatedDate": 1410901295989,\r\n                        "id": 641804,\r\n                        "status": "NEW",\r\n                        "parentAssetId": 0,\r\n                        "masterAssetId": null,\r\n                        "source": "AUTOMATION",\r\n                        "spaceCode": "creative-services",\r\n                        "originatingAssetId": null,\r\n                        "spaceShareCount": null,\r\n                        "childCount": null,\r\n                        "isAllMainFilesPresent": false,\r\n                        "assetPath": [],\r\n                        "assetExtension": {\r\n                            "assetId": 641804,\r\n                            "territoryCode": "LATAM",\r\n                            "languageCode": null,\r\n                            "partnerProvidedLocalizedName": null,\r\n                            "personId": 0,\r\n                            "characterId": 0\r\n                        },\r\n                        "assetFiles": [],\r\n                        "assetAttributes": [],\r\n                        "sharedSpaceCodes": [],\r\n                        "futureEffectiveDate": null,\r\n                        "assetWindows": [\r\n                            {\r\n                                "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",\r\n                                "windowStartDate": 1416016800000,\r\n                                "windowEndDate": 1463281200000\r\n                            }\r\n                        ],\r\n                        "assetSchema": "asset",\r\n                        "slaveAsset": false,\r\n                        "publishedDerivativePresent": false,\r\n                        "sharedAcrossSpace": true,\r\n                        "mainFiles": [],\r\n                        "folderTypeAsset": false,\r\n                        "genericAsset": false,\r\n                        "metaTypeAsset": true,\r\n                        "metaId": 80018039,\r\n                        "sourceFiles": [],\r\n                        "futurePublishedDerivativePresent": false,\r\n                        "characterAsset": false,\r\n                        "personAsset": false,\r\n                        "masterAsset": false,\r\n                        "timestamp": 1410901295989,\r\n                        "_parent": "80018039"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n\r\n========\r\n\r\nelasticsearchserver_node1:7104/metadata/asset/641804?parent=80018039  returns the following result\r\n\r\n{\r\n    "_index": "movie_metadata",\r\n    "_type": "asset",\r\n    "_id": "641804",\r\n    "_version": 1,\r\n    "found": true,\r\n    "_source": {\r\n        "createdDate": 1410901296000,\r\n        "lastUpdatedDate": 1410901295989,\r\n        "id": 641804,\r\n        "status": "NEW",\r\n        "parentAssetId": 0,\r\n        "masterAssetId": null,\r\n        "source": "AUTOMATION",\r\n        "spaceCode": "creative-services",\r\n        "originatingAssetId": null,\r\n        "spaceShareCount": null,\r\n        "childCount": null,\r\n        "isAllMainFilesPresent": false,\r\n        "assetPath": [],\r\n        "assetExtension": {\r\n            "assetId": 641804,\r\n            "territoryCode": "LATAM",\r\n            "languageCode": null,\r\n            "partnerProvidedLocalizedName": null,\r\n            "personId": 0,\r\n            "characterId": 0\r\n        },\r\n        "assetFiles": [],\r\n        "assetAttributes": [],\r\n        "sharedSpaceCodes": [],\r\n        "futureEffectiveDate": null,\r\n        "assetWindows": [\r\n            {\r\n                "partnerId": "dcbd39e0-eff9-11e2-882e-12313924e0a4",\r\n                "windowStartDate": 1416016800000,\r\n                "windowEndDate": 1463281200000\r\n            }\r\n        ],\r\n        "assetSchema": "asset",\r\n        "slaveAsset": false,\r\n        "publishedDerivativePresent": false,\r\n        "sharedAcrossSpace": true,\r\n        "mainFiles": [],\r\n        "folderTypeAsset": false,\r\n        "genericAsset": false,\r\n        "metaTypeAsset": true,\r\n        "metaId": 80018039,\r\n        "sourceFiles": [],\r\n        "futurePublishedDerivativePresent": false,\r\n        "characterAsset": false,\r\n        "personAsset": false,\r\n        "masterAsset": false,\r\n        "timestamp": 1410901295989,\r\n        "_parent": "80018039"\r\n    }\r\n}\r\n\r\n\r\n===='
7999,'suyograo','Admin: Filter non data nodes from _cat/allocation output\nCloses #7998'
7998,'suyograo','_cat/allocation output should filter non data nodes\nShard allocation cannot happen on non data nodes, so there is no use showing `0b` for a non data node in the output. It is better to remove those rows\r\n\r\nExample -- \r\n```\r\n5 61.5gb 403.5gb 465.1gb 13 Suyogs-MacBook-Pro.local 192.168.1.6 Sphinxor\r\n0     0b                    Suyogs-MacBook-Pro.local 192.168.1.6 Hoder\r\n```'
7990,'bleskes','Move Clear Indices Cache, Flush, Optimize, Recovery, Refresh, Segments, Indices Stats, Indices Status  API to use TransportNodesOperationAction\nAll of those APIs send requests to all shards (both primary and replicas) of their relevant indices. They currently use the TransportBroadcastOperationAction infrastructure.\r\n\r\nTransportBroadcastOperationAction is built to send request for one shard per replication group and send a request per shard it needs (and falls back  upon failure). The above API do not need this fall back mechanism and are commonly sent to many shards at once (for example, `GET _stats` goes to all shards of all indices). This results in an inefficiency where we may send hundreds of requests to a single node (one per every shard it holds). It will be more efficient to group those request per node and use the TransportNodesOperationAction model. \r\n'
7985,'jpountz','Disabling doc values silently ignored\nIf you create a field with `doc_values` set to `false` (or not specified), then trying to update the mapping to `true` throws an exception.\r\n\r\nHowever, if `doc_values` is initially set to `true`, then trying to update it to `false` is silently ignored:\r\n\r\n    DELETE /t\r\n    PUT /t\r\n    POST /t/_mapping/t\r\n    {\r\n      "properties": {\r\n        "foo": {\r\n          "type": "long",\r\n          "doc_values": true\r\n        }\r\n      }\r\n    }\r\n    \r\n    POST /t/_mapping/t\r\n    {\r\n      "properties": {\r\n        "foo": {\r\n          "type": "long",\r\n          "doc_values": false\r\n        }\r\n      }\r\n    }\r\n    \r\n    GET /t/_mapping\r\n\r\nReturns:\r\n\r\n    {\r\n       "t": {\r\n          "mappings": {\r\n             "t": {\r\n                "properties": {\r\n                   "foo": {\r\n                      "type": "long",\r\n                      "doc_values": true\r\n                   }\r\n                }\r\n             }\r\n          }\r\n       }\r\n    }'
7961,'jpountz','Fielddata: Add doc values support to boolean fields.\nThis pull request makes boolean handled like dates and ipv4 addresses: things\r\nare stored as as numerics under the hood and aggregations add some special\r\nformatting logic in order to return true/false in addition to 1/0.\r\n\r\nFor example, here is an output of a terms aggregation on a boolean field:\r\n```\r\n   "aggregations": {\r\n      "top_f": {\r\n         "doc_count_error_upper_bound": 0,\r\n         "buckets": [\r\n            {\r\n               "key": 0,\r\n               "key_as_string": "false",\r\n               "doc_count": 2\r\n            },\r\n            {\r\n               "key": 1,\r\n               "key_as_string": "true",\r\n               "doc_count": 1\r\n            }\r\n         ]\r\n      }\r\n   }\r\n```\r\n\r\nSorted numeric doc values are used under the hood.\r\n\r\nClose #4678\r\nClose #7851'
7929,'GaelTadh','Indexed script fixes\n On Disk Scripts : Make on disk scripts re-load timing dynamically configurable.\r\nThis change adds a new resource watcher level of CUSTOM and allows the addition of custom levels of timing.\r\nThis is used by the ScriptService to register the FileWatcher.\r\n\r\nSearch Templates: Add render endpoint for rendering templates\r\nThis commit adds a render endpoint to allow rendering of templates.\r\n````\r\nPOST /_render/template\r\n\'{\r\n  "template":\r\n  {\r\n    "query":\r\n      {\r\n        "{{foo}}": {}\r\n      },\r\n    "size": "{{my_{{}}size}}"\r\n   },\r\n   "params": { "foo": "match_all", "my_size": 10}\r\n}\'\r\n````\r\nWill render\r\n````\r\n{\r\n  "template" :\r\n  {\r\n    "query" : {\r\n      "match_all" : {}\r\n    },\r\n    "size" : 10\r\n  }\r\n}\r\n````\r\n\r\nCloses #6821'
7891,'s1monw','Cleaner query parse error feedback\nProvides the line and column number in user input where errors were spotted and a succinct parse error message, free of server-side stack trace info.\r\n\r\nMany "*parser" classes are changed but the core framework changes are:\r\n\r\n1) XContentParser can now return a new "XContentLocation" object describing position of last token parsed\r\n2) Existing SearchParseException and QueryParsingException classes have all constructors changed to take an XContentLocation object to encourage supply of detailed parse feedback\r\n3) Common "UserInputException" interface introduced for exceptions relating to user input (the 2 above).\r\n4) SearchPhaseExecutionException that gathers results from many shards holds onto only the first of several (typically duplicate) UserInputExceptions from shards\r\n5) BytesRestResponse outputs extra JSON fields for parse failures including line, column and succinct error message.\r\n\r\n\r\nCloses #3303'
7887,'imotov','Update indexes settings during a Snapshot restore\nIt could be usefull to have a way to update settings during a restore snapshot. For example, you can set replica to 0 when restoring a snapshot on a "dev" environment.\r\n\r\n'
7883,'clintongormley','Proper Debian repository\nHi,\r\n\r\nI\'ve been using Elasticsearch in production for a couple of years, a various Debian (stable) servers.\r\n\r\nI really appreciate having a .deb available on your website, and I\'ve cooked myself a small script to upgrade when a new version is released.\r\n\r\nIt would be even more awesome if you had [a proper Debian repository](https://wiki.debian.org/HowToSetupADebianRepository) set up. We\'d have to add it in our `sources.list` file and upgrading would be even easier than it is now.\r\n\r\nI\'m not familiar with the amount of work necessary for setting up such a repository, but I\'m pretty sure all your Debian and Ubuntu users would be pleased and a few of them might even offer help.\r\n\r\nI hope you won\'t file this in the "Unfulfilled Promises and Broken Dreams" [category](https://github.com/rubygems/rubygems/issues?q=is%3Aopen+is%3Aissue+milestone%3A%22Unfulfilled+Promises+and+Broken+Dreams%22).\r\n\r\nThanks a ton for making Elasticsearch et al.'
7878,'s1monw',"Added graceful shutdown behaviour\nWe at Crate recently implemented a procedure to gracefully stop nodes moving shards away before finally shutting down in order keep all the data available.\r\nThis can be used to perform a rolling upgrade of the cluster **with strong data availability guarantees**.\r\n\r\nWe've also seen, that there were already some discussions about how to implement that in ES here: https://github.com/elasticsearch/elasticsearch/issues/4248\r\n\r\nThis PR contains the following changes:\r\n\r\n- added new settings called:\r\n`cluster.graceful_stop.min_availability`, \r\n`cluster.graceful_stop.reallocate`, \r\n`cluster.graceful_stop.timeout`, \r\n`cluster.graceful_stop.force` \r\nto control the shutdown behaviour\r\n(documented here: https://crate.io/docs/en/latest/configuration.html#graceful-stop)\r\n\r\n- added signal handling to perform a graceful shutdown on receiving the `USR2` signal (the current implementation depends on classes that are only available in the oracle and openjdk VMs)\r\n\r\n- added DISABLED state for `LifecycleComponent` and transition methods\r\n\r\nThe shutdown process works as follows:\r\n\r\n1. after sending graceful shutdown signal, all services of InternalNode will be disabled, the current implementation only rejects new HTTP requests on this node as transport connections are usually steady connections\r\n\r\n2. depending on the graceful_stop settings listed above the node will deallocate its shards using the AllShardsDeallocator/PrimariesDeallocator classes\r\n\r\n3. node shuts down once de-/reallocation is finished or returns back to its prior state on any error (if `cluster.graceful_stop.force` is not set to true)\r\n"
7859,'imotov','Getting all snapshots is blocked by in-progress snapshot\nThis would block until current snapshot is completed. This could take a looooooooong time.\r\n\r\n```\r\ncurl http://es:9200/_snapshot/<s3_storage>/_all?pretty\r\n```\r\n\r\nHappens with s3 plugin, maybe this issue belongs to it.\r\n\r\ncc @imotov '
7851,'jpountz','Core type Boolean cannot be set as Doc_Value\nI suggest that the core type ‘Boolean’ can be set with Doc_Value attribute, just like the other core types – string, number, date and binary. \r\nIn system with many different bools the fielddata use a lot of memory and doc_values is great to prevent this.\r\n'
7850,'brwe','significant terms: add scriptable significance heuristic\nThis commit adds scripting capability to significant_terms.\r\nCustom heuristics can be implemented with a script that provides\r\nparameters subset_freq, superset_freq,subset_size, superset_size.\r\n\r\nScript heuristic and results have to be serialized. When deserializing the ScriptService is not available and the new score cannot be computed immediately after reading the result.\r\nTherefore, instead of re-computing the score the score is serialized with each bucket. \r\nFor the reduce phase the script is initialized before buckets are reduced (https://github.com/brwe/elasticsearch/compare/elasticsearch:master...brwe:significant-terms-scripting?expand=1#diff-ba5a615f489137f2b16d37bc54dd3f8dR174) which seems a little clumsy to me. Happy for any suggestion.'
7847,'areek','Suggest functional test not working\nI\'m building a small library to interface with elasticsearch more like Django querysets, and part of the functionality involves suggest. In the test suite I have a fake index with mapping and add a bunch of documents using elasticsearch.py with `refresh=True` The issue is with adding extra parameters to term suggest, specifically `min_word_length`, `min_doc_freq`, and `max_term_freq`. \r\n\r\nhere is a general overview of the snippets\r\n```\r\ncurl -XPUT "http://localhost:9200/foo" -d \'{\r\n    "mappings": {\r\n        "my_doc_type": {\r\n            "properties": {\r\n                "location": {\r\n                    "type": "geo_point"\r\n                },\r\n                "foo_loc": {\r\n                    "type": "geo_point"\r\n                },\r\n                "child": {\r\n                    "type": "nested"\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XPOST "http://localhost:9200/foo/_refresh"\r\n\r\n# Helper function that takes the index name and document json\r\ndef add_document(index, document):\r\n    curl -XPOST "http://localhost:9200/foo/my_doc_type?refresh=true&op_type=create" -d document\r\n\r\n# This is what the min_word_length query looks like\r\nquery = {\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "suggest": {\r\n    "spelling": {\r\n      "text": "bazba",\r\n      "term": {\r\n        "sort": "score",\r\n        "field": "bar",\r\n        "suggest_mode": "missing",\r\n        "max_edits": 2,\r\n        "prefix_length": 1,\r\n        "min_doc_freq": 0,\r\n        "max_term_freq": 0.01,\r\n        "min_word_length": 5\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\nThis query/setup returns suggestions with length 4'
7838,'bleskes','Index overwrites alias\nusing 1.3.2\r\nhere is the scenario:\r\n- i have an alias which points to an index\r\n- i constantly write data to that alias\r\n- the cluster (all 4 nodes) failed with an oome (because of an aggregation with lots of nested cardinalities) and i restarted all nodes at once\r\n- i continue to write data while cluster is being restarted\r\n- after restart i see that the alias is gone and is replaced by an index with the same name'
7836,'s1monw','Fix unnecessary cache evictions when setting indices.fielddata.cache.size \nAs described in user group posts including \r\nhttps://groups.google.com/d/msgid/elasticsearch/07cfb705-1f13-40d5-b7b2-c5b84e328ddb%40googlegroups.com and https://groups.google.com/d/msgid/elasticsearch/14703918-0450-42c1-810b-edf3967951b6%40googlegroups.com\r\n\r\nGuava needs some changes to better utilize ram when using eviction by size (e.g., when indices.fielddata.cache.size is set in ES).\r\n\r\nProposed changes to Guava are here https://code.google.com/r/craigwi-guava/.\r\n\r\nUpdate: also opened Guava issue: https://code.google.com/p/guava-libraries/issues/detail?id=1854.'
7831,'imotov','Snapshot/Restore: Add repository validation to URL Repository\nFollow-up to #7096'
7828,'dadoonet','java: QueryBuilders cleanup\nSome QueryBuilders are missing or have a different naming than the other ones:\r\n\r\n* template query does not exist in QueryBuilders. We should add `templateQuery(...)`.\r\n* `commonTerms(...)` should be deprecated and `commonTermsQuery(...)` should be added.\r\n* `textPhrase(...)` was deprecated in 2011. Could be removed.\r\n* `textPhrasePrefix(...)` was deprecated in 2011. Could be removed.\r\n* `queryString(...)` should be deprecated and `queryStringQuery(...)` should be added.\r\n* `simpleQueryString(...)` should be deprecated and `simpleQueryStringQuery(...)` should be added.\r\n* `fieldMaskingSpanQuery(...)` was never documented. Could potentially be removed.\r\n* both `filtered(...)` and `filteredQuery(...)` exist. We could deprecate potentially `filtered(...)`.\r\n* `inQuery(...)` was never documented. Could potentially be removed.\r\n* `wrapperQuery(...)` was never documented. Could potentially be removed.\r\n\r\n@s1monw WDYT?'
7826,'dadoonet','Docs: java add missing queries / filters\nSince 1.0, new queries and filters has been added to elasticsearch but were not documented in Java guide:\r\n\r\nQueries:\r\n\r\n* commonTerms\r\n* functionScoreQuery\r\n* simpleQueryString\r\n* regexpQuery\r\n* spanMultiTermQueryBuilder\r\n* TemplateQueryBuilder\r\n\r\nFilters:\r\n\r\n* geoHashCellFilter\r\n* regexpFilter\r\n'
7814,'brwe','function_score: match only document with score above custom score thresh...\n...old\r\n\r\nfuncton_score matched each document regardless of the computed score.\r\nThis commit adds a query parameter `min_score` (-Float.MAX_VALUE default).\r\nDocuments that have a score lower than this threshold will not be mached.\r\n\r\ncloses #6952'
7805,'martijnvg','Issue when applying nested aggregation and nested filter\nConsider a scenario.\r\nI\'ve a mapping like this, \r\n\r\n    PUT /x1/x4/_mapping\r\n    {\r\n      "x4": {\r\n        "properties": {\r\n          "playerYears": {\r\n            "type": "nested",\r\n            "properties": {\r\n              "schoolsOfInterest": {\r\n                "type": "nested",\r\n                "properties": {\r\n                  "name": {\r\n                    "type": "string",\r\n                    "index": "not_analyzed"\r\n                  }\r\n                }\r\n              },\r\n              "rating": {\r\n                "type": "float"\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nConsider simple data, \r\n\r\n    POST /x1/x4/1\r\n    {\r\n      "playerYears": {\r\n        "rating": 10,\r\n        "schoolsOfInterest": {\r\n          "name": "x"\r\n        }\r\n      }\r\n    }\r\n\r\nAnd, When I apply filter like below,\r\n\r\n    POST /x1/x4/_search\r\n    {\r\n      "size": 0,\r\n      "aggs": {\r\n        "rating": {\r\n          "nested": {\r\n            "path": "playerYears"\r\n          },\r\n          "aggs": {\r\n            "rating-filtered": {\r\n              "filter": {\r\n                "nested": {\r\n                  "path": "playerYears.schoolsOfInterest",\r\n                  "query": {\r\n                    "match_all": {}\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      "query": {\r\n        "match_all": {}\r\n      }\r\n    }\r\n\r\n\r\nThen result is zero for `rating-filtered` aggregation. \r\n\r\nI am not able to figure it out why, can anyone help ? \r\n\r\n'
7798,'s1monw','Delete Index failed - not acked\nI am getting this error intermittently in the teardown of our tests (using ElasticsearchIntegrationTest).\r\n\r\nElastic 1.3.2 on Java 1.7 u65 on linux.\r\n\r\nI cannot see why we should be getting this.\r\n\r\n```\r\njava.lang.AssertionError: Delete Index failed - not acked\r\nExpected: <true>\r\n     but: was <false>\r\n    at __randomizedtesting.SeedInfo.seed([AB0B449635AD3C03:63F22B491E5E9323]:0)\r\n    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)\r\n    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:111)\r\n    at org.elasticsearch.test.hamcrest.ElasticsearchAssertions.assertAcked(ElasticsearchAssertions.java:107)\r\n    at org.elasticsearch.test.TestCluster.wipeIndices(TestCluster.java:128)\r\n    at org.elasticsearch.test.TestCluster.wipe(TestCluster.java:70)\r\n    at org.elasticsearch.test.ElasticsearchIntegrationTest.afterInternal(ElasticsearchIntegrationTest.java:540)\r\n    at org.elasticsearch.test.ElasticsearchIntegrationTest.after(ElasticsearchIntegrationTest.java:1547)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\r\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n    at java.lang.reflect.Method.invoke(Method.java:606)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1618)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:885)\r\n    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:50)\r\n    at org.apache.lucene.util.TestRuleFieldCacheSanity$1.evaluate(TestRuleFieldCacheSanity.java:51)\r\n    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:49)\r\n    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\r\n    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:793)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:453)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:836)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:738)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:772)\r\n    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:783)\r\n    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:46)\r\n    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:42)\r\n    at com.carrotsearch.randomizedtesting.rules.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:55)\r\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:39)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:43)\r\n    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:48)\r\n    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:65)\r\n    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:55)\r\n    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)\r\n    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:360)\r\n    at java.lang.Thread.run(Thread.java:745)\r\n```'
7796,'s1monw',"Adding 'y' and 'M' to fuzziness parameter\nIdea about how to add year and month as parameters to fuzziness search.\r\nWithout having to lock the M to 30 days and Year to 365 or something like that."
7789,'GaelTadh',"Create Backwards Compatiblity Tests for Templates\nWe should create a backwards compatibility test for templates so that we don't hjork things going forward with new changes,"
7779,'jpountz','Multiple dateRange aggregations produce wrong results\nI\'m testing  the ElasticSearch 1.3.2.\r\nWhen testing the dateRange aggregation query, I found that two buckets with same predicate produce the different result. it seems strange.\r\n\r\nhere\'s a example below\r\n\r\n```\r\ncurl -XGET \'http://localhost:9200/_all/logs/_search?search_type=count\' -d \'{\r\n    "aggs": {\r\n        "rxxxx": {\r\n            "date_range": {\r\n                "field": "@timestamp",\r\n                "ranges": [\r\n                    { "from": "now-1h"},{ "from": "now-5m" },{ "from": "now-1h" }\r\n                ]\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nresult\r\n```\r\n{\r\n    "took": 55,\r\n    "timed_out": false,\r\n    "_shards": {\r\n        "total": 15,\r\n        "successful": 15,\r\n        "failed": 0\r\n    },\r\n    "hits": {\r\n        "total": 8119472,\r\n        "max_score": 0.0,\r\n        "hits": []\r\n    },\r\n    "aggregations": {\r\n        "rxxxx": {\r\n            "buckets": [{\r\n                "key": "2014-09-18T09:15:08.073Z-*",\r\n                "from": 1.411031708073E12,\r\n                "from_as_string": "2014-09-18T09:15:08.073Z",\r\n                "doc_count": 523450\r\n            }, {\r\n                "key": "2014-09-18T10:10:08.073Z-*",\r\n                "from": 1.411035008073E12,\r\n                "from_as_string": "2014-09-18T10:10:08.073Z",\r\n                "doc_count": 523450\r\n            }, {\r\n                "key": "2014-09-18T09:15:08.073Z-*",\r\n                "from": 1.411031708073E12,\r\n                "from_as_string": "2014-09-18T09:15:08.073Z",\r\n                "doc_count": 30200\r\n            }]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\n\r\n\r\n'
7705,'jeff4','Make the elasticsearch blog searchable\nHi,\r\n\r\nReading the blog regularly, there are interesting articles that I don\'t need for the moment but want do keep in mind for future developments. I\'d appreciate to be able to search for them, the same way we can search for the documentation today (http://www.elasticsearch.org/guide/).\r\n\r\nMoreover, the types of articles in the blog fall into a few categories. I\'ve listed:\r\n- Weekly news ("This week in Elasticsearch")\r\n- Events & meetings ("Where in the World is Elasticsearch")\r\n- Releases announcements ("Elasticsearch <product> <version> released")\r\n- Others (mainly tips/howto use a particular feature, like "Using the percolator for geo tagging")\r\n\r\nHaving in the /blog page the possibility to filter by category seems useful to me.\r\n\r\nIk'
7704,'javanna','add support for resetting dynamic cluster settings\nThis PR implements support for resetting transient & persistent cluster settings to, either the configuration file value defined at node startup, or if it does not existed, the default value.\r\n\r\nResetting settings is done using the normal cluster update settings API, by just assigning a JSON `null` to the setting one want to reset. See `update-settings.asciidoc` for details.\r\n\r\nThere are maybe other/smarter ways to do that, but I think this is just straight-forward and it works ;)\r\nThis is related to https://github.com/elasticsearch/elasticsearch/issues/6732, but does only apply to cluster settings not custom analyzer.'
7699,'mikemccand','reclaim_deletes_weight doesn\'t have any effect\nI\'ve been trying to play with merge settings lately and I _think_ ```reclaim_deletes_weight``` isn\'t getting set dynamically.  I turned up the logging and got [this](https://gist.github.com/nik9000/9a533d33986241524fe3) for an index where I cranked the weight up to a ridiculous number (64 or something).  When I run the math for calculating the score it looks like it is using the default of 2 no matter what I set.\r\n\r\nHere is the command I use to set the weight:\r\n```js\r\ncurl -XPUT \'elastic1001:9200/enwiktionary_content/_settings\' -d \'{\r\n  "index" : {\r\n    "merge" : {\r\n      "policy": {\r\n        "max_merge_at_once": 4,\r\n        "segments_per_tier": 4,\r\n        "reclaim_deletes_weight": 32.0\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\nand I get this log:\r\n```\r\n[<faked>][INFO ][index.merge.policy       ] [elastic1013] [enwiktionary_content_1407996688][2] updating [reclaim_deletes_weight] from [8.0] to [64.0]\r\n```\r\n\r\n'
7673,'jpountz','Strange bucket for date_histogram with negative post_zone and zero min_doc_count\nI sent following request to my elasticsearch 1.3.2 server `http://localhost:9200/payment_prod/2002/_search?search_type=count` with negative `post_zone` and `pre_zone`:\r\n\r\n```json\r\n{\r\n    "query" : {\r\n        "filtered" : {\r\n            "query" : {\r\n                "match_all" : {}\r\n            }\r\n        }\r\n    },\r\n    "aggs" : {\r\n        "by_time" : {\r\n            "date_histogram" : {\r\n                "field" : "date",\r\n                "interval" : "month",\r\n                "post_zone" : -2,\r\n                "pre_zone" : -2,\r\n                "min_doc_count" : 0,\r\n                "format" : "yyyy-MM-dd--HH:mm:ss.SSSZ"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIt seems to me that I shouldn\'t get the bucket 2013-07-30--22:00:00.000+0000 in the response.\r\n\r\n```json\r\n{\r\n    "took" : 105,\r\n    "timed_out" : false,\r\n    "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 5,\r\n        "failed" : 0\r\n    },\r\n    "hits" : {\r\n        "total" : 4018428,\r\n        "max_score" : 0.0,\r\n        "hits" : []\r\n    },\r\n    "aggregations" : {\r\n        "by_time" : {\r\n            "buckets" : [{\r\n                    "key_as_string" : "2013-06-30--22:00:00.000+0000",\r\n                    "key" : 1372629600000,\r\n                    "doc_count" : 235258\r\n                }, {\r\n                    "key_as_string" : "2013-07-30--22:00:00.000+0000",\r\n                    "key" : 1375221600000,\r\n                    "doc_count" : 0\r\n                }, {\r\n                    "key_as_string" : "2013-07-31--22:00:00.000+0000",\r\n                    "key" : 1375308000000,\r\n                    "doc_count" : 341928\r\n                }, {\r\n                    "key_as_string" : "2013-08-31--22:00:00.000+0000",\r\n                    "key" : 1377986400000,\r\n                    "doc_count" : 330148\r\n                }\r\n            ]\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nWith a small update on the request, post and pre zone with positive values, `http://localhost:9200/payment_prod/2002/_search?search_type=count`:\r\n\r\n```json\r\n{\r\n    "query" : {\r\n        "filtered" : {\r\n            "query" : {\r\n                "match_all" : {}\r\n            }\r\n        }\r\n    },\r\n    "aggs" : {\r\n        "by_time" : {\r\n            "date_histogram" : {\r\n                "field" : "date",\r\n                "interval" : "month",\r\n                "post_zone" : 2,\r\n                "pre_zone" : 2,\r\n                "min_doc_count" : 0,\r\n                "format" : "yyyy-MM-dd--HH:mm:ss.SSSZ"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nIn such case, response seems valid for me without strange bucket:\r\n\r\n```json\r\n{\r\n    "took" : 87,\r\n    "timed_out" : false,\r\n    "_shards" : {\r\n        "total" : 5,\r\n        "successful" : 5,\r\n        "failed" : 0\r\n    },\r\n    "hits" : {\r\n        "total" : 4018428,\r\n        "max_score" : 0.0,\r\n        "hits" : []\r\n    },\r\n    "aggregations" : {\r\n        "by_time" : {\r\n            "buckets" : [{\r\n                    "key_as_string" : "2013-07-01--02:00:00.000+0000",\r\n                    "key" : 1372644000000,\r\n                    "doc_count" : 233384\r\n                }, {\r\n                    "key_as_string" : "2013-08-01--02:00:00.000+0000",\r\n                    "key" : 1375322400000,\r\n                    "doc_count" : 341918\r\n                }\r\n            ]\r\n        }\r\n    }\r\n```\r\n\r\nIn fact problem occurs with post_zone < 0 and min_doc_count = 0. Without one of those predicates, result seems more reliable.\r\n\r\nAm I  wrong or is there a problem with elasticsearch post_zone management?'
7672,'dadoonet','[plugin] support SNAPSHOT version download from sonatype repo\nFor now, when we use the plugin manager to install a JVM plugin, we download it from:\r\n* elasticsearch download service (official plugins)\r\n* maven central (community based plugins)\r\n\r\nSometime developers would like to share a SNAPSHOT version of a plugin. Artifacts are in that case most of the time available on [Sonatype](https://oss.sonatype.org/) repository.\r\n\r\nPlugin manager should try to download the latest SNAPSHOT version available using Sonatype repo in addition to Maven central.\r\n\r\nSo the following could work:\r\n\r\n```\r\nbin/plugin install org.elasticsearch/elasticsearch-analysis-icu/3.0.0-SNAPSHOT\r\n```'
7670,'electrical',"nice try to detect JAVA_HOME\nit's boring to set up JAVA_HOME on Windows.\r\nin most cases we can guess it."
7667,'martijnvg','Closes #7663 (add routing for pre-indexed shapes)\nAs discussed early in #7663, there no way to provide routing information for pre-indexed shape in GeoShape filters and queries. This PR contains fixes for this issue:\r\nAdds optional `routing` parameter to `indexed_shape` object in GeoShape builder/parser for filter and query.\r\nAdds test for mandatory-routed pre-indexed GeoShape search.\r\nAdds `routing` parameter description into geo_shape filter asciidoc.'
7659,'alexksikes','mlt fails when document has none of the given mlt_fields\nnot sure if this is a bug or a feature, but personally I would prefer getting a 200 with an empty collection rather than interpreting the 500 to something meaningful. \r\n\r\nthis was tested against 1.3.2\r\n\r\n```\r\ncurl -XPOST http://localhost:9200/foo\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{"content":"foo bar qux", "content_2":"foo bar qux"}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/2 -d \'{"content_2":"foo bar qux", "content_3":"foo bar qux"}\'\r\ncurl -XPUT http://localhost:9200/foo/bar/3 -d \'{"content_3":"foo bar qux", "content_4":"foo bar qux"}\'\r\n\r\ncurl -XGET \'http://localhost:9200/foo/bar/1/_mlt?mlt_fields=content_3,content_4\'\r\n{"error":"ElasticsearchException[No fields found to fetch the \'likeText\' from]","status":500}%                                                                                                                                                                                \r\n\r\ncurl -XGET \'http://localhost:9200/foo/bar/3/_mlt?mlt_fields=content_3,content_4\'\r\n{"took":55,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":0,"max_score":null,"hits":[]}}%\r\n```'
7650,'areek','NRTSuggester: Support returning StoredFields with suggestions & Optional output duplication\n**NOTE:** This is a PR against `feature/nrt_suggester` and not the `master`!\r\n\r\nThe ability to return field values for given storedField(s), along with returned suggestions makes the suggesters more flexible, allowing to return associated field values for every suggestion without bloating up the in-memory FST (using `payload` increases the FST size) or having to specify the fields to return at index-time.\r\n \r\n**Implementation details:**\r\n - Support returning associated StoredFields of suggestions\r\n   - Lucene docid is extracted from suggestion and used to get named StoredField value(s) at query-time\r\n   - User can specify arbitrary number of StoredFields to be returned\r\n   - Returned `StoredField(s)` can be accessed through the new `XLookupResult`\r\n   - Currently supports StoredField values of type: `String`, `BytesRef` and `Number`\r\n - Optional deduplication (query time option)\r\n - Remove exactFirst option - simplify lookup code\r\n - Minor changes to `TopNSearcher` & how suggestions are collected\r\n - Introduce `XLookup` interface \r\n - Benchmarks\r\n\r\n**Benchmark Results:**\r\n - **data set**: LineFileDocs\r\n - `AnalyzingSuggester` : vanilla Lucene suggester built from `InputIterator`, uses `payload` to store field value.\r\n - `XAnalyzingSuggester` : current ES suggester built with custom postings format, uses `payload` to store field value\r\n - `XNRTSuggester` : new NRT suggester built with custom postings format, retrieves  field value from provided stored field name.\r\n - **Summary**\r\n   - number of results returned is inversely proportional to QPS\r\n   - longer prefix length yields to higher QPS\r\n   - Retrieving `storedField` is slower than using `payload`, at the cost of more flexibility\r\n   - Due to the relatively small size of the dataset used, the benchmark assumes StoredFields to be in RAM\r\n```\r\n....\r\n-- Stored Field Retrieval Performance\r\n  -- prefixes: 2-4, num: 2\r\n    AnalyzingSuggester queries: 3472, time[ms]: 83 [+- 8.64], ~kQPS: 42\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 62 [+- 1.48], ~kQPS: 56\r\n    XNRTSuggester queries: 3472, time[ms]: 174 [+- 5.04], ~kQPS: 20\r\n  -- prefixes: 2-4, num: 4\r\n    AnalyzingSuggester queries: 3472, time[ms]: 122 [+- 3.34], ~kQPS: 29\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 96 [+- 2.35], ~kQPS: 36\r\n    XNRTSuggester queries: 3472, time[ms]: 307 [+- 6.29], ~kQPS: 11\r\n  -- prefixes: 2-4, num: 6\r\n    AnalyzingSuggester queries: 3472, time[ms]: 172 [+- 5.38], ~kQPS: 20\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 135 [+- 4.58], ~kQPS: 26\r\n    XNRTSuggester queries: 3472, time[ms]: 425 [+- 6.98], ~kQPS: 8\r\n  -- prefixes: 3-6, num: 2\r\n    AnalyzingSuggester queries: 3472, time[ms]: 72 [+- 2.84], ~kQPS: 48\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 60 [+- 3.02], ~kQPS: 58\r\n    XNRTSuggester queries: 3472, time[ms]: 169 [+- 4.06], ~kQPS: 20\r\n  -- prefixes: 3-6, num: 4\r\n    AnalyzingSuggester queries: 3472, time[ms]: 107 [+- 4.29], ~kQPS: 32\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 85 [+- 3.98], ~kQPS: 41\r\n    XNRTSuggester queries: 3472, time[ms]: 266 [+- 5.58], ~kQPS: 13\r\n  -- prefixes: 3-6, num: 6\r\n    AnalyzingSuggester queries: 3472, time[ms]: 139 [+- 2.76], ~kQPS: 25\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 112 [+- 3.71], ~kQPS: 31\r\n    XNRTSuggester queries: 3472, time[ms]: 350 [+- 5.82], ~kQPS: 10\r\n  -- prefixes: 100-200, num: 2\r\n    AnalyzingSuggester queries: 3472, time[ms]: 134 [+- 3.11], ~kQPS: 26\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 122 [+- 3.85], ~kQPS: 28\r\n    XNRTSuggester queries: 3472, time[ms]: 191 [+- 7.18], ~kQPS: 18\r\n  -- prefixes: 100-200, num: 4\r\n    AnalyzingSuggester queries: 3472, time[ms]: 136 [+- 2.72], ~kQPS: 26\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 123 [+- 3.85], ~kQPS: 28\r\n    XNRTSuggester queries: 3472, time[ms]: 191 [+- 4.50], ~kQPS: 18\r\n  -- prefixes: 100-200, num: 6\r\n    AnalyzingSuggester queries: 3472, time[ms]: 136 [+- 4.07], ~kQPS: 26\r\n    XAnalyzingSuggester queries: 3472, time[ms]: 122 [+- 4.15], ~kQPS: 28\r\n    XNRTSuggester queries: 3472, time[ms]: 191 [+- 3.81], ~kQPS: 18\r\n\r\n - Storage benchmark\r\n    AnalyzingSuggester (with payload) size[B]:    1,198,856\r\n    XAnalyzingSuggester (with payload) size[B]:      762,592\r\n    XNRTSuggester size[B]:      774,504\r\n...\r\n```\r\nrelated to #7353'
7644,'alexksikes','Indices API returns wrong search stats groups in ES 1.3\nHi, \r\n\r\nWe encountered a problem with the search stats groups in Indices API. \r\n\r\nThe query_count for stats groups is false at shards level. It make fine query monitoring at shards/nodes level impossible.\r\n\r\nI have created a curl recreation. It has been tested with ES 1.3.2 on my recent MBP.\r\n\r\nhttps://gist.github.com/belevian/46026b88500d264fbc66\r\n\r\nThanks in advance to fix this problem.\r\n\r\nDo not hesitate to ask if you need more informations.\r\n\r\nRegards\r\n\r\nBenjamin'
7641,'areek','Duplicate results with completion suggester\nGiven the following document\r\n\r\n`{\r\n ...\r\n "input" : ["foobar", "barfoo", "foo"]\r\n ...\r\n}`\r\n\r\nand the query `foo`, the document will show up twice in the result set when using completion suggester. I\'m currently using a custom method to clean my input data to make sure the same prefix is not being used twice for a given document but it somehow feels wrong. With a regular search a document will also not show multiple times if a keyword is contained more than once in the document, is the completion suggester working this way by design?'
7640,'pickypg','Improve recognition of CBOR data format\nCurrently we only check if the first byte of the body is a `BYTE_OBJECT_INDEFINITE` to determine whether the content is CBOR or not.  However, what we should actually do is to check whether the "major type" is an object.\r\n\r\nSee:\r\n\r\n* https://github.com/FasterXML/jackson-dataformat-cbor/blob/master/src/main/java/com/fasterxml/jackson/dataformat/cbor/CBORParser.java#L614\r\n* https://github.com/FasterXML/jackson-dataformat-cbor/blob/master/src/main/java/com/fasterxml/jackson/dataformat/cbor/CBORParser.java#L682\r\n\r\nAlso, CBOR can be prefixed with a self-identifying tag, `0xd9d9f7`,  which we should check for as well.  Currently Jackson doesn\'t recognise this tag, but it looks like that will change in the future: https://github.com/FasterXML/jackson-dataformat-cbor/issues/6'
7635,'jpountz','search by type in URL finds a document of a different type\nWhen searching for a object of type \'stc_formula\' on a single index stored in a single shard ES-1.3.0 is finding a document of type \'stc_field\':\r\n```\r\n$ curl -XGET "http://localhost:9200"\r\n{\r\n  "status" : 200,\r\n  "name" : "Prowler",\r\n  "version" : {\r\n    "number" : "1.3.0",\r\n    "build_hash" : "1265b1454eee7725a6918f57415c480028700fb4",\r\n    "build_timestamp" : "2014-07-23T13:46:36Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.9"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n\r\n$ curl -XPOST \'http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_cache/clear\'\r\n{"_shards":{"total":1,"successful":1,"failed":0}}~\r\n\r\n# Edit.... thanks Clinton!\r\n#$ curl -XPOST \'http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/_cache/clear\'\r\n# The actual search URL:\r\n$ curl -XPOST "http://localhost:9200/u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358/stc_formula/_search?pretty=true" -d \'\r\n{\r\n "query": { "match_all": {} },\r\n "explain": true,\r\n "_source": false\r\n}\r\n\'\r\n\r\n{\r\n  "took" : 72,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_shard" : 0,\r\n      "_node" : "Rno76WbaR5S16cN9mAf7ww",\r\n      "_index" : "u-localhost-tmp-75994729-9054-40ec-9bdd-414366739e51-v20140903133358",\r\n      "_type" : "stc_field",\r\n      "_id" : "ba706935-1d7f-42b9-9b02-c0c5b5897bf4",\r\n      "_score" : 1.0,\r\n      "_explanation" : {\r\n        "value" : 1.0,\r\n        "description" : "ConstantScore(cache(stc_formula:)), product of:",\r\n        "details" : [ {\r\n          "value" : 1.0,\r\n          "description" : "boost"\r\n        }, {\r\n          "value" : 1.0,\r\n          "description" : "queryNorm"\r\n        } ]\r\n      }\r\n    } ]\r\n  }\r\n}\r\n```\r\n\r\nIf I use a type filter in the search body, either as filtered query or as a top-level filter then the results are empty as expected.\r\n\r\nThanks for your help!'
7634,'clintongormley',"Add indexCreated flag to IndexResponse action\nIt would be really useful to know if a new index was automatically created after performing an index action. It'd be a much better way to trigger some task, such as alias creation or doing some caching, than cronning jobs."
7633,'mikemccand','Require units for time and byte sized settings\nI fixed ImmutableSettings (and the time/byte size value parsers it uses) to by default require units for time and byte/memory-size settings, and fixed a bunch of tests that were missing units in their settings.\r\n\r\nSizeValue I left alone since the "singles" inherently has no unit.\r\n\r\nI also added a boolean bwc setting `settings_require_units` (defaults to true) that you can set to false to get back to lenient default units.  I put this in InternalSettingsPreparer so it runs "early on", and it\'s a static boolean in ImmutableSettings.  Hopefully most users don\'t need to set this (i.e. they simply add the missing units to their settings on upgrade), but if they need to, they can just set it in config/elasticsearch.yml.\r\n\r\nI fixed the parsing APIs to optionally take a setting name, so that the resulting ElasticsearchIllegalArgumentException includes the name of the troubled setting.\r\n\r\nWe still accept 0 and -1 without units...\r\n\r\nCloses #7616\r\n'
7631,'clintongormley',"Native support for rolling index (aka Index Per Day)\nThe Index per day is a very common pattern and widely used in projects with ES.\r\nIt's a ~~best~~ must practice by this days. Everybody is implementing it by hand, with hard to maintain and error-prone jobs or with extra hits to the cluster.\r\n\r\nSo it would be nice if it was implemented out of the box. It may be implemented as the well known rolling strategies of the logging frameworks. Something like this:\r\n\r\n* Enabled in the index settings (available for using it in templates)\r\n* The index name is just the base part of the final name\r\n* A formatted date(time) is appended to the base name\r\n* A date format could be specified but a default one is used otherwise\r\n* A rolling period could be specified (30m, 6h, 2d). With default to 1d\r\n* Every time a new index is rolled an alias to the base name is created"
7616,'mikemccand','[core] for settings that take units (byte size, time), we should require a unit\nToo many users have fallen into the trap of thinking refresh_interval of "1" means "1s" but in fact it means "1ms" ... I think it should be a hard error if this setting (and maybe any setting that takes units) is missing the units?\r\n\r\nWe should require that the unit is explicit so users don\'t fall into this trap?'
7612,'simianhacker','Marvel Sense - allow different themes on panels\nThe original sense plugin had a light theme on the left hand coing panel, and a dark theme on the result right hand panel, currently marvel only lets you set a light or dark theme for both...suggestion is to allow setting themes on the individual panels'
7598,'electrical','Add dependencies to generated RPM.\n'
7589,'brwe','Mapping: Return `_boost` and `_analyzer` in the GET field mapping API\n...so\r\n\r\n_boost and _analyzer mapping can now be retrieved by their default name\r\nor by given name ("path" for _analyzer, "name" for _boost)\r\nBoth still appear with their default name when just retrieving the mapping.\r\n(see SimpleGetFieldMappingTests#testGet_boostAnd_analyzer and\r\n     SimpleGetMappingTests#testGet_boostAnd_analyzer)\r\n\r\nThe index_name handling is removed from _boost. This never worked anyway,\r\nsee this test: https://github.com/brwe/elasticsearch/commit/36450043640f49f959d953dfd546f33606cb953a\r\n\r\nChange in behavior:\r\n\r\n_analyzer was never a field mapper. When defining an analyzer\r\nin a document, the field (_analyzer or custom name) was indexed\r\nwith default string field properties. These could be overwritten by\r\ndefining an explicit mapping for this field, for example:\r\n\r\n```\r\nPUT testidx/doc/_mapping\r\n{\r\n  "_analyzer": {\r\n    "path": "custom_analyzer"\r\n  },\r\n  "properties": {\r\n    "custom_analyzer": {\r\n      "type": "string",\r\n      "store": true\r\n    }\r\n  }\r\n}\r\n```\r\nNow, this explicit mapping will be ignored completely, instead\r\none can only set the "index" option in the definition of _analyzer\r\nEvery other option will be ignored.\r\n\r\nReason for this change:\r\nThe documentation says\r\n\r\n"By default, the _analyzer field is indexed, it can be disabled by settings index to no in the mapping."\r\n\r\nThis was not true - the setting was ignored. There was a test\r\nfor the explicit definition of the mapping (AnalyzerMapperTests#testAnalyzerMappingExplicit)\r\nbut this functionallity was never documented so I assume it is not in use.\r\n\r\ncloses #7237\r\n\r\nThings that worry me:\r\n\r\nI made it work, but am unsure if this is too hacky. I just made use of the fact that four different names are used for mappers (name, indexName, indexNameClean and full name) and set the name at the fitting place. However, there is plans for deprecating indexName (#6677) so I am unsure how long this solution will have any worth. \r\n\r\nIn addition the overwriting of the properties mapping relies on the fact that the order in which mappings are parsed is never changed. \r\n\r\nAlso, I wonder if the change in behavior for _analyzer qualifies as "breaking change".'
7572,'bleskes','[Indexing] A network partition can cause in flight documents to be lost\n\r\nThis ticket is meant to capture an issue which was discovered as part of the work done in #7493 , which contains a [failing reproduction test](https://github.com/elasticsearch/elasticsearch/blob/596a4a073584c4262d574828c9caea35b5ed1de5/src/test/java/org/elasticsearch/discovery/DiscoveryWithServiceDisruptions.java#L375) with @awaitFix.\r\n\r\nIf a network partition separates a node from the master, there is some window of time before the node detects it. The length of the window is dependent on the type of the partition. This window is extremely small if a socket is broken. More adversarial partitions, for example, silently dropping requests without breaking the socket can take longer (up to 3x30s using current defaults).\r\n\r\nIf the node hosts a *primary* shard at the moment of partition, and ends up being isolated from the cluster (which could have resulted in Split Brain before), some documents that are being indexed into the primary *may* be lost if they fail to reach one of the allocated replicas  (due to the partition) and that replica is later promoted to primary by the master.'
7563,'s1monw','[ENGINE] Flush IndexWriter to disk  on close and shutdown\nToday we trash everything that has been indexed but not flushed to disk\r\nif the engine is closed. This might not be desired if we shutting down a\r\nnode for restart / upgrade or if we close / archive an index. In such a\r\ncase we would like to flush the transaction log and commit everything to\r\ndisk. This commit adds a flag to the close method that is set on close\r\nand shutdown but not when we remove the shard due to relocations.'
7561,'spinscale','Transport: Remove parsing of port ranges\nUnicastZenDiscovery allowed you to specify port ranges for hosts to connect to.\r\nHowever only the first port was used at all, when specifying something like\r\n`host[9300-9400]`. This commit only accepts `host:port` pairs and thus makes\r\nthe configuration reflect the execution as even in the old setup only a single\r\nhost:port pair was pinged at all, as this was very expensive.\r\n\r\nNote there is a coming PR for 1.x, which deprecates this setting and emits a warning instead of removing it.'
7550,'mikemccand','Indexing: make indices.memory.index_buffer_size dynamically updatable\nThis is PR for #7045 \r\n\r\nIt also makes related settings (min/max_index_buffer_size and min/max_shard_index_buffer_size dynamically updatable.\r\n\r\nBut I have some concerns I wasn\'t sure about.  First, whether the ShardsIndicesStatusChecker.run (the methods that "bubbles down" the allowed RAM to all active shards) is thread-safe or not since it can now be called from two threads at once.  Probably I will just put a lock around it to be safe ...\r\n\r\nSecond, I just added these to ClusterDynamicSettingsModule, yet I use NodeSettingsService.addListener to subscribe to changes; is this right?  Also, in the test, I\'m using "persistent" cluster settings, but I\'m not familiar with this (where do these changes persist?) and I couldn\'t figure out how to do a full restart and confirm the dynamic changes did in fact persist somewhere ... so I could use some pointers here :)'
7549,'dakrone','Jepsen transient failures under network partition conditions\nHi! Jepsen tests include five nemeses (test scenarios) that introduce different types of network partitions (see [here](http://aphyr.com/posts/317-call-me-maybe-elasticsearch)). The tests add documents to index before, during and after these partitions, and verify that the documents which were acknowledged during the partitions are retrievable afterwards. Sometimes the tests indicate that a number of documents were indexed, but are not retrievable---however, this does not happen on every run (of the same scenario). For example, in a run of 20 times each (against 598854dd72d7fb01a7e26a9dad065de3deaa5eb7), the following :lost-frac amounts were reported:\r\n\r\nisolate-self-primaries-nemesis 244/361, 2/733, 1/607, 1/603, 1/213, 65/216 (and 14 times 0)\r\nnemesis/partition-random-halves 1/355, 1/226, 4/733, 1/433 (and 16 times 0)\r\nnemesis/partition-halves 1/65, 1/438, 4/715, 2/457, 6/731, 1/435, 9/433 (and 13 times 0)\r\nnemesis/partitioner nemesis/bridge 2/415, 3/253, 2/383, 7/754, 1/786, 1/767 (and 14 times 0)\r\nnemesis/partition-random-node does not report any lost documents.\r\n\r\nIn total, out of a 100 runs, 23 failed.'
7540,'imotov',"Escaping index name when creating a snapshot\nIn some cases the index name might not play well with the file system of the snapshot repository. \r\n\r\nFor example, trying to snapshot 'index_crawl:1' (which includes a colon) would fail in for plugins using URI-based filesystems (HDFS for example) due to filesystem restrictions. "
7537,'imotov','Shards stuck initalizing after restart if snapshot repository is on unavailable nfs\nusing 1.3.2 \r\ni have a snapshot repository of type fs pointing to a hard nfs mount point\r\nif i restart the cluster it will be stuck trying initializing shards on nodes where nfs is inaccessible for some reason\r\nimo a broken snapshot repository should not prevent cluster from functioning'
7479,'s1monw','Cleanup StoreFileMetadata.java\nStoreFileMetadata.java uses a `isSame` methods which is essentially `equals` I think we should clean it up and also provide a `hashCode` implementation.'
7472,'areek','Phrase suggester with suggest_mode: "missing" across multiple indexes yields suggestions when results exist in one index\nI have found a few odd cases where the phrase suggester with suggest_mode: "missing" generates a suggestion even though the query returns results. I am querying across two indexes but the query term only exists in one of the indexes. I get the search results I expect, but I do not expect to get a suggestion.\r\n\r\nI am trying to come up with a curl recreation but I haven\'t been able to reproduce it yet in a smaller corpus. \r\n\r\nI believe what is happening is that the phrase suggestions are getting merged together in the same way the query results are merged together across multiple indexes. If I have query results from test_1 and no results from test_2, then I expect to see results from test_1. But if I have a suggestion from test_1 and no suggestion from test_2 (due to suggest_mode: "missing"), I do not expect to see a suggestion.'
7451,'jpountz','document field parsing: refactor to remove external value mechanism\nI just make this pull request for having a better basis to discuss potential changes in the way external values are handled right now. I do not think it is ready for pulling in yet. Feel free to comment.\r\n \r\nThe external value mechanism was a workaround for not being able to set\r\na value while parsing a field value. It was used for example by geo point\r\nto set the values for the lat and lon field. However, its the major\r\ndisadvantage was that it made the code hard to read.\r\nThis commit removes the external value from parse context and instead\r\nadds a new method FieldMapper.addValue() which can be used to add fields\r\nto a mapper that are unrelated to the currently parsed source.\r\nIn addition, this method is now used also by the multi_field handling so that\r\nparsing is called only once and the parsed value (or any other) is passed\r\nto multi_fields explicitely. Before, multi_fields relied on the parser\r\nbeing at a particular position.\r\nThis means that now values which are objects or arrays in a json structure\r\ncan also be passed to multi_fields which was not possible before\r\n(see BaseTypeMapperTests where all test except for testMultiFieldsWithArray\r\nfailed without this change).\r\n'
7441,'mikemccand',"indices.memory.index_buffer_size is spread evenly across active shards\nRight now ```indices.memory.index_buffer_size``` is divided evenly on all active shards on the node.  It'd be more useful for those of us with many shards if we could have more control over how its divided up or if it were just a single pool.  I imagine single pool would be difficult but giving us a knob for weight or minimum size or something on the index level might be more doable.\r\n\r\nI'm actually not super sure how important this is from a performance standpoint - but my gut says raising this on my larger, hotter shards will make fewer segments.  Maybe its not that big a deal because the segments stay small any way...."
7424,'dadoonet','Avoid overwriting classloader on plugin initialization\nThis fix is for issue #6931. The plugin initialization code in PluginsService.updatedSettings overrides any custom classloader set in Settings on node initialization. I have changed the call to use an overloaded version that does not reset the classloader variable.\r\n\r\nTest suite completed without any errors.'
7395,'javanna','Custom logging.yml location\nAdding the ability to specify an alternate logging configuration file via system property. Documentation updated.\r\n\r\nCloses #2044'
7388,'areek','Context Suggester: Need better error message if you omit category when indexing.\nExample below (I am omitting the color field category when I index). Error message I get back is:\r\n\r\n{"error":"ElasticsearchIllegalArgumentException[one or more prefixes needed]","status":400}\r\n\r\nWould be nice to instead say something like "color category must be specified/is missing."\r\n\r\ncurl -XPUT localhost:9200/test_1 -d \'\r\n{\r\n "settings": {\r\n   "index": {\r\n     "number_of_replicas": "0",\r\n     "number_of_shards": "1"\r\n   }\r\n },\r\n "mappings": {\r\n   "doc": {\r\n     "properties": {\r\n       "context_suggest": {\r\n         "type": "completion",\r\n         "context": {\r\n           "color": {\r\n             "type": "category",\r\n             "path": "color",\r\n             "default": []\r\n           }\r\n         }\r\n       }\r\n     }\r\n   }\r\n }\r\n}\'\r\n\r\ncurl -XPUT localhost:9200/test_1/doc/1 -d \'\r\n{\r\n  "context_suggest": {\r\n    "input": ["test"],\r\n    "output": "test"\r\n  }\r\n}\'\r\n\r\n'
7383,'electrical','Request: expand JAVA_HOME search paths for Debian/Ubuntu\nHello,\r\n\r\nfor Debian/Ubuntu there is a nice package "java-package" which automatically builds JDK/JRE packages for Oracle Java 6/7 and perhaps already version 8 in testing release... \r\n\r\nTherefore it would be nice to add in this line of elasticsearch/src/deb/init.d/elasticsearch:\r\n```\r\n# The first existing directory is used for JAVA_HOME (if JAVA_HOME is not defined in $DEFAULT)\r\nJDK_DIRS="/usr/lib/jvm/java-7-oracle /usr/lib/jvm/java-7-openjdk /usr/lib/jvm/java-7-openjdk-amd64/ /usr/lib/jvm/java-7-openjdk-armhf /usr/lib/jvm/java-7-openjdk-i386/ /usr/lib/jvm/default-java"\r\n```\r\n\r\nalso this path:\r\n```\r\n /usr/lib/jvm/j2sdk1.7-oracle\r\n```\r\n\r\nBTW: \r\nWould be nice to get  also a check for existing java binary with correct version in the init script (and perhaps already as package dependencies ?) so that the init script do more then starting as "OK" without doing anything ;) ... \r\n\r\nThanks\r\n\r\nEDIT: sorry, dependencies are not good ... in Saltstack/puppet templates java ist often installed directly from tar files ... so better as Recommend packages if it should be setup in a .deb package.'
7357,'martijnvg','The has_parent request is broken for self-referential parent types\nWe don\'t explicitly prohibit types from referring to themselves as parents. This functionality worked fine before 0.90 release, but since 0.90 it seems to be broken for the `has_parent` query. In the following script, both search requests return results in 0.20.x but only the second search request returns results in 0.90.x and above.\r\n\r\n```\r\ncurl -XDELETE localhost:9200/test-idx\r\ncurl -XPUT localhost:9200/test-idx -d \'{\r\n    "settings": {\r\n        "index.number_of_shards": 1,\r\n        "index.number_of_replicas": 0\r\n    },\r\n    "mappings": {\r\n        "doc": {\r\n            "_parent": {\r\n                "type": "doc"\r\n            }\r\n        }\r\n    }\r\n}\'\r\ncurl -XPUT "localhost:9200/test-idx/doc/1?routing=1&pretty" -d \'{"name": "doc_1"}\'\r\ncurl -XPUT "localhost:9200/test-idx/doc/2?parent=1&pretty" -d \'{"name": "doc_2"}\'\r\ncurl -XPOST "localhost:9200/test-idx/_refresh?pretty"\r\necho\r\ncurl "localhost:9200/test-idx/doc/_search?pretty" -d \'{\r\n    "query": {\r\n        "has_parent": {\r\n            "type": "doc",\r\n            "query" : {\r\n                "match_all": {\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\necho\r\ncurl "localhost:9200/test-idx/doc/_search?pretty" -d \'{\r\n    "query": {\r\n        "has_child": {\r\n            "type": "doc",\r\n            "query" : {\r\n                "match_all": {\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nsearch result in 0.20.6:\r\n```\r\n{\r\n  "took" : 54,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test-idx",\r\n      "_type" : "doc",\r\n      "_id" : "2",\r\n      "_score" : 1.0, "_source" : {"name": "doc_2"}\r\n    } ]\r\n  }\r\n}\r\n{\r\n  "took" : 2,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test-idx",\r\n      "_type" : "doc",\r\n      "_id" : "1",\r\n      "_score" : 1.0, "_source" : {"name": "doc_1"}\r\n    } ]\r\n  }\r\n}\r\n```\r\n\r\nsearch result in the current master:\r\n```\r\n{\r\n  "took" : 66,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 0,\r\n    "max_score" : null,\r\n    "hits" : [ ]\r\n  }\r\n}\r\n\r\n{\r\n  "took" : 16,\r\n  "timed_out" : false,\r\n  "_shards" : {\r\n    "total" : 1,\r\n    "successful" : 1,\r\n    "failed" : 0\r\n  },\r\n  "hits" : {\r\n    "total" : 1,\r\n    "max_score" : 1.0,\r\n    "hits" : [ {\r\n      "_index" : "test-idx",\r\n      "_type" : "doc",\r\n      "_id" : "1",\r\n      "_score" : 1.0,\r\n      "_source":{"name": "doc_1"}\r\n    } ]\r\n  }\r\n}\r\n```\r\n'
7343,'jpountz','sorting does not work when querying an alias\nusing 1.3.1 and 1.3.2 the ordering of results differs when searching on alias vs searching on the index with actual data from that alias (only one index in the alias has any data in the type)\r\n```\r\nGET myindex_search/_alias\r\n{\r\n   "myindex_7": {\r\n      "aliases": {\r\n         "myindex_search": {},\r\n         "myindex": {}\r\n      }\r\n   },\r\n   "users_idx3": {\r\n      "aliases": {\r\n         "users_idx": {},\r\n         "myindex_search": {},\r\n         "users_idx_search": {},\r\n      }\r\n   }\r\n}\r\n```\r\n```\r\nGET myindex_search/mytype/_search\r\n{\r\n  "query": {\r\n    "constant_score": {\r\n      "filter": {\r\n        "and": {\r\n"filters": [\r\n              {"term": {\r\n                "user_id": 582313\r\n              }}\r\n\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "sort": [\r\n    {\r\n      "data.start_date": {\r\n        "order": "asc",\r\n        "ignore_unmapped": "true"\r\n      }\r\n    }\r\n  ]\r\n,\r\n"aggs": {\r\n  "max": {\r\n    "max": {\r\n      "field": "data.order_date"\r\n    }\r\n  }\r\n}, \r\n  "size": 110\r\n}\r\n```\r\nreturns\r\n```\r\n{\r\n   "took": 5,\r\n   "timed_out": false,\r\n   "_shards": {\r\n      "total": 20,\r\n      "successful": 20,\r\n      "failed": 0\r\n   },\r\n   "hits": {\r\n      "total": 16,\r\n      "max_score": null,\r\n      "hits": [\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408441377.377091-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 1210.14,\r\n                  "end_date": 1408449667.817116,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408441377.377091,\r\n                  "number_of_steps": 1620,\r\n                  "duration": 8290.440024495125,\r\n                  "order_date": 1408449667.817116,\r\n                  "id": "1408441377.377091-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958624986e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408452938.713164-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 10.458,\r\n                  "end_date": 1408453879.153991,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408452938.713164,\r\n                  "number_of_steps": 14,\r\n                  "duration": 940.4408271312714,\r\n                  "order_date": 1408453879.153991,\r\n                  "id": "1408452938.713164-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958682104e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408050000-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5336.568,\r\n                  "end_date": 1408136400,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408050000,\r\n                  "number_of_steps": 7144,\r\n                  "duration": 86400,\r\n                  "order_date": 1408136400,\r\n                  "id": "1408050000-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408050000\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408222800-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 4076.379,\r\n                  "end_date": 1408309200,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408222800,\r\n                  "number_of_steps": 5457,\r\n                  "duration": 86400,\r\n                  "order_date": 1408309200,\r\n                  "id": "1408222800-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408222800\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408463037.221965-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 177.786,\r\n                  "end_date": 1408470644.548557,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408463037.221965,\r\n                  "number_of_steps": 238,\r\n                  "duration": 7607.326591610909,\r\n                  "order_date": 1408470644.548557,\r\n                  "id": "1408463037.221965-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408463037.221965\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408456930.040181-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5.229,\r\n                  "end_date": 1408463006.851273,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408456930.040181,\r\n                  "number_of_steps": 7,\r\n                  "duration": 6076.811092078686,\r\n                  "order_date": 1408463006.851273,\r\n                  "id": "1408456930.040181-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.95870183e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407790800-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 2.241,\r\n                  "end_date": 1407823148.624126,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407790800,\r\n                  "number_of_steps": 3,\r\n                  "duration": 32348.62412595749,\r\n                  "order_date": 1407823148.624126,\r\n                  "id": "1407790800-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407790800\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407877200-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 4134.645,\r\n                  "end_date": 1407963600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407877200,\r\n                  "number_of_steps": 5535,\r\n                  "duration": 86400,\r\n                  "order_date": 1407963600,\r\n                  "id": "1407877200-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407877200\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407963600-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 6652.035,\r\n                  "end_date": 1408050000,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407963600,\r\n                  "number_of_steps": 8905,\r\n                  "duration": 86400,\r\n                  "order_date": 1408050000,\r\n                  "id": "1407963600-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407963600\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408136400-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 10236.141,\r\n                  "end_date": 1408222800,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408136400,\r\n                  "number_of_steps": 13703,\r\n                  "duration": 86400,\r\n                  "order_date": 1408222800,\r\n                  "id": "1408136400-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408136400\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408309200-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 3161.304,\r\n                  "end_date": 1408395600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408309200,\r\n                  "number_of_steps": 4232,\r\n                  "duration": 86400,\r\n                  "order_date": 1408395600,\r\n                  "id": "1408309200-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408309200\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408395600-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 2337.363,\r\n                  "end_date": 1408474407.530582,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408395600,\r\n                  "number_of_steps": 3129,\r\n                  "duration": 78807.53058201075,\r\n                  "order_date": 1408474407.530582,\r\n                  "id": "1408395600-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408395600\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408449687.813152-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5.229,\r\n                  "end_date": 1408450160.890107,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408449687.813152,\r\n                  "number_of_steps": 7,\r\n                  "duration": 473.0769553780556,\r\n                  "order_date": 1408450160.890107,\r\n                  "id": "1408449687.813152-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.95866604e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407823148.624126-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 1706.148,\r\n                  "end_date": 1407877200,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407823148.624126,\r\n                  "number_of_steps": 2284,\r\n                  "duration": 54051.37587404251,\r\n                  "order_date": 1407877200,\r\n                  "id": "1407823148.624126-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407823148.624126\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408387971.543879-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 871.749,\r\n                  "end_date": 1408395600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408387971.543879,\r\n                  "number_of_steps": 1167,\r\n                  "duration": 7628.456121563911,\r\n                  "order_date": 1408395600,\r\n                  "id": "1408387971.543879-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958361125e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408450305.447627-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 15.687,\r\n                  "end_date": 1408452812.031366,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408450305.447627,\r\n                  "number_of_steps": 21,\r\n                  "duration": 2506.583738684654,\r\n                  "order_date": 1408452812.031366,\r\n                  "id": "1408450305.447627-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958669096e-315\r\n            ]\r\n         }\r\n      ]\r\n   },\r\n   "aggregations": {\r\n      "max": {\r\n         "value": 4743694330017250000\r\n      }\r\n   }\r\n}\r\n```\r\nas opposed to correct sorting of data when querying single index\r\n```\r\nGET myindex_7/mytype/_search\r\n{\r\n  "query": {\r\n    "constant_score": {\r\n      "filter": {\r\n        "and": {\r\n"filters": [\r\n              {"term": {\r\n                "user_id": 582313\r\n              }}\r\n\r\n          ]\r\n        }\r\n      }\r\n    }\r\n  },\r\n  "sort": [\r\n    {\r\n      "data.start_date": {\r\n        "order": "asc",\r\n        "ignore_unmapped": "true"\r\n      }\r\n    }\r\n  ]\r\n,\r\n"aggs": {\r\n  "max": {\r\n    "max": {\r\n      "field": "data.order_date"\r\n    }\r\n  }\r\n}, \r\n  "size": 110\r\n}\r\n```\r\n```\r\n{\r\n   "took": 4,\r\n   "timed_out": false,\r\n   "_shards": {\r\n      "total": 8,\r\n      "successful": 8,\r\n      "failed": 0\r\n   },\r\n   "hits": {\r\n      "total": 16,\r\n      "max_score": null,\r\n      "hits": [\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408387971.543879-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 871.749,\r\n                  "end_date": 1408395600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408387971.543879,\r\n                  "number_of_steps": 1167,\r\n                  "duration": 7628.456121563911,\r\n                  "order_date": 1408395600,\r\n                  "id": "1408387971.543879-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958361125e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408441377.377091-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 1210.14,\r\n                  "end_date": 1408449667.817116,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408441377.377091,\r\n                  "number_of_steps": 1620,\r\n                  "duration": 8290.440024495125,\r\n                  "order_date": 1408449667.817116,\r\n                  "id": "1408441377.377091-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958624986e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408449687.813152-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5.229,\r\n                  "end_date": 1408450160.890107,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408449687.813152,\r\n                  "number_of_steps": 7,\r\n                  "duration": 473.0769553780556,\r\n                  "order_date": 1408450160.890107,\r\n                  "id": "1408449687.813152-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.95866604e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408450305.447627-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 15.687,\r\n                  "end_date": 1408452812.031366,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408450305.447627,\r\n                  "number_of_steps": 21,\r\n                  "duration": 2506.583738684654,\r\n                  "order_date": 1408452812.031366,\r\n                  "id": "1408450305.447627-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958669096e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408452938.713164-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 10.458,\r\n                  "end_date": 1408453879.153991,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408452938.713164,\r\n                  "number_of_steps": 14,\r\n                  "duration": 940.4408271312714,\r\n                  "order_date": 1408453879.153991,\r\n                  "id": "1408452938.713164-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.958682104e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408456930.040181-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5.229,\r\n                  "end_date": 1408463006.851273,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408456930.040181,\r\n                  "number_of_steps": 7,\r\n                  "duration": 6076.811092078686,\r\n                  "order_date": 1408463006.851273,\r\n                  "id": "1408456930.040181-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               6.95870183e-315\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407790800-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 2.241,\r\n                  "end_date": 1407823148.624126,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407790800,\r\n                  "number_of_steps": 3,\r\n                  "duration": 32348.62412595749,\r\n                  "order_date": 1407823148.624126,\r\n                  "id": "1407790800-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407790800\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407823148.624126-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 1706.148,\r\n                  "end_date": 1407877200,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407823148.624126,\r\n                  "number_of_steps": 2284,\r\n                  "duration": 54051.37587404251,\r\n                  "order_date": 1407877200,\r\n                  "id": "1407823148.624126-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407823148.624126\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407877200-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 4134.645,\r\n                  "end_date": 1407963600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407877200,\r\n                  "number_of_steps": 5535,\r\n                  "duration": 86400,\r\n                  "order_date": 1407963600,\r\n                  "id": "1407877200-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407877200\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1407963600-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 6652.035,\r\n                  "end_date": 1408050000,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1407963600,\r\n                  "number_of_steps": 8905,\r\n                  "duration": 86400,\r\n                  "order_date": 1408050000,\r\n                  "id": "1407963600-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1407963600\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408050000-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 5336.568,\r\n                  "end_date": 1408136400,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408050000,\r\n                  "number_of_steps": 7144,\r\n                  "duration": 86400,\r\n                  "order_date": 1408136400,\r\n                  "id": "1408050000-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408050000\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408136400-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 10236.141,\r\n                  "end_date": 1408222800,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408136400,\r\n                  "number_of_steps": 13703,\r\n                  "duration": 86400,\r\n                  "order_date": 1408222800,\r\n                  "id": "1408136400-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408136400\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408222800-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 4076.379,\r\n                  "end_date": 1408309200,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408222800,\r\n                  "number_of_steps": 5457,\r\n                  "duration": 86400,\r\n                  "order_date": 1408309200,\r\n                  "id": "1408222800-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408222800\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408309200-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 3161.304,\r\n                  "end_date": 1408395600,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408309200,\r\n                  "number_of_steps": 4232,\r\n                  "duration": 86400,\r\n                  "order_date": 1408395600,\r\n                  "id": "1408309200-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408309200\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408395600-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 2337.363,\r\n                  "end_date": 1408474407.530582,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408395600,\r\n                  "number_of_steps": 3129,\r\n                  "duration": 78807.53058201075,\r\n                  "order_date": 1408474407.530582,\r\n                  "id": "1408395600-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408395600\r\n            ]\r\n         },\r\n         {\r\n            "_index": "myindex_7",\r\n            "_type": "mytype",\r\n            "_id": "582313_1408463037.221965-0",\r\n            "_score": null,\r\n            "_source": {\r\n               "data": {\r\n                  "distance": 177.786,\r\n                  "end_date": 1408470644.548557,\r\n                  "calories": 0,\r\n                  "motion_type": "stationary",\r\n                  "start_date": 1408463037.221965,\r\n                  "number_of_steps": 238,\r\n                  "duration": 7607.326591610909,\r\n                  "order_date": 1408470644.548557,\r\n                  "id": "1408463037.221965-0"\r\n               },\r\n               "user_id": 582313\r\n            },\r\n            "sort": [\r\n               1408463037.221965\r\n            ]\r\n         }\r\n      ]\r\n   },\r\n   "aggregations": {\r\n      "max": {\r\n         "value": 4743694330017250000\r\n      }\r\n   }\r\n}\r\n```\r\nnevermind the broken aggregations and "sort" field values, this started to happen only after upgrade to 1.3.2'
7339,'javanna',"Internal: Move plugin manager to new CLI\nPluginManager could now be rewritten using the new Command Line Interface framework.\r\n\r\n# How does it work?\r\n\r\nThe new `bin/plugin` tool works now as this:\r\n\r\n```\r\nbin/plugin command [options] <command_parameter>\r\n```\r\n\r\n`command_parameter` could be mandatory depending on the `command`.\r\n\r\n## Generic options\r\n\r\nSome options could be applied to any plugin command:\r\n\r\n* `--help`, `-h`\r\n* `--silent`, `-s`\r\n* `--verbose`, `-v`\r\n\r\n## Install\r\n\r\n```\r\nbin/plugin install [options] <plugin_name>\r\n```\r\n\r\n`plugin_name` is mandatory. Still using the same form as used before with the plugin manager.\r\n\r\nOptions could be:\r\n\r\n`--timeout <timeout_value>`, `-t <timeout_value>`\r\n`--url <url>`, `-u <url>`\r\n\r\n## Uninstall (Remove)\r\n\r\n```\r\nbin/plugin uninstall [options] <plugin_name>\r\n```\r\n\r\n`plugin_name` is mandatory. Still using the same form as used before with the plugin manager.\r\n\r\n## List\r\n\r\n```\r\nbin/plugin list [options]\r\n```\r\n\r\n# Breaking changes\r\n\r\nSome important changes for `bin/plugin` application:\r\n\r\n|      Old option       |      New option        |\r\n|-----------------------|--------------------------|\r\n| `--install pluginname`              |`install pluginname`                     |\r\n| `-i pluginname`                       |`install pluginname`                     |\r\n| `-install pluginname`               |`install pluginname`                     |\r\n| `--remove pluginname`              |`uninstall pluginname`                     |\r\n| `-r pluginname`                       |`uninstall pluginname`                     |\r\n| `-remove pluginname`               |`uninstall pluginname`                     |\r\n| `remove pluginname`               |`uninstall pluginname`                     |\r\n| `-l`                       |`list`                     |\r\n| `--list`               |`list`                     |\r\n| `-url <url>`                       |`--url <url>` or `-u <url>`                     |\r\n| `url <url>`                       |`--url <url>` or `-u <url>`                     |\r\n| `-verbose`                       |`--verbose` or `-v`                     |\r\n| `verbose`                       |`--verbose` or `-v`                     |\r\n| `-silent`                       |`--silent` or `-s`                     |\r\n| `silent`                       |`--silent` or `-s`                     |\r\n| `-timeout <time>`                       |`--timeout <time>` or `-t <time>`                     |\r\n| `timeout <time>`                       |`--timeout <time>` or `-t <time>`                     |\r\n\r\nBut the new PluginTool still support for elasticsearch 1.4 old options/commands format and will translate old options to the new one.\r\n\r\nDeprecated format will be removed in an upcoming version (1.5? or 2.0?)\r\n\r\n# Tests\r\n\r\nMany tests have been adapted to the new CLI. Some randomization has been added as well.\r\nNew tests have been added as well.\r\n\r\nAlso, I ran some manual end user tests such as:\r\n\r\n```sh\r\n# Test non existing plugin\r\nbin/plugin install dummy/donotexist\r\nbin/plugin install dummy/donotexist -v\r\nbin/plugin install dummy/donotexist -s\r\n# Test es.org download service\r\nbin/plugin install elasticsearch/elasticsearch-cloud-azure/2.4.0\r\nbin/plugin uninstall elasticsearch/elasticsearch-cloud-azure/2.4.0\r\n# Test maven central download\r\nbin/plugin install org.elasticsearch/elasticsearch-cloud-azure/2.4.0 -v\r\n# Test install from github master download\r\nbin/plugin install mobz/elasticsearch-head -v\r\nbin/plugin uninstall mobz/elasticsearch-head\r\n# Test install using URL\r\nbin/plugin install head -v -u https://github.com/mobz/elasticsearch-head/archive/master.zip\r\n# Test list plugins (with empty or one or many plugins)\r\nbin/plugin list\r\n```\r\n\r\n# Remaining questions\r\n\r\n* We change `PluginManager` class name to `PluginTool` class name. Should we keep the old name (so we don't change `bin/plugin` and `bin/plugin.bat` files)?\r\n* This PR contains a fix when CLI is set to `silent` but in case of error it still prints out an error message. Should this be part of another PR? See https://github.com/dadoonet/elasticsearch/commit/25d332b8d5335f22693d26bc4e57d5d5522b5c7c\r\n\r\nRelated to #7094\r\n"
7335,'GaelTadh','Indexed Scripts/Templates: Return error message on 404\nThis commit adds support for messages such as :\r\n````\r\n{\r\n  "status" : 404,\r\n  "error" : "IndexedScriptMissingException[[asdasdasfe] missing]"\r\n}\r\n{\r\n  "status" : 404,\r\n  "error" : "IndexedSearchTemplateMissingException[[asdasdasdas] missing]"\r\n}\r\n````\r\nWhen non existant indexed scripts and templates are requested from get and delete REST endpoints.\r\n\r\nSee #7325'
7334,'GaelTadh',"SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.\nThis fixes #7317.\r\nSearchRequestBuilder.toString now attempts to render the request's source before falling back to using the internal builder.\r\n\r\nCloses #5576 \r\nCloses #5555"
7325,'GaelTadh','_scripts end point does not return a request body for 404 responses\nTested against both indexed scripts and indexed search templates in 1.3.1:\r\n\r\nA HTTP GET to fetch back a non-existing script or search template returns an empty HTTP body with a HTTP header (404):\r\n\r\n```\r\n_scripts/groovy/asdasdasfe\r\n_search/template/asdasdasdas\r\n```\r\n\r\nWould be nice to return an actual HTTP body to be consistent with our other APIs.  Perhaps something like the following?\r\n\r\n```\r\n{\r\n  "status" : 404,\r\n  "error" : "IndexedScriptMissingException[[asdasdasfe] missing]"\r\n}\r\n{\r\n  "status" : 404,\r\n  "error" : "IndexedSearchTemplateMissingException[[asdasdasdas] missing]"\r\n}\r\n```\r\n\r\n\r\n\r\n\r\n'
7321,'dadoonet','Add Java API example for storing search templates in .scripts\nWould be nice to provide a Java API example for storing search templates in the .scripts index. \r\n\r\nHere is an example:\r\n\r\n```java\r\nimport org.elasticsearch.action.indexedscripts.put.PutIndexedScriptResponse;\r\nimport org.elasticsearch.script.ScriptService;\r\n\r\nimport java.util.HashMap;\r\nimport java.util.Map;\r\n\r\n            PutIndexedScriptResponse scriptResponse = client.preparePutIndexedScript("mustache", "mysearchtemplate",\r\n                    "{" +\r\n                    "\\"template\\":{" +\r\n                        "\\"query\\":{" +\r\n                            "\\"match\\":{" +\r\n                                "\\"comments\\" : \\"{{query_string}}\\"}" +\r\n                            "}" +\r\n                        "}" +\r\n                    "}").get();\r\n            Map<String, String> template_params = new HashMap<String, String>();\r\n            template_params.put("query_string", "some_text");\r\n            SearchResponse searchResponse = client.prepareSearch("my_index").setTypes("my_type").\r\n                    setTemplateName("mysearchtemplate").setTemplateType(ScriptService.ScriptType.INDEXED).setTemplateParams(template_params).get();\r\n```'
7317,'GaelTadh','SearchRequestBuilder.toString modifies the SearchRequestBuilder wiping any source set.\nIf you use setSource on a SearchRequestBuilder and then call to string the SearchRequest will appear to be cleared. \r\n\r\n````\r\nSearchRequestBuilder srb = client().prepareSearch().setSource("{\\"query\\":\\"match\\":{\\"foo\\":\\"bar\\"}}");\r\nsrb.execute().get();\r\n```` \r\nWill execute the correct search, however \r\n````\r\nSearchRequestBuilder srb = client().prepareSearch().setSource("{\\"query\\":\\"match\\":{\\"foo\\":\\"bar\\"}}");\r\nlogger.debug("About to execute [{}]",srb.toString());\r\nsrb.execute().get();\r\n````\r\nWill execute an empty search. \r\n\r\nThe problem is that ````toString()```` calls ````internalBuilder()```` which calls ````sourceBuilder()```` \r\n````\r\n    private SearchSourceBuilder sourceBuilder() {\r\n        if (sourceBuilder == null) {\r\n            sourceBuilder = new SearchSourceBuilder();\r\n        }\r\n        return sourceBuilder;\r\n    }\r\n````\r\nThen when the ````SearchRequestBuilder.execute```` is called the request that was constructed via ````setSource```` is replaced by an empty sourceBuilder.\r\n\r\n````\r\n    @Override\r\n    protected void doExecute(ActionListener<SearchResponse> listener) {\r\n        if (sourceBuilder != null) {\r\n            request.source(sourceBuilder());\r\n        }\r\n        client.search(request, listener);\r\n    }\r\n````\r\n'
7297,'bly2k','Feature: Ability to specify and log slow _percolate calls\nWe currently have the capability to log slow queries that exceed a threshold using the following settings:\r\n\r\nindex.search.slowlog.*\r\n\r\nFor _percolate calls, it is also possible to run into situations where a we might be interested to know when percolation is having bottlenecks and it would be a nice option to log percolate calls that exceed a threshold just like the above settings for search.'
7266,'rmuir',"Elasticsearch hangs on input containing a huge string without spaces\nWhen the input for indexing contains a huge string without spaces, Elasticsearch hangs without responding to the request. I was expecting either to discard or truncate the token or get an error response instead of server hanging. All subsequent valid requests to the server also times out. \r\n\r\nSteps to reproduce\r\n\r\n1. Create a  test index using default so that the standard analyzer is used.\r\n2. Send the attached document to index\r\n     curl -XPUT localhost:9200/test/test/1 -d @bad_input.json\r\n\r\n\r\nI couldn't find a way to attach a file to the bug, so I am sharing the link to the file bad_input.json file on google drive.\r\n\r\nhttps://drive.google.com/file/d/0B4p5T-L62D0SR3dXMDlxemUweU0/edit?usp=sharing\r\n"
7256,'uboness',' Ignore allow_explicit_index when accessing root /_bulk URL\nWhen url-based access control is used for bulk requests\r\n\r\nrest.action.multi.allow_explicit_index: false\r\n\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/url-access-control.html\r\nIt forbids explicitly setting the index in the request body regardless of the bulk url used.\r\n\r\nWould it be possible to allow setting explicit indexes when the URL accessed is the /_bulk root, with no index specified in the URL?\r\n\r\nThis way if a user is allowed to access /_bulk, he can work as if allow_explicit_index is false, while if a user is only allowed to access specific {index}/_bulk urls, he is effectively contained.\r\n\r\nWith the current rules, the only way to allow bulk access with explicit index to one user is to set allow_explicit_index to true and thus allow full access to everybody with bulk access.\r\n\r\nMaybe this feature is not that high-priority, I see that access control in general does not seem to be the focus of elasticsearch. But if this is an easy change, would this work?\r\n'
7246,'clintongormley','Include groovy community\nThis change includes the a new community: groovy to point to the official client.'
7244,'mikemccand',"[Core] Make it easier to disable bloom filter indexing\nWe recently turned off bloom filter loading at search time by default (#6349), but we still pay the index-time (and merging) cost to build the bloom filters.  For append-only apps, this is really just unnecessary and we should make it possible to just use Lucene's default Codec/PostingsFormat."
7237,'brwe','Mapping: Get field mapping does not return _analyzer and _boost fields\nThe [get field mapping API](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html#indices-get-field-mapping) \r\n\r\nreturns all the [special fields](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-fields.html) except for the [_analyzer field](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-analyzer-field.html) and [_boost](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-boost-field.html) fields.\r\n\r\nGiven the following mapping\r\n\r\n```\r\nPUT /nest_test_data-1340/specialdto/_mapping\r\n{\r\n  "specialdto": {\r\n    "_id": {\r\n      "path": "myOtherId",\r\n      "index": "not_analyzed",\r\n      "store": false\r\n    },\r\n    "_source": {\r\n      "enabled": false,\r\n      "compress": true,\r\n      "compress_threshold": "200b",\r\n      "includes": [\r\n        "path2.*"\r\n      ],\r\n      "excludes": [\r\n        "path1.*"\r\n      ]\r\n    },\r\n    "_type": {\r\n      "index": "analyzed",\r\n      "store": true\r\n    },\r\n    "_all": {\r\n      "enabled": true,\r\n      "store_term_vector_positions": true,\r\n      "index_analyzer": "default",\r\n      "search_analyzer": "default"\r\n    },\r\n    "_analyzer": {\r\n      "index": "yes",\r\n      "path": "name"\r\n    },\r\n    "_boost": {\r\n      "name": "boost",\r\n      "null_value": 1.0\r\n    },\r\n    "_parent": {\r\n      "type": "person"\r\n    },\r\n    "_routing": {\r\n      "required": true,\r\n      "path": "name"\r\n    },\r\n    "_index": {\r\n      "enabled": false,\r\n      "store": true\r\n    },\r\n    "_size": {\r\n      "enabled": false,\r\n      "store": true\r\n    },\r\n    "_timestamp": {\r\n      "enabled": true,\r\n      "path": "timestamp",\r\n      "format": "yyyy"\r\n    },\r\n    "_ttl": {\r\n      "enabled": false,\r\n      "default": "1d"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nDoing a GET for all the special fields:\r\n\r\n```\r\nGET http://localhost:9200/nest_test_data-1340/_mapping/specialdto/field/_%2A\r\n```\r\n\r\nreturns:\r\n\r\n```\r\n{\r\n  "nest_test_data-1340" : {\r\n    "mappings" : {\r\n      "specialdto" : {\r\n        "_type" : {\r\n          "full_name" : "_type",\r\n          "mapping":{"_type":{"store":true}}\r\n        },\r\n        "_version" : {\r\n          "full_name" : "_version",\r\n          "mapping":{}\r\n        },\r\n        "_id" : {\r\n          "full_name" : "_id",\r\n          "mapping":{"_id":{"index":"not_analyzed","path":"myOtherId"}}\r\n        },\r\n        "_source" : {\r\n          "full_name" : "_source",\r\n          "mapping":{"_source":{"enabled":false,"compress":true,"compress_threshold":"200b","includes":["path2.*"],"excludes":["path1.*"]}}\r\n        },\r\n        "_routing" : {\r\n          "full_name" : "_routing",\r\n          "mapping":{"_routing":{"required":true,"path":"name"}}\r\n        },\r\n        "_timestamp" : {\r\n          "full_name" : "_timestamp",\r\n          "mapping":{"_timestamp":{"enabled":true,"path":"timestamp","format":"yyyy"}}\r\n        },\r\n        "_index" : {\r\n          "full_name" : "_index",\r\n          "mapping":{"_index":{"enabled":false}}\r\n        },\r\n        "_ttl" : {\r\n          "full_name" : "_ttl",\r\n          "mapping":{"_ttl":{"enabled":false}}\r\n        },\r\n        "_size" : {\r\n          "full_name" : "_size",\r\n          "mapping":{"_size":{"enabled":false}}\r\n        },\r\n        "_uid" : {\r\n          "full_name" : "_uid",\r\n          "mapping":{}\r\n        },\r\n        "_all" : {\r\n          "full_name" : "_all",\r\n          "mapping":{"_all":{"store_term_vectors":true,"store_term_vector_positions":true,"analyzer":"default"}}\r\n        },\r\n        "_parent" : {\r\n          "full_name" : "_parent",\r\n          "mapping":{"_parent":{"type":"person"}}\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nwithout including `_analyzer` and `_boost` in the response.\r\n'
7209,'areek','Completion & Context Suggester: Support expressions for suggestion scoring\nCurrently users have to explicitly set score to suggestion entries via the `weight` parameter, while indexing. It would be nice to be able to just specify an `expression` instead and also accept hard-coded values (as done today).\r\n\r\nIf a weight of a suggestion is calculated by a combination of field values (`popularity`, `recency` for example), then the following format can be used:\r\n\r\n```bash\r\ncurl -X PUT \'localhost:9200/music/song/1?refresh=true\' -d \'{\r\n    "name": ...,\r\n    "suggest": {\r\n        "input": [\r\n           ...\r\n        ],\r\n        "output": ...,\r\n        "payload": {\r\n            ...\r\n        },\r\n        "weight": "doc[\'popularity\'].value * 0.8 + doc[\'recency\'].value * 0.2"\r\n    }\r\n}\'\r\n```\r\nthe example above assumes `popularity` and `recency` to be already defined in the mapping and both have `Numeric` values.\r\n\r\nrelated #6818'
7199,'johtani','Parse synonyms with the same analysis chain\nSynonyms are currently tokenized with the whitespace tokenizer.  Really they should be tokenized with whatever tokenizer and token filters appear before it in the chain (ie the same analysis chain that is used on the text).'
7174,'areek','Add slop param to completion suggester\nI would be very useful if completion suggester will accept a `slop` param. It will solve problem of suggesting for "aaaa bbbb" text in docs with `[ "bbbb aaaa", "bbbb", "aaaa" ]` input.'
7171,'s1monw','Add shard size allocation balance function\nFrom the docs:\r\n`cluster.routing.allocation.balance.size`::\r\n     Defines a weight factor for the number of similarly sized shards allocated\r\n      on a node (float). Defaults to `0.00f` because it is new and we\'re not\r\n      yet sure what a good default is.  Raising this raises the tendency to\r\n      equalize the number of similarly sized shards across all nodes in the\r\n      cluster.  "Similarly sized" means that this formula spits out the same\r\n      number:\r\n        size <= 10^7^ bytes ? 0 : floor(log10(size))\r\n      So 1GB and 5GB shards are "similar" and so are 1KB and 40MB.\r\n\r\nSide effects:\r\n1.  Enables cluster info collection on all master eligible nodes so that\r\nthey can make informed decisions based on shard sizes as soon as they get\r\nelected master.  They don\'t have to wait for the job to return when they\r\nare elected.\r\n2.  Removes the disabled flag for the cluster info job.  Its now complicate\r\nto know when you can disable it.  Besides, the disk allocation decider is\r\non by default now and that needs it too.\r\n3.  Reworks BalanceConfigurationTests quite a bit adding a test for the\r\nshard size balance and a test that random balance settings actually converge.\r\n4.  Fixes some of the cluster rebalance docs that were confusing on reread.\r\n\r\nCloses #7155'
7169,'ppf2','Create a cluster restart doc page\nCurrently, cluster rolling restart process is documented as part of the upgrade process.\r\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-upgrade.html#rolling-upgrades\r\nWould be great to add a doc page specific to performing a full cluster restart that is not part of the upgrade steps.'
7155,'dakrone',"Add shard balance function that tries to equalize similarly sized (on disk) shards\nIt'd be nice to have a balance function that tries to equalize the number of similarly sized shards on each node.  This would have several advantages for clusters with unevenly sized indexes:\r\n* Help prevent #6168 by making it less likely that lots of large shards end up on a single node, pushing its disk space much much higher then the rest.\r\n* Help balance disk IO load by keeping the large shards away from each other.\r\n* Prevent one node from ending up fallow because it only contains small shards just by luck.\r\n\r\n"
7153,'martijnvg','`ignore_unavailable=true` ignored when a single index is specified\nWe noticed an edge case today when using the `ignore_unavailable` parameter against indices that are closed. Running the following in Sense produces a 403 error, which was very unexcepted.\r\n\r\n```sh\r\ncurl -XPOST "http://localhost:9200/test_index/test_type" -d\'\r\n{\r\n  "some": "value"\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/test/_close"\r\n\r\ncurl -XPOST "http://localhost:9200/test/_search?ignore_unavailable=true"\r\n# {\r\n#   "error": "ClusterBlockException[blocked by: [FORBIDDEN/4/index closed];]",\r\n#   "status": 403\r\n# }\r\n```\r\n\r\nAlternatively, if I send `test*` I get the expected empty result.\r\n```sh\r\ncurl -XPOST "http://localhost:9200/test*/_search?ignore_unavailable=true"\r\n# {\r\n#   "took": 1,\r\n#   "timed_out": false,\r\n#   "_shards": {\r\n#     "total": 0,\r\n#     "successful": 0,\r\n#     "failed": 0\r\n#    },\r\n#    // etc.\r\n# }\r\n```'
7147,'GaelTadh','Scripting: Add configuration parameter to control execution of indexed s...\n...cripts.\r\n\r\nThis commit adds the `script.disable_indexed` parameter to the config system.\r\nIf this is set to `true` indexed scripts cannot be created or used at search time.\r\nIf this is set to `sandbox` only indexed script that use sandboxed languages can be created or\r\nused at search time.\r\nIf this is set to `false` indexed scripts from any supported language can be created or used at\r\nsearch time.\r\nThe default value is `sandbox`.\r\nThis setting is independent of the `script.disable_dynamic` setting.\r\n\r\nSee #6922'
7111,'bleskes','Marvel: Can we please show the following Node info ?\nCan we please see some hardware/OS info for the server a given node is running on?\r\n\r\nCpu: num cores/procs, arch, brand/model\r\nOS: name, version, distro, etc\r\n\r\nIts useful, when managing a number of different nodes running on different servers, to be able to see if a given node is running on an odd piece of hardware (usually the result of a linux VM mis-configuration)'
7104,'spinscale','OS X: elasticsearch --argument hangs\nHi,\r\n\r\nThere is an issue in https://github.com/elasticsearch/elasticsearch/blob/master/bin/elasticsearch script: on OS X `elasticsearch --version` and alike commands hang forever. The reason is [here](https://github.com/elasticsearch/elasticsearch/blob/521f8b28b5a5912debd07426aad10c19861e28ed/bin/elasticsearch#L158): with a single `--parameter` script runs `shift 2`, trying to take 2 arguments from input. In dash (Ubuntu) this reduces `$#` below zero and the loop stops, but on OS X sh keeps `$#` equal to 1 in this case, making the loop spin forever.'
7083,'dadoonet','Add an UpdateRequest example to Java API doc\nThe Java API guide does not currently have an example of an UpdateRequest.  Would be nice to see an example of this in the Java API guide with scripting (to show the end user how they can specify scripting via the .scriptLang and .script methods) without having to look at the test cases (eg. https://github.com/elasticsearch/elasticsearch/blob/2024067465503a53d5292c67626998942dfffec6/src/test/java/org/elasticsearch/action/bulk/BulkRequestTests.java) :)'
7057,'jpountz','Mappings: Deprecate `index_name`.\nClose #6677'
7045,'mikemccand',"Make indices.memory.index_buffer_size dynamically updateable\nElasticsearch updates this internally, when an index becomes idle, or when active shards change but we don't let the user change it today, I think."
7041,'areek','Count API: exploit terminate_after setting for performing minimal shard requests\nCurrently the Count API broadcasts requests to all the shards, even when terminate_after setting has been set. It would be ideal to terminate ongoing shard requests, when the count == terminate_after. This would make the API a bit faster when there is a large number of shards to be queried but the terminate_after setting is relatively a small number.'
7038,'martijnvg','unable to re-nest on same field after reverse nesting \nI\'m having trouble performing a certain aggregation on a nested field.\r\n\r\nI have a field of nested type, that holds an array of object. What I want to do is this:\r\n\r\nAggregate on the field to find all the nested documents that contain a certain value, then reverse-nest back to the root, giving me the entire set of root documents which have the nested field object I\'m looking for.\r\n\r\nThen, further aggregate on that set of documents, against the same nested field, this time against a different value. \r\n\r\nThe purpose is to build something like a funnel, where I keep narrowing down the aggregation with more and more criteria, based on that nested field.\r\n\r\nWhat appears to happen though is that when perform my second nested agg, itself with a nested->reverse_nested agg, it looks as though it can\'t see any of nested fields anymore. If you run this simple CURL, looks at the value of \'into_nested_again\'. It\'s 0, but it should be 3, just like the value of the original \'into_nested\' agg. \r\n\r\n```\r\ncurl -XDELETE "http://localhost:9200/nested-test"\r\n\r\ncurl -XPUT "http://localhost:9200/nested-test" -d\'\r\n{\r\n  "mappings": {\r\n    "my_type": {\r\n      "properties": {\r\n        "my_nested_field": {\r\n          "type": "nested"\r\n       }\r\n      } \r\n       \r\n    }\r\n  }\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/nested-test/my_type" -d\'\r\n{\r\n  "my_nested_field" : [\r\n    {\r\n      "my_key": "value1"\r\n    }\r\n  ]\r\n}\'\r\n\r\ncurl -XPOST "http://localhost:9200/nested-test/my_type" -d\'\r\n{\r\n  "my_nested_field" : [\r\n    {\r\n      "my_key": "value1"\r\n    },\r\n    {\r\n      "my_key": "value2"\r\n    }\r\n  ]\r\n}\'\r\n\r\ncurl -XGET "http://localhost:9200/nested-test/_search" -d\'\r\n{\r\n  "aggs": {\r\n    "into_nested": {\r\n      "nested": {\r\n        "path": "my_nested_field"\r\n      },\r\n      "aggs": {\r\n        "by_value1": {\r\n          "filter": {\r\n            "term": {\r\n              "my_nested_field.my_key": "value1"\r\n            }\r\n          },\r\n          "aggs": {\r\n            "by_parent": {\r\n              "reverse_nested": {}\r\n              , "aggs": {\r\n                "into_nested_again": {\r\n                  "nested": {\r\n                    "path": "my_nested_field"\r\n                  }\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n```\r\n'
7030,'markharwood','Support RFC 6902 style PATCH updates\nIt would be nice to have a "patch language" that can be used to update documents, or in the `transform` API without using scripting.'
7025,'kimchy','Master election and discovery in ZenDiscovery module should be pluggable\nWe are implementing a custom master election strategy, but still want to use the zen module  for other actions such as fault detection, sending join requests, processing cluster state, etc.  There should be a way to pass in custom implementations of ElectMasterService and findMaster() to enable reuse of code without requiring copy-pasted code in custom discovery plugins.'
7002,'clintongormley','1.3.0 ElasticsearchException: Failed to load logging configuration\nLogConfigurator.configure() now requires a config file, while previously it did not.\r\n\r\ncode to reproduce:\r\n```\r\n        final Node node = NodeBuilder.nodeBuilder()\r\n                .local(true)\r\n                .build();\r\n        LogConfigurator.configure(node.settings());\r\n```\r\n\r\nexception:\r\n```\r\nException in thread "main" org.elasticsearch.ElasticsearchException: Failed to load logging configuration\r\n\tat org.elasticsearch.common.logging.log4j.LogConfigurator.resolveConfig(LogConfigurator.java:117)\r\n\tat org.elasticsearch.common.logging.log4j.LogConfigurator.configure(LogConfigurator.java:81)\r\n```'
6952,'brwe',"Query DSL: Add min_score support to function_score query\nI've sometimes needed to use the `function_score` query to generate some custom score, and then I want to exclude documents that don't meet a minimum score.\r\n\r\nCurrently I can do this by setting the `_score` to zero then using the `min_score` parameter  in the search request.  However, sometimes I want to apply the same logic further down in the hierarchy.\r\n\r\nAdding `min_score` support to the `function_score` query would allow me to achieve that, ie filter out all documents whose score is lower than the specified.  "
6932,'jpountz',"Match query rewrite and fuzzy_rewrite not applied\nThere are various bugs to do with `match` query rewriting.\r\n\r\nWrong `fuzzy_rewrite` default\r\n--------------------------------------\r\nThe docs for the [boolean match query](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-match-query.html#_boolean) say that fuzziness should use a `constant_score` rewrite method by default.  Instead it uses the `top_terms_N` method.\r\n\r\n`fuzzy_rewrite` never applied\r\n-------------------------------------\r\nThe `fuzzy_rewrite` parameter is never applied, so it is impossible to change it.  This is fixed with the following patch:\r\n\r\n    diff --git a/src/main/java/org/elasticsearch/index/search/MatchQuery.java b/src/main/java/org/elasticsearch/index/search/MatchQuery.java\r\n    index b776416..7cc1f4f 100644\r\n    --- a/src/main/java/org/elasticsearch/index/search/MatchQuery.java\r\n    +++ b/src/main/java/org/elasticsearch/index/search/MatchQuery.java\r\n    @@ -303,10 +303,11 @@ public class MatchQuery {\r\n                     if (query instanceof FuzzyQuery) {\r\n                         QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod);\r\n                     }\r\n    +                return query;\r\n                 }\r\n                 int edits = fuzziness.asDistance(term.text());\r\n                 FuzzyQuery query = new FuzzyQuery(term, edits, fuzzyPrefixLength, maxExpansions, transpositions);\r\n    -            QueryParsers.setRewriteMethod(query, rewriteMethod);\r\n    +            QueryParsers.setRewriteMethod((FuzzyQuery) query, fuzzyRewriteMethod);\r\n                 return query;\r\n             }\r\n             if (mapper != null) {\r\n    @@ -318,4 +319,4 @@ public class MatchQuery {\r\n             return new TermQuery(term);\r\n         }\r\n\r\n    -}\r\n    \\ No newline at end of file\r\n    +}\r\n\r\nUnused `rewrite` parameter\r\n------------------------------------\r\nThe `match` query accepts a `rewrite` parameter which is never used.  \r\n\r\nBad rewriting of `match_phrase_prefix`\r\n--------------------------------------------------\r\nThe `match_phrase_prefix` should (IMO) use a `constant_score_auto` rewrite on the final term (and this should be settable with `rewrite`), but this functionality is implemented with the MultiPhraseQuery which doesn't support the rewrite methods.  "
6922,'GaelTadh',"Indexed scripting should be controlled from a different settings flag than dynamic scripting\nCurrently if dynamic scripting is disabled indexed scripts won't work, this is due to the fact that if an attacker has access to the REST endpoints they could just index a doc into .scripts and then run it from a query.\r\n\r\nI would be great if there was a different config setting so sysadmins could protect the endpoints and script index with a proxy disable dynamic script but allow running indexed scripts."
6914,'imotov','Task management\nWe need a task management API to allow management of long running tasks, like snapshot/restore, benchmarking, update-by-query etc.\r\n\r\nThe API should allow listing and aborting of ongoing tasks.'
6909,'spinscale',"Prevent init script from returning when the service isn't actually started\nHello,\r\nSince the init script uses start-stop-daemon's `-b` option, it might return from _start_ before the service is actually started. If the init script's _status_ action is called immediately after that, It will return `elasticsearch is not running` and exit with a non-zero code.\r\nThis causes systems like Heartbeat to incorrectly start multiple instances of Elasticsearch at once.\r\nThis pull request aims at fixing this issue by waiting until the Elasticsearch process is started.\r\nThere might be a nicer way to achieve this but this is what came to mind.\r\n"
6899,'rmuir','Failure recovery\nWhile i restart my cluster, i saw the exception: \r\n\r\norg.elasticsearch.index.gateway.IndexShardGatewayRecoveryException: [tmh_session][4] failed recovery\r\n\tat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:185)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.elasticsearch.index.engine.EngineCreationFailureException: [tmh_session][4] failed to open reader on writer\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:301)\r\n\tat org.elasticsearch.index.shard.service.InternalIndexShard.performRecoveryPrepareForTranslog(InternalIndexShard.java:709)\r\n\tat org.elasticsearch.index.gateway.local.LocalIndexShardGateway.recover(LocalIndexShardGateway.java:204)\r\n\tat org.elasticsearch.index.gateway.IndexShardGatewayService$1.run(IndexShardGatewayService.java:132)\r\n\t... 3 more\r\nCaused by: org.apache.lucene.index.CorruptIndexException: Invalid fieldsStream maxPointer (file truncated?): maxPointer=3411245664, length=214433792\r\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsReader.<init>(CompressingStoredFieldsReader.java:136)\r\n\tat org.apache.lucene.codecs.compressing.CompressingStoredFieldsFormat.fieldsReader(CompressingStoredFieldsFormat.java:113)\r\n\tat org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:129)\r\n\tat org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:101)\r\n\tat org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:142)\r\n\tat org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:236)\r\n\tat org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:99)\r\n\tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:385)\r\n\tat org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)\r\n\tat org.apache.lucene.search.SearcherManager.<init>(SearcherManager.java:89)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1364)\r\n\tat org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:291)\r\n\t... 6 more'
6897,'martijnvg','ids filter in percolator\nAs the ids filter is not of any usage in the percolator api it should not be possible to add it to a percolator.\r\n\r\nHeres a gist to test it:\r\nhttps://gist.github.com/julianhille/f4d1014a0072cc3233fb'
6871,'martijnvg','"matched_queries" does not include queries within a wrapper query\nI have a document in an index such that the following request returns a match, with the `matched_queries` element containing `query1`:\r\n\r\n    curl -XPOST \'http://localhost:9200/index/_search\' –d \'\r\n    {\r\n      "query": {\r\n        "match": {\r\n          "stuff": {\r\n            "_name": "query1",\r\n            "query": "blah"\r\n          }\r\n        }\r\n      }\r\n    }\r\n    \'\r\n\r\nHowever, if that query is wrapped in a wrapper query, then the name does not appear in the list of `matched_queries`:\r\n\r\n    curl -XPOST \'http://localhost:9200/index/_search\' –d \'\r\n    {\r\n      "query": {\r\n        "wrapper": {\r\n          "query": "eyJtYXRjaCI6IHsic3R1ZmYiOiB7Il9uYW1lIjogInF1ZXJ5MSIsICJxdWVyeSI6ICJibGFoIn19fQ=="\r\n        }\r\n      }\r\n    }\r\n    \'\r\n\r\nI am using the java api and the `WrapperQueryBuilder`, but the effect is the same whether using the java api or the rest interface.\r\n\r\n(gist with set-up steps in case it\'s useful: https://gist.github.com/tstibbs/645e01c5dcdfa9d2a193)'
6821,'GaelTadh','Add endpoint to test/debug/view a rendered template query\nIt would be nice to be able view what the rendered mustache template query looks like after passing in various params.  \r\n\r\nAn endpoint where you could send a template along with the parameters and it returned the resulting query without actually executing the query would be extremely useful.'
6803,'dadoonet','Add basic authentication authentication for plugin manager\nThe plugin downloader which is invoked by the plugin command line does not support downloading the plugin from a URL which is secured using basic auth.\r\n\r\nUse Case\r\n-------\r\n\r\nIn the continuous integration process if we want to install a plugin where the plugin artifact is available on the CI server which is restricted with basic auth this is not possible.\r\n\r\nWorkaround\r\n----------\r\n\r\nDownload using wget passing auth details and use the elasticsearch plugin command line passing the *file* url instead of *http* url\r\n\r\nGood to have\r\n-----------\r\n\r\nPlugin command line to have options to take in basic auth credentials and pass it on to the downloader.\r\n\r\nThanks to @katta for the original PR!\r\n\r\nCloses #2550.'
6798,'electrical',"RPM-Installation not complete\nI'm on a `SUSE Linux Enterprise Server 11 (x86_64)` and evaluating ElasticSearch to replace an existing solution.\r\n\r\nI downloaded [the rpm-release](https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-1.2.1.noarch.rpm) and tried to install it using `rpm -i elasticsearch-1.2.1.noarch.rpm` but neither the group shown in [preinstall](https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/scripts/preinstall) is added nor the [init-script](https://github.com/elasticsearch/elasticsearch/blob/master/src/rpm/init.d/elasticsearch) is moved/copied to `/etc/init.d`.\r\n\r\nAm I missing something or is the installation routine messed up for SLES?"
6796,'markharwood','significant_terms agg new sampling option.\nAvoids the use of FieldData in high-cardinality free-text fields by re-tokenizing samples of the top matching documents.\r\nAlso provides a de-duplication option to remove duplicated sections of text commonly found in free-text document fields in order to avoid skewing word usage stats and making bad keyword suggestions as a result.'
6788,'s1monw','Rescorer ignores track_score\nThe track_scores param is ignored when combined with the rescore features.\r\nThis is not the behavior I expect with this feature.\r\n\r\nCan you confirm this is a bug ?\r\nIf so can we excpect a bugfix ?\r\n\r\nThanks,'
6763,'clintongormley','Search suggestions not provided on 404 page\nRepro:\r\n\r\n1. open http://www.elasticsearch.org/guide/en/elasticsearch/0.90/current/query-dsl-filtered-query.html#query-dsl-filtered-query\r\n2. Click to the search text field (btw, why I can not navigate into it using the `TAB` key?)\r\n3. No suggestions are provided as your type the text\r\n'
6759,'imotov','enhance wait_for_status\nIt would be nice if `wait_for_status` could be enhanced so it does strict status checks not just the current (at least status with green>yellow>red). This would allow one to use it to potentially monitor critical changes for a certain index and react to them.'
6755,'kimchy','Hide internal versions of dependencies from IDE auto-import\nElasticsearch packages many dependencies. E.g. `com.google.common.base.Preconditions` can also be found at `org.elasticsearch.common.base.Preconditions` This leads to folks on my team often accidentally importing the wrong class and a frustrating experience including elasticsearch in our project.\r\n\r\nThese can be hidden from IDE auto-import. [Google Guice has done the same thing](https://code.google.com/p/google-guice/wiki/Guice40), so it would be a great example to look at:\r\n\r\n> Many things inside com.google.inject.internal changed and/or moved. This is especially true for repackaged Guava (formerly Google Collections), cglib, and asm classes. All these classes are now hidden from IDE auto-import suggestions and are in new locations. You will have to update your code if you relied on any of these classes. '
6736,'clintongormley','Define valid index, type, field, id, routing values\nCurrently we have no specification of allowed values for index names, type names, IDs, field names or routing values.\r\n\r\nThis issue is an attempt to document and improve the existing specs to prevent inconsistencies.\r\n\r\nIndex names\r\n-----------------\r\nIndex names are limited by the file system.  They may only be lower case, and my not start with an underscore.  While we don\'t prevent index names starting with a `.`, we reserve those for internal use. Clearly, `.` and `..` cannot be used.\r\n\r\nThese characters are already illegal: `\\`, `/`, `*`, `?`, `"`, `<`, `>`, `|`, ` `, `,`.  We should also add the null byte.\r\n\r\nThere are other filenames which are illegal in Windows, but we probably don\'t need to check for those.\r\n\r\nType names\r\n----------------\r\nType names can contain any character (except null bytes, which currently we don\'t check) but may not start with an underscore.  \r\n\r\nIDs\r\n----\r\nIDs can contain any character (except null bytes, which currently we don\'t check). IDs should not begin with an underscore.\r\n\r\nCurrently IDs are not checked for underscores and IDs with underscores may exist.  These can clash with eg `_mapping` and so should be prevented.  This is a backwards incompatible change.\r\n\r\nRouting & Parent\r\n--------------------\r\nRouting and parent values should be the same as IDs, ie any chars except for the null byte.  The problem is that multiple routing values are passed in the query string as comma-separated values, eg `?routing=foo,bar`.\r\n\r\nIf a single routing value contains a comma, it will be misinterpreted as two routing values.  One idea is to pass multiple routing values as eg `?routing=foo&routing=bar,baz`. Unfortunately, this is not backwards compatible and isn\'t supported by a number of client libraries.\r\n\r\nThe only solution I can think of is to support some escaping of commas, eg `foo\\,bar`.  This would mean that `\\` would need to be escaped as well, ie: `foo\\bar` -> `foo\\\\bar`.  Support for this escaping would need to be added to Elasticsearch and to the client libraries. \r\n'
6732,'s1monw',"Improve cluster/index settings \nCurrently there is no way of seeing all settings that are enforced.  For instance, defaults are not available and settings from the `config/elasticsearch.yml` are not available.\r\n\r\nAny setting which is different from the default should be returned by these APIs:\r\n\r\n    GET /{index}/_settings\r\n    GET /_cluster/settings\r\n\r\nBoth should also support the `include_defaults` query string parameter, which would return all settings, including the defaults.\r\n\r\nAlso, deleting a setting should reset the setting to the default.\r\n\r\nSetting an unknown setting, or a setting that can't be changed should throw an error.\r\n\r\nRelates to #5018, #3670, #2738, #3572, #2628, #3671, #5019, #6309\r\n"
6731,'rmuir',"Add boolean or constant_score similarity\nThe idea here is that some use cases don't need or want full-text ranking. In a lot of cases it can even hurt, for example: trying to use fuzzy query to find misspellings in a database of place names or something like that.\r\n\r\nToday I think its too hard to turn off various measures such as IDF, we could just make it easy by providing a simple similarity out of box that users could enable."
6727,'dakrone','Make similarities dynamically updatable where possible\nThe core similarities can be swapped in dynamically on an existing index, as long as `discount_overlaps` is the same.  Currently we disallow updating similarities, because custom similarities may not be compatible.\r\n\r\nThe logic for deciding whether a similarity can be changed should be more fine grained.'
6725,'aleph-zero','Add operation preference support to aliases\nAllow setting preference (eg `_local`)  in aliases.\r\n\r\nFixes #1307 '
6713,'dakrone','Filters: Terms filter with `fielddata` execution mode should use ordinals\nThe terms filter supports a `fielddata` execution mode that runs on field data instead of the inverted index. However, in case of strings, the current implementation filters based on values, which can be super slow. It should use ordinals instead.'
6709,'martijnvg','Native search script working for searches but not for percolation\nThe same native AbstractSearchScript that works in classic searches do not work from percolators (tested with Elasticsearch 2.0.0 snapshot).\r\n\r\nScript code:\r\npackage test;\r\nimport java.util.Map;\r\nimport org.elasticsearch.common.Nullable;\r\nimport org.elasticsearch.common.component.AbstractComponent;\r\nimport org.elasticsearch.common.inject.Inject;\r\nimport org.elasticsearch.common.settings.Settings;\r\nimport org.elasticsearch.node.Node;\r\nimport org.elasticsearch.script.ExecutableScript;\r\nimport org.elasticsearch.script.NativeScriptFactory;\r\npublic class CooccurenceScriptFactory extends AbstractComponent implements NativeScriptFactory{\r\n    private final Node node;\r\n    @SuppressWarnings("unchecked")\r\n    @Inject\r\n    public CooccurenceScriptFactory(Node node, Settings settings) {\r\n        super(settings);\r\n        this.node = node;\r\n    }\r\n    @Override public ExecutableScript newScript (@Nullable Map<String,Object> params){\r\n        return new CooccurenceScript(node.client(), logger, params);\r\n      }\r\n}\r\npackage test;\r\nimport org.elasticsearch.ElasticsearchIllegalArgumentException;\r\nimport org.elasticsearch.client.Client;\r\nimport org.elasticsearch.common.Nullable;\r\nimport org.elasticsearch.common.logging.ESLogger;\r\nimport org.elasticsearch.common.xcontent.support.XContentMapValues;\r\nimport org.elasticsearch.script.AbstractSearchScript;\r\nimport org.elasticsearch.search.lookup.SourceLookup;\r\nimport java.util.List;\r\nimport java.util.Map;\r\npublic class CooccurenceScript extends AbstractSearchScript {\r\n    private List<String> list = null;\r\n    @SuppressWarnings("unchecked")\r\n    public CooccurenceScript(Client client, ESLogger logger, @Nullable Map<String,Object> params) {\r\n        Map<String, Object> map = params == null ? null : XContentMapValues.nodeMapValue(params.get("map"), null);\r\n        if (map == null) {\r\n            throw new ElasticsearchIllegalArgumentException("Missing the map parameter");\r\n        }\r\n        list = (List<String>) map.get("list");\r\n        if (list == null || list.isEmpty()) {\r\n            throw new ElasticsearchIllegalArgumentException("Missing the list parameter or list is empty");\r\n        }\r\n    }\r\n    @Override\r\n    public java.lang.Object run() {\r\n        SourceLookup source = source();\r\n        @SuppressWarnings("unchecked")\r\n        List<Object> values = (List<Object>) source.get("source_field");\r\n        if (values == null || values.isEmpty()) {\r\n            return false;\r\n        }\r\n        for (Object localValue : values) {\r\n            boolean result = true;\r\n            for (String s : list) {\r\n                result &= ((String) localValue).contains(s);\r\n            }\r\n            if (result) {\r\n                return true;\r\n            }\r\n        }\r\n        return false;\r\n    }\r\n}\r\n\r\nSearch test:\r\n# Create index and type arefresh\r\ncurl -XDELETE http://localhost:9200/index1\r\ncurl -XPOST http://localhost:9200/index1\r\ncurl -XPOST http://localhost:9200/index1/mytype/_mapping -d \'{\r\n  "mytype": {\r\n    "properties": {\r\n      "source_field": { "type": "string" }\r\n    }\r\n  }\r\n}\'\r\n\r\n# Create one document\r\ncurl -XPOST "http://localhost:9200/index1/mytype" -d \'{\r\n  "source_field" : [ "this a that" ]\r\n}\'\r\ncurl -XGET "http://localhost:9200/index1/_refresh"\r\n\r\n# Search the document\r\ncurl \'http://localhost:9200/index1/mytype/_search\' -d \'{\r\n  "query": {\r\n    "constant_score": {\r\n      "filter": {\r\n        "script": {\r\n          "script": "cooccurenceScript",\r\n          "params": {\r\n            "map": { "list" : [ "a", "this" ] }\r\n          },\r\n          "lang": "native"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\nResult:\r\n{"took":1,"timed_out":false,"_shards":{"total":5,"successful":5,"failed":0},"hits":{"total":1,"max_score":1.0,"hits":[{"_index":"index1","_type":"mytype","_id":"cwrZpe8hR_KPPBVVB30AKw","_score":1.0,"_source":{\r\n  "source_field" : [ "this a that" ]\r\n}}]}}\r\n\r\nPercolation test:\r\n# Add a percolator using the same native script\r\ncurl -XPUT "http://localhost:9200/index1/.percolator/1" -d \'{\r\n  "query": {\r\n    "constant_score": {\r\n      "filter": {\r\n        "script": {\r\n          "script": "cooccurenceScript",\r\n          "params": {\r\n            "map": { "list" : [ "a" ] }\r\n          },\r\n          "lang": "native"\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\'\r\n\r\n# Percolate an identical document\r\ncurl -XPOST "http://localhost:9200/index1/mytype/_percolate" -d \'\r\n{\r\n  "doc" : {\r\n    "source_field" : [ "this a that" ]\r\n  }\r\n}\'\r\nResult:\r\n{"took":3,"_shards":{"total":5,"successful":5,"failed":0},"total":0,"matches":[]}\r\n'
6699,'polyfractal','Feature: Add ability to profile queries\nThis PR adds the ability to profile queries and determine which portions of the query are the slowest.  All timings are relative and displayed in microseconds.\r\n\r\nTimings are not guaranteed to match the `took` time of the query...in fact, they will likely be larger.  This is because timings are aggregated from all shards, often executing in parallel, so the profiled time is more akin to CPU time than wallclock time.\r\n\r\nIt is also important to understand that Profile times represetn relative costs, rather than absolute times.  The act of profiling adds considerable overhead to the querying process since all components in the query must be wrapped and timed.\r\n\r\n### Enabling Profiling\r\n\r\nProfling can be enabled with a query param, URI param or dedicated endpoint:\r\n\r\n```bash\r\n# Query param\r\ncurl -XGET localhost:9200/my_index/my_type/_search -d \'{\r\n  "profile": true,\r\n  "query" : { ... }\r\n}\'\r\n\r\n# URI param\r\ncurl -XGET localhost:9200/my_index/my_type/_search?profile=true\r\n\r\n# Endpoint\r\ncurl -XGET localhost:9200/my_index/my_type/_profile -d \'{\r\n  "query" : { ... }\r\n}\'\r\n```\r\n\r\n### Response output\r\n\r\nWhen enabled, the response will contain an additional `profile` element.  This will contain a tree of "profile components", where each component represents a portion of the query.  Often, this tree of components will be larger than you expect - Elasticsearch and Lucene will expand and rewrite portions of the query, which introduces more pieces than you might expect.\r\n\r\nEach component contains the following:\r\n\r\n- **type** - The name of the Elasticsearch/Lucene class for this part of the query, e.g. `TermQuery`\r\n- **time** - The total aggregated time at this level in the tree, which includes all children and all shards.  Displayed as microseconds\r\n- **relative** - Relative contribution of this level in the tree.  This metric is `time / total_time`.  Displayed as a percentage\r\n- **lucene** - A rough approximation of the Lucene syntax for the component, which is often helpful in differentiating many similar components (e.g. differentiating `term: { "field1": "fred" }` from `term: { "field1": "bob" }`, since both are `TermQuery`s)\r\n- **components** - Zero or more additional components which are children to this level.  For example, a Bool will have children components representing the various clauses.\r\n\r\n```json\r\n{\r\n   "took": 99,\r\n   "timed_out": false,\r\n   "_shards": { ... },\r\n   "hits": { ... },\r\n   "profile": [\r\n      {\r\n         "type": "XFilteredQuery",\r\n         "time": 153996,\r\n         "relative": "100.00%",\r\n         "lucene": "filtered(filtered(my_field:the my_field:quick my_field:fox)->cache(input1:[1 TO 1]))->cache(_type:test)",\r\n         "components": [\r\n            {\r\n               "type": "XFilteredQuery",\r\n               "time": 108036,\r\n               "relative": "70.155%",\r\n               "lucene": "filtered(my_field:the my_field:quick my_field:fox)->cache(input1:[1 TO 1])",\r\n               "components": [\r\n                  {\r\n                     "type": "BooleanQuery",\r\n                     "time": 62053,\r\n                     "relative": "40.295%",\r\n                     "lucene": "my_field:the my_field:quick my_field:fox",\r\n                     "components": [\r\n                        {\r\n                           "type": "TermQuery",\r\n                           "time": 20366,\r\n                           "relative": "13.225%",\r\n                           "lucene": "my_field:the"\r\n                        },\r\n                        {\r\n                           "type": "TermQuery",\r\n                           "time": 1117,\r\n                           "relative": "0.72534%",\r\n                           "lucene": "my_field:quick"\r\n                        },\r\n                        {\r\n                           "type": "TermQuery",\r\n                           "time": 391,\r\n                           "relative": "0.25390%",\r\n                           "lucene": "my_field:fox"\r\n                        }\r\n                     ]\r\n                  },\r\n                  {\r\n                     "type": "FilterCacheFilterWrapper",\r\n                     "time": 3983,\r\n                     "relative": "2.5864%",\r\n                     "lucene": "cache(input1:[1 TO 1])"\r\n                  }\r\n               ]\r\n            },\r\n            {\r\n               "type": "FilterCacheFilterWrapper",\r\n               "time": 3835,\r\n               "relative": "2.4903%",\r\n               "lucene": "cache(_type:test)"\r\n            }\r\n         ]\r\n      }\r\n   ]\r\n}\r\n```\r\n\r\n### Todo / Known Issues\r\n\r\n- More tests!  The current tests check for basic validity and overt failure, but I think we need a better way to check the contents of the profile response.  + Unit tests for things like the ProfileVisitors\r\n- Shards can rewrite queries differently (e.g. if the term is not present on one shard), which means chunks of the output could potentially be skipped.  Naively handling this by just ignoring components which don\'t match.\r\n- Potentially gross output, especially with numerics being expressed as byte strings (e.g. `\\0001`)\r\n- Fairly abusive use of Visitor pattern.  In particular, I\'m not overly happy with the ProfileCollapsingVisitor which is used to collect timings from the query tree...but I have no idea how to better structure it.  The biggest problem is that all query types need to be hardcoded, so this will be a maintenance tar pit.\r\n- Audit of the supported Query/Filter types...I imagine there are others that I\'m missing which need to be added to both visitors\r\n-  `/_profile` endpoint could use some love, ideally extending the search endpoint instead of c/p\r\n'
6697,'jpountz',"Aggregations: Memory-bound terms\nOur terms aggregations implementations use an amount of memory that is linear with the cardinality of the value source they run on. Thanks to global ordinals, we only require 4 bytes per unique term to track the count, so even with a field that has a cardinality of 10M, that would only be 40MB of memory.\r\n\r\nHowever, things get worse when using sub aggregations, especially the memory-intensive ones such as `percentiles`, `cardinality`, `top_hits` or bucket aggregations. Ideally we would want memory usage to depend on `size` instead of the cardinality of the value source.\r\n\r\nI have been looking recently at the `Space-Saving` algorithm described in section 3.1 of [`Efficient Computation of Frequent and Top-k Elements in Data Streams`](https://icmi.cs.ucsb.edu/research/tech_reports/reports/2005-23.pdf). Although described for computing top buckets based on counts, I think it would be possible to use it when sorting by term or sub-aggregation. But you don't get optimized memory usage for free so it would have two main drawbacks compared to the current implementation:\r\n 1. it would only work correctly on skewed distributions\r\n 2. there is no clear separation between buckets and some buckets might aggregate document from other buckets\r\n\r\nI think `1` is fine since top terms tend to make more sense on skewed distributions anyway, eg. most frequent terms in natural language text, or top ip addresses that hit a website.\r\n\r\n`2` is more inconvenient, especially if there are other aggregations under this terms aggregation. One good news is that we could know what buckets might have aggregated data that is not theirs (they would be those whose term ordinal has been updated during collection), so we could have it as part of the response if necessary. On the other hand, if there are no sub aggregations, it would be more acceptable to use this implementation since [counts are inaccurate anyway](https://github.com/elasticsearch/elasticsearch/issues/1305). And it would also work nicely with https://github.com/elasticsearch/elasticsearch/issues/6696 since the maximum error can be estimated."
6689,'rjernst','Eats some stack traces\nI have a bug in my highlighter but I\'m having trouble tracking it down because Elasticsearch is eating the stack trace from the root cause exception.  My logs:\r\n```\r\nCaused by: org.elasticsearch.search.fetch.FetchPhaseExecutionException: [simplewiki_content_1404072971][0]: query[filtered(+ConstantScore(_uid:page#425633) +(auxiliary_text:what auxiliary_text:love?))->cache(org.elasticsearch.index.search.nested.NonNestedDocsFilter@17097ef8)],from[0],size[10]: Fetch Failed [Failed to highlight field [auxiliary_text]]\r\n        at org.elasticsearch.search.highlight.ExperimentalHighlighter.highlight(ExperimentalHighlighter.java:89)\r\n        at org.elasticsearch.search.highlight.HighlightPhase.hitExecute(HighlightPhase.java:126)\r\n        at org.elasticsearch.search.fetch.FetchPhase.execute(FetchPhase.java:211)\r\n        at org.elasticsearch.search.SearchService.executeFetchPhase(SearchService.java:340)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:751)\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$SearchQueryFetchTransportHandler.messageReceived(SearchServiceTransportAction.java:740)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:744)\r\nCaused by: java.lang.ArrayIndexOutOfBoundsException\r\n```\r\n\r\nWhy only print the name of the cause?  I\'d much prefer the full, gory stack trace to show up in the logs.  It\'d make it easier to reproduce the problem in a development environment.\r\n\r\nI\'m not sure where the comes from.  I certainly don\'t attempt to clear the stack trace any where.  The code that catches my exception and rethrows it to Elasticsearch is just:\r\n```java\r\n        } catch (Exception e) {\r\n            throw new FetchPhaseExecutionException(context.context, "Failed to highlight field ["\r\n                    + context.fieldName + "]", e);\r\n        }\r\n```\r\nwhich seems pretty standard.'
6687,'clintongormley','Simplifying JSON API in 2.0+\n*Note:  This is a rehash of my [mailing list post](https://groups.google.com/forum/#!topic/elasticsearch/Md3nOZ89eoU) which didn\'t seem to generate any discussion.  I wanted to bring it over here in hopes of finding the right audience.*\r\n\r\n\r\nI was curious if there were any plans to update or modify the JSON query API in ES 2.0+?\r\n\r\nWhile I find the API to very powerful, it is confusing to construct a valid request and requires special casing a lot of rules.  I have some thoughts below on what I see as the current issues, and some suggestions to correct them.  I don\'t intend for this to be a rant, just to provoke discussion.  This is done purely from the point of view of constructing queries (not parsing them), and only for the JSON DSL query syntax for searching (not percolate or aggregators).\r\n\r\n\r\nIt is currently hard to construct small parts of a JSON query without knowing all of the elements involved. Looking at a simple query and a filtered query:\r\n\r\nSimple Query:\r\n\r\n```json\r\n{\r\n    "query": {\r\n        "match_all": {}\r\n    }\r\n}\r\n```\r\n\r\nFiltered Query:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "filter": {\r\n        "and": [\r\n          {\r\n            "term": {\r\n              "foo": "bar"\r\n            }\r\n          }\r\n        ]\r\n      },\r\n      "query": {\r\n        "match_all": {}\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nWe see the syntax tree change so that the initial \'query\' becomes nested and the root of the tree changes.  Once we add a scoring function, it morphs even further.\r\n\r\nScored Query:\r\n\r\n```json\r\n{\r\n  "query": {\r\n    "function_score": {\r\n      "query": {\r\n        "filtered": {\r\n          "filter": {\r\n            "and": [\r\n              {\r\n                "term": {\r\n                  "foo": "bar"\r\n                }\r\n              }\r\n            ]\r\n          },\r\n          "query": {\r\n            "match_all": {}\r\n          }\r\n        }\r\n      },\r\n      "script_score": {\r\n        "script": "result = 0.0 + 1.0;"\r\n      },\r\n      "boost_mode": "replace"\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThis follows some of the same rules (nested inside a new scope), however, not all of the changes get placed together.  We have both a \'script_score\' block and a \'boost_mode\' section.  This means that when I want to add scoring to my query I need to know my scoring block as well as the rest of the query tree so that I can properly place \'boost_mode\'. \r\n\r\n\r\nA simple(r) example.  In a simple scored query, if I want to modify my \'match_all\' block, my path becomes\r\n`"query" -> "function_score" -> "query"`\r\n\r\nOnce I add filtering to the query, the path changes, causing a broken query if I insert in the old location.\r\n`"query" -> "function_score" -> "query" -> "filtered" -> "query"`\r\n\r\nIt would be much simpler if I could define my scoring block, and throw it in to a query at a static path without worrying what else is in the query.  This case is a simple illustration, but the JSON query DSL contains many instances, especially around cases like \'scoring\' where using a single scoring block vs. multiple scoring functions radically changes the structure of the scoring section.\r\n\r\nI understand that this was designed iteratively, and that the syntax will not be perfect of both parsing and construction.  Now that the JSON query DSL seems to have a stable set of elements, it would be useful to set it up so that it can be written in a simple manner.  A few considerations:\r\n\r\n1. When adding an element such as scoring, have it only modify elements below it in the tree (aside from its initial insertion point)\r\n2. Keep the root of the tree static and have the existence of a top level key modify behavior, instead of needing change the nesting of elements. \r\n3. Somehow stop nesting the term "query" all over the place, definitely the most confusing thing for new users in my experience. =D\r\n\r\nHere is a proposed top level DSL example.  It\'s incomplete and probably missing some things but useful as an illustration:\r\n\r\n```json\r\n{\r\n  "filter": {\r\n    "and": [\r\n      {\r\n        "term": {\r\n          "foo": "bar"\r\n        }\r\n      }\r\n    ]\r\n  },\r\n  "query": {\r\n    "match_all": {}\r\n  },\r\n  "scoring": {\r\n      "script_score": {\r\n        "script": "result = 0.0 + 1.0;"\r\n      },\r\n      "boost_mode": "replace"\r\n  },\r\n  "sort": [\r\n    {\r\n      "foo": {\r\n        "order": "desc",\r\n        "mode": "average"\r\n      }\r\n    }\r\n  ]\r\n}\r\n```\r\n\r\n\r\nThanks for reading over this.  I was unable to find a roadmap for prospective features, so if there are already plans to work on this feel free to disregard my comments.\r\n\r\nThanks,\r\nAndrew'
6682,'electrical','APT repository format can cause problems\nThe current apt [repository format](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/setup-repositories.html) is not following the [Debian repository guidelines] (https://wiki.debian.org/HowToSetupADebianRepository#APT_Archive_Types). The version number on the path of the repository can cause confusion. \r\n\r\nPackage naming also violates Debian guidelines, leading to versioning problems. Version upgrade is also problematic, as the entire line in sources.list has to be changed for upgrade, while the Debian guidelines recommends package pinning to keep a specific version. \r\n\r\nThere are several issues open about this (#5536, #1726) that can help solving this problem.'
6679,'bleskes','[Code] Use EnumSets<> when testing for multiple shard states\nAt the moment we have places that check whether a shard is in any of a couple of possible states before performing an action. This can look like this:\r\n\r\n```\r\nif (state != IndexShardState.STARTED && state != IndexShardState.RELOCATED && state != IndexShardState.RECOVERING && state != IndexShardState.POST_RECOVERY) {\r\n   throw new IllegalIndexShardStateException(shardId, state, "operations only allowed when started/relocated");\r\n}\r\n```\r\n\r\nWe should move to using static final enum sets that communicate the intention of the test, for example:\r\n\r\n```\r\nprivate static final EnumSet<IndexShardState> CAN_WRITE_TO_SHARD_STATES= EnumSet.of(IndexShardState.POST_RECOVERY, IndexShardState.STARTED, IndexShardState.RELOCATED, IndexShardState.RECOVERING);\r\n\r\nif (!CAN_WRITE_TO_SHARD_STATES.contains(state)) {\r\n   throw new IllegalIndexShardStateException(shardId, state, "operations only allowed when started/relocated");\r\n}\r\n\r\n```\r\n\r\nThis makes the code cleaner and easier to read\r\n'
6677,'jpountz',"Mappings: Deprecate custom `index_name`s\nToday we allow for custom index names. This adds ambiguity as fields might be referred to either according to their index name or their field name. What should happen when several fields have the same index name or if the index name of a field is the same as the full name of another field?\r\n\r\nMy understanding is that custom `index_name`s are not useful anymore now that we have `copy_to` in order to index several logical fields into a single physical one, so I'd like to deprecate custom `index_name`s so that the index name would always be the same as the full name.\r\n\r\nRelates to #4081"
6674,'markharwood','Wildcards for fields in fuzzy like this query\nRight now you can use fields property in fuzzy like this query to specify list of fields to query against, but if your documents have nested objects inside and you want to search only against all fields inside particular nested object, you have to specify list of all properties with full path. For example with the following document:\r\n```\r\n{"id": "some_id",\r\n"en": {\r\n    "title": "en_title",\r\n    "description": "en_description",\r\n    /* ... */\r\n},\r\n"nl": {\r\n    "title": "nl_title",\r\n    "description": "nl_description",\r\n    /* ... */\r\n}}\r\n```\r\n\r\nIn some cases you want to search only through **en** subdocument, then fuzzy like this should be:\r\n```\r\n{"query": {\r\n    "flt": {\r\n        "like_text": "search string",\r\n        "fields": [ "en.title", "en.description", /* all other fields under en */ ]\r\n}}}\r\n```\r\n\r\nIt would be convenient to use wildcards to specify fields especially if you don\'t have defined list of fields here or list is too big, to have query like this:\r\n```\r\n{"query": {\r\n    "flt": {\r\n        "like_text": "search string",\r\n        "fields": [ "en.*" ]\r\n}}}\r\n```\r\nMay be it\'s worth to add include/exclude patterns. It\'s more or less the same feature as for _source: [source filtering](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-source-filtering.html)\r\n```\r\n{"query": {\r\n    "flt": {\r\n        "like_text": "search string",\r\n        "fields": {\r\n            "include": [ "obj1.*", "obj2.*" ],\r\n            "exclude": [ "categories.*" ]\r\n}}}}'
6653,'dakrone','Script engines should be aware of the script name\nIt would be nice if we could access the name of the script from the script engine framework so it could be logged when errors occur. This would be especially helpful for additional logging for users with dynamic scripting disabled.'
6650,'s1monw',"[TEST] Randomize the logging level between DEBUG and INFO\nwe should randomize the logging level to make sure we don't hide any NPEs etc."
6638,'imotov','Add ability to snapshot to file system of a single node.\nAdd new "local" snapshot repository type that would be similar to shared filesystem repository but will allow storing all snapshot file on a local file system of a single node.'
6622,'areek',"Move `RamDirectoryService` to the test package\nWe have `RamDirectoryService` in our core code that allows you to use lucene's `RAMDirectory` I think this is an awful trap and we should only use it for certain unit test."
6606,'clintongormley',"[guide]  More shard size discussion\nIt'd be nice to have more discussion on the shard size in the guide.  Now it says this:\r\n```\r\nWhile there is no theoretical limit to the amount of data that a primary shard can hold, there is a practical limit. What constitutes the maximum shard size depends entirely on your use case: the hardware you have, the size and complexity of your documents, how you index and query your documents, and your expected response times.\r\n```\r\n\r\nSomethings I've found anecdotally that would be nice to have in there if they are true:\r\n* Having more total shards then nodes slows down queries quite a bit.\r\n* While most things parallelize reasonably well highlighting multi term queries and the term and phrase suggester have pretty high per shard costs.\r\n* If you have more then 500 indexes you should keep each index in fewer shards to keep the cluster state smaller.\r\n* Shards bigger then a few gigabytes take a long time to copy.\r\n\r\n"
6594,'areek',"term,phrase suggester couldn't support double-byte word like Chinese.\nI test these two suggesters.Find, they just support english,and don't support Chinese.So, I suspect they just support one-byte word like English."
6593,'spinscale','Completion stats walk through all fields\nTo compute completion stats we currently walk through all the lucene fields and all segments in our shards. This is inefficient as we only need to walk through fields used for completion. This becomes even more extreme for people not using the completion suggester.\r\n\r\nRelevant code: \r\nhttps://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/search/suggest/completion/Completion090PostingsFormat.java#L360'
6583,'bleskes','Replication of new documents from 1.1.2 primary to 1.0.3 replica not working during upgrade\nDuring a rolling upgrade of our cluster from 1.0.3 to 1.1.2, we observed issues with replication of newly indexed documents from 1.0.3 data nodes holding primary shards to 1.1.2 data nodes holding their replicas.  Replication between 1.0.3 primaries and 1.0.3 replicas, between 1.1.2 primaries and 1.0.3 replicas, and 1.1.2 primaries and 1.1.2 replicas continued to work correctly throughout the upgrade.  Documents were being indexed via the _bulk API.\r\n\r\nLogs on both 1.0.3 and 1.1.2 nodes logged three different types of unusual messages, which did not cease until every node in the cluster was upgraded:\r\n\r\n[2014-06-18 15:29:33,057][WARN ][transport.netty          ] [T02-C01-A02] Message not fully read (request) for [2092] and action [indices/recovery/s], resetting\r\n\r\n[2014-06-18 20:47:37,234][WARN ][transport.netty          ] [T02-C01-A02] Message not fully read (response) for [41328828] handler org.elasticsearch.action.support.broadcast.TransportBroadcastOperationAction$AsyncBroadcastAction$3@1c73aab6, error [false], resetting\r\n\r\n[2014-06-19 21:47:52,935][DEBUG][discovery.zen            ] [T02-C01-A02] received cluster state from [[ES2PROD-M01][ViFfCcmsRLKAEkoli2hfWQ][es2prod-m01][inet[/10.0.64.103:9300]]{updateDomain=3, data=false, faultDomain=1, master=true}] which is also master but with cluster name [null]\r\n\r\nWe are running our nodes in Azure on Windows 2012 R2 Datacenter using the following JVM:\r\n            "version": "1.7.0_55",\r\n            "vm_name": "OpenJDK 64-Bit Server VM",\r\n            "vm_version": "24.55-b03",\r\n            "vm_vendor": "Azul Systems, Inc."\r\n\r\nAlso, it should be noted that /_cat/shards and /_cat/indices both returned null pointer exceptions during the upgrade process.'
6569,'bleskes','NullPointerException corrupts index\n[2014-06-19 10:57:34,834][ERROR][index.engine.internal    ] [a3] [pl][5] failed to acquire searcher, source load_version\r\njava.lang.NullPointerException\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:649)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1221)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.innerCreate(InternalEngine.java:414)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.create(InternalEngine.java:386)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.create(InternalIndexShard.java:384)\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:572)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio\r\nnOperationAction.java:249)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio\r\nnOperationAction.java:228)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n[2014-06-19 10:57:34,838][ERROR][index.engine.internal    ] [a3] [pl][5] failed to acquire searcher, source load_version\r\njava.lang.NullPointerException\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.acquireSearcher(InternalEngine.java:649)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.loadCurrentVersionFromIndex(InternalEngine.java:1221)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.innerCreate(InternalEngine.java:414)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.create(InternalEngine.java:386)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.create(InternalIndexShard.java:384)\r\n        at org.elasticsearch.action.bulk.TransportShardBulkAction.shardOperationOnReplica(TransportShardBulkAction.java:572)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio\r\nnOperationAction.java:249)\r\n        at org.elasticsearch.action.support.replication.TransportShardReplicationOperationAction$ReplicaOperationTransportHandler.messageReceived(TransportShardReplicatio\r\nnOperationAction.java:228)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n...\r\n\r\n[2014-06-19 10:57:34,838][WARN ][indices.recovery         ] [a3] [pl][5] recovery from [[a1][VmKnyScUS968jh3MsqNh5g][a1.example.com][inet[/1.2.3.4:9301]]{datacenter=abc}] failed\r\norg.elasticsearch.transport.RemoteTransportException: [a1][inet[/1.2.3.4:9301]][index/shard/recovery/startRecovery]\r\nCaused by: org.elasticsearch.index.engine.RecoveryEngineException: [pl][5] Phase[2] Execution failed\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.recover(InternalEngine.java:1011)\r\n        at org.elasticsearch.index.shard.service.InternalIndexShard.recover(InternalIndexShard.java:631)\r\n        at org.elasticsearch.indices.recovery.RecoverySource.recover(RecoverySource.java:122)\r\n        at org.elasticsearch.indices.recovery.RecoverySource.access$1600(RecoverySource.java:62)\r\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:351)\r\n        at org.elasticsearch.indices.recovery.RecoverySource$StartRecoveryTransportRequestHandler.messageReceived(RecoverySource.java:337)\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler$RequestHandler.run(MessageChannelHandler.java:270)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n        at java.lang.Thread.run(Thread.java:745)\r\n\r\n\r\nCaused by: org.apache.lucene.index.CorruptIndexException: checksum failed (hardware problem?) : expected=ddd229e actual=e296bd98 (resource=BufferedChecksumIndexInput(MMapIndexInput(path="/home/abc/elasticsearch_new/data/elasticsearch/nodes/0/indices/pl/5/index/_9yef_es090_0.tip")))\r\n        at org.apache.lucene.codecs.CodecUtil.checkFooter(CodecUtil.java:211)\r\n        at org.apache.lucene.codecs.CodecUtil.checksumEntireFile(CodecUtil.java:268)\r\n        at org.apache.lucene.codecs.BlockTreeTermsReader.<init>(BlockTreeTermsReader.java:140)\r\n        at org.apache.lucene.codecs.lucene41.Lucene41PostingsFormat.fieldsProducer(Lucene41PostingsFormat.java:441)\r\n        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat$BloomFilteredFieldsProducer.<init>(BloomFilterPostingsFormat.java:133)\r\n        at org.elasticsearch.index.codec.postingsformat.BloomFilterPostingsFormat.fieldsProducer(BloomFilterPostingsFormat.java:104)\r\n        at org.elasticsearch.index.codec.postingsformat.Elasticsearch090PostingsFormat.fieldsProducer(Elasticsearch090PostingsFormat.java:79)\r\n        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat$FieldsReader.<init>(PerFieldPostingsFormat.java:195)\r\n        at org.apache.lucene.codecs.perfield.PerFieldPostingsFormat.fieldsProducer(PerFieldPostingsFormat.java:251)\r\n        at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:116)\r\n        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:101)\r\n        at org.apache.lucene.index.ReadersAndUpdates.getReader(ReadersAndUpdates.java:142)\r\n        at org.apache.lucene.index.ReadersAndUpdates.getReadOnlyClone(ReadersAndUpdates.java:236)\r\n        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:99)\r\n        at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:385)\r\n        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:112)\r\n        at org.apache.lucene.search.SearcherManager.<init>(SearcherManager.java:89)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.buildSearchManager(InternalEngine.java:1364)\r\n        at org.elasticsearch.index.engine.internal.InternalEngine.start(InternalEngine.java:291)\r\n        ... 7 more'
6555,'s1monw','Unassigned shards: Failed to start shard\nHi All,\r\n\r\nI\'ve recently been struggling with some intermittent issues with unassigned shards. Once a day or two I get into a red state with a handful of unassigned shards and have to use the reroute api to get them assigned.\r\n\r\nI was able to find the following line from the logs on the master node:\r\n\r\n[2014-06-18 20:19:10,644][WARN ][cluster.action.shard     ] [prod-sidekiq2] [1815_2014_18][1] received shard failed for [1815_2014_18][1], node[eMYIqhQTR0S1m4UeRJlLxg], [P], s[INITIALIZING], indexUUID [uHW84Y49Qg2KTZnI9dmLJw], reason [Failed to start shard, message [IndexShardGatewayRecoveryException[[1815_2014_18][1] failed to fetch index version after copying it over]; nested: IndexShardGatewayRecoveryException[[1815_2014_18][1] shard allocated for local recovery (post api), should exist, but doesn\'t, current files: [segments.gen, _checksums-1399837226100]]; nested: FileNotFoundException[segments_8]; ]]\r\n\r\n\r\nI found issue #4674, which sounds very similar as I am using multi data paths, but I\'m running 1.0.0 so I guess the fix for this got into that release.\r\n\r\n```\r\ncurl localhost:9200\r\n{\r\n  "status" : 200,\r\n  "name" : "prod-es3",\r\n  "version" : {\r\n    "number" : "1.0.0",\r\n    "build_hash" : "a46900e9c72c0a623d71b54016357d5f94c8ea32",\r\n    "build_timestamp" : "2014-02-12T16:18:34Z",\r\n    "build_snapshot" : false,\r\n    "lucene_version" : "4.6"\r\n  },\r\n  "tagline" : "You Know, for Search"\r\n}\r\n```\r\n\r\nAny one have any suggestions as to how this might be diagnosed/fixed?\r\n\r\nWould this issue be resolved with an upgrade to 1.0.3 or 1.2.?'
6533,'electrical','Added SLES11 Support to Initscript\nAdded SLES11 Support to Initscript'
6522,'dakrone',"Index prioritization during recovery\nES currently does not have the ability to prioritize indices during recovery.  It would be useful for the node to recover certain shards first, using an algorithm chosen by the user.  Some possibilities could be 'smaller indices first' or 'larger indices first' or 'most recently created indices first'.  That would allow for fast recovery of a current day's log index (for example) to get that index green and ready for ingestion as soon as possible."
6512,'areek','Context Suggester: Context does not work with non strings in mapping\nHey,\r\n\r\nthe context suggester only works with strings in the mapping if you specify a field, see this example\r\n\r\n```\r\nDELETE /services\r\nPUT /services\r\nPUT /services/service/_mapping\r\n{\r\n    "service": {\r\n        "properties": {\r\n            "network_id": {\r\n                "type" : "long"\r\n            },\r\n            "suggest_field": {\r\n                "type": "completion",\r\n                "context": {\r\n                    "network": { \r\n                        "type": "category",\r\n                        "path": "network_id"\r\n                    }\r\n                }\r\n            },\r\n            "suggest_field2": {\r\n                "type": "completion",\r\n                "context": {\r\n                    "network": { \r\n                        "type": "category"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPUT /services/service/1\r\n{\r\n    "name": "knapsack",\r\n    "network_id": "1",\r\n    "suggest_field": {\r\n        "input": ["knacksack", "backpack", "daypack"]\r\n    },\r\n    "suggest_field2": {\r\n        "input": ["knacksack", "backpack", "daypack"],\r\n        "context" : {\r\n          "network" : "1"\r\n        }\r\n    }\r\n}\r\n\r\nPOST services/_suggest?pretty\'\r\n{\r\n    "suggest" : {\r\n        "text" : "k",\r\n        "completion" : {\r\n            "field" : "suggest_field",\r\n            "size": 10,\r\n            "context": {\r\n                "network": "1"\r\n            }\r\n        }\r\n    },\r\n    "suggest2" : {\r\n        "text" : "k",\r\n        "completion" : {\r\n            "field" : "suggest_field2",\r\n            "size": 10,\r\n            "context": {\r\n                "network": "1"\r\n            }\r\n        }\r\n    }\r\n}\r\n```\r\n\r\nSolution 1: Call `toString()` for everything\r\nSolution 2: Reject contexts for anything else than strings'
6511,'martijnvg','Add doc values support to _parent field\nPR for #6107'
6507,'bleskes','Retry if initial contact fails\nWe have a 5-node cluster. As part of a full application redeploy, we are restarting every node in sequence.\r\nHere the rebooted 2nd node ("Moondark", new node, logs attached) tries to reach the old 3rd node ("Volstagg", old master) right as it shuts down as part of the restart. The new 3rd node to replace Volstagg has not yet come up.\r\nMoondark then figures out that the master is unreachable, and *gives up*.\r\n\r\nShouldn\'t a new node trigger another round of pings and a master connect attempt, in the case that the old master dies during initial connect?\r\n\r\nLike this, the new node never joins the cluster. I let the node live for an hour just to confirm.\r\n\r\n```\r\n2014-06-16 10:18:21,114 INFO  node                                      - [Moondark] version[1.1.1], pid[3867], build[f1585f0/2014-04-16T14:27:12Z]\r\n2014-06-16 10:18:21,118 INFO  node                                      - [Moondark] initializing ...\r\n2014-06-16 10:18:21,125 INFO  plugins                                   - [Moondark] loaded [], sites [head, bigdesk, elasticsearch_hq, kibana]\r\n2014-06-16 10:18:23,008 DEBUG discovery.zen.ping.unicast                - [Moondark] using initial hosts [tsl0mag12:12500, tsl0mag14:12500, tsl0mag15:12500, tsl0mag16:12500, tsl0mag17:12500], with concurrent_connects [10]\r\n2014-06-16 10:18:23,022 DEBUG discovery.zen                             - [Moondark] using ping.timeout [3s], master_election.filter_client [true], master_election.filter_data [false]\r\n2014-06-16 10:18:23,025 DEBUG discovery.zen.elect                       - [Moondark] using minimum_master_nodes [3]\r\n2014-06-16 10:18:23,026 DEBUG discovery.zen.fd                          - [Moondark] [master] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n2014-06-16 10:18:23,045 DEBUG discovery.zen.fd                          - [Moondark] [node  ] uses ping_interval [1s], ping_timeout [30s], ping_retries [3]\r\n2014-06-16 10:18:24,003 INFO  node                                      - [Moondark] initialized\r\n2014-06-16 10:18:24,003 INFO  node                                      - [Moondark] starting ...\r\n2014-06-16 10:18:24,097 INFO  transport                                 - [Moondark] bound_address {inet[/0:0:0:0:0:0:0:0:12500]}, publish_address {inet[/151.187.99.214:12500]}\r\n2014-06-16 10:18:24,119 TRACE discovery                                 - [Moondark] waiting for 30s for the initial state to be set by the discovery\r\n2014-06-16 10:18:24,162 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:24,163 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:24,162 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]\r\n2014-06-16 10:18:24,167 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:24,169 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connecting (light) to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:24,222 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:24,224 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:24,227 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:24,227 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:24,229 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:24,228 TRACE discovery.zen.ping.unicast                - [Moondark] [1] connected to [#zen_unicast_2#][tsl0mag14.skead.no][inet[tsl0mag14/151.187.99.214:12500]]\r\n2014-06-16 10:18:24,230 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]\r\n2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1\r\ncG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH\r\n_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:24,275 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1\r\ncG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg\r\n][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:24,274 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:24,280 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:25,664 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:25,665 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]\r\n2014-06-16 10:18:25,666 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:25,667 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:25,668 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:25,672 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:25,673 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:25,674 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,175 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:27,176 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]\r\n2014-06-16 10:18:27,177 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:27,178 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:27,178 TRACE discovery.zen.ping.unicast                - [Moondark] [1] sending to [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:27,180 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,180 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,182 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,182 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,184 TRACE discovery.zen.ping.unicast                - [Moondark] [1] received response from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]: [ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}, ping_response{target [[Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]], master [null], cluster_name[IRIS-GP2-N-S]}]\r\n2014-06-16 10:18:27,185 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_4#][tsl0mag14.skead.no][inet[tsl0mag16/151.187.99.216:12500]]\r\n2014-06-16 10:18:27,194 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_5#][tsl0mag14.skead.no][inet[tsl0mag17/151.187.99.217:12500]]\r\n2014-06-16 10:18:27,195 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [Moondark][lJ5VZ7IGQ5C1cG38rxoSZA][tsl0mag14.skead.no][inet[/151.187.99.214:12500]]\r\n2014-06-16 10:18:27,223 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_1#][tsl0mag14.skead.no][inet[tsl0mag12/151.187.99.212:12500]]\r\n2014-06-16 10:18:27,223 TRACE discovery.zen.ping.unicast                - [Moondark] [1] disconnecting from [#zen_unicast_3#][tsl0mag14.skead.no][inet[tsl0mag15/151.187.99.215:12500]]\r\n2014-06-16 10:18:27,226 TRACE discovery.zen                             - [Moondark] full ping responses:\r\n        --> target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n2014-06-16 10:18:27,227 DEBUG discovery.zen                             - [Moondark] filtered ping responses: (filter_client[true], filter_data[false])\r\n        --> target [[Shatterstar][IQoquIjSRjOpbMenBqAv3A][tsl0mag12.skead.no][inet[/151.187.99.212:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Sushi][iCNA-wEbSXOSn71Gngr_Qg][tsl0mag16.skead.no][inet[/151.187.99.216:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n        --> target [[Venus Dee Milo][ZogIMgj1R5WedZVR7f2A_w][tsl0mag17.skead.no][inet[/151.187.99.217:12500]]], master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]]\r\n2014-06-16 10:18:27,472 DEBUG discovery.zen.fd                          - [Moondark] [master] starting fault detection against master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [initial_join]\r\n2014-06-16 10:18:27,483 DEBUG discovery.zen.fd                          - [Moondark] [master] stopping fault detection against master [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [master failure, failed to perform initial connect [[Volstagg][inet[/151.187.99.215:12500]] connect_timeout[30s]]]\r\n2014-06-16 10:18:27,483 INFO  discovery.zen                             - [Moondark] master_left [[Volstagg][PKLH_LAsQhOBmv7U2vk_5A][tsl0mag15.skead.no][inet[/151.187.99.215:12500]]], reason [failed to perform initial connect [[Volstagg][inet[/151.187.99.215:12500]] connect_timeout[30s]]]\r\n2014-06-16 10:18:27,489 TRACE discovery                                 - [Moondark] initial state set from discovery\r\n2014-06-16 10:18:27,489 INFO  discovery                                 - [Moondark] IRIS-GP2-N-S/lJ5VZ7IGQ5C1cG38rxoSZA\r\n2014-06-16 10:18:27,498 INFO  http                                      - [Moondark] bound_address {inet[/0:0:0:0:0:0:0:0:12400]}, publish_address {inet[/151.187.99.214:12400]}\r\n2014-06-16 10:18:27,510 INFO  node                                      - [Moondark] started\r\n```'
6498,'electrical','RPM repository broken on RHEL5: «Error performing checksum»\nYum complains about wrong checksums when the RPM repository is used in RHEL 5:\r\n\r\n>    $ yum list\r\n>    Loaded plugins: rhnplugin, security\r\n>    This system is receiving updates from RHN Classic or RHN Satellite.\r\n>    elasticsearch-0.90/primary_db                            | 9.1 kB     00:00\r\n>    http://packages.elasticsearch.org/elasticsearch/0.90/centos/repodata/primary.sqlite.bz2: [Errno -3] Error performing checksum\r\n>    Trying other mirror.\r\n>    Error: failure: repodata/primary.sqlite.bz2 from elasticsearch-0.90: [Errno 256] No more mirrors to try.\r\n\r\nThis problem has been dissected in this blog post: <http://prefetch.net/blog/index.php/2009/11/26/dealing-with-yum-checksum-errors/>.\r\n\r\nThe solution is to create the repositores using `createrepo` with the `-s sha1` option that enables the creation of SHA1 checksums instead of SHA256.'
6469,'bleskes','Failure to recover from 1-way network partition\nAfter a 1-way network partition scenario, two nodes might be stuck in different opinions of which nodes are currently in the cluster.\r\n\r\nI.e. with 5 nodes:\r\nNode 1 sees node 1, 3 and 5. This is a RED cluster. \r\nNode 2 sees node 1, 2, 4 and 5. This is a GREEN cluster.\r\nAll of this according to the `nodes` endpoint.\r\n\r\nThe cluster is *stuck in this state*, even when every node could reach every other node (verified with curl on the es http port and binary port).\r\n\r\nSome of these nodes give an NPE for the `_status` endpoint:\r\n`{"error":"NullPointerException[null]","status":500}`\r\n\r\nAll nodes are configured to discover all the others through unicast.\r\nDiscovered on ES 0.90.13 on RHEL 6.5.'
6449,'dakrone','Minimium_should_match doesn\'t work with simple_query_string\nWhen I add a minimum_should_match field to my simple_query_string object, it returns an error about an unsupported field.\r\n\r\nThe error:\r\n```\r\nQueryParsingException[[owlin-production] [simple_query_string] unsupported field [minimum_should_match]]; }]","status":400}\r\n```\r\n\r\nThe query:\r\n```\r\n{\r\n    simple_query_string: {\r\n        fields: [\r\n            \'header\',\r\n            \'description\'\r\n        ],\r\n        query: \'foo\',\r\n        minimum_should_match: 2,\r\n        default_operator: \'OR\'\r\n    }\r\n}\r\n```'
6444,'areek','context suggester not working in elasticsearch\nrunning the example given in the documentation is throwing an error:\r\n\r\n\r\n{\r\n   "_shards": {\r\n      "total": 5,\r\n      "successful": 4,\r\n      "failed": 1,\r\n      "failures": [\r\n         {\r\n            "index": "services",\r\n            "shard": 2,\r\n            "reason": "BroadcastShardOperationFailedException[[services][2] ]; nested: ElasticsearchException[failed to execute suggest]; nested: NullPointerException; "\r\n         }\r\n      ]\r\n   }\r\n}\r\n\r\n\r\n\r\nhere\'s the code example given:\r\n\r\nPUT /services\r\n\r\nPUT /services/service/_mapping\r\n{\r\n    "service": {\r\n        "properties": {\r\n            "name": {\r\n                "type" : "string"\r\n            },\r\n            "tag": {\r\n                "type" : "string"\r\n            },\r\n            "suggest_field": {\r\n                "type": "completion",\r\n                "context": {\r\n                    "color": { \r\n                        "type": "category",\r\n                        "path": "color_field",\r\n                        "default": ["red", "green", "blue"]\r\n                    },\r\n                    "location": { \r\n                        "type": "geo",\r\n                        "precision": "5m",\r\n                        "neighbors": true,\r\n                        "default": "u33"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\r\n\r\nPUT services/service/1\r\n{\r\n    "name": "knapsack",\r\n    "suggest_field": {\r\n        "input": ["knacksack", "backpack", "daypack"],\r\n        "context": {\r\n            "color": ["red", "yellow"]\r\n        }\r\n    }\r\n}\r\n\r\n\r\nPOST services/_suggest?pretty\'\r\n{\r\n    "suggest" : {\r\n        "text" : "m",\r\n        "completion" : {\r\n            "field" : "suggest_field",\r\n            "size": 10,\r\n            "context": {\r\n                "color": "red"\r\n            }\r\n        }\r\n    }\r\n}'
6428,'dakrone','Scripts should have a "cache" they can work with\nIt\'d be useful if scripts could dump arbitrary values into a string keyed map that is reused across executions of the script.  In the case of function scoring you\'d want it reused for all the scored documents on the same shard.  For script filters you\'d want it to reused for all filtered documents on the shard.'
6357,'brwe','Random shards do not recover if one index has faulty synonym filter defined\nI use a synonym filter with the synonyms defined in a text file. 1 node, several indexes with several shards, only one of them uses the synonym filter. When I remove the textfile that contains the synonyms many indexes do not recover when I restart the node. This is random, affected shards and indexes vary.\r\n\r\n\r\nTo reproduce:\r\n\r\n1. start node and create some indexes with documents\r\n1. place synonym file in correct folder, create index that uses synonym filter with synonyms from file and index documents (see example below for details)\r\n2. stop node\r\n3. remove the synonym file\r\n4. start node again\r\n\r\nI would have expected only the affected index shards not to recover but instead this is completely random.\r\n\r\n\r\nExample index:\r\n\r\n\r\n```\r\nDELETE synonymindex\r\n\r\nPUT synonymindex\r\n{\r\n   "settings": {\r\n      "index": {\r\n         "analysis": {\r\n            "analyzer": {\r\n               "synonym": {\r\n                  "tokenizer": "whitespace",\r\n                  "filter": [\r\n                     "synonym"\r\n                  ]\r\n               }\r\n            },\r\n            "filter": {\r\n               "synonym": {\r\n                  "type": "synonym",\r\n                  "synonyms_path": "analysis/synonym.txt"\r\n               }\r\n            }\r\n         }\r\n      }\r\n   },\r\n   "mappings": {\r\n      "sometype": {\r\n         "properties": {\r\n            "text": {\r\n               "type": "string",\r\n               "analyzer": "synonym"\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\r\nPOST synonymindex/sometype\r\n{\r\n    "text": "foo"\r\n}\r\n```\r\n\r\nthe file analysis/synonym.txt contains only one line:\r\n\r\n```\r\nfoo, bar\r\n```'
6340,'martijnvg','Failure in saving percolate query with date type function score \nWhen I try to save a query in percolate which has function score on a date field, it fails with error Failed to parse NullPointerException. If function score is on another data type field like number, it works fine. \r\n\r\n```\r\nPUT testindex\r\n\r\nPUT testindex/document/1\r\n{\r\n  "actionDate" : "2014-05-01",\r\n  "score" : 5\r\n}\r\n\r\nPUT testindex/.percolator/1\r\n{\r\n  "query" : {\r\n    "function_score" : {\r\n      "functions" : [\r\n        {\r\n          "exp" :{ \r\n              "actionDate" : {\r\n                "origin" : "now",\r\n                "scale" : "10d"\r\n              }\r\n          }\r\n        }\r\n      ],\r\n      "query" : {\r\n        "match_all": {}\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n{\r\n   "error": "RemoteTransportException[[nylxdev1.node][inet[/172.17.9.175:9301]][index]]; nested: PercolatorException[[testindex] failed to parse query [1]]; nested: QueryParsingException[[testindex] Failed to parse]; nested: NullPointerException; ",\r\n   "status": 500\r\n}\r\n```'
6290,'javanna','Adding `Exceptions` utility to enable code contracts that fail fast\nProvides `Exceptions` utility to enable fail fast behavior with minimal boilerplate code.\r\n\r\nCloses #6289'
6273,'jpountz','null buckets missing from terms aggregation\nThe terms aggregation creates a bucket for each value but doesn\'t include a `null` bucket. When there is a doc with a key set to `null` the cardinality is currently incorrect. \r\n\r\nUsers may or may not want to include a `null` bucket so it should be configurable (default should be false in line with current behaviour), I propose\r\n\r\n    {\r\n        "aggs" : {\r\n            "genders" : {\r\n                "terms" : { \r\n                    "field" : "gender",\r\n                    "null": true\r\n                }\r\n            }\r\n        }\r\n    }\r\n\r\nAnd the response\r\n\r\n    {\r\n        ...\r\n\r\n        "aggregations" : {\r\n            "genders" : {\r\n                "buckets" : [\r\n                    {\r\n                        "key" : "male",\r\n                        "doc_count" : 10\r\n                    },\r\n                    {\r\n                        "key" : "female",\r\n                        "doc_count" : 10\r\n                    },\r\n                    {\r\n                        "key" : null,\r\n                        "doc_count" : 1  // indicates doc without gender field or field set to null\r\n                    },\r\n                ]\r\n            }\r\n        }\r\n    }'
6261,'polyfractal','Indices Stats: Add FieldData and Filter eviction rates\nThis adds one-, five- and fifteen-minute eviction rates for FieldData and Filter stats.  Cumulative eviction counts don\'t give much insight into the nature and frequency of evictions.  Particularly as server uptime increases, cumulative counts become relatively useless.\r\n\r\nBy measuring and the rates over short time intervals, the user can more easily understand how their system is behaving.  Rates are backed by an exponentially weighted moving average, so more recent evictions contribute to the counter moreso than older evictions.\r\n\r\nCurrent example output (updated 06/27/2014):\r\n\r\n```json\r\n{\r\n    "filter_cache": {\r\n       "memory_size_in_bytes": 62312,\r\n       "evictions": 135,\r\n       "evictions_per_sec": {\r\n         "1m": 58.4 , \r\n         "5m": 13.8, \r\n         "15m": 4.7\r\n       }\r\n    },\r\n    "fielddata": {\r\n        "memory_size_in_bytes": 2101455,\r\n        "evictions": 42,\r\n        "evictions_per_sec": {\r\n          "1m": 21.4 , \r\n          "5m": 18.8, \r\n          "15m": 6.7\r\n        }\r\n    }\r\n}\r\n```\r\n'
6259,'s1monw',"Drop unnecessary BW compat checks on master\nWe don't need to keep the BW compat code on master for conditional version reads / writes etc. Yet I kept the BW compat for indices and analysis though."
6229,'bleskes','Remove backward compatibility layer introduced in #6149\nPR #6149 changed the `Versions.MATCH_ANY` value  from 0 to -3. This was done to allow 0 to be a valid external version. This commit removes the backward compatibility code from v2.0 and up.'
6227,'spinscale','More strict parsing of ISO dates\nIf you are using the default date or the named identifiers of dates,\r\nthe current implementation was allowed to read a year with only one\r\ndigit. In order to make this more strict, this fixes a year to be at\r\nleast 4 digits.\r\n\r\nCloses #6158'
6224,'aleph-zero','Add mock benchmark service for testing\nThe benchmark API needs better mock classes for testing.'
6198,'areek','Adds toString() functionality to SuggestRequestBuilder + SuggestBuilder\nJust like SearchRequestBuilder, this adds toString() functionality to SuggestRequestBuilder + SuggestBuilder'
6173,'aleph-zero','Add pause feature to benchmark API\nDuring testing it came up that having the ability to pause an active benchmark would be very useful.'
6168,'dakrone',"disk.threshold_enabled allocation creates dynamics that can cause runaway disk space usage\nEnabling disk.threshold_enabled for shard allocation can cause shards to continuously get relocated around the cluster and potentially lead to completely running out of disk space if the shards (including replicas) is at least half of total disk space. \r\n\r\nThe mechanism is as follows:\r\n1. Run a cluster restart or have a substantial percentage of nodes restart, triggering many shards to reinitialize\r\n2. Shards will get allocated onto other nodes and begin initializing. Some of those nodes may trigger the high disk threshold level for the disk allocation.\r\n3. When the high disk threshold is triggered shards will start relocating. \r\n4. Relocation effectively creates a duplicate shard, increasing disk space across the cluster.\r\n5. Because other nodes are also initializing/relocating nodes, new shard relocations may again trigger the high threshold\r\n6. Repeat to step 3. (Slowly increasing the number of simultaneously relocating shards in the cluster.)\r\n\r\nHere's a graph showing relocating shards continuously increasing:\r\n![image](https://cloud.githubusercontent.com/assets/820871/2971078/dd98e172-db65-11e3-9ae5-29ac509d10f1.png)\r\n\r\ncluster.routing.allocation.cluster_concurrent_rebalance was set to 6. This was observed in ES 1.1.1\r\n\r\nA number of other config options affect the dynamics:\r\n- Shard size and number of shards (more likely to happen with big shards that take longer to move)\r\n- Throttling of recovery/relocation\r\n- Shard Allocation Awareness limiting where nodes can be moved\r\n\r\nSome possible solutions:\r\n- Treat cluster_concurrent_rebalance as a hard maximum (see also #6141)\r\n- Have the relocation/initialization algorithm calculate what disk space will be once all shards are fully moved rather than what disk space currently is.\r\n\r\nMaybe related to #4790 \r\n"
6158,'spinscale','Dynamic date detection accepting years < 4 digits\nDynamic date detection should only detect 4 digit years, but it is misinterpreting the `5` in `5/12/14` as the year `0005`:\r\n\r\n    DELETE /t\r\n\r\n    PUT /t/t/1\r\n    {\r\n      "d": "5/12/14"\r\n    }\r\n\r\n    GET /t/_search\r\n    {\r\n      "aggs": {\r\n        "foo": {\r\n          "date_histogram": {\r\n            "field": "d",\r\n            "interval": "day"\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\nReturns: \r\n\r\n    "aggregations": {\r\n      "foo": {\r\n         "buckets": [\r\n            {\r\n               "key_as_string": "0005/12/14 00:00:00",\r\n               "key": -61979385600000,\r\n               "doc_count": 1\r\n            }\r\n         ]\r\n      }\r\n    }\r\n\r\n\r\n'
6141,'dakrone','Shard rebalance not obeying cluster_concurrent_rebalance\nDuring full cluster restart relocating shards steadily increased and did not respond to setting cluster.routing.allocation.cluster_concurrent_rebalance. This behavior is pretty clearly cause by having disk.threshold_enabled set.\r\n\r\n- 00:30: Relocating shards was around 86 causing us to get tight on disk space (a node ran out of space and we deleted a number of shards with rm before restarting it).\r\n- 00:57: Set cluster_concurrent_rebalance to 10 (trying to limit relocating shards)\r\n- 01:00 Set to 4 (after realizing it was already set to 6 in the config).\r\n- 01:33 Set to 2 (in case something had changed and it was now per node not for the whole cluster)\r\n- 02:00 Relocating shards hit its peak of ~140\r\n\r\n![image](https://cloud.githubusercontent.com/assets/820871/2953053/8521b826-da4c-11e3-8b56-749eca93943a.png)\r\n\r\nI\'m guessing there is an interaction with disk threshold and/or shard allocation. We\'re seeing a lot of disk space taken up by old shards that haven\'t yet been cleaned up (note this is from 03:38):\r\n\r\n![image](https://cloud.githubusercontent.com/assets/820871/2953176/4722b9f4-da50-11e3-8469-b661b9d9ff9f.png)\r\n\r\nSystem config:\r\n- 175 primary shards (plus 2 replicas per shard) = 525 shards total\r\n- 36 data nodes (3 master nodes).\r\n- Total disk space in cluster is 26TB. Total of all primaries is about 4.6TB (x3 = 13.8TB). Should be plenty of space.\r\n- Some shards are pretty big. Max of about 45GB. Generally range from 15-40GB. \r\n- Just ran a full cluster restart to go from v0.90.12 to v1.1.1 (went pretty smoothly otherwise)\r\n- Shard allocation awareness: DC = dfw, iad, sat; PARITY=0,1 (so 6 zones with 6 servers in each)\r\n\r\nRelevant config settings:\r\n```\r\ncluster:\r\n  routing:\r\n    allocation:\r\n      node_initial_primaries_recoveries: 8\r\n      node_concurrent_recoveries: 15\r\n      cluster_concurrent_rebalance: 6\r\n      awareness.attributes: dc, parity\r\n      disk.threshold_enabled: true\r\n\r\nindices:\r\n  recovery:\r\n    max_bytes_per_sec: 100mb\r\n    concurrent_streams: 5\r\n```\r\n\r\n```\r\n{\r\n  "cluster_name" : "es_glbl_cluster",\r\n  "status" : "yellow",\r\n  "timed_out" : false,\r\n  "number_of_nodes" : 39,\r\n  "number_of_data_nodes" : 36,\r\n  "active_primary_shards" : 175,\r\n  "active_shards" : 469,\r\n  "relocating_shards" : 135,\r\n  "initializing_shards" : 56,\r\n  "unassigned_shards" : 0\r\n}\r\n```\r\n'
6131,'spinscale','Making rpm update init.d script even if its been locally changed\nsince otherwise, you get a broken elasticsearch install, whenever the elasticsearch binary, that the init script runs (/usr/share/elasticsearch/bin/elasticsearch) changes something (like the startup needing -d - for it to be daemonized in 1.1.1 :) which makes the old init script not work.\r\n'
6107,'martijnvg','Add doc values support to _parent field data\nThe `_parent` field can easily have a high cardinality, as a consequence field data for this field can take a lot of memory. It would be useful to have the ability to store this mapping on disk using Lucene doc values.\r\n\r\nDoc values proved to perform very well for aggregations in combination with global ordinals (https://github.com/elasticsearch/elasticsearch/pull/5672). So now that parent/child queries use global ordinals as well (https://github.com/elasticsearch/elasticsearch/pull/5846) I think doc values could even be the default for the `_parent` field?'
6077,'jpountz','Allow binary sort values\nIn order to support the new ICUCollationKeyAnalyzer https://github.com/elasticsearch/elasticsearch-analysis-icu/issues/28 we need to be able to handle binary values as sort keys.  The problem comes when serializing the binary keys into JSON.\r\n\r\nPossibly we can detect whether the analyzer creates binary token streams.\r\n\r\nAn alternative would be to add a dedicated `icu_collation_key` field type: https://github.com/elasticsearch/elasticsearch-analysis-icu/issues/29'
6069,'bleskes','Make cluster recovery near instantaneous if all shards are present and accounted for\nWhen restarting a cluster from green state, each shard appears to undergo some form of checksum to verify it before bringing it online.\r\n\r\nIs there a way to journal writes so that recovery is much much faster, in the way that the xfs filesystem does it.\r\n\r\nOnly review the data that was being written to at the time of the outage or shutdown so that only the in-progress write data needs to be checked.\r\n\r\nFor a clean shutdown, maybe a complete cluster restart command could tell all nodes to shutdown in a clean state then turn off, allowing a near instantaneous recovery on startup. Like stop allocation then flush all translogs, then shut down,etc.\r\n\r\nWould just take lots of the pain out of cluster restarts.\r\n\r\nJust an idea'
6036,'pickypg',"Update index.asciidoc\nBecause this client doesn't apparently work with the 1.x elasticsearch (see https://github.com/elasticsearch/elasticsearch-lang-groovy/issues/20 ) and you can't find the source code either, we should tell people so they don't waste a lot of time :-)"
5977,'spinscale','Expose a PluginManager.isPluginInstalled(String name) method\n@nickminutello did a nice work to expose `isInstalled(String)` method in PR #5566.\r\n\r\nAfter reviewing it, I made the `Plugin` class static.\r\nThis changes needs also to be reviewed :)'
5945,'GaelTadh','InternalIndexShard callback handling of failure is missing/wrong\nIn `InternalIndexShard#index` method calls `failedIndex` callback (to reduce current counters) when a `RuntimeException` happens, it should do it for any `Throwable`. Also, we are missing similar logic in create, and we should properly handle delete as well.\r\n\r\nMight make sense to have a single post callback, and not post/failed, with an Throwable passed to indicate if it was a failure or not.'
5866,'martijnvg',"The scroll, clear scroll and analyze apis should support json in request body\nThe scroll, clear scroll and analyze apis are the only apis not accepting json, yaml etc in the request body. In order to be consistent with all the other apis they should support this. These apis just assume the scroll_id / text to analyze is specified as a string in the request body.\r\n\r\nSupporting json, yaml etc. can be done in bwc manner via content type sniffing and if the content type can't figured out then the request body is assumed to be consumed as a plain string."
5861,'karmi','Settings: Trimmed the main `elasticsearch.yml` configuration file\nThis patch significantly trims the main `elasticsearch.yml` configuration file.\r\n\r\nThe current `elasticsearch.yml` file is a relic of the past, when Elasticsearch\r\ndocumentation has been more dispersed and incomplete.\r\n\r\nIt has improved significantly, so the main configuration can be slimmed down,\r\nand contain only the most important settings for development and production use.\r\n\r\nRelated: 8d0f1a7d123f579fc772e82ef6b9aae08f6d13fd'
5851,'clintongormley','Prepend the type name to the index_name automatically\nFields in different types that share the same path end up being indexed into the same inverted index.  This can be a surprising gotcha for new users, who expect that types are as separate as tables in a traditional database.\r\n\r\nOne possibility, suggested in #4081, is to prepend the type name to the `index_name` (the name of the inverted index) automatically.  \r\n\r\nThis can be done manually now, by specifying a different `index_name` per field, but I\'m wondering if it should become the default.  What would be the disadvantages of doing this?  I can think of these:\r\n\r\n* field data would not be shared between types, leading to more memory usage\r\n* you wouldn\'t be able to query the `foo` field across all types in an index, instead you\'d have to query `*.foo`\r\n\r\nThe upsides are:\r\n\r\n* different mappings would not clash\r\n* different field data data types would not clash\r\n* term frequencies would be preserved per type\r\n\r\nI\'m guessing that most of the time, fields with the same name in different types usually represent the same "thing", and so would be mapped in the same way, which leads me to think that we should leave things as they are, and allow users to use the `index_name` to configure this manually where needed.\r\n\r\nAny thoughts?'
5832,'javanna',"Allow unittests to run against an external cluster\nToday we run our unittests against an internal cluster but we already have support for `ImmutableCluster` that is used by tests that done't need to modify the cluster. Yet, those tests can easily run against an external cluster. as a first simple step we can just use some Junit magic to make this work ie. we add support for an annotation like `@NeedsJVMLocalCluster` or `@NeedsMutableCluster` and given the cluster we use to run the tests we can just exclude those tests. \r\nA pure external cluster can then run on all tests that don't have either of these annotations. In a second step we can add a `MutableExternalCluster` impl that can start external processes and tear them down as needed given the test is not annotated with `@NeedsJVMLocalCluster`"
5829,'mikemccand','Track min/max on numerics in field data per segment\nIf we track for numeric values the min/max values in field data, we can potentially use it in several places to optimize execution.\r\n\r\nFor example, in range filter, *if* the field data for a field is loaded, it can be used to check if the term / range filter needs to be executed at all, or it can work as a match all. Potentially, also adding improvements to boolean filter to have a special case for match all.\r\n\r\nAnother option to use this is in aggs, where this can be used to do bucket estimations.'
5794,'spinscale','Stats API fetching all indices information if given regex has no matches\nHi,\r\nI expect to get empty response for scenario;\r\n\r\n```ssh\r\n#create an index\r\ncurl -XPOST http://localhost:9200/test-1\r\n\r\n# execute _stats for indices starting with xyz (ensure this regex has no matches)\r\ncurl -XPOST http://locahost:9200/xyz*/_stats\r\n```\r\n\r\nreturns stats of all indices. I think this should return empty response.\r\n\r\nTried with 1.0.1 and 1.1.0\r\n'
5781,'jpountz','Suboptimal performance when trying to get ids\nI have dense set of documents with ids from 1 to roughly 80 000 000. The whole db takes 70gb on disk.\r\n\r\nI\'m trying to iterate items in database with the following query:\r\n\r\n```json\r\n{"size":20000,"fields":[],"query":{"filtered":{"filter":{"bool":{"must":[{"term":{"is_mobile":true}},{"range":{"user_id":{"gt":0,"lte":20000},"_cache":false}}]}}}}}\r\n```\r\n\r\nHere range changes every requests by 20 000 forward. There are around 1 000 mobile users per 20 000 users if that matters.\r\n\r\nWhen I set size to 0 iteration finishes very quickly and each iteration takes 1-10ms,\r\nbut when I set size to 20 000 every step takes up to 200-300ms. The root cause of it\r\nis fetch stage: with size=0 disk io is ~10mbps, with size=20000 disk io is ~220mbps.\r\n\r\nHere are some screenshots from bigdesk during my tests:\r\n\r\n![one](http://puu.sh/84Br6.png)\r\n![two](http://puu.sh/84BrU.png)\r\n![three](http://puu.sh/84Bte.png)\r\n\r\nFirst test with size=0 has spike in query time, second test with size=20000 has spike in fetch time.\r\n\r\nI\'m using elasticsearch 1.1.0 and my first idea was that sorting was to blame like in #5573, but restored node behaved the same. This node has 10g of heap, 2g of field data cache and 3g of filter cache, total ram is 16g.\r\n\r\nIs there a reason to fetch 220 megabytes from disk when the only thing I need from a document is an id? Looks like this instance was much faster a week ago, configuration and search/indexing didn\'t change.\r\n\r\n4.5 minutes * 60 seconds * 220mbps = 60 gigabytes read from disk during second test, but only 19 200 000 docs were fetched. How can I figure out what causes such load?\r\n\r\nLet me know if I could provide more info.'
5770,'spinscale',"ES 1.0.2 debian package fails on removing dirs when purged\nPurging package doesn't work in any way:\r\n\r\nWhen /var/lib/elasticsearch exists and is a directory, it tries to rmdir /var/lib\r\n\r\n    (Reading database ... 159672 files and directories currently installed.)\r\n    Removing elasticsearch (1.0.2) ...\r\n    rmdir: failed to remove directory '/var/lib'\r\n    dpkg: error processing package elasticsearch (--purge):\r\n     subprocess installed post-removal script returned error exit status 1\r\n    Errors were encountered while processing:\r\n     elasticsearch\r\n    E: Sub-process /usr/bin/dpkg returned an error code (1)\r\n    A package failed to install.  Trying to recover:\r\n    Press Return to continue.\r\n\r\nWhen /var/lib/elasticsearch doesn't exist, it tries to remove it and fails:\r\n\r\n    (Reading database ... 159672 files and directories currently installed.)\r\n    Removing elasticsearch (1.0.2) ...\r\n    rmdir: failed to remove '/var/lib/elasticsearch': No such file or directory\r\n    dpkg: error processing package elasticsearch (--purge):\r\n     subprocess installed post-removal script returned error exit status 1\r\n    Errors were encountered while processing:\r\n     elasticsearch\r\n    E: Sub-process /usr/bin/dpkg returned an error code (1)\r\n    A package failed to install.  Trying to recover:\r\n    Press Return to continue.\r\n\r\n\r\nWhen /var/lib/elasticsearch is a mounted partition, it tries to remove dir and fails at it:\r\n\r\n    (Reading database ... 159672 files and directories currently installed.)\r\n    Removing elasticsearch (1.0.2) ...\r\n    rmdir: failed to remove '/var/lib/elasticsearch'\r\n    dpkg: error processing package elasticsearch (--purge):\r\n     subprocess installed post-removal script returned error exit status 1\r\n    Errors were encountered while processing:\r\n     elasticsearch\r\n    E: Sub-process /usr/bin/dpkg returned an error code (1)\r\n    A package failed to install.  Trying to recover:\r\n    Press Return to continue.\r\n\r\n\r\nIt also should probably do same stuff *DB packages do: ask if you want to keep your data when purging package\r\n"
5758,'jpountz',"add ipv6 field support\nWhat I've done is copied the `NumericTokenStream`, `NumericRangeFilter`, `NumericRangeQuery` and `NumericUtils` from lucene and change those to support `BigInteger`\r\n\r\nthe term query and range query/filter are both working, but there are still other things like field data need to do\r\n\r\n@kimchy @jpountz @dadoonet can you have a quick look just to make sure I've done the correct thing so far?\r\n\r\ncloses #3714"
5686,'bleskes','indexation versioning with 0 does not complain\nI\'m aware that the documentation says that internal versioning starts at 1, I was just wondering why ES didn\'t complain when I run the command below several times in a row.\r\n\r\nThe value 0 seems to be handled inconsistently compared to every other value; negative values are reported as illegal values, above 0 the regular version control is applied.\r\n\r\nIf ES would report this, it might help prevent bugs, especially in languages like Java where 0 is the default value for a lot of primitive types when not initialized. If one forgets to set it, the versioning will not kick without the user being aware of it.\r\n\r\nTested in v1.0.0 and v1.1.0\r\n\r\n```\r\ncurl -XPUT \'http://localhost:9200/testindex/testtype/1?version=0\' -d \'{\r\n"title": "test"\r\n}\'\r\n```'
5682,'areek','Completion suggester - duplicates - `index_analyzer` and `preserve_separators` ignored\nI would expect that `index_analyzer: standard` and `preverve_separators: true` would lowercase and normalize the `output` but it turns out that they don\'t. This causes the completion suggester to return duplicated results.\r\n\r\n```curl -XPOST "/test"```\r\n\r\n```\r\ncurl -XPOST "/test/_mapping/test" -d\'\r\n{\r\n    "test":{\r\n        "properties": {\r\n            "name":{\r\n                "type": "string",\r\n                "copy_to":"name_suggest"\r\n            },\r\n            "name_suggest":{\r\n                "type": "completion",\r\n                "index_analyzer": "standard",\r\n                "search_analyzer": "standard",\r\n                "preserve_position_increments":false,\r\n                "payloads":false,\r\n                "preserve_separators":false\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n```\r\ncurl -XPOST "/test/test" -d\'\r\n{\r\n    "name":"bmw"\r\n}\'\r\n```\r\n```\r\ncurl -XPOST "/test/test" -d\'\r\n{\r\n    "name":"BMW"\r\n}\'\r\n```\r\n```\r\ncurl -XPOST "/test/test" -d\'\r\n{\r\n    "name":"bmw "\r\n}\'\r\n```\r\n```\r\ncurl -XPOST "/test/_suggest" -d\'\r\n{\r\n    "text":"b",\r\n    "car":{\r\n        "completion":{\r\n            "field":"name_suggest"\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nResult\r\n\r\n```javascript\r\n{\r\n   "_shards": {\r\n      "total": 5,\r\n      "successful": 5,\r\n      "failed": 0\r\n   },\r\n   "car": [\r\n      {\r\n         "text": "b",\r\n         "offset": 0,\r\n         "length": 1,\r\n         "options": [\r\n            {\r\n               "text": "BMW",\r\n               "score": 1\r\n            },\r\n            {\r\n               "text": "bmw",\r\n               "score": 1\r\n            },\r\n            {\r\n               "text": "bmw ",\r\n               "score": 1\r\n            }\r\n         ]\r\n      }\r\n   ]\r\n}\r\n```'
5576,'GaelTadh','SearchRequestBuilder#toString causes the content of the request to change\nUsing the Java API, If one sets the content of a search request through `SearchRequestBuilder#setSource` methods and then calls `toString` to see the result, not only the content of the request is not returned as it wasn\'t set through `sourceBuilder()`,  the content of the request gets also reset due to the `internalBuilder()` call in `toString`.\r\n\r\nHere is a small failing test that demontrates it:\r\n\r\n```\r\nSearchRequestBuilder searchRequestBuilder = new SearchRequestBuilder(client()).setSource("{\\n" +\r\n                "            \\"query\\" : {\\n" +\r\n                "            \\"match\\" : {\\n" +\r\n                "                \\"field\\" : {\\n" +\r\n                "                    \\"query\\" : \\"value\\"" +\r\n                "                }\\n" +\r\n                "            }\\n" +\r\n                "        }\\n" +\r\n                "        }");\r\nString preToString = searchRequestBuilder.request().source().toUtf8();\r\nsearchRequestBuilder.toString();\r\nString postToString = searchRequestBuilder.request().source().toUtf8();\r\nassertThat(preToString, equalTo(postToString));\r\n```'
5555,'GaelTadh','CountRequestBuilder toString()\nSearchRequestBuilder has a friendly toString method, but not same for CountRequestBuilder.\r\nWill be a good to have feature for debugging.'
5512,'brwe','Add K-means clustering feature\nAdd k-means clustering to allow detection of clusters in data sets.\r\nhttp://en.wikipedia.org/wiki/K-means_clustering\r\n\r\nWould be useful for geo points but also other use cases too.\r\n\r\nThanks to https://github.com/koobs for suggesting this one in Sydney Elastic Training.\r\n'
5464,'dadoonet','1.0.1 Java API compilation error on source download\nAfter downloading sources i started to get compilation errors like\r\n````\r\njava: for-each not applicable to expression type\r\n  required: array or java.lang.Iterable\r\n  found:    org.elasticsearch.common.collect.UnmodifiableIterator<org.elasticsearch.cluster.metadata.IndexMetaData>\r\n````\r\nWith 0.90x versions this was not a problem. What is temporary solution and what is proper solution?'
5442,'aleph-zero','REST API _suggest endpoint mistakenly creates documents with id = _suggest\nUsing the REST API, it is possible to create documents with id  = _suggest if one tries to add a type parameter to the request. While technically correct, this behavior is confusing and should be altered to return a 400 BAD_REQUEST.'
5436,'areek','Support for the Lucene FreeText suggester\nInitial version for review of the addition of the free text suggester to the elasticsearch suggest mechanism'
5399,'martijnvg','Term query for _parent doesn\'t work for grandchild\nWith a Parent > Child > Grandchild relationship, you can use a `term` query on the `_parent` field to find children with a particular `$parent`, but not grandchildren with parent `$Child`:\r\n\r\n    DELETE /myindx\r\n\r\n    PUT /myindex\r\n    {\r\n      "mappings": {\r\n        "Parent": {\r\n          "properties": {\r\n            "name": {\r\n              "type": "string",\r\n              "index": "analyzed"\r\n            }\r\n          }\r\n        },\r\n        "Child": {\r\n          "_parent": {\r\n            "type": "Parent"\r\n          },\r\n          "properties": {\r\n            "name": {\r\n              "type": "string",\r\n              "index": "analyzed"\r\n            }\r\n          }\r\n        },\r\n        "GrandChild": {\r\n          "_parent": {\r\n            "type": "Child"\r\n          },\r\n          "properties": {\r\n            "name": {\r\n              "type": "string",\r\n              "index": "analyzed"\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n\r\n    POST /_bulk \r\n    { "index" : { "_index" : "myindex", "_type" : "Parent", "_id" : "alice" } }\r\n    { "name" : "Alice" }\r\n    { "index" : { "_index" : "myindex", "_type" : "Child", "_id" : "bob", "parent" : "alice" } }\r\n    { "name" : "Bob"}\r\n    { "index" : { "_index" : "myindex", "_type" : "GrandChild", "_id" : "gc1", "parent" : "bob", "routing" : "alice" } }\r\n    { "name" : "grand child 1" }\r\n    { "index" : { "_index" : "myindex", "_type" : "GrandChild", "_id" : "gc2", "parent" : "bob", "routing" : "alice" } }\r\n    { "name" : "grand child 2"}\r\n     \r\n    GET /_search?fields=_parent,_routing\r\n\r\nThis query works:\r\n\r\n    POST /myindex/Child/_search\r\n    {\r\n      "query": {\r\n        "term": {\r\n          "_parent": "alice"\r\n        }\r\n      }\r\n    }\r\n\r\nThis query doesn\'t:\r\n\r\n    POST /myindex/GrandChild/_search\r\n    {\r\n      "query": {\r\n        "term": {\r\n          "_parent": "bob"\r\n        }\r\n      }\r\n    }\r\n\r\nBut performing the same lookup with `has_parent` does work:\r\n\r\n    POST /myindex/GrandChild/_search\r\n    {\r\n      "query": {\r\n        "has_parent": {\r\n          "parent_type": "Child",\r\n          "query": {\r\n            "filtered": {\r\n              "filter": {\r\n                "term": {\r\n                  "_id": "bob"\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }'
5378,'rmuir',"Add a char_filter cutting the field into sentences\nAdds a char_filter that cuts the field down to the configured number of\r\nsentences.  Defaults to just the first sentence.\r\n\r\nThere is some impedence between the incoming Reader and the CharacterIterator\r\nthat has to be passed to the BreakIterator.  We copy a configurable number\r\nof characters into a character array and shove those into the BreakIterator\r\nvia Lucene's CharArrayIterator rather than try anything fancy.  Hopefully\r\nthis is fine from a performance perspective.\r\n\r\nCloses #5377"
5329,'electrical','CONF_FILE in /etc/sysconfig not used by startup script on RedHat\nI\'m facing a issue with node default parameters in the system configuration file (/etc/sysconfig/elasticsearch) on a RedHat Linux installation : the \r\ndefinition of the node configuration file is ignored by the standard startup script.\r\n\r\nEnvironment:\r\n- Redhat Linux RHEL 6.2\r\n- Elasticsearch 1.0.1 (elasticsearch-1.0.1-1.noarch.rpm)\r\n\r\n### Problems:\r\n\r\n1) custom config file defined in /etc/sysconfig is ignored\r\n\r\nThe file /etc/sysconfig/elasticsearch contains default parameters used by the startup script /etc/init.d/elasticsearch to intialize the node. The parameter CONF_FILE, as its name suggests, contains the name of the configuration file. By default, it is defined like this:\r\n    \r\n```\r\n    # Elasticsearch configuration file (elasticsearch.yml)\r\n    CONF_FILE=/etc/elasticsearch/elasticsearch.yml \r\n```\r\n\r\nIf this value is changed to another location, the node will start but the new configuration will be ignored.\r\n   \r\n2) relative config file in /etc/sysconfig prevents node startup\r\n  \r\nIn file /etc/sysconfig/elasticsearch, another parameter named CONF_DIR defines the Elasticsearch configuration directory, where other config files like logging.yml will be found:\r\n    \r\n```\r\n    # Elasticsearch conf directory\r\n    CONF_DIR=/etc/elasticsearch\r\n```\r\n\r\nAccording to the CONF_FILE comment, it should be possible to give a relative file name (elasticsearch.yml), but it does not work, the node will not start using standard service command.\r\n\r\n### How to reproduce:\r\n\r\n0) prerequisites\r\n\r\n- install elasticsearch-1.0.1-1.noarch.rpm on a RedHat/Centos system\r\n- create a new configuration file called foobar.yml in /etc/elasticsearch with specific parameters, for example the cluster name:\r\n    \r\n    ```\r\n    cluster.name: foobar\r\n    ```\r\n          \r\n1) custom config file\r\n      \r\n- edit file /etc/sysconfig/elasticsearch and change the CONF_FILE value to the new configuration file:\r\n    \r\n    ``` \r\n    CONF_FILE=/etc/elasticsearch/foobar.yml\r\n    ```\r\n          \r\n- start the server using the service command:\r\n    \r\n    ```\r\n    service elasticsearch start\r\n    ```\r\n        \r\n- check the cluster name using the REST API:\r\n    \r\n    ``` \r\n    curl http://localhost:9200/_nodes?pretty=true\r\n    ```\r\n         \r\n- the cluster name is still the default "elasticsearch", new configuration file was not used.    \r\n\r\n2) relative standard config file\r\n\r\n- edit file /etc/sysconfig/elasticsearch and change the CONF_FILE value to a relative name of an existing file in the CONF_DIR directory:\r\n    \r\n    ```\r\n    CONF_FILE=elasticsearch.yml\r\n    ```\r\n        \r\n- start the server using the service command:\r\n    \r\n    ```\r\n    service elasticsearch start\r\n    ```\r\n        \r\n- the startup fails with exit code 6 (configuration file not found), however the file $CONF_DIR/$CONF_FILE exits !      \r\n\r\n### Workaround:\r\n\r\nIn file /etc/sysconfig/elasticsearch, pass the config file in the Java options : \r\n  \r\n```\r\n    ES_JAVA_OPTS="-Des.config=/etc/elasticsearch/foobar.yml"\r\n```\r\n    \r\nIf the config argument is not absolute (foobar.yml), the file will be searched in the $CONF_DIR directory during Elasticsearch startup (Java code). But it\'s not very clean, because :\r\n  \r\n- you can\'t use CONF_FILE in JAVA_OPTS, because it\'s defined later in the sysconfig file\r\n- you still need an absolute CONF_FILE, because it\'s existence is directly checked in the init.d script    \r\n- you\'ll have to remember that CONF_FILE is not really used by the node...\r\n\r\n### How to solve:\r\n\r\n1) in /etc/init.d/elasticsearch, line 88, in the daemon launch command line, define the es.config property using the $CONF_FILE variable :\r\n  \r\n```\r\n    daemon --user $ES_USER --pidfile $pidfile $exec -p $pidfile -d \\\r\n        -Des.default.path.home=$ES_HOME \\\r\n        -Des.default.path.logs=$LOG_DIR -Des.default.path.data=$DATA_DIR \\\r\n        -Des.default.path.work=$WORK_DIR -Des.default.path.conf=$CONF_DIR \\\r\n        -Des.config=$CONF_FILE\r\n```\r\n    \r\n(maybe es.default.config would be better than es.config ?)\r\n    \r\nThis way, the configuration specified in the sysconfig file is really used by the node (solves problem n°1).  \r\n    \r\n2) in /etc/init.d/elasticsearch, line 68, replace the current configuration file existence test :\r\n  \r\n```\r\n    [ -f $CONF_FILE ] || exit 6        # line 68\r\n```\r\n\r\nby this one:\r\n\r\n```\r\n    [ -f $CONF_FILE ] || [ -f $CONF_DIR/$CONF_FILE ] || exit 6\r\n```\r\n\r\nThis test reproduce better the one made in the Java code (first check for an absolute configuration file, then for a relative one in the configuration directory, then in the classpath, else fail). Now, the configuration file can be absolute or relative, it works without surprise (solves problem n°2).\r\n\r\nWhat do you think ?\r\n'
5324,'jpountz','Add support for "missing" to all bucket aggregations\nNEED: In many (if not majority cases) when present users with business analytics, the user would want to see numbers for complete data set. No matter how you aggregate it should present the same data with the same number of documents. Inability to handle "missing" values exclude those from analysis making analyzed data set incomplete and grand totals dependent on which field(s) the aggregation is done. It is impossible to explain to the users why the lower level totals do not add up to the upper level ones!  \r\n\r\nWORKAROUND: Currently field based bucket aggregations (term, range etc) have no way to aggregate missing values. The only way is to use missing aggregation on the same level and the same field as the term aggregation itself. It is easy enough when dealing with one level aggregations but if you have 2-3 level aggregation number of "missing" aggregations (and complete lower level aggregation to be repeated in them) mushrooms very quickly to the point that the query is huge, convoluted and not debuggable. It may affect performance  as well. Also fetched date needs to be heavily post-processed to extract multiple levels aggregation buckets from under various "missing" elements and put them inline with the regular aggregation values. Below please see a simple query to do 2 level aggregation with just one sum metrics\r\n\r\nPROPOSAL: I would suggest that any aggregation operating on a field should have a missing option. If missing config is specified, aggregation should accumulate missing values under that value and honor any nested aggregations within. It should never assume any value like 0 or _missing since it may clash with actual keys. If it is not specified the aggregation should skip missing values as it does now.\r\n\r\nThis approach makes it entirely compatible with existing logic and give developers complete control over whether to aggregate missing and under what key. In cases when it is not needed (and not specified) there will be no performance overhead. But when it needed it will work faster as we would not need to do missing aggregation and aggregations under it separately (same goes for "other" aggregation)\r\n\r\nTo be honest, I would love to see the same handling for "other" - documents that have not been included in aggregation due to the aggregation size constraints. Again the same rationale - ability to slice complete data set regardless of aggregation structure. It is just as needed as "missing" and just as troublesome to calculate but \r\nI could understand if you did not add it as it may be not compatible with your algorithms but  PLEASE PLEASE add "missing" handling at least\r\n\r\n```\r\n{\r\n      "total": {\r\n        "sum": {\r\n          "field": "money.totals.obligationTotal"\r\n        }\r\n      },\r\n      "missing": {\r\n        "missing": {\r\n          "field": "division"\r\n        },\r\n        "aggs": {\r\n          "total": {\r\n            "sum": {\r\n              "field": "money.totals.obligationTotal"\r\n            }\r\n          },\r\n          "missing": {\r\n            "missing": {\r\n              "field": "fy"\r\n            }\r\n          },\r\n          "group": {\r\n            "terms": {\r\n              "field": "fy",\r\n              "order": { "_term": "asc" }\r\n            },\r\n            "aggs": {\r\n              "total": {\r\n                "sum": {\r\n                  "field": "money.totals.obligationTotal"\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      },\r\n      "group": {\r\n        "terms": {\r\n          "field": "division",\r\n          "order": { "_term": "asc" },\r\n          size:100\r\n        },\r\n        "aggs": {\r\n          "total": {\r\n            "sum": {\r\n              "field": "money.totals.obligationTotal"\r\n            }\r\n          },\r\n          "missing": {\r\n            "missing": {\r\n              "field": "fy"\r\n            },\r\n            "aggs": {\r\n              "total": {\r\n                "sum": {\r\n                  "field": "money.totals.obligationTotal"\r\n                }\r\n              }\r\n            }\r\n          },\r\n          "group": {\r\n            "terms": {\r\n              "field": "fy",\r\n              "order": { "_term": "asc" }\r\n            },\r\n            "aggs": {\r\n              "total": {\r\n                "sum": {\r\n                  "field": "money.totals.obligationTotal"\r\n                }\r\n              }\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n```\r\n\r\ncc @uboness, @jpountz   '
5317,'spinscale','Packaging: Log stdout output into file\nUntil stdout was ignored during start up of our packages. This could\r\nresult in problems when specyfying something like an invalid heap size\r\n(2gb instead of 2g) as this was not logged anywhere.\r\n\r\nFor init.d style startup, errors are now logged into\r\n/var/log/elasticsearch/elasticsearch-stdout.log\r\n\r\nFor systemd startup, one can now use journalctl to see these errors.\r\n\r\nThis PR is based on #4429 (which only supported the debian package)'
5306,'martijnvg','Aggregations that can count only parent documents with matching children documents\nFeature request: \r\nAggregations that can count only parent documents with matching children documents. \r\n\r\nI\'ve been working on a BI system with ES 0.90 and we needed count "users" which have certain attributes, for instance let\'s say gender and star sign. A user is a parent-level document and the attributes are child documents. \r\n\r\nFrom the sample above, we were doing so by creating a query for each combination of male / female and the star signs and querying individually, as one can imagine, this was slow, but the results are exactly what we want. We could run this in roughly 2 minutes.\r\n\r\nWe considered using the msearch query to get these results in a single query and we ended up with something similar to this: https://gist.github.com/chaos-generator/9133118\r\nThe sample above runs in 40 seconds give or take.\r\n\r\nAnd along came elastic search 1.0.0 and now we have aggregations, so we simplified our query to this: https://gist.github.com/chaos-generator/9133139\r\nThis runs lightning fast and we get the results in 200ms on average, which is ideal for us, BUT we get the total number of documents with the attributes, rather than the count on the parent documents.\r\n\r\nOur problem, as you can see in the msearch gist, is that we have a parent level document and child documents, which would only be updated if another document with the exact same attributes came in, this means that a parent level user document can have three child documents that will have gender and star sign, but I only want to count the parent document, rather than each individual child document.\r\n\r\nAs we don\'t know in advance the attributes our users will be searching, we cannot use a script in index time to help us do this aggregation. We tried to use a script in search time like this:  https://gist.github.com/chaos-generator/9133321 , but it didn\'t work as we wanted too:\r\n\r\nYou can use this gist to simulate the issue we have: https://gist.github.com/chaos-generator/9143655'
5269,'martijnvg','Remove id cache from stats apis\nSince #4930 is in the id cache can now also be removed from the stats apis. The id cache inES it self has already been removed.'
5261,'costin',"Plugin isolation\nFor 1.x, we talked about having class space isolated between plugins so their dependencies don't interact with each other.\r\nTo solve this issue, a parent-last `ClassLoader` will be used meaning that each plugin can be loaded into its own `ClassLoader` which will first try to load classes from itself (meaning the plugin class space) and only then fall back to its parent, or the Elasticsearch `ClassLoader`.\r\n\r\nUnder the current enhancement can have an additional `property` in its `es-plugin.properties` called `isolation` which indicates whether the plugin requires its own class space or not. As of now, all existing plugins lack the property which means the default ES settings will be applied.\r\nES provides a new setting, `plugin.isolation` which indicates the behaviour to use for plugins that do not specify any isolation. By default this would be true but can be changed to false to revert to the default / 1.0 behaviour.\r\n\r\nBy enabling isolation out of the box, we theoretically prevent any clashing problems regarding the classpath and at the same time, keep a close eye on problematic plugins. If they appear, folks can turn off `plugin.isolation` (by setting it to false) or simply cherry pick the problematic plugin and configure no isolation for it.\r\n"
5255,'brwe','geo_distance - can\'t filter by inner (object) type ` UncheckedExecutionException - java.lang.NumberFormatException`\nAccording to [docs](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl-geo-distance-filter.html) I should be able to filter by inner (not nested) object attribute (in my case place.point)\r\n\r\nMy mapping looks like:\r\n```javascript\r\n{\r\n  "proposalsets": {\r\n    "_parent": {\r\n      "type": "service"\r\n    },\r\n    "_ttl": {\r\n      "enabled": true\r\n    },\r\n    "properties": {\r\n      "place": {\r\n        "type": "object",\r\n        "properties": {\r\n          "name": {\r\n            "type": "string",\r\n            "index": "analyzed",\r\n            "analyzer": "polish",\r\n            "fields": {\r\n              "raw": {\r\n                "type": "string",\r\n                "index": "not_analyzed"\r\n              }\r\n            }\r\n          },\r\n          "point": {\r\n            "type": "geo_point",\r\n            "fielddata": {\r\n              "format": "compressed",\r\n              "precision": "3m"\r\n            }\r\n          },\r\n          "city": {\r\n            "type": "string",\r\n            "index": "not_analyzed"\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nThe query I am trygin to execute is :\r\n```javascript\r\n{\r\n   "query": {\r\n      "filtered": {\r\n         "filter": {\r\n            "geo_distance": {\r\n               "distance": "100km",\r\n               "place.point": {\r\n                  "lat": 40.73,\r\n                  "lon": -74.1\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n```\r\n\r\n```javascript\r\n{\r\n   "error": "SearchPhaseExecutionException[Failed to execute phase [query], all shards failed; shardFailures {[OnU2dSO5ROKwYQWpQzpmEA][pl][5]: QueryPhaseExecutionException[[pl][5]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"@\\u0018B\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"@\\u0018B\\"]; nested: NumberFormatException[For input string: \\"@\\u0018B\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][4]: QueryPhaseExecutionException[[pl][4]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: empty String]; nested: UncheckedExecutionException[java.lang.NumberFormatException: empty String]; nested: NumberFormatException[empty String]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][7]: QueryPhaseExecutionException[[pl][7]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"(`\\u0012:v\\n~\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"(`\\u0012:v\\n~\\"]; nested: NumberFormatException[For input string: \\"(`\\u0012:v\\n~\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][6]: QueryPhaseExecutionException[[pl][6]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"@%\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"@%\\"]; nested: NumberFormatException[For input string: \\"@%\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][1]: QueryPhaseExecutionException[[pl][1]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"$\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"$\\"]; nested: NumberFormatException[For input string: \\"$\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][0]: QueryPhaseExecutionException[[pl][0]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"@\\u0019&\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"@\\u0019&\\"]; nested: NumberFormatException[For input string: \\"@\\u0019&\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][3]: QueryPhaseExecutionException[[pl][3]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"$\\f\\u0001C\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"$\\f\\u0001C\\"]; nested: NumberFormatException[For input string: \\"$\\f\\u0001C\\"]; }{[OnU2dSO5ROKwYQWpQzpmEA][pl][2]: QueryPhaseExecutionException[[pl][2]: query[filtered(ConstantScore(GeoDistanceFilter(place.point, SLOPPY_ARC, 100000.0, 40.73, -74.1)))->cache(_type:proposalsets)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: ElasticsearchException[java.lang.NumberFormatException: For input string: \\"$\\"]; nested: UncheckedExecutionException[java.lang.NumberFormatException: For input string: \\"$\\"]; nested: NumberFormatException[For input string: \\"$\\"]; }]",\r\n   "status": 500\r\n}\r\n```\r\n\r\nThis happens even when the index is empty (but has mapping). \r\nWhen I move the point from the place to the root of the document, there seems to be no problem.'
5232,'kimchy','Slow cluster startup with zen discovery.\nWhen a cluster with a large number of nodes starts up, the joining of the nodes becomes slow, as the cluster state update is blocking. The master node adds the nodes one by one and waits after every join (zen-disco-receive) for the updated cluster state to be distributed.\r\n\r\nThis issue occurs in elasticsearch version 1.0 and is related to #3736 introducing the wait during the processing of the cluster state updates.\r\n\r\nWhen setting discovery.zen.publish_timeout:0 the startup of the cluster is as fast as before, as the master node is not waiting for the individual updates to be acked by the client nodes.\r\n\r\nA solution to the problem might be that the updates of the cluster state would be processed on the master and only distributed after all have been applied. Or that the master would not wait for the state to be acked by the client nodes during startup.\r\n'
5209,'bleskes',"GetFieldMapping API is not available in ES 1.0.0 \nWe just installed ES 1.0.0 but the GetFieldMapping API always returned empty response.\r\n\r\nhttp://localhost:9200/index/doc/_mapping/field/*\r\nalways responded {} \r\n\r\nThis problem is causing Kibana 3 milestone 5 to display the following error:\r\n\r\nNo index found at https://server/logstash-2014.02.20/_mapping/field/*\r\nPlease create at least one index.If you're using a proxy ensure it is configured correctly."
5191,'clintongormley','Add support for matching mode when attaching REST handlers\nAt the moment, ElasticSearch does a matching mode `equals` when attaching REST handlers using the method `RestController#registerHandler`:\r\n\r\n    \t@Inject\r\n\tpublic MyRestHandler(Settings settings, Client client, RestController controller) {\r\n\t\tsuper(settings, client);\r\n\r\n\t\t// Define REST endpoints\r\n\t\tcontroller.registerHandler(Method.GET, "(...)",this);\r\n\r\nIn some cases, it\'s not suitable since we would want to attach an handle on a subset of URIs. For such cases, the matching mode `starts with` would be more suitable:\r\n\r\n    controller.registerHandler(Method.GET, "/myuri",\r\n                       RestMatchingMode.STARTS_WITH, this);\r\n\r\nThis means that all sub URIs (like `/myuri/segment1`, `/myuri/segment2/segment3/segment4`, and so on) would handle by the same class. This would be useful if we want manage URIs in a custom way (URI format, ...) that isn\'t supported by ElasticSearch. This is typically the case for OData URIs.\r\n\r\nTo implement such feature, we need to add an enumeration to list all support matching modes:\r\n\r\n    public enum RestMatchingMode {\r\n        EQUALS, STARTS_WITH\r\n    }\r\n\r\nWe also need to update the `RestController` to add the corresponding methods:\r\n\r\n    public void registerHandler(RestRequest.Method method,\r\n                                          String path, RestHandler handler) {\r\n        registerHandler(method, path, RestMatchingMode.EQUALS, handler);\r\n    }\r\n\r\n    public void registerHandler(RestRequest.Method method, String path,\r\n                                         RestMatchingMode matchingMode, RestHandler handler) {\r\n        (...)\r\n    }\r\n\r\nAs the matching is handled by the classes `PathTrie` and `TreeNode`, we need to add in it the support of matching mode at both methods insert and retrieve.\r\n\r\n    public synchronized void insert(String[] path, int index,\r\n                         RestMatchingMode matchingMode, T value) {\r\n       (...)\r\n       TrieNode<T> node = children.get(key);\r\n       if (node == null) {\r\n            if (index == (path.length - 1)) {\r\n                node = new TrieNode<T>(token, value, this, wildcard, matchingMode);\r\n            } else {\r\n                node = new TrieNode<T>(token, null, this, wildcard, RestMatchingMode.EQUALS);\r\n            }\r\n            children = newMapBuilder(children).put(key, node).immutableMap();\r\n        (...)\r\n    }\r\n\r\n    public T retrieve(String[] path, int index, Map<String, String> params) {\r\n        (...)\r\n        if (index == (path.length - 1)) {\r\n            return node.value;\r\n        }\r\n\r\n        // In the case of starts with matching, return the value\r\n        if (RestMatchingMode.STARTS_WITH.equals(node.getMatchingMode())) {\r\n            return node.value;\r\n        }\r\n        (...)\r\n\r\nMoreover we simply need to add the property matchingMode in the class TreeNode\r\n\r\n    public class TrieNode<T> {\r\n        (...)\r\n        private RestMatchingMode matchingMode; \r\n        (...)\r\n\r\n        public TrieNode(String key, T value, TrieNode parent, String wildcard,\r\n                                   RestMatchingMode matchingMode) {\r\n            this.key = key;\r\n            this.wildcard = wildcard;\r\n            this.isWildcard = (key.equals(wildcard));\r\n            this.parent = parent;\r\n            this.value = value;\r\n            this.children = ImmutableMap.of();\r\n            this.matchingMode = matchingMode;\r\n            (...)\r\n        }\r\n        (...)\r\n    }\r\n\r\n\r\n'
5119,'bleskes',"Delay startup of marvel plugin\nWhen starting up an ES node with the marvel plugin installed, before the node can connect to or start a cluster it complains every second that it cannot connect to upload stats which generates a lot of noise in the logs;\r\n```\r\n[2014-02-14 10:32:10,991][ERROR][marvel.agent.exporter    ] error connecting to [localhost:9200]\r\njava.net.ConnectException: Connection refused\r\n        at java.net.PlainSocketImpl.socketConnect(Native Method)\r\n        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)\r\n        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)\r\n        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)\r\n        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)\r\n        at java.net.Socket.connect(Socket.java:579)\r\n        at sun.net.NetworkClient.doConnect(NetworkClient.java:175)\r\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)\r\n        at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)\r\n        at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)\r\n        at sun.net.www.http.HttpClient.New(HttpClient.java:308)\r\n        at sun.net.www.http.HttpClient.New(HttpClient.java:326)\r\n        at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)\r\n        at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)\r\n        at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)\r\n        at org.elasticsearch.marvel.agent.exporter.ESExporter.openConnection(ESExporter.java:317)\r\n        at org.elasticsearch.marvel.agent.exporter.ESExporter.openExportingConnection(ESExporter.java:198)\r\n        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportXContent(ESExporter.java:246)\r\n        at org.elasticsearch.marvel.agent.exporter.ESExporter.exportNodeStats(ESExporter.java:135)\r\n        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.exportNodeStats(AgentService.java:274)\r\n        at org.elasticsearch.marvel.agent.AgentService$ExportingWorker.run(AgentService.java:174)\r\n        at java.lang.Thread.run(Thread.java:744)\r\n[2014-02-14 10:32:10,991][ERROR][marvel.agent.exporter    ] could not connect to any configured elasticsearch instances: [localhost:9200]\r\n```\r\n\r\nIt'd be nicer if marvel could wait until cluster state is established and then start stats collection."
5064,'tlrx',"Updating plugins\nTo update a plugin, we need to remove it first otherwise, elasticsearch will complain that it's already installed:\r\n```\r\nsudo bin/plugin -install mobz/elasticsearch-head\r\n\r\n-> Installing mobz/elasticsearch-head...\r\nFailed to install mobz/elasticsearch-head, reason: plugin directory /usr/share/elasticsearch/plugins/head already exists. To update the plugin, uninstall it first using -remove mobz/elasticsearch-head command\r\n```\r\n\r\nI propose that we provide an `update` switch:\r\n```\r\nsudo bin/plugin -update mobz/elasticsearch-head\r\n```\r\n\r\nThis should be some sugar that runs the following:\r\n```\r\nsudo bin/plugin -remove mobz/elasticsearch-head\r\nsudo bin/plugin -install mobz/elasticsearch-head\r\n```\r\n\r\n"
4978,'spinscale',"sysctl: permission denied on key 'vm.max_map_count'  - OpenVZ Elasticsearch 0.90.9 compatibility issue\nHello,\r\n\r\nElasticsearch 0.90.9 and higher do not work with openvz (tested with debian wheezy i386) with the following error showing on startup:\r\n\r\nStarting ElasticSearch Server:sysctl: permission denied on key 'vm.max_map_count'  "
4976,'dadoonet','cleanup for the internal plugin infrastructure\n- Introduced the `JvmPlugin` wrapper (to add additional internal functionality to the plugin)\r\n- fixed `hashcode()` on the `PluginInfo`\r\n- extracted the `PluginsInfo` caching logic to `CachableReference` construct in utils\r\n- cleaned up the `PluginsService` class to make the logic there more structured/understandable'
4914,'martijnvg','has_parent doesn\'t set the default type to the parent\nTrying to reference a field in the parent document using a script like `doc["field"]` fails because the field doesn\'t exist in the *child* mapping.  Prepending the parent type ( `doc["parentType.field"]` finds the field correctly.\r\n\r\nThe default type within a `has_parent` or `has_child` query or filter should be the parent/child type respectively.\r\n\r\n    curl -XPUT "http://localhost:9200/t" -d\'\r\n    {\r\n      "mappings": {\r\n        "myParent": {\r\n          "properties": {\r\n            "weight": {\r\n              "type": "double"\r\n            }\r\n          }\r\n        },\r\n        "myChild": {\r\n          "_parent": {\r\n            "type": "myParent"\r\n          },\r\n          "_routing": {\r\n            "required": true\r\n          }\r\n        }\r\n      }\r\n    }\'\r\n\r\n\r\n    curl -XPUT "http://localhost:9200/t/myParent/1" -d\'\r\n    {\r\n      "weight": 2\r\n    }\'\r\n\r\n    curl -XPUT "http://localhost:9200/t/myChild/3?parent=1" -d\'\r\n    {}\'\r\n\r\n    curl -XGET "http://localhost:9200/t/myChild/_search" -d\'\r\n    {\r\n      "query": {\r\n        "has_parent": {\r\n          "query": {\r\n            "function_score": {\r\n              "script_score": {\r\n                "script": "_score * doc[\\"myParent.weight\\"].value"\r\n              }\r\n            }\r\n          },\r\n          "parent_type": "myParent",\r\n          "score_type": "score"\r\n        }\r\n      }\r\n    }\'\r\n\r\nThis query:\r\n\r\n    curl -XGET "http://localhost:9200/t/myChild/_search" -d\'\r\n    {\r\n      "query": {\r\n        "has_parent": {\r\n          "query": {\r\n            "function_score": {\r\n              "script_score": {\r\n                "script": "_score * doc[\\"weight\\"].value"\r\n              }\r\n            }\r\n          },\r\n          "parent_type": "myParent",\r\n          "score_type": "score"\r\n        }\r\n      }\r\n    }\'\r\n\r\nFails with:\r\n\r\n    "QueryPhaseExecutionException[[t][2]: query[filtered(ParentQuery[myParent](filtered(function score (ConstantScore(*:*),function=script[_score * doc[\'weight\'].value], params [null]))->cache(_type:myParent)))->cache(_type:myChild)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: RuntimeException[[Error: No field found for [weight] in mapping with types [myChild]]\r\n    [Near : {... _score * doc[\'weight\'].value ....}]\r\n                 ^\r\n    [Line: 1, Column: 1]]; nested: CompileException[[Error: No field found for [weight] in mapping with types [myChild]]\r\n    [Near : {... _score * doc[\'weight\'].value ....}]\r\n                 ^\r\n    [Line: 1, Column: 1]]; nested: ElasticsearchIllegalArgumentException[No field found for [weight] in mapping with types [myChild]];\r\n\r\n'
4847,'jpountz','Histogram aggregations: support decimal intervals\nFork of #3810 and #3799. Decimal intervals would be useful for certain data types such as currencies, temperatures, lengths ...'
4759,'areek',"Improve result ordering logic for completion suggester to account for quality of match\nWe're using shingles for the inputs and we have many inputs per suggestion, plus synonyms (for [postal address abbreviations](https://www.usps.com/send/official-abbreviations.htm)) and we'd like to use fuzzy searching as well.\r\n\r\nWhat we're finding is that short inputs will match many suggestions which are then ordered only by `weight`, regardless of the 'quality' of the input match. So the top `size` results _often don't include items that are much closer to the actual input_, and sometimes don't include the item that's an exact match! This understandably causes confusion, especially when users know that an exact match exists.\r\n\r\nWe're working around it by requesting suggestions with a much larger `size` value, then reordering the results with extra code before returning the 'top N' to the client.\r\n\r\nI'm still working out the best logic to use for the reordering but we're limited in what we can achieve on the client-side. We'll probably put prefix matches first, then substring matches, then everything else.\r\n\r\nObviously we'd rather ES did this for us because it would do a better job (it has more information, such as synonyms and fuzzy edit distance), would apply to all matching results not just the top `size`, and would be faster.\r\n\r\nPossibly related to #3791."
4758,'imotov',"Get repository should support wildcards\nThese work:\r\n\r\n    GET /_snapshot/\r\n    GET /_snapshot/_all\r\n    GET /_snapshot/name\r\n    GET /_snapshot/name,name\r\n\r\nBut wildcards don't:\r\n\r\n    GET /_snapshot/*\r\n    GET /_snapshot/prefi*\r\n\r\n\r\n\r\n\r\n\r\n\r\n"
4678,'jpountz',"Create dedicated fielddata implementation for boolean fields\nJust like we have for bytes & numerics, we should have a dedicated field data implementation for boolean values. This will be much more optimized memory-wise and overall performance-wise than what we have today (today, we treat booleans as bytes - `T` & `F`)\r\n\r\nThis will also solve the formatting issues in the different APIs (eg. facets & aggregations) where instead of returning `T` & `F` we'll be able to return `true` & `false`.\r\n\r\nrelated to: #2462 "
4672,'dakrone',"Cache estimated sizes of per-segment per-field field data to short-circuit circuit breaker\nIt could be helpful to have something like a `Map<SegmentReader.CoreCacheKey|FieldName, Long>` cache so that multiple requests all attempting to load field data for the same field can first check to see whether the there is enough space for the data, and short-circuit if not.\r\n\r\nThis should help for multiple requests so we don't have to continually estimate if we know it's going to break regardless."
4642,'costin',"Add percentage for other stats (affects _stats and _cat)\nPercentages are quite useful for doing quick diagnosis - there's such an indicator for ram/heap but we could/should extend them to other runtime stats as well such as disk used/free for example."
4586,'markharwood',"A new generic timeout handler and changes to existing search classes to use it\nA more effective approach to time-limiting activities such as search requests. Special runtime exceptions can now short-cut the execution of long-running calls to Lucene classes and are caught and reported back, not as a fatal error but using the existing “timedOut” flag in results.\r\n\r\nPhases like the FetchPhase can now exit early and so also have a timed-out status. The SearchPhaseController does its best to assemble whatever hits, aggregations and facets have been produced within the provided time limits rather than returning nothing and throwing an error.\r\n\r\nActivityTimeMonitor is the new central class for efficiently monitoring all forms of thread overrun in a JVM.\r\nThe SearchContext setup is modified to register the start and end of query tasks with ActivityTimeMonitor.\r\nStore.java is modified to add timeout checks (via calls to ATM) in the low-level file access routines by using a delegating wrapper for Lucene's IndexInput and IndexInputSlicer.\r\nContextIndexSearcher is modified to catch and unwrap ActivityTimedOutExceptions that can now come out of the Lucene calls and report them as timeouts along with any partial results.\r\nFetchPhase is similarly modified to deal with the possibility of timeout errors."
4576,'jpountz','CompressorFactory.uncompressIfNeeded is fragile\nCompressorFactory.uncompressIfNeeded checks for a particular header to know whether data is compressed or not. This results in an unspecified behavior in the rare case that data is uncompressed and the first bytes match the header for compressed data.'
4562,'jpountz',"Don't index geo points by default, use doc values instead\nToday, geo points are indexed as a single token `<latitude>,<longitude>`. This index is later uninverted into field data which is in-turn used for geo-distance computations. In practice, we never do term-based search on geo points so we should rather use doc values instead of the inverted index: we wouldn't waste time inverting and later uninverting the data, and this would be more space efficient.\r\n\r\nIn order to keep the experience consistent with previous versions, by default field data is going to be loaded into memory from doc values. In order to leave data on disk, the `doc_values` field data format will need to be configured explicitely."
4538,'martijnvg','Parent is not getting set, using UpdateRequest in JavaAPI\nI found a similar bug https://github.com/elasticsearch/elasticsearch/issues/3444, but it is not fixed in JavaAPI with requesting using UpdateReuest.\r\nTo resolve this, "UpdateRequest.java" should have property "parent" & it should be used while generating upsertRequest. (I am using version 0.90.5)\r\nTest with the following code : \r\n\r\ncurl -XPOST \'http://localhost:9200/123\'\r\ncurl -XPOST \'http://localhost:9200/123/TableWithParent/_mapping\' -d \'\r\n{"TableWithParent" : {"_parent" : { "type" : "Table1" }}}\'\r\n\r\nInserting data using JavaAPI:\r\n--\r\npublic static void main(String[] args) {\r\n\t\tClient client = NodeBuilder.nodeBuilder().node().client();\r\n\t\tMap<String, Object> data = new HashMap<String, Object>();\r\n\t\tdata.put("column1", "colVal1");\r\n\t\tUpdateRequest updateRequest = new UpdateRequest();\r\n\t\tupdateRequest.index("123");\r\n\t\tupdateRequest.docAsUpsert(true);\r\n\t\tupdateRequest.routing("123");\r\n\t\tupdateRequest.parent("parentID");\r\n\t\tupdateRequest.id("123|8|627110220645727|662022187154530");\r\n\t\tupdateRequest.type("TableWithParent");\r\n\t\tupdateRequest.doc(data);\r\n\t\tclient.update(updateRequest);\r\n\t}\r\n\r\nBelow query, should return parent object:\r\ncurl -XGET \'http://localhost:9200/123/TableWithParent/123|8|627110220645727|662022187154530?routing=123&pretty&fields=\\_parent,\\_source\'\r\n\r\nThanks'
4537,'javanna','rivers cleanup task deleting valid rivers\nafter issuing two commands below i would expect the river to be created:\r\n```java\r\nMap<String, Object> source = new HashMap<String, Object>();\r\nsource.put("type", "dummy");\r\n        \r\n_client.admin().indices().prepareDeleteMapping() \r\n            .setIndices("_river")\r\n            .setType("my-river")\r\n            .get();\r\n\r\n_client.prepareIndex()\r\n            .setIndex("_river")\r\n            .setType("my-river")\r\n            .setId("_meta")\r\n            .setSource(source)\r\n            .get();\r\n```\r\ninstead it looks like if the commands execute fast enough one after another (happens every time on single node deployment),\r\nthere river is created and then deleted by riverClusterStateListener task\r\n\r\nlog dump for the case above:\r\n```\r\n[2013-12-23 17:23:43,794][INFO ][river.dummy              ] [test] [dummy][my-river] close\r\n[2013-12-23 17:23:43,854][INFO ][river.dummy              ] [test] [dummy][my-river] create\r\n[2013-12-23 17:23:43,854][INFO ][river.dummy              ] [test] [dummy][my-river] start\r\n[2013-12-23 17:23:44,355][INFO ][river.dummy              ] [test] [dummy][my-river] close\r\n```'
4531,'bleskes',"ClusterHealthResponse.validationFailures now maps to RoutingTableValidation.allFailures()\nThe ClusterHealthResponse.validationFailures is currently wired to RoutingTable.failures , which contains cluster level validation errors. That means that the list doesn't contain any index level validation failure, if exists. Since we by default return only the top level information (`level=cluster`), this important information is hidden to the rest layer. This PR adds the index validation failures to this list.  The Java API always return index level information but one needs to check it for every index, which means this change is imho good here as well.\r\n\r\n"
4441,'areek','Take fuzzy edit distance into account when computing the score for completion suggestion\nCurrently the completion suggester uses the configured weight as the score of the suggestion. It would be nice to take the edit distance into account when calculating the score for the completion suggester used with fuzzy option as the term suggester does.\r\n'
4435,'s1monw',"Calculate weight of shards\nLet Elasticsearch figure out how much work it is spending on each shard\r\nso it can do intelligent things with that data.  Intelligent things not\r\nincluded in this commit.\r\n\r\nThe weights are calculated based on exponentially weighted moving averages\r\nof the amount of time performing various tasks on the shard.  These times\r\ncould themselves be weighted or not.  That is an open question.  Either\r\nway the weight of a shard is calculated as the proportion of that total\r\nweight that is spent on the shard compared to either the cluster as a whole\r\nto get the cluster weight or the node to get the node weight.\r\n\r\nThere are actually six weights based on the six moving averages mainted\r\nfor each time stat: 1 minute, 5 minutes, 15 minutes, 1 hour, 1 day, 1 week.\r\nI'm not sure which one will be useful for things like shard balancing so\r\nI calculated them all.\r\n\r\nCloses #4434\r\n\r\nWork in progress because:\r\nI don't trust EWMA because it uses LongAdder's getAndReset method which\r\nis documented as not working while threads are updating it.  Maybe it is\r\ngood enough for this.  Would it make more sense to just pass in the delta\r\nbetween this tick and the last from the sum in MeanMetric.\r\n\r\nWhen a shard is first made should it's averages default to the rate of\r\nthe first five seconds like it does now?  That is probably ok intelligent\r\nthings will have to take that into account.  I think routing would be better\r\noff assuming that it is 2 or something like that because the shard will\r\nsoon get updates, probably more than others because it is new and phasing\r\nin the actual weight with the a priori weight over time or maybe all at\r\nonce after a few minutes.\r\n\r\nIs it OK to start so many jobs ticking every five seconds?\r\n\r\nWill those jobs tick close enough to five seconds to make the figures\r\nuseful?\r\n\r\nShould all the jobs run in the GENERAL thread pool like they are now?\r\n\r\nI don't stop any of the jobs that I start.  Ever.  Insanity.\r\n\r\nNot building weight using enough time statistics.  Which ones should I use?\r\n\r\nShould the weight include things other than cpu times?\r\n\r\nWhat should I do about the statistics of moved shard?  Copy their rates\r\nover from the old shard and continue where it left off and let it start\r\nover?  If I don't copy the rates, what good will the one week average be?\r\n\r\nNot sure if times should be weighted.\r\n\r\nFormatting.\r\n\r\nDocumentation."
4429,'spinscale','Log JVM stdout on service start\nBased on http://stackoverflow.com/questions/8251933/how-can-i-log-the-stdout-of-a-process-started-by-start-stop-daemon\r\n\r\nCloses #4428'
4424,'spinscale','dynamic templates not handled properly\nI\'ve set up dynamic templates in config/default-mapping.json, and have new index created for each day. Occasionally, definitions in config/default-mapping.json are not used properly. The file has the following content:\r\n\r\n{\r\n    "_default_" : {\r\n        "dynamic_templates" : [\r\n            {\r\n                "template_1" : {\r\n                    "match" : "http_domain",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "template_2" : {\r\n                    "match" : "client",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "template_3" : {\r\n                    "match" : "username",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "template_4" : {\r\n                    "match" : "ids_sig_text",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "template_5" : {\r\n                    "match" : "reason",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            },\r\n            {\r\n                "template_6" : {\r\n                    "match" : "action",\r\n                    "mapping" : {\r\n                        "type" : "string",\r\n                        "index" : "not_analyzed"\r\n                    }\r\n                }\r\n            }\r\n        ]\r\n    }\r\n}\r\n\r\nIn most cases, the definitions are handled properly, but for some days, the above fields are split into terms which should not happen. This also happened for today and when querying the index mapping, I saw the following strange result:\r\n\r\n curl -XGET \'http://zapata.ad4.seb.net:9200/rsyslog-2013-12-12/_mapping?pretty=true\'\r\n\r\n{\r\n  "rsyslog-2013-12-12" : {\r\n    "logs" : {\r\n      "dynamic_templates" : [ {\r\n        "template_1" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "wwwdomain"\r\n        }\r\n      } ],\r\n\r\n...\r\n...\r\n    "events" : {\r\n      "dynamic_templates" : [ {\r\n        "template_1" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "wwwdomain"\r\n        }\r\n      }, {\r\n        "template_2" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "client"\r\n        }\r\n      }, {\r\n        "template_3" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "username"\r\n        }\r\n      }, {\r\n        "template_4" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "ids_sig_text"\r\n        }\r\n      }, {\r\n        "template_5" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "reason"\r\n        }\r\n      }, {\r\n        "template_6" : {\r\n          "mapping" : {\r\n            "type" : "string",\r\n            "index" : "not_analyzed"\r\n          },\r\n          "match" : "action"\r\n        }\r\n      } ],\r\n\r\n...\r\n...\r\n\r\nAs you can see, the mappings are different for "logs" and "events" although usually they are one and the same. Also "wwwdomain" is a field which I removed more than a month ago from my default-mapping.json file, and it is not used as a fieldname in any of my documents anymore.\r\n\r\nI have seen the same behavior both for elasticsearch 0.90.3 and 0.90.7. What puzzles me most is why is a deleted configuration file entry reappearing in mappings, even though it was last used more than a month ago with 0.90.3, while I am currently running 0.90.7. I have checked all my elasticsearch config files with grep and wwwdomain does not appear anywhere.\r\n\r\nregards,\r\nristo'
4377,'jpountz','Move MappingMetaData to use internal compressed generic content (prefer SMILE)\nToday, `MappingMetaData` has an internal `CompressedString` that is used to store the json serialized version of the mapping. It would be much better if we had it stored as compressed, but in SMILE format, that will be much more storage efficient.\r\n\r\nI think it would make sense to create a `CompressedXContent` class, that would store internally the content compressed as bytes (based on a specific content type if needed). Then, we can use it, in SMILE format, to store the mapping. Also, `CompressedXContent` can include helper methods to convert to json and so on (that can make use of the XContentHelper classes).\r\n\r\nLast, for very large mappings, we can have a paged bytes instead of a single byte array in the `CompressedXContent` class.'
4285,'martijnvg',"Add allocate_all_primaries to cluster reroute\nFrom the docs:\r\n`allocate_all_primaries`::\r\n    Allocate all unallocated primaries to any node that can take them.\r\n    Accepts no parameters.  Each allocation is similar to running `allocate`\r\n    with `allow_primary` so this can cause data loss.  This is useful in the\r\n    same cases as `allocate` with `allow_primary` but doesn't require looking\r\n    up the `index` or `shard` or guessing an appropriate `node`.\r\n\r\nCloses #4206"
4255,'areek','Completion only retrieves one result when multiple documents share same output\nWhen I create multiple documents which have the same output value in the completion field, a suggest completion request only retrieves one object.\r\nI guess it is a feature, however, once we may set different payloads to those documents, it would make sense to retrieve multiple suggestions with the same output.\r\n\r\nMy environment settings:\r\nElasticSearch version number: `1.0.0.Beta1`\r\nLucene version: `4.5.1`\r\n\r\nIn the following example, I create an index with two documents. Each document has a different value in the payload, but the same output and input.\r\nPerforming a suggestion completion request, only one document is retrieved.\r\n\r\nScripts:\r\nCreate and populate index\r\n--\r\n```sh\r\ncurl -XDELETE \'localhost:9200/notebookindex\'\r\n\r\ncurl -XPUT localhost:9200/notebookindex\r\n\r\ncurl -XPUT localhost:9200/notebookindex/friend/_mapping -d \'{\r\n  "friend" : {\r\n        "properties" : {\r\n            "name" : { "type" : "string" },\r\n            "suggestField" : { "type" : "completion", "payloads" : true }\r\n        }\r\n    }\r\n}\'\r\n\r\ncurl -XPUT \'localhost:9200/notebookindex/friend/1\' -d \' {\r\n  "name": "james smith",\r\n  "suggestField": {\r\n    "input": ["james", "smith", "james smith"],\r\n    "output": "james smith",\r\n    "payload": {"id": "1", "phone": "555-55555"}\r\n  }\r\n}\'\r\n\r\ncurl -XPUT \'localhost:9200/notebookindex/friend/2\' -d \'{\r\n  "name": "james smith",\r\n  "suggestField": {\r\n    "input": ["james", "smith", "james smith"],\r\n    "output": "james smith",\r\n    "payload": {"id": "2", "phone": "444-44444"}\r\n  }\r\n}\'\r\n```\r\n\r\nSearch 1: look for friends starting with `j`\r\n--\r\n```sh\r\ncurl -XPOST \'localhost:9200/notebookindex/_suggest?pretty\' -d \'{\r\n  "my-friends-suggest": {\r\n    "text": "j",\r\n    "completion": {\r\n      "field": "suggestField"\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nOnly one `James` is found, even though there are two documents matching the suggestion.\r\n\r\n```JSON\r\n{\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "my-friends-suggest" : [ {\r\n    "text" : "j",\r\n    "offset" : 0,\r\n    "length" : 1,\r\n    "options" : [ {\r\n      "text" : "james smith",\r\n      "score" : 1.0, "payload" : {"id":"2","phone":"444-44444"}\r\n    } ]\r\n  } ]\r\n}\r\n```\r\n\r\nRemoval and Search 2: remove the `James` (`id 2`) previously found and look for friends starting with `j`\r\n--\r\n```sh\r\ncurl -XDELETE \'localhost:9200/notebookindex/friend/2\'\r\n\r\ncurl -XPOST \'localhost:9200/notebookindex/_suggest?pretty\' -d \'{\r\n  "my-friends-suggest": {\r\n    "text": "j",\r\n    "completion": {\r\n      "field": "suggestField"\r\n    }\r\n  }\r\n}\'\r\n```\r\n\r\nThe other `James` is found.\r\n\r\n```JSON\r\n{\r\n  "_shards" : {\r\n    "total" : 5,\r\n    "successful" : 5,\r\n    "failed" : 0\r\n  },\r\n  "my-friends-suggest" : [ {\r\n    "text" : "j",\r\n    "offset" : 0,\r\n    "length" : 1,\r\n    "options" : [ {\r\n      "text" : "james smith",\r\n      "score" : 1.0, "payload" : {"id":"1","phone":"555-55555"}\r\n    } ]\r\n  } ]\r\n}\r\n```'
4217,'javanna','Move delete index api to new acknowledgement mechanism\nMove delete index api to new acknowledgement mechanism introduced in #3786 .'
4177,'dakrone','OutOfMemoryError[Java heap space] when executing a query with a high \'from\' value.\nHi, \r\n\r\nI am executing a simple query (using the Chrome Postman App) on an index with around 80000 records, with a \'from\' value which is way higher than the number of available pages.\r\n\r\n{\r\n  "from" : 200000000,\r\n  "size" : 1,\r\n  "query" : { \r\n    "query_string" : {\r\n\t    "query" : "test"\r\n    }\r\n  }\r\n}\r\n  \r\nThis results in the following response:\r\n\r\n{\r\n  "error" : "ReduceSearchPhaseException[Failed to execute phase [query], [reduce] ]; nested: OutOfMemoryError[Java heap space]; ",\r\n  "status" : 503\r\n}\r\n\r\nInterestingly, when I execute the same query from a Java client using the Java API, the same error occurs but the value can be much lower, around 2000000. Elasticsearch and Client running in separate JVMs,\r\n\r\nOur web application fronting ElasticSearch restricts the page size but not the page number as I assumed this would always return 0 results if the client request exceeds the available number of pages. However, if the client provides a very high number, this OOM occurs.\r\n\r\nSetup:\r\n$ java -version\r\njava version "1.7.0_17"\r\nJava(TM) SE Runtime Environment (build 1.7.0_17-b02)\r\nJava HotSpot(TM) 64-Bit Server VM (build 23.7-b01, mixed mode)\r\nWindows 7\r\nElasticsearch 0.90.5 (standard setup, no settings changed after install)\r\n\r\nES LOG after query direct from postman:\r\n==============================\r\n[2013-11-15 14:05:04,323][INFO ][node                     ] [Guthrie, Jebediah] version[0.90.5], pid[14304], build[c8714e8/2013-09-17T12:50:20Z]\r\n[2013-11-15 14:05:04,324][INFO ][node                     ] [Guthrie, Jebediah] initializing ...\r\n[2013-11-15 14:05:04,330][INFO ][plugins                  ] [Guthrie, Jebediah] loaded [], sites []\r\n[2013-11-15 14:05:07,601][INFO ][node                     ] [Guthrie, Jebediah] initialized\r\n[2013-11-15 14:05:07,602][INFO ][node                     ] [Guthrie, Jebediah] starting ...\r\n[2013-11-15 14:05:07,790][INFO ][transport                ] [Guthrie, Jebediah] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/10.201.87.208:9300]}\r\n[2013-11-15 14:05:11,006][INFO ][cluster.service          ] [Guthrie, Jebediah] new_master [Guthrie, Jebediah][rwzTTiUKQXG7mX0YuYc3SQ][inet[/10.201.87.208:9300]], reason: zen-disco-join (elected_as_master)\r\n[2013-11-15 14:05:11,079][INFO ][discovery                ] [Guthrie, Jebediah] elasticsearch/rwzTTiUKQXG7mX0YuYc3SQ\r\n[2013-11-15 14:05:11,228][INFO ][http                     ] [Guthrie, Jebediah] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/10.201.87.208:9200]}\r\n[2013-11-15 14:05:11,229][INFO ][node                     ] [Guthrie, Jebediah] started\r\n[2013-11-15 14:05:11,942][INFO ][gateway                  ] [Guthrie, Jebediah] recovered [2] indices into cluster_state\r\n[2013-11-15 14:05:44,744][DEBUG][action.search.type       ] [Guthrie, Jebediah] [gtr][0]: Failed to execute [org.elasticsearch.action.search.SearchRequest@2bc44fb7] while moving to second phase\r\njava.lang.OutOfMemoryError: Java heap space\r\n\tat org.apache.lucene.util.PriorityQueue.<init>(PriorityQueue.java:64)\r\n\tat org.apache.lucene.util.PriorityQueue.<init>(PriorityQueue.java:37)\r\n\tat org.elasticsearch.search.controller.ScoreDocQueue.<init>(ScoreDocQueue.java:31)\r\n\tat org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:248)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.moveToSecondPhase(TransportSearchQueryThenFetchAction.java:85)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.innerMoveToSecondPhase(TransportSearchTypeAction.java:409)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.onFirstPhaseResult(TransportSearchTypeAction.java:241)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:219)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$3.onResult(TransportSearchTypeAction.java:216)\r\n\tat org.elasticsearch.search.action.SearchServiceTransportAction.sendExecuteQuery(SearchServiceTransportAction.java:203)\r\n\tat org.elasticsearch.action.search.type.TransportSearchQueryThenFetchAction$AsyncAction.sendExecuteFirstPhase(TransportSearchQueryThenFetchAction.java:80)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:216)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction.performFirstPhase(TransportSearchTypeAction.java:203)\r\n\tat org.elasticsearch.action.search.type.TransportSearchTypeAction$BaseAsyncAction$2.run(TransportSearchTypeAction.java:186)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n[2013-11-15 14:05:46,165][INFO ][cluster.service          ] [Guthrie, Jebediah] added {[Man-Spider][4F01Q8RwQHOhDW_LB2u1YQ][inet[/10.201.87.208:9301]]{data=false, local=false, master=false},}, reason: zen-disco-receive(join from node[[Man-Spider][4F01Q8RwQHOhDW_LB2u1YQ][inet[/10.201.87.208:9301]]{data=false, local=false, master=false}])\r\n\r\nCLIENT Log using Java API:\r\n====================\r\norg.elasticsearch.action.search.ReduceSearchPhaseException: Failed to execute phase [query], [reduce] \r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:180) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:154) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:148) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:251) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:242) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.5.jar:na]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]\r\n        at java.lang.Thread.run(Thread.java:722) ~[na:1.7.0_17]\r\njava.lang.OutOfMemoryError: Java heap space\r\n        at org.apache.lucene.util.PriorityQueue.<init>(PriorityQueue.java:64) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]\r\n        at org.apache.lucene.util.PriorityQueue.<init>(PriorityQueue.java:37) ~[lucene-core-4.4.0.jar:4.4.0 1504776 - sarowe - 2013-07-19 02:53:42]\r\n        at org.elasticsearch.search.controller.ScoreDocQueue.<init>(ScoreDocQueue.java:31) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.search.controller.SearchPhaseController.sortDocs(SearchPhaseController.java:248) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.innerExecuteFetchPhase(TransportSearchDfsQueryThenFetchAction.java:185) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction.executeFetchPhase(TransportSearchDfsQueryThenFetchAction.java:178) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:154) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.action.search.type.TransportSearchDfsQueryThenFetchAction$AsyncAction$3.onResult(TransportSearchDfsQueryThenFetchAction.java:148) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:251) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.search.action.SearchServiceTransportAction$5.handleResponse(SearchServiceTransportAction.java:242) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.handleResponse(MessageChannelHandler.java:153) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.transport.netty.MessageChannelHandler.messageReceived(MessageChannelHandler.java:124) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:791) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:296) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:462) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:443) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:70) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:564) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:559) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:268) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.Channels.fireMessageReceived(Channels.java:255) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:109) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108) ~[elasticsearch-0.90.5.jar:na]\r\n        at org.elasticsearch.common.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42) ~[elasticsearch-0.90.5.jar:na]\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]\r\n'
4160,'jpountz','Make Stored Compression Configurable\nAllow to make the stored compression configurable on the index level (`index.store.compression.level`), and use that when constructing the codec to use.\r\n\r\nIf possible (will be tricky), allow to set it on a live index, so for example, for time base indices, older indices can move to use higher compression mode and optimized (which should probably be done over a different issue once the setting is implemented).'
4081,'clintongormley','Field resolution should be unambiguous\nAs far as I understand it, fields are resolved on a _first found_ basis. So given the following documents:\r\n\r\n    PUT /index/foo/1\r\n    {\r\n        "count": 1,\r\n        "foo": {\r\n            "count": 1\r\n        }\r\n    }\r\n\r\n    PUT /index/bar/2\r\n    {\r\n        "count": 1,\r\n        "foo": {\r\n            "count": 1\r\n        }\r\n    }\r\n\r\n\r\n.... the field `foo.count` could resolve to `foo.count`, `foo.foo.count`, or `bar.foo.count`, depending on which is found first.\r\n\r\nField resolution should be unambiguous. Field names should be grouped by type and sorted in descending order by number of `.`. So the above mappings should result in:\r\n\r\n    bar:\r\n        foo.count\r\n        count\r\n\r\n    foo:\r\n       foo.count\r\n       count\r\n\r\nThen if no type is specified (or multiple types are specified), go through the groups in alphabetical order.\r\n\r\nThis would result in the following resolutions:\r\n\r\n    foo.foo.count   => (foo) foo.count\r\n    foo.count       => (foo) count\r\n    count           => (bar) count\r\n    *.foo.count     => (bar) foo.count\r\n    *.count         => (bar) count\r\n    *.*.count       => (bar) foo.count\r\n    \r\n\r\n'
4075,'alexksikes','Deprecate the `more_like_this` API\nChange the `more_like_this` query to accept an `index`, `type` and `id` parameter and use that to retrieve the doc to be searched.  This way we can deprecate the `more_like_this` API.'
4064,'GaelTadh',"Make script reloading dynamically configurable\nRe #4062 \r\n\r\nNot sure if we need two separate settings for script reloading, ie `watcher.interval` and `script.auto_reload_enabled`.  I'd make it just one: `script.auto_reload.interval` which defaults to `-1`.\r\n\r\nI'd also like it to be dynamically configurable, so that I can turn it on (with a short interval) while making changes, then turn it off again.\r\n\r\nPerhaps we should keep `script.auto_reload_enabled` and default it to `true`, but make it non-dynamic, to avoid a potential exploit."
4063,'rmuir','Analysis Synonym: Shared on node level + auto reload from file system\nTwo important changes to synonyms:\r\n\r\n- Share the synonym FST on the node level, keyed by the `Settings`, so if there are multiple indices sharing the same settings, they would also share the same FST.\r\n\r\n- Using the new file watcher service, we can automatically reload synonyms if configured on the file system and changed. Might make sense to have a "reloadble" token filter support for other cases where we are interested in it.\r\n\r\nThis is a great candidate to backport to 0.90 if implemented.'
3973,'spinscale','Add ES_STOP_TIMEOUT configuration variable to packages\nThis variable allows to configure the waiting time after a TERM signal has\r\nbeen sent. After that waiting time a KILL signal is sent to ensure the\r\nservice is stopped.\r\n\r\nIn case of a bigger installation the default values might be to slow, so it\r\nnow is configurable.\r\n\r\nOriginal work done by @tmclaugh at the PR #3719\r\n\r\nCloses #3719\r\nCloses #3972'
3946,'areek','Search_mode : "always" not working when give the exact keyword\n     {\r\n      "my-suggest-1" : {\r\n        "text" : "Francis",\r\n        "term" : {\r\n          "field" : "director",\r\n          "suggest_mode" : "always"\r\n        }\r\n        \r\n      }\r\n    }\r\n\r\nresult ::\r\n\r\n    {\r\n       "_shards": {\r\n          "total": 90,\r\n          "successful": 88,\r\n          "failed": 0\r\n       },\r\n       "my-suggest-1": [\r\n          {\r\n             "text": "francis",\r\n             "offset": 0,\r\n             "length": 7,\r\n             "options": []\r\n          }\r\n       ]\r\n    }\r\n\r\nhere when the query is made on "Francis":\r\n\r\n    {\r\n       "_shards": {\r\n          "total": 90,\r\n          "successful": 88,\r\n          "failed": 0\r\n       },\r\n       "my-suggest-1": [\r\n          {\r\n             "text": "francis",\r\n             "offset": 0,\r\n             "length": 7,\r\n             "options": []\r\n          }\r\n       ]\r\n    }\r\n\r\nand the result is \r\n\r\n    {\r\n       "_shards": {\r\n          "total": 90,\r\n          "successful": 88,\r\n          "failed": 0\r\n       },\r\n       "my-suggest-1": [\r\n          {\r\n             "text": "franci",\r\n             "offset": 0,\r\n             "length": 6,\r\n             "options": [\r\n                {\r\n                   "text": "francis",\r\n                   "score": 0.8333333,\r\n                   "freq": 1\r\n                }\r\n             ]\r\n          }\r\n       ]\r\n    }\r\n\r\n'
3890,'dakrone',"Explore Littles Law for automatic / dynamic queue sizes\nwe currently use static defaults for queues on threadpools. It might be worth looking into things like http://en.wikipedia.org/wiki/Little's_law for the queue sizes...\r\n"
3889,'s1monw','ActionResponse should implement ToXContent\nHe should across the board implement `ToXContent` and all responses should implement is without surrounding `start/endObject`. I started doing one of them in this PR: https://github.com/elasticsearch/elasticsearch/pull/3871 but as @martijnvg pointed out we should have all of them being consistent about it. \r\n'
3876,'jpountz','Allow users to configure how filters should be cached\nLucene 4.5 changed the way filters are cached by default from bit sets to compressed bit sets: WAH8DocIdSet, which is based on word-aligned hybrid encoding but there is also an interesting in-memory doc id set implementation based on pfor-delta encoding.\r\n\r\nWe should consider allowing our users to use such filters, and maybe even change the defaults.'
3861,'javanna','Get document API can specify an alias, but will return documents that are not part of that alias (as defined by the filter for that alias)\nCreate an index and populate it with two documents. Create an aliases with a filter, such that the alias contains one document.\r\n\r\nA search using the alias will return one result.\r\nA get document request using the alias can retrieve both documents.\r\n\r\nThis may confuse users: "why does this document not turn up in my search results", and makes it hard to implement a security model using aliases.\r\n\r\nThe UidField.loadDocIdAndVersion() method already uses a filter to check whether the document has been deleted. It would be possible to pass in the alias filter, if defined.'
3850,'martijnvg',"Root analyzer is ignored with positions offset gaps.\nRoot analyzer (analyzer on a type) isn't picked for fields with `position_offset_gap` set."
3791,'areek','Completion suggest: support setting different weights per input\nThe completion suggest field mapper today only supports setting a global weight which will be the same for all inputs. It could be useful to support setting different weights per input, depending on how well they match the suggestion.'
3740,'bleskes',"Nodes API: Return failures in nodes response\nToday, if there is a failure executing a nodes related API (nodes info, nodes stats, ...), we don't properly return them. We should."
3731,'imotov','Bulk import stalls on date parsing errors\nSending six insert records with dates that can not be processed causes the _bulk HTTP endpoint to hang indefinitely. With less than six, errors are returned, e.g.: \r\n\r\nMapperParsingException[failed to parse [start_time]]; nested: MapperParsingException[failed to parse date field [1379271987.081399], tried both date format [dateOptionalTime], and timestamp number with locale [null]]; nested: IllegalArgumentException[Invalid format: \\"1379271987.081399\\" is malformed at \\"7.081399\\"]; "}},'
3663,'bleskes',"NodeDoesNotExistOnMasterException Handling\nThis is related to #2117 \r\n\r\nI'm able to reproduce a slight variation with the following steps:\r\nSet discovery.zen.minimum_master_nodes = 2 and node.max_local_storage_nodes = 1. These nodes are deployed on NFS so that in case a machine is fails, an ES process can be started on the same data path from a different machine. Each machine has access to data directories for all es nodes.\r\n\r\nThe Initial deployment was:\r\n* ES1 - Node1:9300 (path.data=/nfs/node1)\r\n* ES2 - Node2:9300 (path.data=/nfs/node2)\r\n* ES3 - Node3:9300 (path.data=/nfs/node3)\r\n\r\nThen I took the following steps:\r\n1. Killed the network on Node2, which resulted in a cluster with two nodes (ES1,ES3). ES2 process was still running on Node2.\r\n2. Started new es process on Node1 with path.data=/nfs/node2. I was assuming since node.max_local_storage_nodes = 1, it will not start as ES2 on Node2:9300 already has lock on it, but it started anyways. The cluster now looked like\r\n  * ES1 - Node1:9300 (path.data=/nfs/node1)\r\n  * ES2 - Node1:9301 (path.data=/nfs/node2)\r\n  * ES3 - Node3:9300 (path.data=/nfs/node3)\r\n\r\nES2 - Node2:9300 (path.data=/nfs/node2) was still running but not part of the cluster.\r\n3. I started the network on Node2 which resulted in following two clusters:\r\n* Cluster1:\r\n    * ES1 - Node1:9300 (path.data=/nfs/node1)\r\n    * ES2 - Node1:9301 (path.data=/nfs/node2)\r\n    * ES3 - Node3:9300 (path.data=/nfs/node3)\r\n* Cluster2:\r\n   * ES1 - Node1:9300 (path.data=/nfs/node1)\r\n   * ES2 - Node2:9300 (path.data=/nfs/node2)\r\n\r\nNow, Node1:9300 is participating in both the clusters\r\n\r\nThis happens because of the way NodeDoesNotExistOnMasterException is handled at https://github.com/elasticsearch/elasticsearch/blob/master/src/main/java/org/elasticsearch/discovery/zen/fd/MasterFaultDetection.java#L315\r\n\r\nWhen Node2 comes into network, it sees that it is no longer a part of the cluster and starts master election resulting in a new master.\r\n\r\n"
3593,'mikemccand','Delete by query should not silently refresh index\nHi this issue caused lots of trouble because it was not clear why this happened. I had some index updates where a (quite common) approach is used:\r\n\r\nI have to update a bulk of documents with some higher level group key (not the uid). Like:\r\n\r\n    doc1: { groupKey: \'foo\', _id: \'bar1\' }\r\n    doc2: { groupKey: \'foo\', _id: \'bar2\' }\r\n    doc3: { groupKey: \'foo\', _id: \'bar3\' }\r\n\r\nThe code that updates this group of documents does not know the real _id of those already in the index (it just knows that the whole group updates), so it first deletes all documents by using deleteByQuery on the group key. After that it reindexes all documents in the group (with possibly different new _id values).\r\n\r\nIf you don\'t disable index refreshing, for a short time, the whole group would be disappearing and reappearing then. So to make the whole group reindex "atomic" you would disable index refreshing before that and reenable it afterwards (or do manual refreshing at all - what I do for this index in any case).\r\n\r\nUnfortunately, deleteByQuery forcefully refreshes the index. Which is hard to understand because its not documented. There is just a comment in the code that the refresh is needed although its heavy, because when executing a Lucene IndexWriter deleteByQuery, ElasticSearch does not know what documents were really deleted, so all internal tracking does not work (it cannot update version consistency,...)\r\n\r\nI was discussing with Martijn on IRC (not even he was aware that deleteByQuery does not work with disabled refreshing), he suggested that maybe the query is executed in ElasticSearch itsself and then it starts a bulk on _uid deletes (this is also one possibility for a workaround in our case if number of deletes is small).\r\n\r\nIn my opinion the better variant would be to do it like in Apache Solr: Apache Solr has 2 different IndexReaders open: One for searching the index (this one is refreshed in those periods of times), but a second one is another NRT reader on the IndexWriter that is used to do some updates of data structures after IndexWriter has written stuff. So updating of the ES internal data should be done with a new NRT reader and not the one used for searching.'
3569,'dakrone',"Allow for 'grace period expiration' before shard reallocation?\nIt would be really useful to allow for a 'grace period' between when ES notices that a particular node has gone down, and shard-reallocation begins. There are times when we might want to do a quick restart of an ES node ... or take one down for a full reboot ... and we don't want to do a re-allocation of shards because thats a very IO-intensive operation. In our case, we also use the Zookeeper plugin, and a shard-reallocation is triggered by a short communication break between the ES nodes and Zookeeper.\r\n"
3565,'areek','Proposed simplifications to _suggest\nRecent changes to configure field suggestions (see http://www.elasticsearch.org/blog/you-complete-me/) are a step forward, but may be improved as follows.\r\n\r\nI would suggest a default configuration that would work on any field (i.e. no special configuration needed in the mapping file).\r\n\r\nCalling a suggest for a field would look like this:\r\n\r\ncurl -X POST localhost:9200/hotels/_suggest -d \'{\r\n  "text": "m",\r\n  "field": "title"\r\n}\'\r\n\r\nThis would work on using all emitted tokens for the "title" field, whatever is specified there (analyzers, whether or not to lowercase, split) and autosuggest words from it. It\'s likely this would be too slow and indeed FSTs need to be stored at index time, and may need to be in the mapping file. If so, then not as a different type, but as an add-on flag:\r\n\r\n{\r\n  "mappings": {\r\n    "hotel": {\r\n      "properties": {\r\n        "title": { "type": "string", "store_suggest_fsts": true}\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nNote that the suggester would use the same field (query) analyzer on the input, so to also split the query!\r\n\r\nThis means that typing in "state un" would be analyzed in 2 tokens: "state" and "un" (because the title field is a default "string"). For both words the suggester could suggest alternatives, but by default it could do it only for the last word (assuming the user is still typing there). This could then match the word "union" if there is a document with "State of the Union" as the title. (of course also many other "un*" words).\r\n\r\nThis is different than the current implementation, because there the suggester only works for full matches from the beginning of the field: if we would have a "title_suggest": {"type":"completion"} field, then typing in "state un" would find NOTHING, because there is no title that matches "state un*"\r\n\r\nThis is better because it could work with 0 or very limited configuration, and also fits the use case of actual suggestions better (where we are not merely matching against a simple prefixable field, but against freetext).\r\n\r\nIt would be nice if the suggester could then also boost the "union" completion because there is a colocated "state" match, but that is left as an exercise for the ElasticSearch gurus ;-)\r\n\r\nBy deferring the definition of the atoms to match against to the existing, flexible analyzers, this gives this much more control. We could even add fuzziness or stemming to the match targets...\r\n\r\nWDYT'
3505,'s1monw','Inconsistent highlighting behavior(normal x vector)\nSo, the issue is reproducible by running(using 0.90.0):\r\n\r\n```\r\ncurl -XPOST http://localhost:9200/foo -d \'{ "mappings": { "bar": { "properties": { "id": { "type": "integer" }, "content": { "type": "string", "store": "yes" } } } } }\'\r\ncurl -XPOST http://localhost:9200/foo2 -d \'{ "mappings": { "bar": { "properties": { "id": { "type": "integer" }, "content": { "type": "string", "store": "yes", "term_vector": "with_positions_offsets" } } } } }\'\r\n\r\ncurl -XPUT http://localhost:9200/foo/bar/1 -d \'{ "id": 1, "content": "North Atlantic Treaty Organization, Zwei, Drei, Vier, Fünf, Sechs, Sieben, Acht, Neun" }\'\r\ncurl -XPUT http://localhost:9200/foo2/bar/1 -d \'{ "id": 1, "content": "North Atlantic Treaty Organization, Zwei, Drei, Vier, Fünf, Sechs, Sieben, Acht, Neun" }\'\r\n\r\ncurl -XPOST http://localhost:9200/foo/bar/_search -d\' { "query": { "bool": { "must": [ { "match": { "content": "North" } }, { "match": { "content": "Zwei" } }, { "match": { "content": "Vier" } }, { "match": { "content": "Drei" } }, { "match": { "content": "Sechs" } }, { "match": { "content": "Sieben" } }, { "match": { "content": "Acht" } }, { "match": { "content": "Neun" } } ] } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }\'\r\n\r\ncurl -XPOST http://localhost:9200/foo2/bar/_search -d\' { "query": { "bool": { "must": [ { "match": { "content": "North" } }, { "match": { "content": "Zwei" } }, { "match": { "content": "Vier" } }, { "match": { "content": "Drei" } }, { "match": { "content": "Sechs" } }, { "match": { "content": "Sieben" } }, { "match": { "content": "Acht" } }, { "match": { "content": "Neun" } } ] } }, "highlight": { "fields": { "content": { "fragment_size": 50, "number_of_fragments": 5 } } } }\'\r\n\r\n```\r\n\r\nFor the first index/query, I get \r\n\r\n```\r\n"highlight":{"content":[", <em>Vier</em>, Fünf, <em>Sechs</em>, <em>Sieben</em>, <em>Acht</em>, <em>Neun</em>","<em>North</em> Atlantic Treaty Organization, <em>Zwei</em>, <em>Drei</em>"]}\r\n```\r\n\r\nwhich is what i would expected. but for the second I get:\r\n\r\n```\r\n"highlight":{"content":["<em>North</em> Atlantic Treaty Organization, <em>Zwei</em>, <em>Drei</em>, Vier","Vier, Fünf, <em>Sechs</em>, <em>Sieben</em>, <em>Acht</em>, <em>Neun</em>"]}\r\n```\r\n\r\nwhere the Vier is missing.\r\n\r\nI don\'t expect both to return the same highlighted fragments, but I don\'t really get why the Vier is not highlighted on the second query(for the second fragment).'
3495,'jpountz','Using a missing filter for attributes of a nested object always returns an empty set\nHi there,\r\n\r\nI was trying to use the `exists`/`missing` filters when I stumbled upon this behavior: When I use the `missing` filter for nested objects, it always returns an empty set if the containing nested object is missing, too.\r\n\r\nHere is my document mapping:\r\n\r\n```javascript\r\n{\r\n  "site": {\r\n    "properties": {\r\n      "host": {\r\n        "type": "string",\r\n        "index": "not_analyzed",\r\n        "omit_norms": true,\r\n        "index_options": "docs"\r\n      },\r\n      "ip": {,\r\n        "type": "string",\r\n        "index": "not_analyzed",\r\n        "omit_norms": true,\r\n        "index_options": "docs"\r\n      },\r\n      ...\r\n      "modules": {\r\n        "type": "nested",\r\n        "properties": {\r\n          "module_id": {\r\n            "type": "integer"\r\n          },\r\n          "name": {\r\n            "type": "string",\r\n            "index": "not_analyzed",\r\n            "omit_norms": true,\r\n            "index_options": "docs"\r\n          },\r\n          ...\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nMy Document looks like this:\r\n\r\n```javascript\r\n{\r\n  "host": "6c1bb1fb58e8c48cabbd1e4382e55871f31ad776.com",\r\n  "ip" : "0.0.0.0",\r\n  ...\r\n  "modules": [ ]\r\n}\r\n```\r\n\r\nIf I now use a query with a  nested filter to select every document where modules.name is missing, I only get an empty set.\r\n\r\n```javascript\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": { "match_all": { } },\r\n\r\n      "filter": {\r\n        "nested": {\r\n          "path": "modules",\r\n          "query": {\r\n            "filtered": {\r\n              "query": { "match_all": { } },\r\n\r\n              "filter": {\r\n                "missing": { "field": "modules.name" }\r\n              }\r\n\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nIt seems to work if I submit a document which contains a module:\r\n\r\n```javascript\r\n{\r\n  "host": "6c1bb1fb58e8c48cabbd1e4382e55871f31ad776.com",\r\n  "ip" : "0.0.0.0",\r\n  ...\r\n  "modules": [ { "version" : "foo" } ]\r\n}\r\n\r\n```\r\n\r\nWhen using documents where the modules object isn\'t empty, use a `missing` filter which looks for "deeper" missing attributes seems to work, too.\r\n\r\n```javascript\r\n{\r\n  "query": {\r\n    "filtered": {\r\n      "query": { "match_all": { } },\r\n\r\n      "filter": {\r\n        "nested": {\r\n          "path": "modules",\r\n          "query": {\r\n            "filtered": {\r\n              "query": { "match_all": { } },\r\n\r\n              "filter": {\r\n                "missing": { "field": "modules.foo.bar.baz" }\r\n              }\r\n\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n```\r\n\r\nI was expecting, that a `missing` filter also returns documents if the containing nested object is missing or empty.\r\n\r\nUpdate: Wrapping an `exists` filter in a `not` filter doesn\'t return any documents, either.'
3445,'dadoonet','Not able to use Java API in JBoss EAP6 module\nIf I use api in simple servlet attaching elasticsearch libs in WEB-INF/lib everything is ok.\r\n\r\nBut if I create a module in JBoss with elasticsearch and give my web-app a dependency on it (via jboss-deployment-structure.xml), it cannot load neither TransportClient, nor Node-client throwing inject errors like these:\r\n\r\n\r\n```\r\norg.elasticsearch.common.inject.CreationException: Guice creation errors:\r\n\r\n1) Error injecting constructor, java.lang.NoClassDefFoundError: sun/misc/Unsafe\r\n  at org.elasticsearch.threadpool.ThreadPool.<init>(Unknown Source)\r\n  while locating org.elasticsearch.threadpool.ThreadPool\r\nCaused by: java.lang.NoClassDefFoundError: sun/misc/Unsafe\r\n\tat org.elasticsearch.common.util.concurrent.jsr166e.Striped64.getUnsafe(Striped64.java:321)\r\n\tat org.elasticsearch.common.util.concurrent.jsr166e.Striped64.<clinit>(Striped64.java:301)\r\n\tat org.elasticsearch.common.metrics.CounterMetric.<init>(CounterMetric.java:28)\r\n\tat org.elasticsearch.common.util.concurrent.EsAbortPolicy.<init>(EsAbortPolicy.java:30)\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.<init>(EsThreadPoolExecutor.java:36)\r\n\tat org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:292)\r\n\tat org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:249)\r\n\tat org.elasticsearch.threadpool.ThreadPool.<init>(ThreadPool.java:119)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\r\n\tat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)\r\n\tat org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:177)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:127)\r\n\tat ru.deltasolutions.switchyard.component.es.ESHandler.doStart(ESHandler.java:117)\r\n\tat org.switchyard.deploy.BaseServiceHandler.start(BaseServiceHandler.java:60)\r\n\tat org.switchyard.deploy.internal.Deployment.deployReferenceBindings(Deployment.java:309)\r\n\tat org.switchyard.deploy.internal.Deployment.start(Deployment.java:141)\r\n\tat org.switchyard.as7.extension.deployment.SwitchYardDeployment.start(SwitchYardDeployment.java:101)\r\n\tat org.switchyard.as7.extension.services.SwitchYardService.start(SwitchYardService.java:73)\r\n\tat org.jboss.msc.service.ServiceControllerImpl$StartTask.startService(ServiceControllerImpl.java:1811)\r\n\tat org.jboss.msc.service.ServiceControllerImpl$StartTask.run(ServiceControllerImpl.java:1746)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\nCaused by: java.lang.ClassNotFoundException: sun.misc.Unsafe from [Module "org.elasticsearch:main" from local module loader @49404e39 (finder: local module finder @1ccfa5c1 (roots: C:\\Java\\jboss-eap-6.1\\modules,C:\\Java\\jboss-eap-6.1\\modules\\system\\layers\\soa,C:\\Java\\jboss-eap-6.1\\modules\\system\\layers\\ds,C:\\Java\\jboss-eap-6.1\\modules\\system\\layers\\base))]\r\n\tat org.jboss.modules.ModuleClassLoader.findClass(ModuleClassLoader.java:196)\r\n\tat org.jboss.modules.ConcurrentClassLoader.performLoadClassUnchecked(ConcurrentClassLoader.java:444)\r\n\tat org.jboss.modules.ConcurrentClassLoader.performLoadClassChecked(ConcurrentClassLoader.java:432)\r\n\tat org.jboss.modules.ConcurrentClassLoader.performLoadClass(ConcurrentClassLoader.java:374)\r\n\tat org.jboss.modules.ConcurrentClassLoader.loadClass(ConcurrentClassLoader.java:119)\r\n\t... 42 more\r\n\r\n2) Error injecting constructor, java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.util.concurrent.jsr166e.LongAdder\r\n  at org.elasticsearch.threadpool.ThreadPool.<init>(Unknown Source)\r\n  while locating org.elasticsearch.threadpool.ThreadPool\r\n    for parameter 1 at org.elasticsearch.transport.netty.NettyTransport.<init>(Unknown Source)\r\n  while locating org.elasticsearch.transport.netty.NettyTransport\r\n  while locating org.elasticsearch.transport.Transport\r\n    for parameter 1 at org.elasticsearch.transport.TransportService.<init>(Unknown Source)\r\n  while locating org.elasticsearch.transport.TransportService\r\nCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.elasticsearch.common.util.concurrent.jsr166e.LongAdder\r\n\tat org.elasticsearch.common.metrics.CounterMetric.<init>(CounterMetric.java:28)\r\n\tat org.elasticsearch.common.util.concurrent.EsAbortPolicy.<init>(EsAbortPolicy.java:30)\r\n\tat org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor.<init>(EsThreadPoolExecutor.java:36)\r\n\tat org.elasticsearch.threadpool.ThreadPool.rebuild(ThreadPool.java:292)\r\n\tat org.elasticsearch.threadpool.ThreadPool.build(ThreadPool.java:249)\r\n\tat org.elasticsearch.threadpool.ThreadPool.<init>(ThreadPool.java:119)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\r\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:525)\r\n\tat org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:54)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.FactoryProxy.get(FactoryProxy.java:52)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42)\r\n\tat org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66)\r\n\tat org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85)\r\n\tat org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:98)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:819)\r\n\tat org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:42)\r\n\tat org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:57)\r\n\tat org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:45)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:200)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:193)\r\n\tat org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:812)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:193)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:175)\r\n\tat org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:110)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:93)\r\n\tat org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70)\r\n\tat org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:59)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:177)\r\n\tat org.elasticsearch.client.transport.TransportClient.<init>(TransportClient.java:127)\r\n\tat ru.deltasolutions.switchyard.component.es.ESHandler.doStart(ESHandler.java:117)\r\n\tat org.switchyard.deploy.BaseServiceHandler.start(BaseServiceHandler.java:60)\r\n\tat org.switchyard.deploy.internal.Deployment.deployReferenceBindings(Deployment.java:309)\r\n\tat org.switchyard.deploy.internal.Deployment.start(Deployment.java:141)\r\n\tat org.switchyard.as7.extension.deployment.SwitchYardDeployment.start(SwitchYardDeployment.java:101)\r\n\tat org.switchyard.as7.extension.services.SwitchYardService.start(SwitchYardService.java:73)\r\n\tat org.jboss.msc.service.ServiceControllerImpl$StartTask.startService(ServiceControllerImpl.java:1811)\r\n\tat org.jboss.msc.service.ServiceControllerImpl$StartTask.run(ServiceControllerImpl.java:1746)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\tat java.lang.Thread.run(Thread.java:722)\r\n\r\n```\r\nand so on up to 56 errors in TransportClient and up to over 2000 errors in Node client.\r\n\r\nAs I see, that\'s because of elasticsearch depends on special (or simple?) classloading mechanism. JBoss AS uses module classloaders and deployment classloader is different from ES module classloader.\r\n\r\nIt\'s a pity, but my deployment format denies me from including ES jars into the deployment, they only could be a module.\r\n\r\nThe last resort is to use Jest to communicate to ES cluster, but it seems to be not fast enough to perform at needed transaction rate...'
3316,'GaelTadh',"Special case the _index field in queries\nYou can already use the `_id` field in a `term`/`terms` query and it does the right thing, in spite of the fact that the `_id` field doesn't really exist.  It would be nice to do the same thing with `_index`, instead of having to resort to the `indices` query.\r\n\r\nThe `_type` field does actually exist and is indexed, so adding `_index` would complete the trinity."
3303,'markharwood','REST error readability\nErrors returned by the REST API are not easily human readable due to the way the data is stored in the `error` property.\r\nEg. https://gist.github.com/missinglink/e3cb9b127ae00e8c561c\r\n\r\nYou can \'prettify\' the results, but it is still un-readable.\r\nEg. https://gist.github.com/pecke01/5956684\r\n\r\nNote: (this specific error was caused by invalid syntax, the outer curly brackets were missing)\r\n\r\nThis issue makes it difficult for beginners to understand syntax errors and for advanced users to debug quickly when they make silly errors.\r\n\r\nIdeally usage of `?pretty=1` would return "developer friendly" messages.\r\n\r\nAny thoughts / suggestions? Maybe there is a tool which may help?'
3297,'jpountz','Weekly DateHistogramFacet producing wrong results\nThis failing test shows how the weekly date histogram facet gets confused once timezones and offets are applied to it. Are they applied in the correct order?'
3278,'martijnvg','Terms Lookup by Query/Filter (aka. Join Filter)\nThis PR adds support for generating a terms filter based on the field values\r\nof documents matching a specified lookup query/filter.  The value of the \r\nconfigurable "path" field is collected from the field data cache for each \r\ndocument matching the lookup query/filter and is then used to filter the main \r\nquery.  This is can also be called a join filter.\r\n\r\nThis PR abstracts the TermsLookup functionality in order to support multiple\r\nlookup methods.  The existing functionality is moved into FieldTermsLookup and\r\nthe new query based lookup is in QueryTermsLookup.  All existing caching \r\nfunctionality works with the new query based lookup for increased performance.\r\n\r\nDuring testing of I found that one of the performance bottlenecks was \r\ngenerating the Lucene TermsFilter on large sets of terms (probably since\r\nit sorts the terms).  I have created a FieldDataTermsFilter that uses the\r\nfield data cache to lookup value of the field being filtered and compare it to\r\nthe set of gathered terms.  This significantly increased performance at the \r\ncost of higher memory usage.  Currently a TermsFilter is used when the number\r\nof filtering terms is less than 1024 and the FieldDataTermsFilter is used\r\nfor everything else.  This should eventually be configurable or we need to\r\nperform some test to find the optimal value.\r\n\r\n\r\nExamples:\r\n\r\nReplicate a has_child query by joining on the child\'s "pid" field to the\r\nparent\'s "id" field for each child that has the tag "something".\r\n```\r\ncurl -XPOST \'http://localhost:9200/parentIndex/_search\' -d \'{\r\n    "query": {\r\n        "constant_score": {\r\n            "filter": {\r\n                "terms": {\r\n                    "id": {\r\n                        "index": "childIndex",\r\n                        "type": "childType",\r\n                        "path": "pid",\r\n                        "query": {\r\n                            "term": {\r\n                                "tag": "something"\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```\r\n\r\nLookup companies that offer products or services mentioning elasticsearch.\r\nNotice that products and services are kept in their own indices.\r\n```\r\ncurl -XPOST \'http://localhost:9200/companies/_search\' -d \'{\r\n    "query": {\r\n        "constant_score": {\r\n            "filter": {\r\n                "terms": {\r\n                    "company_id": {\r\n                        "indices": ["products", "services"],\r\n                        "path": "company_id",\r\n                        "filter": {\r\n                            "term": {\r\n                                "description": "elasticsearch"\r\n                            }\r\n                        }\r\n                    }\r\n                }\r\n            }\r\n        }\r\n    }\r\n}\'\r\n```'
3226,'kimchy',"Upgrade to Netty 4.0\nI'm currently working on this. Writing the issue here so we could discuss.\r\n\r\nNetty 4 is not yet stable, so I won't merge until everything is tested and Netty 4 has a stable release."
3163,'GaelTadh','Multi-field query_string with auto_generate_phrase_queries = true when one of the fields is indexed w/o positions using word delimiter filter [0.90.1]\nWhen running query_string query on multiple fields, one of which is indexed w/o positions (index_options: docs) and the query string contains a token which is split by word delimiter, if auto_generate_phrase_queries is true, the query fails with `IllegalStateException` (`field was indexed without position data; cannot run PhraseQuery`).\r\n\r\nIt was working fine in 0.20.x (just checked in 0.20.5), but doesn’t work in 0.90.1\r\n\r\nLooks like it splits a term into sub-terms and tries to form a PhraseQuery from the tokens and apparently fails to run it. In 0.20.x it seems it was ignoring auto_generate_phrase_queries for fields w/o positions data, so it was getting the results from the other fields.\r\n\r\nExample:\r\n\r\n```\r\ncurl -XPOST "http://localhost:9200/phrases-test/" -d \'\r\n{\r\n   "settings":{\r\n      "index":{\r\n         "analysis":{\r\n            "analyzer":{\r\n                "generic_text":{\r\n                    "type": "custom",\r\n                    "tokenizer": "whitespace",\r\n                    "filter": ["short_word_delimiter", "lowercase"]\r\n                }\r\n            },\r\n            "filter":{\r\n               "short_word_delimiter":{\r\n                    "type": "word_delimiter",\r\n                    "generate_word_parts": true,\r\n                    "generate_number_parts": true,\r\n                    "catenate_words": true,\r\n                    "catenate_numbers": true,\r\n                    "catenate_all": true,\r\n                    "split_on_case_change": true,\r\n                    "preserve_original": true\r\n               }\r\n            }\r\n         }\r\n      }\r\n   }\r\n}\r\n\'\r\n\r\ncurl -XPOST "http://localhost:9200/phrases-test/test/_mapping" -d \'\r\n{\r\n   "test": {\r\n        "properties": {\r\n            "title": {\r\n                "type": "multi_field",\r\n                "fields": {\r\n                    "regular": {\r\n                        "type":"string",\r\n                        "analyzer": "generic_text"\r\n                    },\r\n                    "wo_positions": {\r\n                        "type":"string",\r\n                        "analyzer": "generic_text",\r\n                        "index_options": "docs"\r\n                    }\r\n                }\r\n            }\r\n        }\r\n   }\r\n}\r\n\'\r\n\r\ncurl -XPOST "http://localhost:9200/phrases-test/test/" -d \'\r\n{\r\n   "title": "plain box"\r\n}\r\n\'\r\ncurl -XPOST "http://localhost:9200/phrases-test/test/" -d \'\r\n{\r\n   "title":"box 10x20"\r\n}\r\n\'\r\ncurl -XPOST "http://localhost:9200/phrases-test/test/" -d \'\r\n{\r\n   "title":"box 10x30"\r\n}\r\n\'\r\n```\r\n\r\nWhen you search in 0.90.1:\r\n```\r\ncurl -XPOST "http://localhost:9200/phrases-test/test/_search?pretty=true" -d \'\r\n{\r\n   "query":{\r\n        "query_string" : {\r\n            "fields" : ["title.regular", "title.wo_positions"],\r\n            "query" : "10x30",\r\n            "auto_generate_phrase_queries": true\r\n        }\r\n   }\r\n}\r\n\'\r\n```\r\n\r\nYou get the exception:\r\n```\r\n"status" : 500,\r\n"reason" : "QueryPhaseExecutionException[[phrases-test][0]: query[filtered((title.regular:\\"(10x30 10) x (30 10x30)\\" | title.wo_positions:\\"(10x30 10) x (30 10x30)\\"))->cache(_type:test)],from[0],size[10]: Query Failed [Failed to execute main query]]; nested: IllegalStateException[field \\"title.wo_positions\\" was indexed without position data; cannot run PhraseQuery (term=10x30)]; "\r\n```\r\n\r\nIn 0.20.5 you get the expected results:\r\n```\r\n"hits" : {\r\n    "total" : 1,\r\n    "max_score" : 0.76713204,\r\n    "hits" : [ {\r\n      "_index" : "phrases-test",\r\n      "_type" : "test",\r\n      "_id" : "Oa8-V__fRjO6UrfwRZWmDg",\r\n      "_score" : 0.76713204, "_source" :\r\n{\r\n   "title":"box 10x30"\r\n}\r\n    }\r\n```'
3162,'spinscale',"add Dockerfile to easily build elasticsearch into a runnable linux container\nIt would be nice to have a Dockerfile for this project, to be able to start an ElasticSearch container easily.\r\n\r\n1) Instal docker `http://docker.io`\r\n2) Download the Dockerfile `wget https://raw.github.com/elasticsearch/elasticsearch/dockerfile/Dockerfile`\r\n3) Build the container using the Dockerfile `docker build - < Dockerfile`\r\n4) Start ElasticSearch with `docker run <imageid>`\r\n\r\nThat's it!"
3068,'martijnvg','conflict between alias routing and parent/child routing\nI was trying to create aliases with routing on an index that includes parent/child docs. Posting to an alias endpoint while specifying "parent=X" causes an error. I was thinking it shouldn\'t, because the parent/child routing is to ensure the docs will wind up on the same shard, but this is already guaranteed by the routed alias.\r\n\r\nCurl recreation:\r\n\r\nhttps://gist.github.com/erikcameron/5621421\r\n\r\nAs noted, it looks like it works if you give parent and routing explicitly. \r\n\r\nThanks!\r\n-E'
3022,'martijnvg','Return matching nested inner objects per hit\nAdd support for including the matching nested inner objects per hit element.'
2917,'martijnvg','Sorting based on parent/child relationship\nCurrently there is no way to sort documents based on parent child relation. E.g.\r\nSorting a doc based on child doc field or the opposite.'
2914,'alexksikes','MLT bug when source disabled?\nI\'m trying to use the more_like_this handler in almost the exact same way it\'s used in the documentation here:\r\n\r\nhttp://www.elasticsearch.org/guide/reference/api/more-like-this/\r\n\r\ncurl -XGET "http://localhost:9200/foo/document/1008534/_mlt?mlt_fields=cs,ks,tpcs&min_doc_freq=2"\r\n\r\n{"error":"ElasticSearchException[No fields found to fetch the \'likeText\' from]","status":500}\r\n\r\nI\'m guessing this bug stems from the fact that source is disabled, but I\'m not really sure. If it is the case that source is required for MLT, you should document that fact.'
2908,'bleskes',"Recovery should re-assign shards when a node re-joins the cluster\nWhen a node disconnects and then re-joins the cluster, it is stripped of all its shard assignments, even if it is the best recovery candidate. This significantly prolongs recovery, increases the burden on the cluster, and in observed cases can cause the cluster to go red (more below). \r\n\r\nEither of the following would help the situation:\r\n\r\n1. Re-plan the recovery when a node joins. If the re-joining node has more segments than the current recoverer, stop that recovery and have the re-joining node take it over.\r\n2. Do not un-assign the shards from the errant node. Let the shards become over-replicated, then rebalance (rather than letting them become under-replicated and then rebalancing).\r\n\r\n_____\r\n\r\nWe run nodes with hundreds of GB of data in EC2 instances. The combination of heavy shards and modest network bandwidth means recovery can take tens of minutes. This is much longer than a node would typically be off-line after encountering a fault -- even a full rebuild of a node takes only a few minutes, and comes back with all its data intact thanks to the magic of EBS. In the common case of a rolling restart each node rejoins within a few seconds. \r\n\r\nSuppose in my cluster Kitty has shards 1, 2; Xavier has shards 3, 4; Jane has 1, 3; Hank has 2, 3, 4; Scott has 1, 2, 4. Kitty will phase out (to later re-join); Xavier and Jane will start recover of shards 1 and 2 respectively.\r\n\r\nIn the current world, with `c.r.a.allow_rebalance` set to default, Kitty will re-join and do nothing until recovery is complete - she does not send or receive shards, and does not answer queries. Once the cluster is green, she is then assigned an arbitrary portion of shards and the cluster begins *rebalancing* onto her. In general, only a few of the new shard assignments will overlap and so her first act is to delete most of the data on her disk. With allow rebalance set to `always`, she will at least begin rebalancing immediately on join, but again to an arbitrary set of shards: she'll rejoin with 1 and 2 complete or mostly-complete (due to intervening writes), but the odds are only (1/nshards^2) that a shard is re-assigned.\r\n\r\nThe downsides:\r\n\r\n* Any transient disconnect results in a minimum recovery period of (MB per node) / (MB/s recovery throughput). \r\n* During that time the cluster has gone from 5 strong nodes to 4 nodes doing recovery and serving 125% of their normal data burden.\r\n* The shards are under-replicated during this time. If Scott blips out during recovery as well, you all of a sudden have effectively no replication for shards 1 and 2 even though three machines have the data. If Hank additionally goes down, we've seen (at least in 19.8) a situation where the shards remained un-assigned until a full-cluster stop / full-cluster start could be effected.\r\n\r\nProposed Alternatives:\r\n\r\n1. **Re-plan the recovery when a node joins.** When Kitty phases out, her shards are assigned to Xavier and Jane and recovery initiates as normal. When Kitty re-joins, the master node takes stock of how many segments she has for all under-replicated shards, and forms a new recovery plan. Suppose Kitty rejoins with 60% of the current segments for shard 1 and 95% of shard 2, while the other nodes are 70% through transfer. Xavier will complete the recovery of shard 1, and Kitty will delete hers; Jane will stop recovering shard 2, while Kitty will recover that last 5%. Once the cluster is green, some shard will be rebalanced onto Kitty.\r\n2. **Do not un-assign the shards from the errant node.** When Kitty phases out, assign her shards as usual to Xavier and Jane -- but leave them also assigned to Kitty. If Kitty rejoins, she will complete whatever incremental recovery of her shards are necessary. The cluster will then choose how to discard the over-replicated shards to find optimal balance. The improvement here is that a) however quickly Kitty completes recovery is how quickly you're serving data from a full-strength cluster again; b) you're  spending as little time as possible in an under-replicated state.  It's safer to ski downhill.\r\n"
2867,'rmuir','Proximity BM25 ranking\nSince we have BM25 similarity in 0.90.0.Beta1, is there any way to implement Sphinx-like Proximity BM25 ranking?\r\n\r\n    SPH_RANK_PROXIMITY_BM25, the default ranking mode that uses and combines both phrase proximity and BM25 ranking.'
2572,'martijnvg',"Unicast discovery fails in client mode if #unicast.hosts < mimimum_master_nodes\nWhen using unicast discovery where the ping hosts are a subset of the nodes in the cluster (4 'seeds', 8 nodes total), joining a data node (`node.data: true`) to the cluster works fine, but joining a client node (`node.client: true`) fails as it feels it's not found the master. Pinging the seeding hosts works fine from a network point of view. \r\n\r\nThe join *is* successful if either enough nodes are available in the `discovery.zen.ping.unicast.hosts` setting or `discovery.zen.minimum_master_nodes` is set to a value equal or less than the length of the hosts array."
2318,'javanna','Index aliases and delete operation\nSuppose we use the "usersdata flow" exposed in kimchy\'s "Big Data, Search and Analytics" presentation to index users documents in a big oversharded index with routing+filtering aliases. Some users content are moved to their own index when becoming too big.\r\n\r\nFrom the client perspective we don\'t know if we are manipulating a virtual index via an alias or a real index dedicated to the user and the code should be the same. So if an user is deleted and we want to remove its content the client code do something like curl -XDELETE localhost:9200/$INDEX_NAME. The power of aliasing being that the client is agnostic of knowing if it deals with a real index or an aliased one.\r\n\r\nBut with the current behavior if $INDEX_NAME is an alias the whole big oversharded index will be deleted and all users document are lost!\r\n\r\nIMHO it is really dangerous. I was expecting a simple delete index if the $INDEX_NAME is a real index under the hood and maybe a delete by query (even if it is expensive) or at least a failure or something like that maybe configurable via a parameter if $INDEX_NAME is an alias linking to the big oversharded index.\r\n\r\nWhat do you think?'
2315,'s1monw',"Verify that source object is ended properly and does not contain any tra...\n...iling tokens (this might corrupt client result parsing).\r\n\r\nInvalidate document when the source containins trailing tokens:\r\n\r\nExample (indexing):\r\n\r\n'{}}' => invalid sice it contains an extra '}' after the source object is exited."
2172,'spinscale','upstart script\nThis patch replaces the init.d script with an upstart script:\n\nhttps://github.com/elasticsearch/elasticsearch/pull/2171\n\nit probably needs review.'
2139,'spinscale','adding wrong port exception for misguided HTTP requests\nHi kimchy,\n\nI think it is useful to add a special wrong port handling to netty transport, so people who accidentally connect with a HTTP client to port 9300 will receive a small diagnostic message.\n\nCheers,\n\nJörg'
2118,'s1monw',"Add Explicit Multi & PhraseQuery support to REST & Java API\ncurrently I don't see explicit support for MultiPhraseQuery in ElasticSearch. It would be interesting to be able to combine PhraseQuery with fuzzy terms especially with Lucene 4.0 coming up."
2082,'martijnvg','Inconsistent _parent field query\nThere is a small inconsistency in the _parent field query. \n\nWith term query, only **parent_id** is required, but with e.g. wildcard query, the format **parent_type#parent_id** is required, as described in http://www.elasticsearch.org/guide/reference/api/delete.html\n\nDemonstration: a curl-based parent/child example script can be found here: https://gist.github.com/3047912\n\nAlternative - term query\n\n\tcurl -XGET \'http://localhost:9200/test/library/_search?pretty\' -d \'{\n\t      "query": {\n\t            "term" : {\n\t                  "_parent" : "1"\n\t             }\n\t      }\n\t}\'\n\nAlternative - wildcard query\n\t\n\tcurl -XGET \'http://localhost:9200/test/library/_search?pretty\' -d \'{\n\t      "query": {\n\t            "wildcard" : {\n\t                  "_parent" : "title#1"\n\t             }\n\t      }\n\t}\'\n\nBoth yield same result.\n\nSuggestion: there should be no special format or query types allowed for _parent queries to avoid confusion. Possibly every _parent query should just be forced to become a term query. The docs at http://www.elasticsearch.org/guide/reference/mapping/parent-field.html need a query example. http://www.elasticsearch.org/guide/reference/api/delete.html should be cleaned up.\n'
2044,'javanna',"Feature request: Be able to specify logger.yml path\nI'd like to be able to specify the `logger.yml` path when booting up elasticsearch, similar to the `-Des.config=<path>` option.\n\nUse case: booting up a ES for testing purposes, want to be able to specify a custom logger.yml without having to edit the system-wide one."
2040,'markharwood','Limit faceting to top N hits\nSee\nhttps://groups.google.com/d/msg/elasticsearch/OZsc5ofNhag/9JfwSy-CEMwJ where Shay says "...with the design of the facets, if shouldn\'t be hard to implement..."'
1984,'uboness','Allow for easier implementation of fine-grained security\nI posted the following on the mailing list: https://groups.google.com/forum/?fromgroups#!topic/elasticsearch/lAmxGvP3pos\n\nI\'d like to suggest an implementation which allows override "fields" or "partial_fields" in order to accomplish filtering of certain fields (which are used for fine-grained security).\n\nFor example:\n\n1. if a user passes in a query which has no "fields" or "partial_fields" defined, use a predefined "partial_fields" to prevent sensitive fields from appearing in search results.\n2. if a user passes a query which has "fields" or "partial_fields", then merge or override fields available in search results.\n\nIn other posts I have seen the suggestion to use indexed fields to accomplish fine-grained security. This works well except for the above scenario for trying to filter security-centric fields from search results.\n\nI look forward to hearing about an existing feature or new feature to accomplish the above.'
1956,'s1monw','Provide ability to apply query side synonym changes in real-time\nFor search analyzers that contain a synonym type filter, it would be great to be able to make changes to the synonym file and have those changes recognized immediately by the associated analyzers without having to close/open the index. Ideally, the synonym cache would be refreshed automatically with changes to the corresponding synonym file. Another option would be to provide API call that could be made to refresh cache on demand. For most ecommerce sites, synonyms provide great value, and therefore are used quite frequently, and it is critical that these changes be implemented without any search outage.'
1954,'martijnvg',"Delete child documents when parent is deleted.\nDiscussed here:\nhttps://groups.google.com/forum/?fromgroups#!topic/elasticsearch/nfpTXJFePhA\nChild documents should be deleted when parent documents is deleted. Especially if it's deleted by ttl."
1848,'kimchy','_analyze request issue\nelasticsearch v 0.19.2\r\nelasticsearch.yml:\r\n\r\n```yaml\r\nindex:\r\n  analysis:\r\n    analyzer:\r\n      text_lt:\r\n        type: custom\r\n        tokenizer: standard\r\n        filter: [standard, lowercase, stop]\r\n```\r\n\r\nPOST `http://localhost:9200/_analyze?analyzer=text_lt`\r\nwith data: `message about error is here`\r\n\r\n```json\r\n{"error":"ElasticSearchIllegalArgumentException[failed to find analyzer [text_lt]]","status":400}\r\n```\r\n\r\n```\r\n[2012-04-05 12:37:14,622][DEBUG][action.admin.indices.analyze] [Cassiopea] failed to execute [org.elasticsearch.action.admin.indices.analyze.AnalyzeRequest@673c9f]\r\norg.elasticsearch.ElasticSearchIllegalArgumentException: failed to find analyzer [text_lt]\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:147)\r\n        at org.elasticsearch.action.admin.indices.analyze.TransportAnalyzeAction.shardOperation(TransportAnalyzeAction.java:57)\r\n        at org.elasticsearch.action.support.single.custom.TransportSingleCustomOperationAction$AsyncSingleAction$1.run(TransportSingleCustomOperationAction.java:143)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n        at java.lang.Thread.run(Unknown Source)\r\n```\r\n\r\nPOST `http://localhost:9200/_analyze?analyzer=standard` works as expected:\r\n\r\n```json\r\n{"tokens":[{"token":"message","start_offset":0,"end_offset":7,"type":"<ALPHANUM>","position":1},{"token":"about","start_offset":8,"end_offset":13,"type":"<ALPHANUM>","position":2},{"token":"error","start_offset":14,"end_offset":19,"type":"<ALPHANUM>","position":3},{"token":"here","start_offset":23,"end_offset":27,"type":"<ALPHANUM>","position":5}]}\r\n```'
1563,'Mpdreamz','Node Stats rest api returns open_file_descriptors: -1\n_cluster/nodes/stats :\r\nnodes.nodename.process.open_file_descriptors: -1\r\n\r\nES version 0.18.4\r\nOperations System Windows Xp 32 bit\r\n\r\n'
1412,'alexksikes','Implement "interesting terms" in More Like This handler\nSolr implements the "interesting terms" for MLT queries: http://wiki.apache.org/solr/MoreLikeThisHandler\r\n\r\n"One of: "list", "details", "none" -- this will show what "interesting" terms are used for the MoreLikeThis query. These are the top tf/idf terms. NOTE: if you select \'details\', this shows you the term and boost used for each term. Unless mlt.boost=true all terms will have boost=1.0"'
1322,'kimchy','Allow merging shard replicas based on document versions\nWould it be possible to allow shard replicas to be merged together after a network partition based on the highest version numbers? \r\n\r\nClearly this would be optional, as versions would collide or become out-of-sequence in a default configuration. However, for people who desire this functionality, it\'s easy to generate globally unique, sequential version numbers from the content that won\'t collide (microsecond timestamp + truncated hash of the doc) on the client side and feed these as version numbers into ElasticSearch.\r\n\r\nThe only caveat I can think of is that deletions aren\'t as straightforward. It seems like the system Cassandra uses, which replaces the stored value with a "tombstone" that represents the deletion. The tombstones get cleaned up after a configurable grace time  period. I believe the grace period in Cassandra is 10 days by default -- clearly a very conservative value. This system seems to work well. The delete API already takes a version number, so that could be used to version the tombstones. Again, I think this should be totally optional as keeping tombstones might effect the performance & data size of some existing use cases that don\'t need this feature. If it isn\'t optional or doesn\'t need to be, perhaps the default grace period could be very short, like an hour?\r\n\r\nMergeable replicas would allow the an ElasticSearch cluster to operate as an eventually consistent "AP" system and could make working with multiple datacenters very feasible out-of-the-box. Currently it works as a "CP" system, so the quorum side of a network partition takes precedence and the other side(s) become effectively read-only. It would be nice to be able to continue to receive writes on the non-quorum side(s) and merge these writes into the cluster once it is healed.\r\n\r\nCurrently my strategy involves building a completely isolated ES cluster at each datacenter and distributing index updates using a message queue / worker system. Each datacenter has it\'s own isolated MQ and worker group. Index updates get inserted into a fan-out exchange which inserts an update message in each individual datacenter\'s outbound queue, which the workers pick up and perform the index update. This is complicated and was error-prone to start, and also introduces adds non-trivial extra cost and latency to the process.'
1203,'drewr','Provide online Javadocs\nA hosted version of the Javadocs will greatly help out on the mailing-list/forum when trying to point out a class/method to someone.  The online documentation is great for illustrating concepts, but something at the class level would also help'
1063,'martijnvg','"block until refresh" indexing option\nFeature request:  Provide an option to the index operation that will wait until the next scheduled refresh occurs before returning a response.  After the response is returned, all documents indexed in that operation should be visible for search.\r\n\r\n'
283,'kimchy','ThreadLocals not cleared\nI am running ElasticSearch 0.9.0. My web application uses the Java API to connect to the server. After closing the client node, several API objects are still referenced by ThreadLocals. As a consequence the web application can not be garbage collected after being undeployed. This creates a memory leak.\r\n\r\n    30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap\r\n    SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.inject.InjectorImpl$1@19d44df]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@149895]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.\r\n    30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap\r\n    SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom$1@2d8a59]) and a value of type [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom] (value [org.elasticsearch.common.util.concurrent.jsr166y.ThreadLocalRandom@1d60b6a]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.\r\n    30.07.2010 14:59:07 org.apache.catalina.loader.WebappClassLoader clearThreadLocalMap\r\n    SCHWERWIEGEND: The web application [] created a ThreadLocal with key of type [null] (value [org.elasticsearch.common.inject.InjectorImpl$1@19d44df]) and a value of type [java.lang.Object[]] (value [[Ljava.lang.Object;@1876a6d]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.\r\n'
